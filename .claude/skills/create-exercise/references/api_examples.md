# API Examples for Exercise Creation

Complete examples of API interactions for creating, updating, and managing exercises.

## Base URL

```
http://localhost:8000
```

All endpoints require the backend to be running.

## Getting Chapter Information

### Get Course and Chapters

```bash
# Get course details
GET /api/courses/fundamentals-of-data-engineering

Response:
{
  "id": 1,
  "title": "Fundamentals of Data Engineering",
  "slug": "fundamentals-of-data-engineering",
  "description": "...",
  "created_at": "2024-02-10T10:00:00",
  "updated_at": "2024-02-10T10:00:00"
}
```

```bash
# Get all chapters for a course
GET /api/courses/fundamentals-of-data-engineering/chapters

Response:
[
  {
    "id": 1,
    "order": 1,
    "title": "Data Engineering Described",
    "slug": "data-engineering-described",
    "part_id": 1
  },
  {
    "id": 2,
    "order": 2,
    "title": "The Data Engineering Lifecycle",
    "slug": "the-data-engineering-lifecycle",
    "part_id": 1
  }
]
```

### Get Chapter Details with Content

```bash
# Get full chapter details including HTML content
GET /api/courses/fundamentals-of-data-engineering/chapters/data-engineering-described

Response:
{
  "id": 1,
  "course_id": 1,
  "part_id": 1,
  "order": 1,
  "title": "Data Engineering Described",
  "slug": "data-engineering-described",
  "content": "<h2>Introduction</h2><p>...</p>",
  "image": "/static/images/chapter1.png",
  "created_at": "2024-02-10T10:00:00",
  "updated_at": "2024-02-10T10:00:00"
}
```

## Listing Existing Exercises

### Get All Exercises for a Chapter

```bash
# Get exercises by chapter ID
GET /api/chapters/1/exercises

Response:
[
  {
    "id": 1,
    "chapter_id": 1,
    "order": 1,
    "title": "DÃ©finition du Data Engineering",
    "type": "multiple_choice",
    "content": {
      "question": "Quel est le rÃ´le principal d'un data engineer ?",
      "options": [
        "Analyser les donnÃ©es",
        "Construire l'infrastructure de donnÃ©es",
        "CrÃ©er des modÃ¨les ML",
        "Designer des interfaces"
      ],
      "correct_index": 1,
      "explanation": "..."
    },
    "image": null,
    "auto_generated": false,
    "created_at": "2024-02-10T10:00:00",
    "updated_at": "2024-02-10T10:00:00"
  },
  {
    "id": 2,
    "chapter_id": 1,
    "order": 2,
    "title": "Composants d'un Pipeline",
    "type": "true_false",
    "content": {
      "statement": "Un pipeline de donnÃ©es inclut toujours une Ã©tape de machine learning.",
      "correct_answer": false,
      "explanation": "..."
    },
    "image": null,
    "auto_generated": false,
    "created_at": "2024-02-10T10:00:00",
    "updated_at": "2024-02-10T10:00:00"
  }
]
```

**Note:** Use this to determine the next `order` value and to avoid creating duplicate exercises.

## Creating Exercises

### Create Multiple Choice Exercise

```bash
POST /api/chapters/1/exercises
Content-Type: application/json

{
  "order": 3,
  "title": "Architecture de Data Warehouse",
  "type": "multiple_choice",
  "content": {
    "question": "Quelle architecture est typiquement utilisÃ©e dans un data warehouse ?",
    "options": [
      "Architecture microservices",
      "Architecture en Ã©toile (star schema)",
      "Architecture monolithique",
      "Architecture Ã©vÃ©nementielle"
    ],
    "correct_index": 1,
    "explanation": "Les data warehouses utilisent gÃ©nÃ©ralement une architecture en Ã©toile (star schema) avec une table de faits centrale entourÃ©e de tables de dimensions. Cette structure facilite les requÃªtes analytiques et amÃ©liore les performances. Les autres architectures sont utilisÃ©es pour des systÃ¨mes diffÃ©rents."
  },
  "auto_generated": true
}

Response: 201 Created
{
  "id": 3,
  "chapter_id": 1,
  "order": 3,
  "title": "Architecture de Data Warehouse",
  "type": "multiple_choice",
  "content": { ... },
  "image": null,
  "auto_generated": true,
  "created_at": "2024-02-10T11:00:00",
  "updated_at": "2024-02-10T11:00:00"
}
```

### Create True/False Exercise

```bash
POST /api/chapters/1/exercises
Content-Type: application/json

{
  "order": 4,
  "title": "ScalabilitÃ© Verticale vs Horizontale",
  "type": "true_false",
  "content": {
    "statement": "La scalabilitÃ© horizontale est toujours prÃ©fÃ©rable Ã  la scalabilitÃ© verticale pour les systÃ¨mes de donnÃ©es.",
    "correct_answer": false,
    "explanation": "Faux. Le choix entre scalabilitÃ© horizontale (ajouter des machines) et verticale (augmenter les ressources d'une machine) dÃ©pend du cas d'usage. La scalabilitÃ© verticale peut Ãªtre plus simple et moins coÃ»teuse pour des charges modÃ©rÃ©es, tandis que l'horizontale est meilleure pour une croissance massive. Chaque approche a ses avantages selon le contexte."
  },
  "auto_generated": true
}

Response: 201 Created
{
  "id": 4,
  "chapter_id": 1,
  "order": 4,
  "title": "ScalabilitÃ© Verticale vs Horizontale",
  "type": "true_false",
  "content": { ... },
  "image": null,
  "auto_generated": true,
  "created_at": "2024-02-10T11:01:00",
  "updated_at": "2024-02-10T11:01:00"
}
```

### Create Code Exercise (Python)

```bash
POST /api/chapters/2/exercises
Content-Type: application/json

{
  "order": 1,
  "title": "Filtrage de DonnÃ©es avec Pandas",
  "type": "code",
  "content": {
    "instructions": "Ã‰crivez une fonction qui prend un DataFrame pandas et filtre les lignes oÃ¹ la colonne 'status' est 'active' et la colonne 'created_at' est dans les 30 derniers jours. Le DataFrame contient les colonnes : 'id' (int), 'status' (str), 'created_at' (datetime), 'value' (float).",
    "language": "python",
    "starter_code": "import pandas as pd\nfrom datetime import datetime, timedelta\n\ndef filter_recent_active(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Filtre le DataFrame pour ne garder que les entrÃ©es actives rÃ©centes.\n    \n    Args:\n        df: DataFrame avec colonnes id, status, created_at, value\n    \n    Returns:\n        DataFrame filtrÃ©\n    \"\"\"\n    # TODO: Calculer la date limite (30 jours avant aujourd'hui)\n    \n    # TODO: Filtrer sur status == 'active' ET created_at >= date limite\n    \n    pass",
    "solution": "import pandas as pd\nfrom datetime import datetime, timedelta\n\ndef filter_recent_active(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Filtre le DataFrame pour ne garder que les entrÃ©es actives rÃ©centes.\n    \n    Args:\n        df: DataFrame avec colonnes id, status, created_at, value\n    \n    Returns:\n        DataFrame filtrÃ©\n    \"\"\"\n    # Calculer la date limite (30 jours avant aujourd'hui)\n    cutoff_date = datetime.now() - timedelta(days=30)\n    \n    # Filtrer sur les deux conditions\n    filtered_df = df[\n        (df['status'] == 'active') & \n        (df['created_at'] >= cutoff_date)\n    ]\n    \n    return filtered_df",
    "hints": [
      "Utilisez datetime.now() et timedelta pour calculer une date dans le passÃ©",
      "Pandas permet de combiner plusieurs conditions avec & (et)",
      "N'oubliez pas les parenthÃ¨ses autour de chaque condition lors de la combinaison"
    ]
  },
  "auto_generated": true
}

Response: 201 Created
{
  "id": 5,
  "chapter_id": 2,
  "order": 1,
  ...
}
```

### Create Code Exercise (SQL)

```bash
POST /api/chapters/3/exercises
Content-Type: application/json

{
  "order": 2,
  "title": "AgrÃ©gation avec GROUP BY",
  "type": "code",
  "content": {
    "instructions": "Ã‰crivez une requÃªte SQL qui calcule le nombre de commandes et le montant total par client pour l'annÃ©e 2024. La table 'orders' contient : order_id (int), customer_id (int), order_date (date), amount (decimal). Triez les rÃ©sultats par montant total dÃ©croissant.",
    "language": "sql",
    "starter_code": "-- Calculer les statistiques par client pour 2024\n-- Table: orders (order_id, customer_id, order_date, amount)\n-- RÃ©sultat: customer_id, nombre de commandes, montant total\n\nSELECT \n    customer_id,\n    -- TODO: Compter le nombre de commandes\n    -- TODO: Calculer la somme des montants\nFROM orders\nWHERE \n    -- TODO: Filtrer pour l'annÃ©e 2024\nGROUP BY \n    -- TODO: Grouper par customer_id\nORDER BY \n    -- TODO: Trier par montant total dÃ©croissant\n;",
    "solution": "-- Calculer les statistiques par client pour 2024\nSELECT \n    customer_id,\n    COUNT(*) as order_count,\n    SUM(amount) as total_amount\nFROM orders\nWHERE \n    YEAR(order_date) = 2024\nGROUP BY \n    customer_id\nORDER BY \n    total_amount DESC;",
    "hints": [
      "Utilisez COUNT(*) pour compter les lignes et SUM(amount) pour le total",
      "La fonction YEAR() extrait l'annÃ©e d'une date",
      "ORDER BY peut utiliser les alias dÃ©finis dans SELECT"
    ]
  },
  "auto_generated": true
}

Response: 201 Created
{ ... }
```

## Updating Exercises

### Update Exercise Content

```bash
PUT /api/exercises/3
Content-Type: application/json

{
  "title": "Architecture de Data Warehouse (mis Ã  jour)",
  "content": {
    "question": "Quelle architecture de modÃ©lisation est la plus courante dans un data warehouse ?",
    "options": [
      "ModÃ¨le plat (flat)",
      "ModÃ¨le en Ã©toile (star schema)",
      "ModÃ¨le orientÃ© objets",
      "ModÃ¨le hiÃ©rarchique"
    ],
    "correct_index": 1,
    "explanation": "Le modÃ¨le en Ã©toile (star schema) est l'architecture de modÃ©lisation la plus rÃ©pandue dans les data warehouses. Il organise les donnÃ©es en une table de faits centrale (mesures) entourÃ©e de tables de dimensions (contexte). Cette structure simplifie les requÃªtes analytiques et optimise les performances."
  }
}

Response: 200 OK
{
  "id": 3,
  "chapter_id": 1,
  "order": 3,
  "title": "Architecture de Data Warehouse (mis Ã  jour)",
  "type": "multiple_choice",
  "content": { ... },
  "updated_at": "2024-02-10T12:00:00"
}
```

### Update Only the Title

```bash
PUT /api/exercises/3
Content-Type: application/json

{
  "title": "Nouveau Titre"
}

Response: 200 OK
{
  "id": 3,
  "title": "Nouveau Titre",
  ...
}
```

### Update Exercise Order

```bash
PUT /api/exercises/3
Content-Type: application/json

{
  "order": 1
}

Response: 200 OK
```

**Note:** Be careful with order changes - they can create conflicts if another exercise already has that order.

## Deleting Exercises

### Delete an Exercise

```bash
DELETE /api/exercises/3

Response: 204 No Content
```

**Note:** Exercise is permanently deleted. This does NOT automatically reorder remaining exercises.

## Batch Operations

### Creating Multiple Exercises

When creating multiple exercises at once, send separate requests for each:

```python
import requests

base_url = "http://localhost:8000"
chapter_id = 1

exercises = [
    {
        "order": 3,
        "title": "Exercise 1",
        "type": "multiple_choice",
        "content": { ... },
        "auto_generated": True
    },
    {
        "order": 4,
        "title": "Exercise 2",
        "type": "true_false",
        "content": { ... },
        "auto_generated": True
    },
    {
        "order": 5,
        "title": "Exercise 3",
        "type": "code",
        "content": { ... },
        "auto_generated": True
    }
]

for exercise in exercises:
    response = requests.post(
        f"{base_url}/api/chapters/{chapter_id}/exercises",
        json=exercise
    )
    if response.status_code == 201:
        print(f"âœ… Created: {exercise['title']}")
    else:
        print(f"âŒ Failed: {exercise['title']} - {response.text}")
```

## Error Handling

### Common Errors

#### 404 - Chapter Not Found

```bash
POST /api/chapters/999/exercises

Response: 404 Not Found
{
  "detail": "Chapter 999 not found"
}
```

**Solution:** Verify chapter ID exists via `GET /api/courses/{slug}/chapters`

#### 422 - Validation Error

```bash
POST /api/chapters/1/exercises
{
  "order": 1,
  "title": "Test",
  "type": "invalid_type",
  "content": {}
}

Response: 422 Unprocessable Entity
{
  "detail": [
    {
      "loc": ["body", "type"],
      "msg": "value is not a valid enumeration member; permitted: 'multiple_choice', 'true_false', 'code'",
      "type": "type_error.enum"
    }
  ]
}
```

**Solution:** Check type is one of: `multiple_choice`, `true_false`, `code`

#### 422 - Invalid Content Structure

```bash
POST /api/chapters/1/exercises
{
  "order": 1,
  "title": "Test",
  "type": "multiple_choice",
  "content": {
    "question": "Question?"
    // Missing options and correct_index
  }
}

Response: 422 Unprocessable Entity
{
  "detail": "Invalid content structure for multiple_choice exercise"
}
```

**Solution:** Ensure content structure matches exercise type requirements

#### 409 - Order Conflict

```bash
POST /api/chapters/1/exercises
{
  "order": 1,  // Already exists
  "title": "Test",
  "type": "multiple_choice",
  "content": { ... }
}

Response: 409 Conflict
{
  "detail": "Exercise with order 1 already exists for this chapter"
}
```

**Solution:** Get existing exercises first, use next available order

## Complete Workflow Example

### Creating 3 Exercises for a Chapter

```python
import requests
from datetime import datetime

base_url = "http://localhost:8000"
course_slug = "fundamentals-of-data-engineering"
chapter_slug = "data-engineering-described"

# Step 1: Get chapter ID
response = requests.get(f"{base_url}/api/courses/{course_slug}/chapters/{chapter_slug}")
chapter = response.json()
chapter_id = chapter['id']
print(f"Chapter ID: {chapter_id}")

# Step 2: Get existing exercises to determine next order
response = requests.get(f"{base_url}/api/chapters/{chapter_id}/exercises")
existing_exercises = response.json()
next_order = len(existing_exercises) + 1
print(f"Existing exercises: {len(existing_exercises)}, next order: {next_order}")

# Step 3: Define new exercises
new_exercises = [
    {
        "order": next_order,
        "title": "RÃ´le du Data Engineer",
        "type": "multiple_choice",
        "content": {
            "question": "Quelle est la responsabilitÃ© principale d'un data engineer ?",
            "options": [
                "CrÃ©er des dashboards de visualisation",
                "Construire et maintenir l'infrastructure de donnÃ©es",
                "Analyser les donnÃ©es pour en extraire des insights",
                "DÃ©velopper des modÃ¨les de machine learning"
            ],
            "correct_index": 1,
            "explanation": "Les data engineers sont responsables de la construction et de la maintenance de l'infrastructure qui permet la collecte, le stockage et le traitement des donnÃ©es. Les autres rÃ´les sont assurÃ©s par les data analysts (option A et C) et les ML engineers (option D)."
        },
        "auto_generated": True
    },
    {
        "order": next_order + 1,
        "title": "Evolution du Data Engineering",
        "type": "true_false",
        "content": {
            "statement": "Le data engineering a Ã©mergÃ© comme discipline distincte aprÃ¨s l'apparition du big data dans les annÃ©es 2000.",
            "correct_answer": True,
            "explanation": "Vrai. Bien que les concepts de gestion de donnÃ©es existaient auparavant, le data engineering en tant que discipline distincte a Ã©mergÃ© avec l'avÃ¨nement du big data, notamment aprÃ¨s les publications de Google sur MapReduce et GFS au dÃ©but des annÃ©es 2000."
        },
        "auto_generated": True
    },
    {
        "order": next_order + 2,
        "title": "Pipeline ETL Simple",
        "type": "code",
        "content": {
            "instructions": "CrÃ©ez une fonction Python simple qui lit un fichier CSV, filtre les lignes oÃ¹ 'age' > 18, et Ã©crit le rÃ©sultat dans un nouveau fichier CSV. Utilisez pandas.",
            "language": "python",
            "starter_code": "import pandas as pd\n\ndef filter_adults(input_file: str, output_file: str):\n    \"\"\"\n    Filtre les adultes (age > 18) d'un CSV.\n    \n    Args:\n        input_file: Chemin du CSV source\n        output_file: Chemin du CSV de sortie\n    \"\"\"\n    # TODO: Lire le CSV\n    \n    # TODO: Filtrer age > 18\n    \n    # TODO: Ã‰crire le rÃ©sultat\n    \n    pass",
            "solution": "import pandas as pd\n\ndef filter_adults(input_file: str, output_file: str):\n    \"\"\"\n    Filtre les adultes (age > 18) d'un CSV.\n    \n    Args:\n        input_file: Chemin du CSV source\n        output_file: Chemin du CSV de sortie\n    \"\"\"\n    # Lire le fichier CSV source\n    df = pd.read_csv(input_file)\n    \n    # Filtrer les lignes oÃ¹ age > 18\n    adults_df = df[df['age'] > 18]\n    \n    # Ã‰crire dans le fichier de sortie\n    adults_df.to_csv(output_file, index=False)\n    \n    print(f\"FiltrÃ© {len(adults_df)} adultes sur {len(df)} lignes\")",
            "hints": [
                "Utilisez pd.read_csv() pour lire le fichier",
                "Le filtrage avec pandas : df[df['column'] > value]",
                "Utilisez df.to_csv() avec index=False pour Ã©viter d'Ã©crire l'index"
            ]
        },
        "auto_generated": True
    }
]

# Step 4: Create exercises
created_count = 0
for exercise in new_exercises:
    response = requests.post(
        f"{base_url}/api/chapters/{chapter_id}/exercises",
        json=exercise
    )
    if response.status_code == 201:
        created = response.json()
        print(f"âœ… Created exercise {created['order']}: {created['title']}")
        created_count += 1
    else:
        print(f"âŒ Failed to create: {exercise['title']}")
        print(f"   Error: {response.text}")

print(f"\nğŸ‰ Created {created_count} exercises successfully!")
print(f"ğŸ”— View at: http://localhost:3000/courses/{course_slug}/chapters/{chapter_slug}")
```

This example demonstrates the complete workflow for creating multiple exercises programmatically.
