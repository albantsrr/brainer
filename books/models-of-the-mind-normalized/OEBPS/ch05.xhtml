<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 5</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter5"><a href="contents.xhtml#re_chapter5">CHAPTER FIVE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter5">Excitation and Inhibition</a><a id="page_117"></a></p>
<p class="H1" id="b-9781472966445-ch495-sec5">
<span class="bold">
<span>The balanced network and oscillations</span>
</span></p>
<p class="TXT">Within nearly every neuron, a battle is raging. This fight – a struggle over the ultimate output of the neuron – pits the two fundamental forces of the brain against each other. It’s a battle of excitation versus inhibition. Excitatory inputs encourage a neuron to fire. Inhibitory inputs do the opposite: they push the neuron farther from its threshold for spiking. </p>
<p class="TXI">The balance between these two powers defines the brain’s activity. It determines which neurons fire and when. It shapes their rhythms – rhythms that are implicated in everything from attention to sleep to memory. Perhaps more surprisingly, the balance between excitation and inhibition can also explain a feature of the brain that has haunted scientists for decades: the notorious unreliability of neurons. </p>
<p class="TXI">Eavesdrop on a neuron that should be doing the same thing over and over – for example, one in the motor system that is producing the same movement repeatedly – and you’ll find its activity surprisingly irregular. Rather than repeating the same pattern of spikes verbatim each time, it will fire more on some attempts and less on others. </p>
<p class="TXI">Scientists learned about this peculiar habit of neurons early in the days of neural recordings. In 1932, physiologist Joseph Erlanger made an update to the equipment in his <a id="page_118"></a>St Louis laboratory that let him record neural activity with 20 times the sensitivity previously available. He, along with his colleague E. A. Blair, was finally able to isolate individual neurons in the leg of a frog and record how they responded to precise pulses of electricity – 58 identical pulses per minute to be exact.</p>
<p class="TXI">To their surprise, Erlanger and Blair found that those identical pulses did not produce identical responses: a neuron may respond to one pulse of current, but not the next. There was still a relationship between the strength of the pulse and the response: when weak currents were used, for example, the neuron would respond, say, 10 per cent of the time, medium currents half the time and so on. But beyond these probabilities, how a neuron responded to any given pulse seemed a matter of pure chance. As the pair wrote in their 1933 paper in the <span class="italic">American Journal of Physiology</span>: ‘We were struck by the kaleidoscopic appearance of [responses] obtained from large nerves under absolutely constant conditions.’ </p>
<p class="TXI">This work was one of the first studies to systematically investigate the mysterious irregularity of the nervous system, but it would be far from the last. In 1964, for example, a pair of American scientists applied the same brushing motion to a monkey’s skin over and over. They reported that the activity of neurons responding to this motion appeared as ‘a series of irregularly recurring impulses, so that, in general, no ordered pattern can be detected by visual inspections’. </p>
<p class="TXI">In 1983, a group of researchers from Cambridge and New York noted that: ‘The variability of cortical neuron response[s] is known to be considerable.’ Their study of the visual system in cats and monkeys showed again that <a id="page_119"></a>the neural response to repeats of the same image produced different results. The responses still had <span class="italic">some</span> relation to the stimulus – the cells still changed how much they fired <span class="italic">on average</span> to different images. But exactly which neuron would fire and when for any given instance seemed as unpredictable as next week’s weather. ‘Successive presentations of identical stimuli do not yield identical responses,’ the authors concluded. </p>
<p class="TXI">In 1998, two prominent neuroscientists even went so far as to liken the workings of the brain to the randomness of radioactive decay, writing that neurons have ‘more in common with the ticking of a Geiger counter than of a clock’. </p>
<p class="TXI">Decades of research and thousands of papers have resulted in a tidy message about just how messy the nervous system is. Signals coming into the brain appear to impinge on neurons already flickering on and off at their own whims. Inputs to these neurons can influence their activity, but not control it exactly – there will always be some element of surprise. This presumably useless chatter that distracts from the main message the neuron is trying to send is referred to by neuroscientists as ‘noise’. </p>
<p class="TXI">As Einstein famously said with regard to the new science of quantum mechanics: ‘God does not play dice.’ So why should the brain? Could there be any good reason for evolution to produce noisy neurons? Some philosophers have claimed that the noise in the brain could be a source of our free will – a way to overcome a view of the mind as subject to the same deterministic laws of any machine. Yet others disagree. As British philosopher Galen Strawson wrote: ‘It may be that some <a id="page_120"></a>changes in the way one is are traceable … to the influence of indeterministic or random factors. But it is absurd to suppose that indeterministic or random factors, for which one is [by definition] in no way responsible, can in themselves contribute in any way to one’s being truly morally responsible for how one is.’ In other words, following decisions based on a coin flip isn’t exactly ‘free’ either. </p>
<p class="TXI">Other purposes for this unpredictability have been posited by scientists. Randomness, for example, can help to learn new things. If someone walks the same path to work every day, occasionally taking a random left turn could expose them to an unknown park, a new coffee shop or even a faster path. Neurons might benefit from a little exploration as well and noise lets them do that. </p>
<p class="TXI">In addition to the question of <span class="italic">why</span> neurons are noisy, the question of <span class="italic">how</span> they end up this way has preoccupied neuroscientists. Possible sources of noise exist outside the brain. Photoreceptors in the eye, for example, need to be hit by a certain number of photons before they respond. But even a constant source of light can’t guarantee that a constant stream of photons will reach the eye. In this way, the input to the nervous system itself could be unreliable. </p>
<p class="TXI">In addition, several elements of a neuron’s function depend on random processes. The electrical state of a neuron, for example, will change if the diffusion of ions in the fluid around it does. And neurons, like any other cells, are made up of molecular machines that don’t always function according to plan: necessary proteins might not be produced fast enough, moving parts may get stuck, <span class="italic">etc.</span> While these physical failures could <a id="page_121"></a>contribute to the brain’s noisiness they don’t seem to fully account for it. In fact, when neurons are taken from the cortex and put into a Petri dish they behave remarkably more reliably: stimulating these neurons the same way twice will actually produce similar results. Therefore, basic lapses in cellular machinery – which can happen in the dish just as well as in the brain – seem insufficient to explain the noise that is normally observed. </p>
<p class="TXI">The books are therefore not balanced: the noise put in somehow doesn’t equal the noise produced. We could suspect this is just a curious error in accounting; perhaps there are a few extra unreliable cogs in the neural machinery, or inputs from the world are even less stable than we believe. Such mis-estimates could maybe make up the difference, if it weren’t for one small fact: the very nature of how neurons work makes them noise <span class="italic">reducers</span>. </p>
<p class="TXI">To understand this, imagine you and some friends are playing a game where the goal is simply to see how far you can collectively move a football down a long field before a timer runs out. None of you are very well practised and occasionally you make mistakes – one person misses a pass, another gets tired, someone else trips. You also occasionally exceed your own expectations by running extra fast or passing extra far. If the time allotted is small – say 30 seconds – such momentary lapses or advantages will have big effects on your distance. You could go 150m on one try and 20 the next. But if the time is large, say five minutes, these fluctuations in performance may simply balance each other out: a slow start could be made up for with an intense sprint at the end, or the gain from a long pass could be lost because of a fall. As a result, the longer the <a id="page_122"></a>time, the more similar the distance will be on each try. In other words, the ‘noisiness’ of your athletic ability gets averaged out over time. </p>
<p class="TXI">Neurons find themselves in a similar situation. If a neuron gets enough input within a certain amount of time, it will fire a spike (see Figure 12). The input it gets is noisy because it comes from the firing of other neurons. So, the neuron may receive, say, five inputs at one moment, 13 the next and zero after that. Just like in the game example, if the neuron takes in this noisy input for a long time before deciding if it has enough to spike, the impact of the noise will be reduced. If it only uses a quick snapshot of its input, however, the noise will dominate.</p>
<p class="TXI">So how much time does a neuron combine its inputs over? About 20 milliseconds. That may not seem like long, but for a neuron it’s plenty. A spike only takes about 1 millisecond and a cell can be receiving many at a time from all of its different inputs. Therefore, neurons should be able to take the average over many snapshots of input before deciding to spike. </p>
<p class="TXI">Neuroscientists William Softky and Christof Koch used a simple mathematical model of a neuron – the ‘leaky integrate-and-fire’ model introduced in <a href="chapter2.xhtml#chapter2">Chapter 2</a> – to test just this. In their 1993 study, they simulated a neuron receiving inputs at irregular times. Yet the neuron itself – because it integrated these incoming spikes over time – still produced output spikes that were much more regular than the input it received. This means neurons do have the power to destroy noise – to take in noisy inputs while producing less noisy outputs.</p>
<p class="image-fig" id="fig12.jpg">
<img alt="" src="Images/chapter-05-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 12</span>
</span></p>
<p class="TXI">If neurons weren’t able to quench noise, then the unreliability of the brain wouldn’t be as much of a <a id="page_123"></a>mystery. As mentioned before, we could assume that small amounts of randomness enter the brain – either from the outside world or from inside a cell – and spread through the system via the connections between neurons. If noisy inputs led to outputs that were just as, or possibly more, noisy, this would be a perfectly self-consistent story; noisy neurons would beget noisy neurons. But, according to Softky and Koch’s model, this is not what happens. When passed through a neuron, noise should get weaker. When passed over and over through a whole network of neurons, it should be expected to all but disappear. Yet everywhere neuroscientists look, there it is.</p>
<p class="TXI">Not only is the brain unpredictable, then, but it seems to be encouraging that unpredictability – going against the natural tendency of neurons to squash it. What’s keeping the randomness alive? Does the brain have a random-number generator? Some sort of hidden biological dice? Or, as scientists in the 1990s <a id="page_124"></a>hypothesised, does all this disorder actually result from a more fundamental order, from the balance between excitation and inhibition?</p>
<p class="center">* * *</p>
<p class="TXT">It took Ernst Florey several trips to a Los Angeles horse butcher to uncover the source of inhibition in the brain.</p>
<p class="TXI">It was the mid-1950s when Florey, a German-born neurobiologist who emigrated to North America, was working on this question with his wife Elisabeth. At that time, the fact that neurons communicate by sending chemicals – called neurotransmitters – between each other was largely established. However, the only known neurotransmitters were excitatory – that is, they were chemicals that made a neuron <span class="italic">more</span> likely to fire. Yet since the mid-nineteenth century, it was known that some neurons can actually reduce the electrical activity of their targets. For example, the Weber brothers, Ernst and Eduard, showed in 1845 that electrically stimulating a nerve in the spinal cord could slow down the cells that controlled the beating of the heart, even bringing it to a standstill. This meant the chemical released by these neurons was inhibitory – it made cells less likely to fire. </p>
<p class="TXI">Florey needed specimens for his study on ‘factor I’, his name for the substance responsible for inhibition. So, he regularly rode his 1934 Chevrolet to a horse butcher and picked up some parts less favoured by their average customers: fresh brains and spinal cords. After extracting different substances from this nervous tissue, he checked what happened when each one was applied to living neurons taken from a crayfish. Eventually he identified <a id="page_125"></a>some candidate chemicals that reliably quieted the crayfish neurons. This cross-species pairing was actually a bit of luck on Florey’s part. Neurotransmitters can’t always be assumed to function the same way across animals. But in this case, what was inhibitory for the horse was inhibitory for the crayfish. </p>
<p class="TXI">With the help of professional chemists, Florey then used tissue from yet another animal – 45kg (100lb) of cow brain to be precise – to purify ‘factor I’ down to its most basic chemical structure. In the end he was left with 18mg of gamma-aminobutyric acid. Gamma-aminobutyric acid (or GABA, as it is more commonly known) was the first identified inhibitory neurotransmitter. </p>
<p class="TXI">Whether a neurotransmitter is inhibitory or excitatory is really in the eye of the beholder – or more technically in the receptor of the target neuron. When a neurotransmitter is released from one neuron, the chemical travels the short distance across the synapse between that neuron and its target. It then attaches itself to receptors that line the membrane of the target neuron. These receptors are like little protein padlocks. They require the right key – that is, the correct neurotransmitter – to open. And once they open they’re pretty selective about who they let in. One kind of receptor that GABA attaches to, for example, only lets chloride ions into the cell. Chloride ions have a negative charge and letting more in makes it harder for the neuron to reach the electrical threshold it needs to fire. The receptors to which excitatory neurotransmitters attach let in positively charged ions, like sodium, which bring the neuron <span class="italic">closer</span> to threshold.</p> <a id="page_126"></a>
<p class="TXI">Neurons tend to release the same neurotransmitter on to all of their targets, a principle known as Dale’s Law (named after Henry Hallett Dale who boldly guessed as much in 1934, a time at which only two neurotransmitters had even been identified). Neurons that release GABA are called ‘GABAergic’, although because GABA is the most prevalent inhibitory neurotransmitter in the adult mammalian brain, they’re frequently just called ‘inhibitory’. Excitatory transmitters are a bit more diverse, but the neurons that release them are still broadly classed as ‘excitatory’. Within an area of the cortex, excitatory and inhibitory neurons freely intermix, sending connections to, and receiving them from, each other.</p>
<p class="TXI">In 1991, after many of these facts of inhibition had been established, Florey wrote a retrospective on his role in the discovery of the first – and arguably most important – inhibitory neurotransmitter. He ended it with the sentence: ‘Whatever the brain does for the mind, we can be sure that GABA plays a major role in it.’ Likely unbeknownst to Florey, at the same time a theory that makes inhibition a key player in the production of the brain’s unpredictability was developing. </p>
<p class="center">* * *</p>
<p class="TXT">Returning to the analogy of the timed football game, imagine now that another team is added. Their goal is to fight against you, moving the ball to the opposite end of the field. When the clock stops, whoever is closer to their goal side wins. If the other team is also made up of your semi-athletic friends, then on average both teams <a id="page_127"></a>would perform the same. The noisiness of your performance will still affect the outcome: your team may beat the other by a few metres on one try and get beaten by the same small amount on another. But on the whole, it will be a balanced and tame game. </p>
<p class="TXI">Now consider if the other team was made up of professional athletes – some of the strongest, fastest players around. In this case, you and your friends wouldn’t stand a chance; you’d be clobbered every time. This is why no one would bother watching a competition between career football players and the high school team, Tiger Woods vs your dad, or Godzilla against a literal moth. The outcome of all those matches is just too predictable to be interesting. In other words, unfair fights create consistency; fair fights are more fun. </p>
<p class="TXI">In the cortex, neurons get thousands of connections from both excitatory and inhibitory cells. Because of this, each individual force is strong and would consistently dominate if the other were any weaker. Without the presence of inhibition, for example, the hundreds of excitatory inputs bombarding a cell at any moment would make it fire almost constantly; on the other hand, inhibition alone would drive the cell down to a completely stagnant state. With huge power on each side, the true activity of the neuron is thus the result of a tug of war between giants. What’s happening in the neuron is indeed a balanced fight and it’s the kind you’d see at the Olympics rather than in a schoolyard.</p>
<p class="TXI">Tell this fact to a computer scientist and they may start to get worried. That’s because computer scientists know that taking the difference between too big, noisy numbers can cause big problems. In computers, numbers <a id="page_128"></a>can only be represented with a certain level of precision. This means that some numbers need to be rounded off, introducing error – or noise – into the computation. For example, a computer with only three digits of precision may represent the number 18,231 as 1.82x10<sup>3</sup>; the leftover 31 is lost in the rounding error. When subtracting two roughly equal numbers, the effects of this rounding error can dominate the answer. For example, 18,231 minus 18,115 is 116, yet the computer would calculate this difference as 1.82x10<sup>3</sup> minus 1.81x10<sup>3</sup> which is only 10! That puts the computer off by 106. And the larger the numbers, the larger the error will be. For example, a computer with three-digit precision calculating 182,310 minus 181,150 would produce an answer that is 1,060 less than the true one. </p>
<p class="TXI">You reasonably wouldn’t feel comfortable if your bank or doctor’s office were doing their calculations this way. For this reason, programmers are taught to write their code in a way that avoids subtracting two very large numbers. Yet neurons are subtracting two large numbers – excitation minus inhibition – at every moment. Could such a ‘bug’ really be part of the brain’s operating system? </p>
<p class="TXI">Scientists had been toying with this idea for a while when, in 1994, Stanford neuroscientists Michael Shadlen and William Newsome decided to test it out. Similar to the work of Softky and Koch, Shadlen and Newsome built a mathematical model of a single neuron and fed it inputs. This time, however, the neuron got both noisy excitatory <span class="italic">and</span> noisy inhibitory inputs. When pitting these two forces against each other, sometimes excitation will win and sometimes inhibition will. Would the fight play out like a noisy calculation and create a neuron that <a id="page_129"></a>fired erratically? Or would the neuron still be able to crush the noise in these inputs the way it had the excitatory inputs in Softky and Koch’s work? Shadlen and Newsome found that, indeed, given both these types of inputs – each coming in at the same, high rate – the neuron’s output was noisy. </p>
<p class="TXI">In a boxing match between amateurs, a momentary lapse in attention from one may let the other land a small hit. In a fight between pros, however, that same lapse could lead to a knockout. In general, the stronger that two competing powers are, the bigger the swings in the outcome of their competition. This is how the internal struggle between excitation and inhibition in a neuron can overpower its normal noise-crushing abilities. Because both sources are evenly matched, the neuron’s net input (that is the total excitation minus the total inhibition) is not very big on average. But because both sources are strong, the swings around that average are huge. At one moment the neuron can get pushed far above its threshold for firing and emit a spike. At the next it could be forced into silence by a wave of inhibition. These influences can make a neuron fire when it otherwise wouldn’t have or stay silent when it otherwise would have. In this way, a balance between excitation and inhibition creates havoc in a neuron and helps explain the variability of the brain.</p>
<p class="TXI">The simulation run by Shadlen and Newsome went a long way in helping to understand how neurons could remain noisy. But it didn’t quite go far enough. Real neurons get inputs from other real neurons. For the theory that noise results from a balance between excitation and inhibition to be correct, it thus has to <a id="page_130"></a>work for a whole network of excitatory and inhibitory neurons. That means one in which each neuron’s input comes from the other neurons and its outputs go back to them too. Shadlen and Newsome’s simulation, however, was just of a single neuron, one that received inputs controlled by the model-makers. You can’t just look at the income and expenses of a single household and decide that the national economy is strong. Similarly, simulating a single neuron can’t guarantee that a network of neurons will work as needed. As we saw in the last chapter, in a system with a lot of moving parts, all of them need to move just right to get the desired outcome. </p>
<p class="TXI">To get a whole network to produce reliably noisy neurons requires coordination: each neuron needs to get excitatory and inhibitory input from its neighbours in roughly equal proportions. And the network needs to be <span class="italic">self-consistent</span> – that is, each neuron needs to produce the same amount of noise it receives, no more nor less. Could a network of interacting excitatory and inhibitory cells actually sustain the kind of noisy firing seen in the brain, or would the noise eventually peter out or explode?</p>
<p class="center">* * *</p>
<p class="TXT">When it comes to questions of self-consistency in networks, physicists know what to do. As we saw in the last chapter, physics is full of situations where self-consistency is important: gases made of large numbers of simple particles, for example, where each particle is influenced by all those around it and influences them all <a id="page_131"></a>in return. So, techniques have been developed to make the maths of these interactions easier to work with.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">In the 1980s, Israeli physicist Haim Sompolinsky was using these techniques to understand the ways materials behave at different temperatures. But his interests eventually turned towards neurons. In 1996, Sompolinsky and fellow physicist-turned-neuroscientist Carl van Vreeswijk, applied the physicist’s approach to the question of balance in the brain. Mimicking the mathematics used to understand interacting particles, they wrote down some simple equations that represented a very large population of interacting excitatory and inhibitory cells. The population also received external inputs meant to represent connections coming from other brain areas.</p>
<p class="TXI">With their simple equations, it was possible for van Vreeswijk and Sompolinsky to mathematically define the kind of behaviour they wanted to see from the model. For example, the cells had to be able to keep themselves active, but not too active (they shouldn’t be firing non-stop, for example). In addition, they should respond to increases in external input by increasing their average firing rate. And, of course, the responses had to be noisy.</p>
<p class="TXI">Putting in these demands, van Vreeswijk and Sompolinsky then churned through the equations. They found that, to create a full network that will keep firing <a id="page_132"></a>away irregularly at a reasonable rate, certain conditions had to be met. For example, the inhibitory cells need to have a stronger influence on the excitatory cells than the excitatory cells have on each other. Ensuring that the excitatory cells receive slightly more inhibition than excitation keeps the network activity in check. It’s also important that the connections between neurons be random and rare – each cell should only get inputs from, say, five or ten per cent of the other cells. This ensures no two neurons get locked into the same pattern of behaviour.</p>
<p class="TXI">None of the requirements van Vreeswijk and Sompolinsky found were unreasonable for a brain to meet. And when the pair ran a simulation of a network that met all of them, the necessary balance between excitation and inhibition emerged, and the simulated neurons looked as noisy as any real ones. Shadlen and Newsome’s intuition about how a single neuron can sustain noisy firing did in fact hold in a network of interacting neurons.</p>
<p class="TXI">Beyond just showing that balancing excitation and inhibition was possible in a network, van Vreeswijk and Sompolinsky also found a possible benefit of it: neurons in a tightly balanced network respond quickly to inputs. When a network is balanced, it’s like a driver with each foot pressed equally on the gas and the brake. This balance gets disrupted, however, if the amount of external input changes. Because the external inputs are excitatory – and they target the excitatory cells in the network more than the inhibitory ones – an increase in their firing is like a weight added to the gas pedal. The car then zooms off almost as quickly as the input came in. After this initial <span class="italic">whoosh</span> of a response, however, the <a id="page_133"></a>network regains its balance. The explosion of excitation in the network causes the inhibitory neurons to fire more and – like adding an additional weight to the brake – the network resettles in a new equilibrium, ready to respond again. This ability to act so quickly in response to a changing input could help the brain accurately keep up with a changing world.</p>
<p class="TXI">Knowing that the maths works out is reassuring, but the real test of a theory comes from real neurons. Van Vreeswijk and Sompolinsky’s work makes plenty of predictions for neuroscientists to test, and that’s just what Michael Wehr and Anthony Zador at Cold Spring Harbor Laboratory did in 2003. The pair recorded from neurons in the auditory cortex of rats, which is responsible for processing sound, while different sounds were played to the animal. Normally when neuroscientists drop an electrode into the brain they’re trying to pick up on the output of neurons – that is, their spikes. But these researchers used a different technique to eavesdrop on the input a neuron was getting – specifically to see if the excitatory and inhibitory inputs balanced each other out.</p>
<p class="TXI">What they saw was that, right after the sound turned on, a strong surge of excitation came into the cell. It was followed almost immediately by an equal influx of inhibition – the brake that follows the gas. Therefore, increasing the input to this real network showed just the behaviour expected from the model. Even when using louder sounds that produced more excitation, the amount of inhibition that followed always matched it. Balance seemed to be emerging in the brain just as it did in the model.</p>
<p class="TXI">To explore another prediction of the model, scientists had to get a bit creative. Van Vreeswijk and Sompolinsky <a id="page_134"></a>showed that, to make a well-balanced network, the strength of connections between neurons should depend on the total number of connections: with more connections, each connection can be weaker. Jérémie Barral and Alex Reyes from New York University wanted a way to change the number of connections in a network in order to test this hypothesis.</p>
<p class="TXI">Within a brain it’s hard to control just how neurons grow. So, in 2016, they decided to grow them in a Petri dish instead. It’s an experimental set-up that – in its simplicity, controllability and flexibility – is almost like a live version of a computer simulation. In order to control the number of connections, they simply put different numbers of neurons in the dish; dishes with more neurons packed in made more connections. They then monitored the activity of the neurons and checked their connection strengths. All the populations (which contained both excitatory and inhibitory cells) fired noisily, just as a balanced network should. But the connection strengths varied drastically. In a dish where each neuron only got about 50 connections, the connections were three times stronger than those with 500 connections. In fact, looking across all the populations, the average strength of a connection was roughly equal to one divided by the square root of the number of connections – exactly what van Vreeswijk and Sompolinsky’s theory predicted.</p>
<p class="TXI">As more and more evidence was sought, more was found for the belief that the brain was in a balanced state. But not all experiments went as the theory predicted; tight balance between excitation and inhibition wasn’t always seen. There is good reason to believe that certain <a id="page_135"></a>brain areas engaged in certain tasks may be more likely to exhibit balanced behaviour. The auditory cortex, for example, needs to respond to quick changes in sound frequency to process incoming information. This makes the quick responsiveness of well-balanced neurons a good match. Other areas that don’t require such speed may find a different solution.</p>
<p class="TXI">The beauty of balance is that it takes a ubiquitous inhabitant of the brain – inhibition – and puts it to work solving an equally ubiquitous mystery – noise. And it does it all without any reliance on magic: that is, no hidden source of randomness. The noise comes even while neurons are responding just as they should.</p>
<p class="TXI">This counter-intuitive fact that good behaviour can produce bedlam is important. And it had been observed somewhere else before. Van Vreeswijk and Sompolinsky make reference to this history in the first word of the title of their paper: ‘<span class="italic">Chaos</span> in neuronal networks with balanced excitatory and inhibitory activity’.</p>
<p class="center">* * *</p>
<p class="TXT">Chaos didn’t exist in the 1930s: when neuroscientists were first realising how noisy neurons are, the mathematical theory to understand their behaviour hadn’t yet been discovered. When it was, it happened seemingly by chance. </p>
<p class="TXI">The Department of Meteorology at MIT was founded in 1941, just in time for Edward Lorenz. Lorenz, born in 1917 in a nice Connecticut neighbourhood to an engineer and a teacher, had an early interest in numbers, maps and the planets. He intended to continue on in <a id="page_136"></a>mathematics after earning an undergraduate degree in it, but, as was the case for so many scientists of his time, the war intervened. In 1942 Lorenz was given the task of weather prediction for the US Army Air Corps. To learn how to do this, he took a crash course in meteorology at MIT. When he was done with the army he stayed with meteorology and remained at MIT: first as a PhD student, then a research scientist and finally a professor.</p>
<p class="TXI">If you’ve ever tried to plan a picnic, you know weather prediction is far from perfect. Academic meteorologists, who focus on the large-scale physics of the planet, hardly even consider day-to-day forecasting a goal. But Lorenz remained curious about it and about how a new technology – the computer – could help. </p>
<p class="TXI">The equations that describe the weather are many and complex. Churning through them by hand – to see how the weather right now will lead to the weather later – is an enormous, nearly impossible task (by the time you finish it the weather you’re predicting likely will have passed). But a computer could probably do it much faster. </p>
<p class="TXI">Starting in 1958, Lorenz put this to the test. He boiled the dynamics of weather down to 12 equations, picked some values to start them off with – say, westerly winds at 100km/hr – and let the mathematics run. He printed the output of the model on rolls of paper as it went along. It looked weather-like enough, with familiar ebbs and flows of currents and temperatures. One day he wanted to re-run a particular simulation to see how it would evolve over a longer period of time. Rather than starting it from the very beginning again, he figured he could start it part way along by <a id="page_137"></a>putting in the values from the printout as the starting conditions. Impatience, sometimes, is the mother of discovery. </p>
<p class="TXI">The numbers that the computer printed out, however, weren’t the full thing. To fit more on the page, the printer cut the number of digits after the decimal point from six down to three. So, the numbers Lorenz put in for the second run of the simulation weren’t <span class="italic">exactly</span> where the model was before. But what could a few decimal points matter in a model of the whole world’s weather? Turns out quite a bit. After a few cranks of the mathematical machinery – about two months of weather changes in model time – this second run of the simulation was completely different from the first. What was hot was cold, what was fast was slow. What was supposed to be a replication turned into a revelation. </p>
<p class="TXI">Up until that point, scientists assumed that small changes only beget small changes. A little gust of wind at one point in time should have no power to move mountains later. Under that dogma, what Lorenz observed must’ve come from a mistake, a technical error made by the large, clunky computers of the day, perhaps. </p>
<p class="TXI">Lorenz was willing to see what was truly happening, however. As he wrote in 1991: ‘The scientist must always be on the lookout for other explanations than those that have been commonly disseminated.’ What Lorenz had observed was the true behaviour of the mathematics, as counter-intuitive as it seemed. In certain situations, small fluctuations <span class="italic">can</span> get amplified, making behaviour unpredictable. It’s not a mistake or an error; it’s just how complex systems work. Chaos – the name given to this <a id="page_138"></a>phenomenon by mathematicians<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> – was real and it would do scientists well to try to understand it.</p>
<p class="TXI">Chaotic processes produce outputs that <span class="italic">look</span> random but in fact arise from perfect rule-following. The source of this deception is the upsetting truth that our ability to predict outcomes based on knowing the rules is far more limited than previously thought – especially if those rules are complex. In his book <span class="italic">Chaos: Making a New Science</span>, a sweeping history of how the field emerged, James Gleick wrote: ‘Traditionally, a dynamicist would believe that to write down a system’s equations is to understand the system … But because of the little bits of nonlinearity in these equations, a dynamicist would find himself helpless to answer the easiest practical questions about the future of the system.’ This imbues even the simplest systems of, say, interacting billiard balls or swinging pendulums with the potential to produce something surprising. He continued: ‘Those studying chaotic dynamics discovered that the disorderly behaviour of simple systems acted as a <span class="italic">creative</span> process. It generated complexity: richly organised patterns, sometimes stable and sometimes unstable, sometimes finite and sometimes infinite.’ </p>
<p class="TXI">Chaos was happening in the atmosphere – and if van Vreeswijk and Sompolinsky were right, it was happening in the brain, too. For this reason, explaining why the brain reacts to repeated inputs with a <a id="page_139"></a>kaleidoscopic variety needn’t involve spotty cellular machinery. That’s not to say that there aren’t any sources of noise in the brain (such as unreliable ion channels or broken-down receptors), but just that an object as complex as the brain, with its interacting pools of excitation and inhibition, doesn’t require them to show rich and irregular responses. In fact, in their simulation of a network, all van Vreeswijk and Sompolinsky had to do was change the starting state of a single neuron – from firing to not, or vice versa – to create a completely different pattern of activity across the population.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> If a change so small can create such a disturbance, the brain’s ability to keep noise alive seems less mysterious.</p>
<p class="center">* * *</p>
<p class="TXT">In medical centres around the world, epilepsy patients spend several days – up to a week – stuck in small rooms. These ‘monitoring’ rooms are usually equipped with a TV – for the patients – and cameras that monitor patient movement – for the doctors. All day and night, the patients are connected to an electroencephalogram (EEG) machine that’s capturing their brain’s behaviour. They hope that the information gathered will help to treat their seizures.</p> <a id="page_140"></a>
<p class="TXI">EEG electrodes, attached via stickers and tape to 
the scalp, monitor the electrical activity produced 
by the brain below. Each electrode provides one measurement – a complicated combination of the activity of many, many neurons at once. It’s a signal that varies over time like a seismograph. When patients are awake, the signal is a jagged and squiggly line: it moves slightly up and slightly down at random, but without any strong rhythm. When patients are asleep (particularly in deep dreamless sleep), the EEG makes waves: large movements upwards then downwards extending over a second or more. When the event of interest – a seizure – occurs, the movements are even starker. The signal traces out big, fast sweeps up and down, three to four times a second, like a kid scribbling frantically with a crayon. </p>
<p class="TXI">What are neurons doing to create these strong signals during a seizure? They’re working together. Like a well-trained military formation, they march in lockstep: firing in unison then remaining silent before firing again. The result is a repeated, synchronous burst of activity that drives the EEG signal up and down over and over again. In this way, a seizure is the opposite of randomness – it is perfect order and predictability. </p>
<p class="TXI">The same neurons that produce that seizure also produce the slow waves of sleep and the normal, noisy activity needed for everyday cognition. How can the same circuit exhibit these different behaviours? And how does it switch between them?</p>
<p class="TXI">In the late 1990s, French computational neuroscientist Nicolas Brunel set out to understand the different ways <a id="page_141"></a>circuits can conduct themselves.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> Specifically, building off the work of van Vreeswijk and Sompolinsky, he wanted to investigate how models made of excitatory and inhibitory neurons behave. To do this, Brunel explored the <span class="italic">parameter space</span> of these models. </p>
<p class="TXI">Parameters are the knobs that can be turned on a model. They are values that define specific features, like the number of neurons in the network or how many inputs each gets. Like regular space, parameter space can be explored in many different directions, but here each direction corresponds to a different parameter. The two parameters Brunel chose to explore were, firstly, how much external input the network gets (<span class="italic">i.e.</span>, input from other brain areas) and, secondly, how strong the inhibitory connections in the network are compared with the excitatory ones. By changing each of these parameters a bit and churning through the equations, Brunel could check how the behaviour of the network depends on these values. </p>
<p class="TXI">Doing this for a bunch of different parameter values results in a map of the model’s behaviour. The latitude and longitude on this map (see Figure 13) correspond to the two parameters Brunel varied respectively. For the network at the middle of the map, the inhibition is exactly equal to the excitation and the input to the network is of medium strength. Moving to the left on the map, excitation becomes stronger than inhibition; <a id="page_142"></a>move to the right and vice versa. Move upwards and the input to the network gets stronger, down and it’s weaker. Laid out this way, the network van Vreeswijk and Sompolinsky studied – with the inhibitory connections slightly stronger than excitatory ones – is just off to the right of the middle.</p>
<p class="image-fig" id="fig13.jpg">
<img alt="" src="Images/chapter-05-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 13</span>
</span></p>
<p class="TXI">Brunel surveyed this model landscape looking for any changes in the terrain: do certain sets of parameters make the network behave drastically differently? To find the first striking landmark you don’t have to travel far from van Vreeswijk and Sompolinsky’s original network. Crossing over from the region where inhibition is stronger into the one where excitation is, a sharp transition happens. In mathematics, these <a id="page_143"></a>transitions are known as bifurcations. Like a steep cliff separating a grassy plain from a sea, bifurcations mark a quick change between two separate areas in parameter space. In Brunel’s map, the line where excitation and inhibition are equal separates the networks with irregular, noisy firing on the right from those with rigid, predictable firing on the left. Specifically, when inhibition gets too weak, the neurons in these networks stop their unique pitter-patter and start firing in unison. Their tight synchronous activity – with groups of neurons flicking on and off together – looks a lot like a seizure. </p>
<p class="TXI">Physiologists have known for centuries that certain substances act as convulsants – that is, they induce seizures. With the increased understanding of neurotransmitters that came in the mid-twentieth century, it became clear that many of these drugs interfered with inhibition. Bicuculline, for example, is found in plants across North America and stops GABA from attaching to its receptor. Thujone, present in low doses in absinthe, prevents GABA receptors from letting chloride ions in. Whatever the mechanism, in the end, these drugs are throwing the balance off in the brain, putting inhibitory influences at a disadvantage. Using his bird’s-eye view of the brain’s behaviour, Brunel could see how changing the brain’s parameters – through drugs or otherwise – moved it into different states. </p>
<p class="TXI">Travelling to the other end of Brunel’s map reveals yet another pattern of activity. In this realm, inhibition rules over excitation. If the external input remains at medium strength, the neurons remain noisy here. Move up or down, however, and two similar, but <a id="page_144"></a>different, behaviours appear. With both high and low external input, the neurons show some cohesion. If you added up how many neurons were active at any given time, you’d see waves of activity: brief periods of more-than-average firing followed by less. But unlike the military precision of the seizure state, networks here are more like a percussion section made up of six-year-olds: there may be some organisation, but not everybody is playing together all the time. In fact, an individual neuron in these networks only participates in every third or fourth wave – and even then their timing isn’t always perfect. In this way, these states are both oscillating and noisy. </p>
<p class="TXI">The feature that differentiates behaviour in the top right corner of the map from that in the bottom right is the <span class="italic">frequency</span> of that oscillation. Drive the network with strong external inputs and its average activity will move up and down quickly – as fast as 180 times per second. The strong input drives the excitatory cells that drive the inhibitory cells to shut them down; then the inhibitory cells shut themselves down and the whole thing repeats. Reduce the network’s input and it oscillates more slowly, around 20 times a second. These slow oscillations occur because the external input to the network is so weak, and inhibition is so strong, that many neurons just don’t get enough input to fire. The ones that are firing, however, use their connections to slowly rev the network back up. If too many inhibitory cells get activated, though, the network becomes quiet again. </p>
<p class="TXI">Despite their superficial similarity to a seizure, these messy oscillations aren’t actually debilitating. In fact, scientists have observed oscillations in all different parts <a id="page_145"></a>of the brain under all different conditions. Groups of neurons in the visual cortex, for example, can oscillate at a swift 60 times a second. The hippocampus (the memory-processing machine from last chapter) sometimes oscillates quickly and other times slowly. The olfactory bulb, where odours are processed, generates waves that range from once per second – aligning with inhalation – to a hundred times. Oscillations can be found everywhere if you care to look.</p>
<p class="TXI">Mathematicians are happy to see oscillations. This is because, to a mathematician, oscillations are approachable. Chaos and randomness are a challenge to capture with equations, but perfect periodicity is easy and it’s elegant. Over millennia, mathematicians have developed the equipment not just to describe oscillations, but to predict how they’ll interact and to spot them in signals that – to the untrained eye – may not look like oscillations at all. </p>
<p class="TXI">Nancy Kopell is a mathematician, or at least she used to be. Like her mother and sister before her, Kopell majored in mathematics as an undergraduate. She then got a PhD<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> in it from the University of California, Berkeley, in 1967, and became a professor of mathematics <a id="page_146"></a>at Northeastern University in Boston. But after many years crossing back and forth between the border of mathematics and biology – taking problems from the latter to inspire ideas for the former – she started to feel more settled in the land of biology. As Kopell wrote in an autobiography: ‘My perspective started to shift, and I found myself at least as interested in the physiological phenomena as the mathematics problems that they generated. I didn’t stop thinking mathematically, but problems interested me less if I didn’t see the relevance to specific biological networks.’ Many of the biological networks that interested her were neural ones and throughout her career she’s studied all manner of oscillations in the brain.</p>
<p class="TXI">High-frequency oscillations are referred to by neuroscientists as ‘gamma’ waves. The reason for this is that Hans Berger, the inventor of the original EEG machine, called the big slow waves he could see by eye on his shoddy equipment ‘alpha’ waves and everything else ‘beta’; the scientists who came after him simply followed suit, giving new frequencies they found new Greek letters. Gamma waves, while fast, are usually small – or ‘low amplitude’ in technical terms. Their presence, detectable by a modern EEG or an electrode in the brain, is associated with an alert and attentive mind.</p>
<p class="TXI">In 2005, Kopell and colleagues came up with an explanation for how gamma oscillations could help the brain focus. Their theory stems from the idea that neurons representing the information you’re paying attention to should get a head start in the oscillation. Consider trying to listen to a phone call in the middle of a noisy room. Here, the signal you are paying attention <a id="page_147"></a>to – the voice on the other end of the line – is competing against all the distracting sounds in the room. In Kopell’s model, the voice is represented by one group of excitatory cells and the background chatter by another. Both of these groups send connections to a common pool of inhibitory neurons and both get connections back from it in return. </p>
<p class="TXI">Importantly, the neurons representing the voice – because they are the object of attention – get a little more input than the ‘background’ neurons. This means they’ll fire first and more vigorously. If these ‘voice’ neurons fire in unison, they will – through their connections to the inhibitory cells – cause a big, sharp increase in inhibitory cell-firing. This wave of inhibition will then shut down the cells representing both the voice and the background noise. Because of this, the background neurons never get the chance to fire and therefore can’t interfere with the sound of the voice. It’s as though the voice neurons, by being the first to fire, are pushing themselves through a door and then slamming it shut on the background neurons. And as long as the voice neurons keep getting a little extra input, this process will repeat over and over – creating an oscillation. The background neurons will be forced to remain silent each time. This leaves just the clean sound of the voice in the phone as the only remaining signal. </p>
<p class="TXI">Beyond just this role in attention, neuroscientists have devised countless other ways in which oscillations could help the brain. These include uses in navigation, memory and movement. Oscillations are also supposed to make communication between brain areas better and help organise neurons into separately functioning <a id="page_148"></a>groups. On top of this, theories abound for how oscillations go wrong in diseases like schizophrenia, bipolar disorder and autism.</p>
<p class="TXI">The omnipresence of oscillations may make it seem like their importance would go unquestioned, but this is far from the case. While several different roles for oscillations have been devised, many scientists remain sceptical. </p>
<p class="TXI">Part of the concern comes from the very first step: how oscillations are measured. Instead of recording from many neurons at once, many researchers interested in oscillations use an indirect measure that comes from the fluid that surrounds neurons. Specifically, when neurons are getting a lot of input, the composition of ions in this fluid changes and this can be used as a proxy for how active the population is. But the relationship between ion flows in this fluid and the real activity of neurons is complicated and not completely understood. This makes it hard to know if observed oscillations are actually happening. </p>
<p class="TXI">Scientists can also be swayed by the tools available to them. EEG has been around for a century and it makes the spotting of oscillations easy, even in human subjects; experiments can be performed in an afternoon on willing (usually undergraduate) research participants. As mentioned, a similar ease and familiarity applies to the mathematical tools of analysing oscillations. This may make researchers more likely to seek out these brainwaves, even in cases where they might not provide the best answers. To paraphrase the old adage, when a hammer is the easiest tool you have to use, you start looking for nails.</p> <a id="page_149"></a>
<p class="TXI">Another issue is impact, especially when it comes to fast oscillations like gamma. If one brain state has stronger gamma waves than another, it means more neurons are firing as part of a wave in that state rather than sporadically on their own. But when these waves come so quickly, being part of one may only make a neuron fire a few milliseconds before or after it otherwise would have. Could that kind of temporal precision really matter? Or is all that matters the total number of spikes produced? Many elegant hypotheses about how oscillations can help haven’t been directly tested – and can be quite hard to test – so answers are unknown. </p>
<p class="TXI">As neuroscientist Chris Moore said in a 2019 <span class="italic">Science Daily</span> interview: ‘Gamma rhythms have been a huge topic of debate … Some greatly respected neuroscientists view gamma rhythms as the magic, unifying clock that aligns signals across brain areas. There are other equally respected neuroscientists that, colourfully, view gamma rhythms as the exhaust fumes of computation: They show up when the engine is running but they’re absolutely not important.’ </p>
<p class="TXI">Exhaust fumes may be produced when a car moves, but they’re not directly what’s making it go. Similarly, networks of neurons may produce oscillations when performing computations, but whether those oscillations are what’s doing the computing remains to be seen. </p>
<p class="TXI">As has been shown, the interaction between excitatory and inhibitory cells can create a zoo of different firing patterns. Putting these two forces in conflict has both benefits and risks. It grants the network the ability to respond with lightning speed and to generate the smooth <a id="page_150"></a>rhythms needed for sleep. At the same time, it places the brain dangerously close to seizures and creates literal chaos. Making sense of such a multifaceted system can be a challenge. Luckily a diversity of mathematical methods – those developed for physics, meteorology and understanding oscillations – have helped tame the wild nature of neural firing.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-1" id="fn-1">1</a> ﻿Historically, this set of techniques went under the more obvious name of ﻿‘﻿self-consistent field theory﻿’﻿, but it﻿’﻿s now known as ﻿‘﻿mean-field theory﻿’﻿. The trick behind the mean-field approach is that you don﻿’﻿t need to provide an equation for each and every interacting particle in your system. Instead, you can study a ﻿‘﻿representative﻿’﻿ particle that receives its own output as input. That makes studying self-consistency a lot easier.﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-2" id="fn-2">2</a> ﻿In popular culture it﻿’﻿s better known as the ﻿‘﻿butterfly effect﻿’﻿, the idea that something as insignificant as a butterfly flapping its wings can change the whole course of history. ﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-3" id="fn-3">3</a> ﻿Again, the population will still produce the same amount of spikes ﻿<span class="italic">on average</span>﻿ in response to a given input. It﻿’﻿s just how those spikes are distributed across time and neurons that varies. If your neurons were truly following no rules for how they responded to inputs, you wouldn﻿’﻿t be able to be reading this right now. ﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-4" id="fn-4">4</a> ﻿Brunel, perhaps unsurprisingly at this point, started as a physicist. He learned about neuroscience during his PhD in the early 1990s, when a course exposed him to this new trend of applying the tools of physics to the brain.﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-5" id="fn-5">5</a> ﻿Kopell﻿’﻿s reasons for going to graduate school were somewhat unusual: ﻿‘﻿I had not entered college thinking about going to graduate school. But when my senior year arrived, I was not married and had nothing specific I wanted to do, so graduate school seemed like a good option.﻿’﻿ But the sexism she encountered there was perhaps more expected: ﻿‘﻿It was the unspoken but very widespread assumption that women in math were like dancing bears ﻿–﻿ perhaps they could do it, but not very well, and the attempt was an amusing spectacle.﻿’﻿﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 2</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter2"><a href="contents.xhtml#re_chapter2">CHAPTER TWO</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter2">How Neurons Get Their Spike</a><a id="page_17"></a></p>
<p class="H1" id="b-9781472966445-ch99-sec2">
<span class="bold">
<span>Leaky integrate-and-fire and Hodgkin-Huxley neurons</span>
</span></p>
<p class="TXT">‘The laws of action of the nervous principle are totally different from those of electricity,’ concluded Johannes Müller more than 600 pages into his 1840 textbook <span class="italic">Handbuch der Physiologie des Menschen</span>. ‘To speak, therefore, of an electric current in the nerves, is to use quite as symbolical an expression as if we compared the action of the nervous principle with light or magnetism.’</p>
<p class="TXI">Müller’s book – a wide-ranging tour through the new and uncertain terrain of the field of physiology – was widely read. Its publication (especially its near-immediate translation into English under the title <span class="italic">Elements of Physiology</span>) cemented Müller’s reputation as a trusted teacher and scientist. </p>
<p class="TXI">Müller was a professor at Humboldt University of Berlin from 1833 until his death 25 years later. He had a broad interest in biology and strong intellectual views. He was a believer in vitalism, the idea that life relied on a <span class="italic">Lebenskraft</span>, or vital organising force, that went beyond mere chemical and physical interactions. This philosophy could be found streaking through his physiology. In his book, he claims not only that the activity of nerves is not electric in nature, but that it may ultimately be ‘imponderable’, the question of its essence ‘not capable of solution by physiological facts’.</p> <a id="page_18"></a>
<p class="TXI">Müller, however, was wrong. Over the course of the following century, the spirit that animated the nerves would prove wholly reducible to the simple movement of charged particles. Electricity is indeed the ink in which the neural code is written. The nervous principle was perfectly ponderable after all.</p>
<p class="TXI">More than merely striking down Müller’s vitalism, the identification of this ‘bio-electricity’ in the nervous system provided an opportunity. By forging a path between the two rapidly developing studies of electricity and physiology, it allowed for the tools of the former to be applied to the problems of the latter. Specifically, equations – whittled down by countless experiments to capture the essential behaviours of wires, batteries and circuits – now offered a language in which to describe the nervous system. The two fields would come to share symbols, but their relationship was far more than the merely symbolic one Müller claimed. The proper study of the nervous system depended on collaboration with the study of electricity. The seeds of this collaboration, planted in the nineteenth century, would come to sprout in the twentieth and bloom in the twenty-first. </p>
<p class="center">* * *</p>
<p class="TXT">Walk into the home of an educated member of upper-class society in late eighteenth-century Europe and you may find, among shelves of other scientific tools and curiosities, a Leyden jar. Leyden jars, named after the Dutch town that was home to of one of their inventors, are glass jars like most others. However instead of storing jam or pickled vegetables, Leyden jars store charge. <a id="page_19"></a>Developed in the mid-eighteenth century, these devices marked a turning point in the study of electricity. As a literal form of lightning in a bottle, they let scientists and non-scientists alike control and transmit electricity for the first time – sometimes doling out shocks large enough to cause nosebleeds or unconsciousness. </p>
<p class="TXI">While its power may be large, the Leyden jar’s design is simple (see Figure 1). The bottom portion of the inside of the jar is covered in a metal foil, as is the same region on the outside. This creates a sandwich of glass in between the two layers of metal. Through a chain or rod inserted at the top of the jar, the internal foil gets pumped full of charged particles. Particles of opposite charge are attracted to each other, so if the particles going into the jar are positively charged, for example, then negatively charged ones will start to accrue on the outside. The particles can never reach each other, however, because the glass of the jar keeps them apart. Like two neighbourhood dogs separated by a fence, they can only line up on either side of the glass, desperately wishing to be closer.</p><a id="page_20"></a>
<p class="TXI">We would now call a device that stores charge like the Leyden jar a ‘capacitor’. The disparity in charge on either side of the glass creates a difference in potential energy known as voltage. Over time, as more and more charge is added to the jar, this voltage increases. If the glass barrier disappeared – or another path were provided for these particles to reach each other – that potential energy would turn into kinetic energy as the particles moved towards their counterparts. The higher the voltage was across the capacitor, the stronger this movement of charge – or current – would be. This is exactly how so many scientists and tinkerers ended up shocking themselves. By creating a link between the inside and outside of the jar with their hand, they opened a route for the flow of charged particles right through their body.</p>
<p class="image-fig" id="fig1.jpg">
<img alt="" src="Images/chapter-02-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 1</span>
</span></p>
<p class="TXI">Luigi Galvani was an Italian scientist born in 1737. Strongly religious throughout his life, he considered joining the church before eventually studying medicine at the University of Bologna. There he was educated not just in techniques of surgery and anatomy, but also in the fashionable topic of electricity. The laboratory he kept in his home – where he worked closely with his wife Lucia, the daughter of one of his professors – contained devices for exploring both the biological and the electric: scalpels and microscopes, along with electrostatic machines and, of course, Leyden jars. For his medical experiments, Galvani – like students of biology for centuries before and after him – focused on frogs. The muscles in a frog’s legs can keep working after death, a desirable feature when trying to simultaneously understand the workings of an animal and dissect it.</p> <a id="page_21"></a>
<p class="TXI">It was a result of his lab’s diversity – and potentially disorganisation – that landed Galvani in the pages of science textbooks. As the story goes, someone in the lab (possibly Lucia) touched a metal scalpel to the nerve of a dead frog’s leg at the exact moment that an errant spark from an electrical device caused the scalpel to carry charge. The leg muscles of the frog immediately contracted, an observation Galvani decided to enthusiastically pursue. In his 1791 book he describes many different preparations for his follow-up experiments on ‘animal electricity’, including comparing the efficacy of different types of metal in eliciting contractions and how he connected a wire to a frog’s nerve during a thunderstorm. He watched its legs contract with each lightning flash.</p>
<p class="TXI">There had always been some hints that life was making use of electricity. Ibn Rushd, a twelfth-century Muslim philosopher, anticipated several scientific findings when he noted that the ability of an electric fish to numb the fishermen in its waters may stem from the same force that pulls iron to a lodestone. And in the years before Galvani’s discovery, physicians were already exploring the application of electric currents to the body as a cure for everything from deafness to paralysis. But Galvani’s varied set of experiments took the study of bio-electricity beyond mere speculation and guesswork. He gathered the evidence to show that animal movement follows from the movement of electricity in the animal. He thus concluded that electricity was a force intrinsic to animals, a kind of fluid that flowed through their bodies as commonly as blood. </p>
<p class="TXI">In line with the spirit of amateur science at the time, upon hearing news of Galvani’s work many people set <a id="page_22"></a>out to replicate it. Putting their personal Leyden jars in contact with any frog they could capture, curious laymen saw the same contractions and convulsions as Galvani did. So broad was the impact of Galvani’s work – and along with it the idea of electrical animation – it made its way into the mind of English writer Mary Shelley, forming part of the inspiration for her novel <span class="italic">Frankenstein</span>.</p>
<p class="TXI">A healthy dose of scientific scepticism, however, meant that not all of Galvani’s academic peers were so enthusiastically accepting of his claims. Alessandro Volta – an Italian physicist after whom ‘voltage’ was named – acknowledged that electricity could indeed cause muscle contractions in animals. But he denied that this means animals <span class="italic">normally</span> use electricity to move. Volta didn’t see in Galvani’s experiments any evidence that animals were producing their own electricity. In fact, he found that contact between two different metals could create many, nearly imperceptible, electric forces and therefore any test of animal electricity using metals in contact could be contaminated by externally generated electricity. As Volta wrote in an 1800 publication: ‘I found myself obliged to combat the pretended animal electricity of Galvani and to declare it an external electricity moved by the mutual contact of metals of different kinds’.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">Unfortunately for Galvani, Volta was a younger man, more willing to engage in public debate and on his way up in the field. He was a formidable scientific opponent. The power of Volta’s personality meant Galvani’s ideas, though correct in many ways, would be eclipsed for decades.</p> <a id="page_23"></a>
<p class="TXI">Müller’s textbook came nearly 10 years after Volta’s death, but his objection to animal electricity followed similar lines. He simply didn’t believe electricity was the substance of nervous transmission and the weight of the evidence at the time couldn’t sway him. In addition to his vitalist tendencies, this stubbornness was perhaps due to Müller’s preference for observation over intervention. No matter how many examples of animals responding to externally applied electricity amassed over the years, it would never equal a direct observation of an animal generating its own electricity. ‘Observation is simple, indefatigable, industrious, upright, without any preconceived opinion,’ said Müller in his inaugural lecture at the University of Bonn. ‘Experiment is artificial, impatient, busy, digressive, passionate, unreliable.’ At the time, however, observation was impossible. No tool was powerful enough to pick up on the faint electrical signals carried by nerves in their natural state. </p>
<p class="TXI">That changed in 1847 when Emil du Bois-Reymond – one of Müller’s own students – fashioned a very sensitive galvanometer,<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> a device that measures current through its interaction with a magnetic field. His experiments were an attempt to replicate in nerves what Italian physicist Carlo Matteucci had recently observed in muscles. Using a galvanometer, Matteucci detected a small change in current coming from muscles after forcing them to contract. Searching for this signal in a nerve, however, demanded a stronger magnetic field to pick up the weaker current. In addition to designing proper insulation 
to prevent any interference from outside electricity, <a id="page_24"></a>du Bois-Reymond had to coil more than a mile of wire by hand (producing more than eight times the coils of Matteucci) to get a magnetic field strong enough for his purposes. His handiwork paid off. With his galvanometer measuring its response, du Bois-Reymond stimulated 
a nerve in various ways – including electrically or 
with chemicals like strychnine – and monitored the galvanometer’s reading of how the nerve responded. Each time, he saw the galvanometer’s needle shoot up. Electricity had been spotted at work in the nervous system. </p>
<p class="TXI">Du Bois-Reymond was a showman as much as he was a scientist and he lamented the dry presentation styles of his fellow scientists. To spread the fruits of his labour, he built several public-ready demonstrations of bio-electricity, including a set-up where he could make a needle move by squeezing his arm in a jar of salt water. All this helped ensure that his findings would be noticed and that du Bois-Reymond would be fondly regarded by the minds of his time. As he said: ‘Popularisers of science persist in the public mind as memorial stones of human progress long after the waves of oblivion have surged over the originators of the soundest research.’ </p>
<p class="TXI">Luckily his research was sound as well. Particularly, the follow-up work du Bois-Reymond carried out with his student Julius Bernstein would seal the fate of the theory of nervous electricity. Du Bois-Reymond’s original experiment had succeeded in showing a signature of current change in an activated nerve. But Bernstein, through clever and careful experimental design, was able to both amplify the strength of the signal and record it at a finer timescale – creating the first true observation of the elusive nervous signal.</p><a id="page_25"></a>
<p class="TXI">Bernstein’s experiments worked by first isolating a nerve and placing it on to his device. The nerve was then electrically stimulated at one end and Bernstein would look for the presence of any electrical activity some distance away. By recording with a precision of up to one-third of one-thousandth of a second, he saw how the nerve current changed characteristically over time after each stimulation. Depending on how far his recording site was from the stimulation site, there may be a brief pause as the electric event travelled down the nerve to reach the galvanometer. Once it got to where he was recording, however, he always saw the current rapidly decrease and then more slowly recover to its normal value.</p>
<p class="TXI">Bernstein’s result, published in the inaugural issue of the <span class="italic">European Journal of Physiology</span> in 1868, was the first known recording of what is now referred to as an ‘action potential’. An action potential is defined as a characteristic pattern of changes in the electrical properties of a cell. Neurons have action potentials. Certain other excitable cells, like those in the muscles or heart, do as well. </p>
<p class="TXI">This electrical disturbance travels across a cell’s membrane like a wave. In this way, action potentials help a cell carry a signal from one end of itself to the other. In the heart, for example, the ripple of an action potential helps coordinate a cell’s contraction. Action potentials are also a way for a cell to say something to other cells. In a neuron, when an action potential reaches the knobbly end of an outgrowth known as an axon, it pushes out neurotransmitters. These chemicals can reach other cells and trigger action potentials in them as well. In the case of the familiar frog nerve, the action potentials travelling down the leg lead to the <a id="page_26"></a>release of neurotransmitters on to the leg muscle. Action potentials in the muscle then cause it to twitch. </p>
<p class="TXI">Bernstein’s work was the first word in a long story about the action potential. Now recognised as the core unit of communication in the nervous system, the action potential forms the basis of modern neuroscience. This quick blip of electrical activity connects the brain to the body, the body to the brain and links all the neurons of the brain in between. </p>
<p class="TXI">With his glimpsing of current changes coming from the nerve, du Bois-Reymond wrote: ‘If I do not greatly deceive myself, I have succeeded in realising [...] the hundred years’ dream of physicists and physiologists, to wit, the identity of the nervous principle with electricity.’ The nervous principle was indeed identified in the action potential. Yet du Bois-Reymond had committed himself to a ‘mathematico-physical method’ of explaining biology, and while he had established the physical, he had not quite solved the mathematical. With a growing sense among scientists that proper science involved quantification, the job of describing the physical properties of the nervous principle was far from done. Indeed, it would take roughly another hundred years to capture the essence of the nervous principle in equations. </p>
<p class="center">* * *</p>
<p class="TXT">In contrast to the experience of Johannes Müller, when Georg Ohm published a book of his scientific findings he lost his job. </p>
<p class="TXI">Ohm was born in 1789, the son of a locksmith. He studied for only a short time at the university of his <a id="page_27"></a>hometown, Erlangen in Germany, and then spent years teaching mathematics and physics in various cities. Eventually, with an aim of becoming an academic, he started performing his own small experiments, particularly around the topic of electricity. For one test, he cut wires of various lengths out of different metals. He then applied voltage across the two ends of the wire and measured how much current flowed between them. Through this he was able to deduce a mathematical relationship between the length of a wire and its current: the longer the wire, the lower the current. </p>
<p class="TXI">By 1827, Ohm had collected this and other equations of electricity into his book, <span class="italic">The Galvanic Circuit Investigated Mathematically</span>. Quite contrary to its modern form, the study of electricity in the time of Ohm wasn’t a very mathematical discipline and Ohm’s peers didn’t like his attempts to make it one. One reviewer went so far as to say: ‘He who looks on the world with the eye of reverence must turn aside from this book as the result of an incurable delusion, whose sole effort is to detract from the dignity of nature.’ Having taken time away from his job to write the book in the hope it would land him a promotion, Ohm, with the book’s failure, ended up resigning instead. </p>
<p class="TXI">Ohm, however, was right. The key relationship he observed – that the current that runs through a wire is equal to the voltage across it divided by the wire’s resistance – is a cornerstone of modern electrical engineering taught to first-year physics students worldwide. This is now known as Ohm’s law and the standard unit of resistance is the ‘ohm’. Ohm wouldn’t know the full impact of his work in his lifetime, but he <a id="page_28"></a>did eventually get some recognition. At the age of 63 he was finally appointed a professor of experimental physics at the University of Munich, two years before he died. </p>
<p class="TXI">Resistance, as the name suggests, is a measure of opposition. It’s a description of just how much a material impedes the course of current. Most materials have some amount of resistance, but, as Ohm noted, the physical properties of a material determine just how resistant it is. Longer wires have higher resistance; thicker ones have lower. Just as the narrowing of an hourglass slows the flow of sand, wires with higher resistance hinder the flow of charged particles.</p>
<p class="TXI">Louis Lapicque knew of Ohm’s law. Born in France in 1866, shortly after the first recording of an action potential, Lapicque completed his doctorate at the Paris Medical School. He wrote his thesis on liver function and iron metabolism. Though his studies were scientific, his interests ranged more broadly from history to politics to sailing; he sometimes even took his boat to conferences across the English Channel.</p>
<p class="TXI">It was around the start of the twentieth century that Lapique started studying the nerve impulse. It would be the beginning of a decades-long project with his student-turned-wife-and-colleague Marcelle de Heredia, which centred on the concept of time in nerves. One of their earliest questions was: how long does it take to activate a nerve? It was well established by then that applying voltage across a nerve<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> caused a response – measured either as an action potential observed directly in the <a id="page_29"></a>nerve or as a muscle twitch that resulted from it. It was also clear that the amount of voltage applied mattered: higher voltage and the nerve would respond quicker, lower and it would respond slower. But what was the exact <span class="italic">mathematical</span> relationship between the stimulation applied and the time it took to get a response? </p>
<p class="TXI">This may sound like something of a small research question, a curiosity not of much consequence, but it was Lapicque’s approach to it that mattered. Because a proper physiologist also needed to be an engineer – designing and building all manner of electrical devices for stimulating and recording from nerve fibres – Lapicque knew the rules of electricity. He knew about capacitors, resistance, voltage and Ohm’s law. And it was with this knowledge that he composed a mathematical concept of the nerve that would answer his question – and many more to come. </p>
<p class="TXI">Understanding of the membranes that enclose cells had grown in the decades before Lapicque’s work. It was becoming clear that these bundles of biological molecules worked a little bit like a brick wall: they didn’t let much through. Some of the particles they were capable of keeping apart included ions – atoms of different elements like chloride, sodium or potassium that carry a positive or negative charge. So, just as charged particles could build up on either side of the glass in a Leyden jar, so too could they accrue on the inside and outside of a cell. As Lapicque wrote in his 1907 paper: ‘These ideas lead, when treated in the simplest possible way, to already established equations for the polarisation of metal electrodes.’ </p>
<p class="TXI">He thus came to describe the nerve in terms of an ‘equivalent circuit’. (see Figure 2) That is, he assumed <a id="page_30"></a>that different parts of the nerve acted like the different components of an electrical circuit. The first equivalence was made between the cell membrane and a capacitor, as the membrane could store charge in just the same way. But it was clear that these membranes weren’t acting as perfect capacitors; they couldn’t keep all of the charge apart. Instead, some amount of current seemed to flow between the inside and outside of the cell, allowing it to discharge slightly. A wire with some resistance could play this role. So Lapicque added a resistor to his circuit model of the nerve in parallel with the capacitor. This way, when current is injected into the circuit, some of that charge goes to the capacitor and some goes through the resistor. Trying to create a charge difference across the cell is therefore like pouring water into an imperfect bucket; much of it would stay in the bucket, but some would leak away.</p>
<p class="image-fig" id="fig2.jpg">
<img alt="" src="Images/chapter-02-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 2</span>
</span></p>
<p class="TXI">This analogy between a cell and a circuit made it possible for Lapicque to write down an equation. The equation described how the voltage across the cell <a id="page_31"></a>membrane should change over time, based on how much voltage was being applied to it and for how long. With this formalisation in mind, he could calculate when the nerve would respond.</p>
<p class="TXI">For the data to test his equation on, Lapicque turned to the standard frog-leg experiment: he applied different amounts of voltage to the frog’s nerve and recorded the time it took to see a response. Lapicque assumed that when the frog nerve responded it was because the voltage across its membrane had reached a certain threshold. He therefore calculated how long his model would take to reach that threshold for each different voltage applied. Comparing the predictions from his model with the results of his experiments, Lapicque found a good match. He could predict just how long a certain voltage would need to be applied in order to make the nerve respond.</p>
<p class="TXI">Lapicque wasn’t the first to write down such an equation. A previous scientist, Georges Weiss, offered a guess as to how to describe this relationship between voltage and time. And it was a relatively good guess too; it deviated from Lapicque’s predictions only somewhat, for example, in the case of voltages applied for a long time. But just as the smallest clue at a crime scene can change the narrative of the whole event, this slight difference between the predictions of Lapicque’s equation and what came before actually signalled a deep difference in understanding.</p>
<p class="TXI">Unlike Lapicque’s, Weiss’ equation wasn’t inspired by the mechanics of a cell nor was it meant to be interpreted as an equivalent circuit. It was more a description of the data than a model of it. Whereas a descriptive equation is <a id="page_32"></a>like a cartoon animation of an event – capturing its appearance but without any depth – a model is a re-enactment. A mathematical model of a nerve impulse thus needs to have the same moving parts as the nerve itself. Each variable should be mappable to a real physical entity and their interactions should mirror the real world as well. That is just what Lapicque’s equivalent circuit provided: an equation where the terms were interpretable.</p>
<p class="TXI">Others before Lapicque had seen the similarity between the electrical tools used to study the nerve and the nerve itself. Lapicque was building heavily on the work of Walther Nernst, who noticed that the membrane’s ability to separate ions could underlie the action potential. Another student of du Bois-Reymond, Ludimar Hermann, had spoken of the nerve in terms of capacitors and resistors. And even Galvani himself had a vision of a nerve that worked similarly to his Leyden jar. But with his explicit equivalent circuit and quantitative fit to data, Lapicque went a step further in making an argument for the nerve as a precise electrical device. As he wrote: ‘The physical interpretation that I reach today gives a precise meaning to several important previously known facts on excitability … It seems to me a reason to consider it a step in the direction of realism.’</p>
<p class="TXI">Due to their limited equipment, most neuroscientists of Lapicque’s time were recording from whole nerves. Nerves are bundles of many axons – the fibres through which individual neurons send their signals to other cells. Recording from many axons at once makes it easier to pick up the current changes they produce, but harder to see the detailed shape of those changes. Sticking an electrode into a single neuron, however, makes it possible <a id="page_33"></a>to record the voltage across its membrane directly. Once the technology to observe individual neurons became available in the early twentieth century, the action potential came into much clearer view. </p>
<p class="TXI">One defining feature of the action potential noticed by English physiologist Edgar Adrian in the 1920s is the ‘all-or-nothing’ principle.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> The all-or-nothing principle says that a neuron either emits an action potential or it doesn’t – nothing in between. In other words, any time a neuron gets enough input, the voltage across its membrane changes – and it changes in exactly the same way. So, just as a goal in hockey counts the same no matter how hard the puck is driven into the net, strongly stimulating a neuron doesn’t make its action potential any bigger or better. All stronger stimulation can do is make the neuron emit <span class="italic">more</span> of the exact same action potentials. In this way, the nervous system cares more about quantity than quality. </p>
<p class="TXI">The all-or-nothing nature of a neuron aligns with Lapicque’s intuition about a threshold. He knew that the voltage across the membrane needed to reach a certain value in order to see a response from the nerve. But once it got there, a response was a response. </p>
<p class="TXI">By the 1960s, the all-or-nothing principle was combined with Lapicque’s equation into a mathematical model known as a ‘leaky integrate-and-fire neuron’: ‘leaky’ because the presence of a resistor means some of the current leaks away; ‘integrate’ because the capacitor integrates the rest of it and stores it as charge; and ‘fire’ <a id="page_34"></a>because when the voltage across the capacitor reaches the threshold the neuron ‘fires’, or emits an action potential. After each ‘firing’ the voltage is reset to its baseline, only to reach threshold again if more input is given to the neuron. </p>
<p class="TXI">While the model is simple, it can replicate features of how real neurons fire: for example, with strong and constant input the model neuron will fire action potentials repeatedly, with only a small delay between each one; if the input is low enough, however, it can remain on indefinitely without ever causing a single action potential. </p>
<p class="TXI">These model neurons can also be made to form connections – strung together such that the firing of one generates input to another. This provides modellers with a broader power: to replicate, explore and understand the behaviour of not just individual neurons but whole networks of them. </p>
<p class="TXI">Since their inception, such models have been used to understand countless aspects of the brain, including disease. Parkinson’s disease is a disorder that impacts the firing of neurons in the basal ganglia. Located deep in the brain, the basal ganglia are composed of a variety of regions with elaborate Latin names. When the input to one region – the striatum – is perturbed by Parkinson’s disease, it knocks the rest of the basal ganglia off balance. As a result of changes to the striatum, the subthalamic nucleus (another region of the basal ganglia) starts to fire more, which causes neurons in the globus pallidus external (yet another basal ganglia region) to fire. But those neurons send connections <span class="italic">back</span> to the subthalamic nucleus that <a id="page_35"></a>
<span class="italic">prevent</span> those neurons from firing more – which also, in turn, shuts down the globus pallidus external itself. The result of this complicated web of connections is oscillations: neurons in this network fire more, then less, then more again. These rhythms appear to be related to the movement problems Parkinson’s patients have – tremors, slowed movements and rigidity. </p>
<p class="TXI">In 2011, researchers at the University of Freiburg built a computer model of these brain regions made up of 3,000 leaky integrate-and-fire model neurons. In the model, disturbing the cells that represented the striatum created the same problematic waves of activity seen in the subthalamic nuclei of Parkinson’s patients. With the model exhibiting signs of the disease, it could also be used to explore ways to treat it. For example, injecting pulses of input into the model’s subthalamic nucleus broke these waves down and restored normal activity. But the pulses had to be at just the right pace – too slow and the offending oscillations got worse, not better. Deep brain stimulation – a procedure where pulses of electrical activity are injected into the subthalamic nucleus of Parkinson’s patients – is known to help alleviate tremors. Doctors using this treatment also know that the rate of pulses has to be high, around 100 times per second. This model gives a hint as to why high rates of stimulation work better than lower ones. In this way, modelling the brain as a series of interconnected circuits illuminates how the application of electricity can fix its firing.</p>
<p class="TXI">Lapicque’s original interest was in the timing of neural firing. By piecing together the right components of an electrical circuit he captured the timing of action potentials correctly, but the creation of this circuit stand-in for a neuron did more than that. It formed <a id="page_36"></a>a solid foundation on which to build towering networks of thousands of interconnecting cells. Now computers across the world churn through the equations of these faux neurons, simulating how real neurons integrate and fire in both health and disease. </p>
<p class="center">* * *</p>
<p class="TXT">In the summer of 1939, Alan Hodgkin set out on a small fishing boat off the southern coast of England. His goal was to catch some squid, but mostly what he got was seasick. </p>
<p class="TXI">At the time, Hodgkin, a research fellow at Cambridge University, had only just arrived at the Marine Biological Association in Plymouth ready to embark on a new project studying the electrical properties of the squid giant axon. In particular, he wished to know how the action potential got its characteristic up-down shape (frequently referred to as a ‘spike’<sup><a href="#fn-5" id="fnt-5">5</a>
</sup>). A few weeks later he was joined by an equally green student collaborator, Andrew Huxley. Luckily, the men eventually figured out when and where their subject matter could be found in the sea. </p>
<p class="TXI">Though Huxley was a student of Hodgkin’s, the men were only four years apart in age. Hodgkin looked the role of a proper English gentleman: long face, sharp eyes, his hair neatly parted and swept to the side. Huxley was a bit more boyish with round cheeks and heavy eyebrows. Both men had skill in biology and physics, though each came to this pairing from the opposite side. </p>
<p class="TXI">Hodgkin primarily studied biology, but in his last term he was encouraged by a zoology professor to learn <a id="page_37"></a>as much mathematics and physics as he could. Hodgkin obliged, spending hours with textbooks on differential equations. Huxley was long interested in mechanics and engineering, but switched to a more biological track after a friend told him physiology classes would teach more lively and controversial topics. Huxley may have also been drawn to these subjects by the influence of his grandfather. Biologist Thomas Henry Huxley – known as ‘Darwin’s bulldog’ for his vociferous defence of Darwin’s theory of evolution – described physiology as ‘the mechanical engineering of living machines’. </p>
<p class="TXI">Lapicque’s model predicted when a cell would fire, but it still didn’t explain exactly what an action potential was. At the time of Hodgkin’s boat trip, the going theory of what happens when a neuron emits an action potential was still the one put forward by the original action potential observer himself, Julius Bernstein. It said that, at the time of this electrical event, the cell membrane temporarily breaks down. It therefore lets ions of all different kinds flow through, erasing the charge difference that is normally across it and creating the small current Bernstein saw with his galvanometer. </p>
<p class="TXI">But some of Hodgkin’s previous experiments on crabs told him this may not be quite right. He wanted to follow up on this work with the squid because the large size of the axon running along its mantle made precise measurements easier.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> Sticking an electrode into this axon, Hodgkin and Huxley recorded the voltage changes <a id="page_38"></a>that occurred during an action potential (See Figure 3). What they saw was a clear ‘overshoot’. That is, the voltage didn’t just go to zero, as would happen with a discharged capacitor, but rather it <span class="italic">reversed</span>. While a neuron normally has more positive charge on the outside of the cell than on the inside, during the peak of the action potential this pattern gets inverted and the inside becomes more positively charged than the outside. Simply letting more ions diffuse through the membrane wouldn’t lead to such a separation. Something more selective was at play.</p>
<p class="TXI">Only a short time after Hodgkin and Huxley made this discovery, their work was unfortunately interrupted. Hitler invaded Poland. The men needed to abandon the lab and join the war effort. Solving the mystery of the action potential would have to wait.</p>
<p class="image-fig" id="fig3.jpg">
<img alt="" src="Images/chapter-02-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 3</span>
</span></p>
<p class="TXI">When Hodgkin and Huxley returned to Plymouth eight years later the lab required a bit of reassembling: the building had been bombed in air raids and their equipment had been passed around to other scientists. But the men, each having picked up some extra quantitative skills as a result of their wartime assignments – Huxley performing data analysis for the <a id="page_39"></a>Gunnery Division of the Royal Navy and Hodgkin developing radar systems for the Air Force – were eager to get back to work on the physical mechanisms of the nerve impulse.</p>
<p class="TXI">For many of the following years, Hodgkin and Huxley (helped by fellow physiologist Bernard Katz) played with ions. By removing a certain type of ion from the neuron’s environment, they could determine which parts of the action potential relied on which kinds of charged particles. A neuron kept in a bath with less sodium had less of an overshoot. Add extra potassium to the bath and the undershoot – an effect at the very end of the action potential when the inside of the cell becomes more negative than normal – disappeared. The pair also experimented with a technique that let them directly control the voltage across the cell membrane. Changing this balance of charge created large changes in the flow of ions into and out of the cell. Remove the difference in charge across the membrane and stores of sodium outside the cell suddenly swim inwards; keep the cell in this state a bit longer and potassium ions from inside the cell rush out. </p>
<p class="TXI">The result of all these manipulations was a model. Specifically, Hodgkin and Huxley condensed their hard-won knowledge of the nuances of neural membranes into the form of an equivalent circuit and with it a corresponding set of equations. This equivalent circuit was more complex than Lapicque’s, however. It had more moving parts as it aimed to explain not just <span class="italic">when</span> an action potential happens but the full shape of the event itself. The main difference, though, came down to resistance.</p> <a id="page_40"></a>
<p class="TXI">In addition to the resistor Lapicque put in parallel with the membrane capacitor, Hodgkin and Huxley added two more – one specifically controlling the flow of sodium ions and the other controlling the flow of potassium ions. Such a separation of resistors assumed that different channels in the cell membrane were selectively allowing different ion types to pass. What’s more, the strength of these resistors – that is, the extent to which they block the flow of their respective ions – is not a fixed parameter in the model. Instead, they are dependent on the state of the voltage across the capacitor. The cell accomplishes this by opening or closing its ion channels as the voltage across its membrane changes. In this way, the membrane of the cell acts like the bouncer of a club: it assesses the population of particles on either side of itself and uses that to determine which ions get to enter and exit the cell. </p>
<p class="TXI">Having defined the equations of this circuit, Hodgkin and Huxley wanted to churn through the numbers to see if the voltage across the model’s capacitor really would mimic the characteristic whish and whoosh of an action potential. There was a problem, however. Cambridge was home to one of the earliest digital computers, a device that would’ve greatly sped up Hodgkin and Huxley’s calculations, but it was out of service. So, Huxley turned to a Brunsviga – a large, metal calculator powered by a hand-crank. As he sat for days putting in the value of the voltage at one point in time just to calculate what it would be at the next one-ten-thousandth of a second, Huxley actually found the work somewhat suspenseful. As he said in his Nobel lecture: ‘It was quite often exciting … Would the membrane <a id="page_41"></a>potential get away into a spike, or die in a subthreshold oscillation? Very often my expectations turned out to be wrong, and an important lesson I learnt from these manual computations was the complete inadequacy of one’s intuition in trying to deal with a system of this degree of complexity.’ </p>
<p class="TXI">With the calculations complete, Hodgkin and Huxley had a set of artificial action potentials, the behaviour of which formed a near-perfect mirror image of a real neuron’s spike. </p>
<p class="TXI">When injected with current, the Hodgkin-Huxley model cell displays a complex dance of changing voltage and resistances. First, the input fights against the cell’s natural state: it adds some positive charge to the largely negative inside of the cell. If this initial disturbance in the membrane’s voltage is large enough – that is, if the threshold is met – sodium channels start opening and a glut of positively charged sodium ions flood into the cell. This creates a positive feedback loop: the influx of sodium ions pushes the inside of the cell more positive and the resulting change in voltage lowers the sodium resistance even more. Soon, the difference in charge across the membrane disappears. The inside of the cell is briefly as positive as the outside, and then more so – the ‘overshoot’. As this is happening potassium channels are opening, letting positively charged potassium ions fall out of the cell. The sodium and potassium channels work like saloon doors, one letting ions in and the other out, but now the potassium ions are moving quicker. The work of the potassium ions reverses the trend in voltage. As this exodus of potassium again makes the inside of the cell more negative, sodium channels close. The <a id="page_42"></a>separation of charge across the membrane is being rebuilt. As the voltage nears its original value, positive charge continues to leak out of the still-open potassium channels – the ‘undershoot’. Eventually these too close, the voltage recovers and the cell has returned to normal, ready to fire again. The whole event takes less than one-half of one-hundredth of a second. </p>
<p class="TXI">According to Hodgkin, the pair built this mathematical model because ‘at first it might be thought that the response of a nerve to different electrical stimuli is too complicated and varied to be explained by these relatively simple conclusions’. But explain it they did. Like a juggler, the neuron combines simple parts in simple ways to create a splendidly intricate display. The Hodgkin-Huxley model makes clear that the action potential is a delicately controlled explosion occurring a billion times a second in your brain. </p>
<p class="TXI">The pair published their work – both experimental and computational – in a slew of <span class="italic">Journal of Physiology</span> papers in 1952. Eleven years later, they were awarded two-thirds of the Nobel Prize for ‘their discoveries concerning the ionic mechanisms involved in excitation and inhibition in the peripheral and central portions of the nerve cell membrane’. If doubts remained in the minds of any biologists about whether the nerve impulse was explainable in terms of ions and electricity, the work of Hodgkin and Huxley put those to rest. </p>
<p class="center">* * *</p>
<p class="TXT">‘The body <span class="italic">and dendrites</span> of a nerve cell are specialised for the reception and integration of information, <a id="page_43"></a>which is conveyed as impulses that are fired from other nerve cells along their axons’ (emphasis added). With this modest sentence John Eccles, an Australian neurophysiologist and the third awardee alongside Hodgkin and Huxley, began his Nobel lecture. The lecture goes on to describe the intricacies of ion flows that occur when one cell sends information to another. </p>
<p class="TXI">What the lecture doesn’t discuss is dendrites. Dendrites are the wispy tendrils that grow out of a neuron’s cell body. These offshoots, like tree roots, branch and stretch and branch again to cover a wide area around the cell. A cell casts its dendritic net out among nearby cells to collect input from them. </p>
<p class="TXI">Eccles had a complex relationship with dendrites. The type of neuron he studied – found in the spinal cord of cats – had elaborate dendritic branches. They spanned roughly 20 times the size of the cell’s body in all directions. Yet Eccles didn’t believe this cellular root system was terribly relevant. He conceded that the parts of the dendrites nearest the cell body may have some use: axons from other neurons land on these regions and their inputs will get transported immediately into the cell body where they can contribute to causing an action potential. But, he claimed, those farther out were simply too distant to do much: their signal wouldn’t survive the journey to the cell body. Instead, he assumed the cell used these arms to absorb and expel charged particles in order to keep its overall chemical balance intact. In Eccles’ eyes, therefore, dendrites were – at most – a wick that carried a flame a short way to the cell body and, at least, a straw slurping up some ions. </p>
<p class="TXI">Eccles’ position on dendrites put him at odds with his student, Wilfrid Rall. Rall earned a degree in physics <a id="page_44"></a>from Yale University in 1943 but, after his time working on the Manhattan Project, became interested in biology. He moved to New Zealand to work with Eccles on the effects of nerve stimulation in 1949. </p>
<p class="TXI">Given his background, Rall was quick to turn to mathematical analyses and simulations to understand a system as complex as a biological cell. And he was inspired and galvanised<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> by the work of Hodgkin and Huxley, which he heard about when Hodgkin visited the University of Chicago where Rall was getting his master’s degree. With this mathematical model in mind, Rall suspected that dendrites were capable of more than Eccles gave them credit for. After his time in New Zealand, Rall devoted a good portion of his career to proving the power of dendrites – and, in turn, proving the power of mathematical models to anticipate discoveries in biology. </p>
<p class="TXI">Building on the analogy of the cell as an electrical circuit, Rall modelled the thin cords of dendrites as just what they looked like: cables. The ‘cable theory’ approach to dendrites treats each section of a dendrite as a very narrow wire, the width of which – just as Ohm discovered – determines its resistance. Stringing these sections together, Rall explored how electrical activity at the far end of a dendrite can make its way to the cell body or vice versa. </p>
<p class="TXI">Adding more parts to this mathematical model, however, meant more numbers to crunch. The National Institutes of Health (NIH) in Bethesda, Maryland, where Rall worked, didn’t have a digital computer suitable for some of his larger simulations. When Rall wanted to run <a id="page_45"></a>the equations of a model with extensive dendrites, it was the job of Marjory Weiss, a programmer at the NIH, to drive a box of punch cards with the computer’s instructions to Washington DC to run them on a computer there. Rall couldn’t see the results of his model until she returned the next day. </p>
<p class="TXI">Through his elaborate mathematical explorations, Rall clearly showed – against the beliefs of Eccles – that a cell body with dendrites can have very different electrical properties than one without. A short description of Rall’s calculations, published in 1957, set off a years-long debate between the two men in the form of a volley of publications and presentations.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> Each pointed to experimental evidence and their own calculations to support their side. But slowly, over time, Eccles’ position shifted. By 1966, he had publicly accepted dendrites as a relevant cog in the neural machinery. Rall was right.</p>
<p class="TXI">Cable theory did more than just expose Eccles’ mistake. It also offered a way for Rall to explore in equations the many magical things that dendrites can do before the experimental techniques to do so were available. One important ability Rall identified was detecting order. Rall saw in his simulations that the order in which a dendrite gets input has important consequences for the cell’s response. If an input comes to a dendrite’s far end first, followed by more inputs closer and closer to the cell body, the cell may fire. If the <a id="page_46"></a>pattern is reversed, however, it won’t. This is because inputs coming in far from the cell body take longer to reach it. So, starting the inputs at the far end means they all reach the cell body at the same time. This creates a big change in the membrane’s voltage and possibly a spike. Going the other way, however, inputs come in at different times; this creates only a middling disturbance in voltage. In a race where runners start at different times and locations, the only way to get them to cross the finish line together is to give the farther ones a head start. </p>
<p class="TXI">Rall made this prediction in 1964. In 2010, it was shown to be true in real neurons. To test Rall’s hypothesis, researchers at University College London took a sample of neurons from rat brains. Placing these neurons in a dish, they were able to carefully control the release of neurotransmitters on to specific portions of a dendrite – portions as little as five microns (or the width of a red blood cell) apart. When this input went from the end of a dendrite to its root, the cell spiked 80 per cent of the time. In the other direction, it responded only half as often.</p>
<p class="TXI">This work shows how even the smallest bit of biology has a purpose. That the sections of a dendrite can work like the keys of a piano – where the same notes can be played in different ways to different effect – gives neurons new tricks. Specifically, it imbues neurons with the ability to identify sequences. There are many occasions where inputs sweeping across the dendrite from one direction should be treated differently from inputs sweeping the other way. For example, neurons in the retina have this kind of ‘direction selectivity’. This lets them signal which way objects in the visual field are moving.</p><a id="page_47"></a>
<p class="TXI">In many science classes, students are given small electrical circuit kits to play with. They can use wires of different resistances to connect up capacitors and batteries, maybe making a lightbulb light up or a fan spin. In much the same pick-and-play way, neuroscientists now build models of neurons. With the basic parts list of an electrical circuit, almost any observed property of a neuron’s activity can be mimicked. Rall helped add more parts to the kit.</p>
<p class="center">* * *</p>
<p class="TXT">If a standard neuron model is a small house built out of the bricks of electrical engineering, then the model constructed by the Blue Brain Project in 2015 is an entire metropolis. Eighty-two scientists across 12 institutions worked together as part of this unprecedented collaboration. Their goal was to replicate a portion of the rat brain about the size of a large grain of sand. They combed through previous studies and spent years performing their own experiments to collect every bit of data they could about the neurons in this region. They identified the ion channels they use, the length of their axons, the shapes of their dendrites, how closely they pack together and how frequently they connect. Through this, they identified 55 standard shapes that neurons can take, 11 different electrical response profiles they can have and a host of different ways they can interact.</p>
<p class="TXI">They used this data to build a simulation – a simulation that included more than 30,000 highly detailed model neurons forming 36 million connections. The full model required a specially built supercomputer to run through the billions of equations that defined it. Yet all of this <a id="page_48"></a>complexity still stemmed from the same basic principles of Lapicque, Hodgkin, Huxley and Rall. A lead researcher on the project, Idan Segev, summarised the approach: ‘Use Hodgkin-Huxley in an extended way and build a simulation of the way these cells are active, to get the music – the electrical activity – of this network of neurons that should imitate the real biological network that you’re trying to understand.’ </p>
<p class="TXI">As the team showed in their publication documenting the work, the model was able to reproduce several features of the real biological network. The simulation showed similar sequences of firing patterns over time, a diversity of responses across cell types and oscillations. More than just replicating results of past experiments, this real-to-life model also makes it possible to explore new experiments quickly and easily. Recreating the biology in a computer makes virtual investigations of this brain region as simple as writing a few lines of code – an approach known as ‘in silico’ neuroscience.</p>
<p class="TXI">Running such simulations can only give good predictions if the model underlying them is a reasonable facsimile of biology. Thanks to Lapicque, we know that using the equations of an electrical circuit as a stand-in for a neuron is a solid foundation on which to build models of the brain. It was his analogy that set off the study of the nerve as an electrical device. And the extension of his analogy by countless other scientists – many trained in both physics and physiology – expanded its explanatory power even further. The nervous system – against Müller’s intuitions – is brought to life by the flow of electricity and the study of it has been undeniably animated by the study of electricity.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-1" id="fn-1">1</a> ﻿In the course of proving that contact between dissimilar metals generates electricity, Volta ended up inventing the battery. ﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-2" id="fn-2">2</a> ﻿Named, of course, after our man Galvani.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-3" id="fn-3">3</a> ﻿Applying voltage was an easier way to control the flow of charge than injecting current directly.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-4" id="fn-4">4</a> ﻿More on Adrian, and what his discovery meant about how neurons represent information, in ﻿﻿Chapter 7﻿﻿.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-5" id="fn-5">5</a> ﻿Spike, firing, activity, action potential ﻿–﻿ the sacred emission of a neuron goes by many names.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-6" id="fn-6">6</a> ﻿The ﻿‘﻿squid giant axon﻿’﻿ that Hodgkin and Huxley were studying is a particularly large (about the width of a marker tip) axon of a rather average-sized squid. It is not, as many students of neuroscience initially believe, the axon of a giant squid. ﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-7" id="fn-7">7</a> ﻿Also named after our man Galvani.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-8" id="fn-8">8</a> ﻿According to Rall, Eccles even prevented his work from being published. With respect to a 1958 manuscript: ﻿‘﻿A negative referee persuaded the editors to reject this manuscript. The fact that this referee was Eccles, was clear from many marginal notes on the returned manuscript.﻿’﻿﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 5</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter5"><a href="contents.xhtml#re_chapter5">CHAPTER FIVE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter5">Excitation and Inhibition</a><a id="page_117"></a></p>
<p class="H1" id="b-9781472966445-ch495-sec5">
<span class="bold">
<span>The balanced network and oscillations</span>
</span></p>
<p class="TXT">Within nearly every neuron, a battle is raging. This fight – a struggle over the ultimate output of the neuron – pits the two fundamental forces of the brain against each other. It’s a battle of excitation versus inhibition. Excitatory inputs encourage a neuron to fire. Inhibitory inputs do the opposite: they push the neuron farther from its threshold for spiking. </p>
<p class="TXI">The balance between these two powers defines the brain’s activity. It determines which neurons fire and when. It shapes their rhythms – rhythms that are implicated in everything from attention to sleep to memory. Perhaps more surprisingly, the balance between excitation and inhibition can also explain a feature of the brain that has haunted scientists for decades: the notorious unreliability of neurons. </p>
<p class="TXI">Eavesdrop on a neuron that should be doing the same thing over and over – for example, one in the motor system that is producing the same movement repeatedly – and you’ll find its activity surprisingly irregular. Rather than repeating the same pattern of spikes verbatim each time, it will fire more on some attempts and less on others. </p>
<p class="TXI">Scientists learned about this peculiar habit of neurons early in the days of neural recordings. In 1932, physiologist Joseph Erlanger made an update to the equipment in his <a id="page_118"></a>St Louis laboratory that let him record neural activity with 20 times the sensitivity previously available. He, along with his colleague E. A. Blair, was finally able to isolate individual neurons in the leg of a frog and record how they responded to precise pulses of electricity – 58 identical pulses per minute to be exact.</p>
<p class="TXI">To their surprise, Erlanger and Blair found that those identical pulses did not produce identical responses: a neuron may respond to one pulse of current, but not the next. There was still a relationship between the strength of the pulse and the response: when weak currents were used, for example, the neuron would respond, say, 10 per cent of the time, medium currents half the time and so on. But beyond these probabilities, how a neuron responded to any given pulse seemed a matter of pure chance. As the pair wrote in their 1933 paper in the <span class="italic">American Journal of Physiology</span>: ‘We were struck by the kaleidoscopic appearance of [responses] obtained from large nerves under absolutely constant conditions.’ </p>
<p class="TXI">This work was one of the first studies to systematically investigate the mysterious irregularity of the nervous system, but it would be far from the last. In 1964, for example, a pair of American scientists applied the same brushing motion to a monkey’s skin over and over. They reported that the activity of neurons responding to this motion appeared as ‘a series of irregularly recurring impulses, so that, in general, no ordered pattern can be detected by visual inspections’. </p>
<p class="TXI">In 1983, a group of researchers from Cambridge and New York noted that: ‘The variability of cortical neuron response[s] is known to be considerable.’ Their study of the visual system in cats and monkeys showed again that <a id="page_119"></a>the neural response to repeats of the same image produced different results. The responses still had <span class="italic">some</span> relation to the stimulus – the cells still changed how much they fired <span class="italic">on average</span> to different images. But exactly which neuron would fire and when for any given instance seemed as unpredictable as next week’s weather. ‘Successive presentations of identical stimuli do not yield identical responses,’ the authors concluded. </p>
<p class="TXI">In 1998, two prominent neuroscientists even went so far as to liken the workings of the brain to the randomness of radioactive decay, writing that neurons have ‘more in common with the ticking of a Geiger counter than of a clock’. </p>
<p class="TXI">Decades of research and thousands of papers have resulted in a tidy message about just how messy the nervous system is. Signals coming into the brain appear to impinge on neurons already flickering on and off at their own whims. Inputs to these neurons can influence their activity, but not control it exactly – there will always be some element of surprise. This presumably useless chatter that distracts from the main message the neuron is trying to send is referred to by neuroscientists as ‘noise’. </p>
<p class="TXI">As Einstein famously said with regard to the new science of quantum mechanics: ‘God does not play dice.’ So why should the brain? Could there be any good reason for evolution to produce noisy neurons? Some philosophers have claimed that the noise in the brain could be a source of our free will – a way to overcome a view of the mind as subject to the same deterministic laws of any machine. Yet others disagree. As British philosopher Galen Strawson wrote: ‘It may be that some <a id="page_120"></a>changes in the way one is are traceable … to the influence of indeterministic or random factors. But it is absurd to suppose that indeterministic or random factors, for which one is [by definition] in no way responsible, can in themselves contribute in any way to one’s being truly morally responsible for how one is.’ In other words, following decisions based on a coin flip isn’t exactly ‘free’ either. </p>
<p class="TXI">Other purposes for this unpredictability have been posited by scientists. Randomness, for example, can help to learn new things. If someone walks the same path to work every day, occasionally taking a random left turn could expose them to an unknown park, a new coffee shop or even a faster path. Neurons might benefit from a little exploration as well and noise lets them do that. </p>
<p class="TXI">In addition to the question of <span class="italic">why</span> neurons are noisy, the question of <span class="italic">how</span> they end up this way has preoccupied neuroscientists. Possible sources of noise exist outside the brain. Photoreceptors in the eye, for example, need to be hit by a certain number of photons before they respond. But even a constant source of light can’t guarantee that a constant stream of photons will reach the eye. In this way, the input to the nervous system itself could be unreliable. </p>
<p class="TXI">In addition, several elements of a neuron’s function depend on random processes. The electrical state of a neuron, for example, will change if the diffusion of ions in the fluid around it does. And neurons, like any other cells, are made up of molecular machines that don’t always function according to plan: necessary proteins might not be produced fast enough, moving parts may get stuck, <span class="italic">etc.</span> While these physical failures could <a id="page_121"></a>contribute to the brain’s noisiness they don’t seem to fully account for it. In fact, when neurons are taken from the cortex and put into a Petri dish they behave remarkably more reliably: stimulating these neurons the same way twice will actually produce similar results. Therefore, basic lapses in cellular machinery – which can happen in the dish just as well as in the brain – seem insufficient to explain the noise that is normally observed. </p>
<p class="TXI">The books are therefore not balanced: the noise put in somehow doesn’t equal the noise produced. We could suspect this is just a curious error in accounting; perhaps there are a few extra unreliable cogs in the neural machinery, or inputs from the world are even less stable than we believe. Such mis-estimates could maybe make up the difference, if it weren’t for one small fact: the very nature of how neurons work makes them noise <span class="italic">reducers</span>. </p>
<p class="TXI">To understand this, imagine you and some friends are playing a game where the goal is simply to see how far you can collectively move a football down a long field before a timer runs out. None of you are very well practised and occasionally you make mistakes – one person misses a pass, another gets tired, someone else trips. You also occasionally exceed your own expectations by running extra fast or passing extra far. If the time allotted is small – say 30 seconds – such momentary lapses or advantages will have big effects on your distance. You could go 150m on one try and 20 the next. But if the time is large, say five minutes, these fluctuations in performance may simply balance each other out: a slow start could be made up for with an intense sprint at the end, or the gain from a long pass could be lost because of a fall. As a result, the longer the <a id="page_122"></a>time, the more similar the distance will be on each try. In other words, the ‘noisiness’ of your athletic ability gets averaged out over time. </p>
<p class="TXI">Neurons find themselves in a similar situation. If a neuron gets enough input within a certain amount of time, it will fire a spike (see Figure 12). The input it gets is noisy because it comes from the firing of other neurons. So, the neuron may receive, say, five inputs at one moment, 13 the next and zero after that. Just like in the game example, if the neuron takes in this noisy input for a long time before deciding if it has enough to spike, the impact of the noise will be reduced. If it only uses a quick snapshot of its input, however, the noise will dominate.</p>
<p class="TXI">So how much time does a neuron combine its inputs over? About 20 milliseconds. That may not seem like long, but for a neuron it’s plenty. A spike only takes about 1 millisecond and a cell can be receiving many at a time from all of its different inputs. Therefore, neurons should be able to take the average over many snapshots of input before deciding to spike. </p>
<p class="TXI">Neuroscientists William Softky and Christof Koch used a simple mathematical model of a neuron – the ‘leaky integrate-and-fire’ model introduced in <a href="chapter2.xhtml#chapter2">Chapter 2</a> – to test just this. In their 1993 study, they simulated a neuron receiving inputs at irregular times. Yet the neuron itself – because it integrated these incoming spikes over time – still produced output spikes that were much more regular than the input it received. This means neurons do have the power to destroy noise – to take in noisy inputs while producing less noisy outputs.</p>
<p class="image-fig" id="fig12.jpg">
<img alt="" src="Images/chapter-05-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 12</span>
</span></p>
<p class="TXI">If neurons weren’t able to quench noise, then the unreliability of the brain wouldn’t be as much of a <a id="page_123"></a>mystery. As mentioned before, we could assume that small amounts of randomness enter the brain – either from the outside world or from inside a cell – and spread through the system via the connections between neurons. If noisy inputs led to outputs that were just as, or possibly more, noisy, this would be a perfectly self-consistent story; noisy neurons would beget noisy neurons. But, according to Softky and Koch’s model, this is not what happens. When passed through a neuron, noise should get weaker. When passed over and over through a whole network of neurons, it should be expected to all but disappear. Yet everywhere neuroscientists look, there it is.</p>
<p class="TXI">Not only is the brain unpredictable, then, but it seems to be encouraging that unpredictability – going against the natural tendency of neurons to squash it. What’s keeping the randomness alive? Does the brain have a random-number generator? Some sort of hidden biological dice? Or, as scientists in the 1990s <a id="page_124"></a>hypothesised, does all this disorder actually result from a more fundamental order, from the balance between excitation and inhibition?</p>
<p class="center">* * *</p>
<p class="TXT">It took Ernst Florey several trips to a Los Angeles horse butcher to uncover the source of inhibition in the brain.</p>
<p class="TXI">It was the mid-1950s when Florey, a German-born neurobiologist who emigrated to North America, was working on this question with his wife Elisabeth. At that time, the fact that neurons communicate by sending chemicals – called neurotransmitters – between each other was largely established. However, the only known neurotransmitters were excitatory – that is, they were chemicals that made a neuron <span class="italic">more</span> likely to fire. Yet since the mid-nineteenth century, it was known that some neurons can actually reduce the electrical activity of their targets. For example, the Weber brothers, Ernst and Eduard, showed in 1845 that electrically stimulating a nerve in the spinal cord could slow down the cells that controlled the beating of the heart, even bringing it to a standstill. This meant the chemical released by these neurons was inhibitory – it made cells less likely to fire. </p>
<p class="TXI">Florey needed specimens for his study on ‘factor I’, his name for the substance responsible for inhibition. So, he regularly rode his 1934 Chevrolet to a horse butcher and picked up some parts less favoured by their average customers: fresh brains and spinal cords. After extracting different substances from this nervous tissue, he checked what happened when each one was applied to living neurons taken from a crayfish. Eventually he identified <a id="page_125"></a>some candidate chemicals that reliably quieted the crayfish neurons. This cross-species pairing was actually a bit of luck on Florey’s part. Neurotransmitters can’t always be assumed to function the same way across animals. But in this case, what was inhibitory for the horse was inhibitory for the crayfish. </p>
<p class="TXI">With the help of professional chemists, Florey then used tissue from yet another animal – 45kg (100lb) of cow brain to be precise – to purify ‘factor I’ down to its most basic chemical structure. In the end he was left with 18mg of gamma-aminobutyric acid. Gamma-aminobutyric acid (or GABA, as it is more commonly known) was the first identified inhibitory neurotransmitter. </p>
<p class="TXI">Whether a neurotransmitter is inhibitory or excitatory is really in the eye of the beholder – or more technically in the receptor of the target neuron. When a neurotransmitter is released from one neuron, the chemical travels the short distance across the synapse between that neuron and its target. It then attaches itself to receptors that line the membrane of the target neuron. These receptors are like little protein padlocks. They require the right key – that is, the correct neurotransmitter – to open. And once they open they’re pretty selective about who they let in. One kind of receptor that GABA attaches to, for example, only lets chloride ions into the cell. Chloride ions have a negative charge and letting more in makes it harder for the neuron to reach the electrical threshold it needs to fire. The receptors to which excitatory neurotransmitters attach let in positively charged ions, like sodium, which bring the neuron <span class="italic">closer</span> to threshold.</p> <a id="page_126"></a>
<p class="TXI">Neurons tend to release the same neurotransmitter on to all of their targets, a principle known as Dale’s Law (named after Henry Hallett Dale who boldly guessed as much in 1934, a time at which only two neurotransmitters had even been identified). Neurons that release GABA are called ‘GABAergic’, although because GABA is the most prevalent inhibitory neurotransmitter in the adult mammalian brain, they’re frequently just called ‘inhibitory’. Excitatory transmitters are a bit more diverse, but the neurons that release them are still broadly classed as ‘excitatory’. Within an area of the cortex, excitatory and inhibitory neurons freely intermix, sending connections to, and receiving them from, each other.</p>
<p class="TXI">In 1991, after many of these facts of inhibition had been established, Florey wrote a retrospective on his role in the discovery of the first – and arguably most important – inhibitory neurotransmitter. He ended it with the sentence: ‘Whatever the brain does for the mind, we can be sure that GABA plays a major role in it.’ Likely unbeknownst to Florey, at the same time a theory that makes inhibition a key player in the production of the brain’s unpredictability was developing. </p>
<p class="center">* * *</p>
<p class="TXT">Returning to the analogy of the timed football game, imagine now that another team is added. Their goal is to fight against you, moving the ball to the opposite end of the field. When the clock stops, whoever is closer to their goal side wins. If the other team is also made up of your semi-athletic friends, then on average both teams <a id="page_127"></a>would perform the same. The noisiness of your performance will still affect the outcome: your team may beat the other by a few metres on one try and get beaten by the same small amount on another. But on the whole, it will be a balanced and tame game. </p>
<p class="TXI">Now consider if the other team was made up of professional athletes – some of the strongest, fastest players around. In this case, you and your friends wouldn’t stand a chance; you’d be clobbered every time. This is why no one would bother watching a competition between career football players and the high school team, Tiger Woods vs your dad, or Godzilla against a literal moth. The outcome of all those matches is just too predictable to be interesting. In other words, unfair fights create consistency; fair fights are more fun. </p>
<p class="TXI">In the cortex, neurons get thousands of connections from both excitatory and inhibitory cells. Because of this, each individual force is strong and would consistently dominate if the other were any weaker. Without the presence of inhibition, for example, the hundreds of excitatory inputs bombarding a cell at any moment would make it fire almost constantly; on the other hand, inhibition alone would drive the cell down to a completely stagnant state. With huge power on each side, the true activity of the neuron is thus the result of a tug of war between giants. What’s happening in the neuron is indeed a balanced fight and it’s the kind you’d see at the Olympics rather than in a schoolyard.</p>
<p class="TXI">Tell this fact to a computer scientist and they may start to get worried. That’s because computer scientists know that taking the difference between too big, noisy numbers can cause big problems. In computers, numbers <a id="page_128"></a>can only be represented with a certain level of precision. This means that some numbers need to be rounded off, introducing error – or noise – into the computation. For example, a computer with only three digits of precision may represent the number 18,231 as 1.82x10<sup>3</sup>; the leftover 31 is lost in the rounding error. When subtracting two roughly equal numbers, the effects of this rounding error can dominate the answer. For example, 18,231 minus 18,115 is 116, yet the computer would calculate this difference as 1.82x10<sup>3</sup> minus 1.81x10<sup>3</sup> which is only 10! That puts the computer off by 106. And the larger the numbers, the larger the error will be. For example, a computer with three-digit precision calculating 182,310 minus 181,150 would produce an answer that is 1,060 less than the true one. </p>
<p class="TXI">You reasonably wouldn’t feel comfortable if your bank or doctor’s office were doing their calculations this way. For this reason, programmers are taught to write their code in a way that avoids subtracting two very large numbers. Yet neurons are subtracting two large numbers – excitation minus inhibition – at every moment. Could such a ‘bug’ really be part of the brain’s operating system? </p>
<p class="TXI">Scientists had been toying with this idea for a while when, in 1994, Stanford neuroscientists Michael Shadlen and William Newsome decided to test it out. Similar to the work of Softky and Koch, Shadlen and Newsome built a mathematical model of a single neuron and fed it inputs. This time, however, the neuron got both noisy excitatory <span class="italic">and</span> noisy inhibitory inputs. When pitting these two forces against each other, sometimes excitation will win and sometimes inhibition will. Would the fight play out like a noisy calculation and create a neuron that <a id="page_129"></a>fired erratically? Or would the neuron still be able to crush the noise in these inputs the way it had the excitatory inputs in Softky and Koch’s work? Shadlen and Newsome found that, indeed, given both these types of inputs – each coming in at the same, high rate – the neuron’s output was noisy. </p>
<p class="TXI">In a boxing match between amateurs, a momentary lapse in attention from one may let the other land a small hit. In a fight between pros, however, that same lapse could lead to a knockout. In general, the stronger that two competing powers are, the bigger the swings in the outcome of their competition. This is how the internal struggle between excitation and inhibition in a neuron can overpower its normal noise-crushing abilities. Because both sources are evenly matched, the neuron’s net input (that is the total excitation minus the total inhibition) is not very big on average. But because both sources are strong, the swings around that average are huge. At one moment the neuron can get pushed far above its threshold for firing and emit a spike. At the next it could be forced into silence by a wave of inhibition. These influences can make a neuron fire when it otherwise wouldn’t have or stay silent when it otherwise would have. In this way, a balance between excitation and inhibition creates havoc in a neuron and helps explain the variability of the brain.</p>
<p class="TXI">The simulation run by Shadlen and Newsome went a long way in helping to understand how neurons could remain noisy. But it didn’t quite go far enough. Real neurons get inputs from other real neurons. For the theory that noise results from a balance between excitation and inhibition to be correct, it thus has to <a id="page_130"></a>work for a whole network of excitatory and inhibitory neurons. That means one in which each neuron’s input comes from the other neurons and its outputs go back to them too. Shadlen and Newsome’s simulation, however, was just of a single neuron, one that received inputs controlled by the model-makers. You can’t just look at the income and expenses of a single household and decide that the national economy is strong. Similarly, simulating a single neuron can’t guarantee that a network of neurons will work as needed. As we saw in the last chapter, in a system with a lot of moving parts, all of them need to move just right to get the desired outcome. </p>
<p class="TXI">To get a whole network to produce reliably noisy neurons requires coordination: each neuron needs to get excitatory and inhibitory input from its neighbours in roughly equal proportions. And the network needs to be <span class="italic">self-consistent</span> – that is, each neuron needs to produce the same amount of noise it receives, no more nor less. Could a network of interacting excitatory and inhibitory cells actually sustain the kind of noisy firing seen in the brain, or would the noise eventually peter out or explode?</p>
<p class="center">* * *</p>
<p class="TXT">When it comes to questions of self-consistency in networks, physicists know what to do. As we saw in the last chapter, physics is full of situations where self-consistency is important: gases made of large numbers of simple particles, for example, where each particle is influenced by all those around it and influences them all <a id="page_131"></a>in return. So, techniques have been developed to make the maths of these interactions easier to work with.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">In the 1980s, Israeli physicist Haim Sompolinsky was using these techniques to understand the ways materials behave at different temperatures. But his interests eventually turned towards neurons. In 1996, Sompolinsky and fellow physicist-turned-neuroscientist Carl van Vreeswijk, applied the physicist’s approach to the question of balance in the brain. Mimicking the mathematics used to understand interacting particles, they wrote down some simple equations that represented a very large population of interacting excitatory and inhibitory cells. The population also received external inputs meant to represent connections coming from other brain areas.</p>
<p class="TXI">With their simple equations, it was possible for van Vreeswijk and Sompolinsky to mathematically define the kind of behaviour they wanted to see from the model. For example, the cells had to be able to keep themselves active, but not too active (they shouldn’t be firing non-stop, for example). In addition, they should respond to increases in external input by increasing their average firing rate. And, of course, the responses had to be noisy.</p>
<p class="TXI">Putting in these demands, van Vreeswijk and Sompolinsky then churned through the equations. They found that, to create a full network that will keep firing <a id="page_132"></a>away irregularly at a reasonable rate, certain conditions had to be met. For example, the inhibitory cells need to have a stronger influence on the excitatory cells than the excitatory cells have on each other. Ensuring that the excitatory cells receive slightly more inhibition than excitation keeps the network activity in check. It’s also important that the connections between neurons be random and rare – each cell should only get inputs from, say, five or ten per cent of the other cells. This ensures no two neurons get locked into the same pattern of behaviour.</p>
<p class="TXI">None of the requirements van Vreeswijk and Sompolinsky found were unreasonable for a brain to meet. And when the pair ran a simulation of a network that met all of them, the necessary balance between excitation and inhibition emerged, and the simulated neurons looked as noisy as any real ones. Shadlen and Newsome’s intuition about how a single neuron can sustain noisy firing did in fact hold in a network of interacting neurons.</p>
<p class="TXI">Beyond just showing that balancing excitation and inhibition was possible in a network, van Vreeswijk and Sompolinsky also found a possible benefit of it: neurons in a tightly balanced network respond quickly to inputs. When a network is balanced, it’s like a driver with each foot pressed equally on the gas and the brake. This balance gets disrupted, however, if the amount of external input changes. Because the external inputs are excitatory – and they target the excitatory cells in the network more than the inhibitory ones – an increase in their firing is like a weight added to the gas pedal. The car then zooms off almost as quickly as the input came in. After this initial <span class="italic">whoosh</span> of a response, however, the <a id="page_133"></a>network regains its balance. The explosion of excitation in the network causes the inhibitory neurons to fire more and – like adding an additional weight to the brake – the network resettles in a new equilibrium, ready to respond again. This ability to act so quickly in response to a changing input could help the brain accurately keep up with a changing world.</p>
<p class="TXI">Knowing that the maths works out is reassuring, but the real test of a theory comes from real neurons. Van Vreeswijk and Sompolinsky’s work makes plenty of predictions for neuroscientists to test, and that’s just what Michael Wehr and Anthony Zador at Cold Spring Harbor Laboratory did in 2003. The pair recorded from neurons in the auditory cortex of rats, which is responsible for processing sound, while different sounds were played to the animal. Normally when neuroscientists drop an electrode into the brain they’re trying to pick up on the output of neurons – that is, their spikes. But these researchers used a different technique to eavesdrop on the input a neuron was getting – specifically to see if the excitatory and inhibitory inputs balanced each other out.</p>
<p class="TXI">What they saw was that, right after the sound turned on, a strong surge of excitation came into the cell. It was followed almost immediately by an equal influx of inhibition – the brake that follows the gas. Therefore, increasing the input to this real network showed just the behaviour expected from the model. Even when using louder sounds that produced more excitation, the amount of inhibition that followed always matched it. Balance seemed to be emerging in the brain just as it did in the model.</p>
<p class="TXI">To explore another prediction of the model, scientists had to get a bit creative. Van Vreeswijk and Sompolinsky <a id="page_134"></a>showed that, to make a well-balanced network, the strength of connections between neurons should depend on the total number of connections: with more connections, each connection can be weaker. Jérémie Barral and Alex Reyes from New York University wanted a way to change the number of connections in a network in order to test this hypothesis.</p>
<p class="TXI">Within a brain it’s hard to control just how neurons grow. So, in 2016, they decided to grow them in a Petri dish instead. It’s an experimental set-up that – in its simplicity, controllability and flexibility – is almost like a live version of a computer simulation. In order to control the number of connections, they simply put different numbers of neurons in the dish; dishes with more neurons packed in made more connections. They then monitored the activity of the neurons and checked their connection strengths. All the populations (which contained both excitatory and inhibitory cells) fired noisily, just as a balanced network should. But the connection strengths varied drastically. In a dish where each neuron only got about 50 connections, the connections were three times stronger than those with 500 connections. In fact, looking across all the populations, the average strength of a connection was roughly equal to one divided by the square root of the number of connections – exactly what van Vreeswijk and Sompolinsky’s theory predicted.</p>
<p class="TXI">As more and more evidence was sought, more was found for the belief that the brain was in a balanced state. But not all experiments went as the theory predicted; tight balance between excitation and inhibition wasn’t always seen. There is good reason to believe that certain <a id="page_135"></a>brain areas engaged in certain tasks may be more likely to exhibit balanced behaviour. The auditory cortex, for example, needs to respond to quick changes in sound frequency to process incoming information. This makes the quick responsiveness of well-balanced neurons a good match. Other areas that don’t require such speed may find a different solution.</p>
<p class="TXI">The beauty of balance is that it takes a ubiquitous inhabitant of the brain – inhibition – and puts it to work solving an equally ubiquitous mystery – noise. And it does it all without any reliance on magic: that is, no hidden source of randomness. The noise comes even while neurons are responding just as they should.</p>
<p class="TXI">This counter-intuitive fact that good behaviour can produce bedlam is important. And it had been observed somewhere else before. Van Vreeswijk and Sompolinsky make reference to this history in the first word of the title of their paper: ‘<span class="italic">Chaos</span> in neuronal networks with balanced excitatory and inhibitory activity’.</p>
<p class="center">* * *</p>
<p class="TXT">Chaos didn’t exist in the 1930s: when neuroscientists were first realising how noisy neurons are, the mathematical theory to understand their behaviour hadn’t yet been discovered. When it was, it happened seemingly by chance. </p>
<p class="TXI">The Department of Meteorology at MIT was founded in 1941, just in time for Edward Lorenz. Lorenz, born in 1917 in a nice Connecticut neighbourhood to an engineer and a teacher, had an early interest in numbers, maps and the planets. He intended to continue on in <a id="page_136"></a>mathematics after earning an undergraduate degree in it, but, as was the case for so many scientists of his time, the war intervened. In 1942 Lorenz was given the task of weather prediction for the US Army Air Corps. To learn how to do this, he took a crash course in meteorology at MIT. When he was done with the army he stayed with meteorology and remained at MIT: first as a PhD student, then a research scientist and finally a professor.</p>
<p class="TXI">If you’ve ever tried to plan a picnic, you know weather prediction is far from perfect. Academic meteorologists, who focus on the large-scale physics of the planet, hardly even consider day-to-day forecasting a goal. But Lorenz remained curious about it and about how a new technology – the computer – could help. </p>
<p class="TXI">The equations that describe the weather are many and complex. Churning through them by hand – to see how the weather right now will lead to the weather later – is an enormous, nearly impossible task (by the time you finish it the weather you’re predicting likely will have passed). But a computer could probably do it much faster. </p>
<p class="TXI">Starting in 1958, Lorenz put this to the test. He boiled the dynamics of weather down to 12 equations, picked some values to start them off with – say, westerly winds at 100km/hr – and let the mathematics run. He printed the output of the model on rolls of paper as it went along. It looked weather-like enough, with familiar ebbs and flows of currents and temperatures. One day he wanted to re-run a particular simulation to see how it would evolve over a longer period of time. Rather than starting it from the very beginning again, he figured he could start it part way along by <a id="page_137"></a>putting in the values from the printout as the starting conditions. Impatience, sometimes, is the mother of discovery. </p>
<p class="TXI">The numbers that the computer printed out, however, weren’t the full thing. To fit more on the page, the printer cut the number of digits after the decimal point from six down to three. So, the numbers Lorenz put in for the second run of the simulation weren’t <span class="italic">exactly</span> where the model was before. But what could a few decimal points matter in a model of the whole world’s weather? Turns out quite a bit. After a few cranks of the mathematical machinery – about two months of weather changes in model time – this second run of the simulation was completely different from the first. What was hot was cold, what was fast was slow. What was supposed to be a replication turned into a revelation. </p>
<p class="TXI">Up until that point, scientists assumed that small changes only beget small changes. A little gust of wind at one point in time should have no power to move mountains later. Under that dogma, what Lorenz observed must’ve come from a mistake, a technical error made by the large, clunky computers of the day, perhaps. </p>
<p class="TXI">Lorenz was willing to see what was truly happening, however. As he wrote in 1991: ‘The scientist must always be on the lookout for other explanations than those that have been commonly disseminated.’ What Lorenz had observed was the true behaviour of the mathematics, as counter-intuitive as it seemed. In certain situations, small fluctuations <span class="italic">can</span> get amplified, making behaviour unpredictable. It’s not a mistake or an error; it’s just how complex systems work. Chaos – the name given to this <a id="page_138"></a>phenomenon by mathematicians<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> – was real and it would do scientists well to try to understand it.</p>
<p class="TXI">Chaotic processes produce outputs that <span class="italic">look</span> random but in fact arise from perfect rule-following. The source of this deception is the upsetting truth that our ability to predict outcomes based on knowing the rules is far more limited than previously thought – especially if those rules are complex. In his book <span class="italic">Chaos: Making a New Science</span>, a sweeping history of how the field emerged, James Gleick wrote: ‘Traditionally, a dynamicist would believe that to write down a system’s equations is to understand the system … But because of the little bits of nonlinearity in these equations, a dynamicist would find himself helpless to answer the easiest practical questions about the future of the system.’ This imbues even the simplest systems of, say, interacting billiard balls or swinging pendulums with the potential to produce something surprising. He continued: ‘Those studying chaotic dynamics discovered that the disorderly behaviour of simple systems acted as a <span class="italic">creative</span> process. It generated complexity: richly organised patterns, sometimes stable and sometimes unstable, sometimes finite and sometimes infinite.’ </p>
<p class="TXI">Chaos was happening in the atmosphere – and if van Vreeswijk and Sompolinsky were right, it was happening in the brain, too. For this reason, explaining why the brain reacts to repeated inputs with a <a id="page_139"></a>kaleidoscopic variety needn’t involve spotty cellular machinery. That’s not to say that there aren’t any sources of noise in the brain (such as unreliable ion channels or broken-down receptors), but just that an object as complex as the brain, with its interacting pools of excitation and inhibition, doesn’t require them to show rich and irregular responses. In fact, in their simulation of a network, all van Vreeswijk and Sompolinsky had to do was change the starting state of a single neuron – from firing to not, or vice versa – to create a completely different pattern of activity across the population.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> If a change so small can create such a disturbance, the brain’s ability to keep noise alive seems less mysterious.</p>
<p class="center">* * *</p>
<p class="TXT">In medical centres around the world, epilepsy patients spend several days – up to a week – stuck in small rooms. These ‘monitoring’ rooms are usually equipped with a TV – for the patients – and cameras that monitor patient movement – for the doctors. All day and night, the patients are connected to an electroencephalogram (EEG) machine that’s capturing their brain’s behaviour. They hope that the information gathered will help to treat their seizures.</p> <a id="page_140"></a>
<p class="TXI">EEG electrodes, attached via stickers and tape to 
the scalp, monitor the electrical activity produced 
by the brain below. Each electrode provides one measurement – a complicated combination of the activity of many, many neurons at once. It’s a signal that varies over time like a seismograph. When patients are awake, the signal is a jagged and squiggly line: it moves slightly up and slightly down at random, but without any strong rhythm. When patients are asleep (particularly in deep dreamless sleep), the EEG makes waves: large movements upwards then downwards extending over a second or more. When the event of interest – a seizure – occurs, the movements are even starker. The signal traces out big, fast sweeps up and down, three to four times a second, like a kid scribbling frantically with a crayon. </p>
<p class="TXI">What are neurons doing to create these strong signals during a seizure? They’re working together. Like a well-trained military formation, they march in lockstep: firing in unison then remaining silent before firing again. The result is a repeated, synchronous burst of activity that drives the EEG signal up and down over and over again. In this way, a seizure is the opposite of randomness – it is perfect order and predictability. </p>
<p class="TXI">The same neurons that produce that seizure also produce the slow waves of sleep and the normal, noisy activity needed for everyday cognition. How can the same circuit exhibit these different behaviours? And how does it switch between them?</p>
<p class="TXI">In the late 1990s, French computational neuroscientist Nicolas Brunel set out to understand the different ways <a id="page_141"></a>circuits can conduct themselves.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> Specifically, building off the work of van Vreeswijk and Sompolinsky, he wanted to investigate how models made of excitatory and inhibitory neurons behave. To do this, Brunel explored the <span class="italic">parameter space</span> of these models. </p>
<p class="TXI">Parameters are the knobs that can be turned on a model. They are values that define specific features, like the number of neurons in the network or how many inputs each gets. Like regular space, parameter space can be explored in many different directions, but here each direction corresponds to a different parameter. The two parameters Brunel chose to explore were, firstly, how much external input the network gets (<span class="italic">i.e.</span>, input from other brain areas) and, secondly, how strong the inhibitory connections in the network are compared with the excitatory ones. By changing each of these parameters a bit and churning through the equations, Brunel could check how the behaviour of the network depends on these values. </p>
<p class="TXI">Doing this for a bunch of different parameter values results in a map of the model’s behaviour. The latitude and longitude on this map (see Figure 13) correspond to the two parameters Brunel varied respectively. For the network at the middle of the map, the inhibition is exactly equal to the excitation and the input to the network is of medium strength. Moving to the left on the map, excitation becomes stronger than inhibition; <a id="page_142"></a>move to the right and vice versa. Move upwards and the input to the network gets stronger, down and it’s weaker. Laid out this way, the network van Vreeswijk and Sompolinsky studied – with the inhibitory connections slightly stronger than excitatory ones – is just off to the right of the middle.</p>
<p class="image-fig" id="fig13.jpg">
<img alt="" src="Images/chapter-05-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 13</span>
</span></p>
<p class="TXI">Brunel surveyed this model landscape looking for any changes in the terrain: do certain sets of parameters make the network behave drastically differently? To find the first striking landmark you don’t have to travel far from van Vreeswijk and Sompolinsky’s original network. Crossing over from the region where inhibition is stronger into the one where excitation is, a sharp transition happens. In mathematics, these <a id="page_143"></a>transitions are known as bifurcations. Like a steep cliff separating a grassy plain from a sea, bifurcations mark a quick change between two separate areas in parameter space. In Brunel’s map, the line where excitation and inhibition are equal separates the networks with irregular, noisy firing on the right from those with rigid, predictable firing on the left. Specifically, when inhibition gets too weak, the neurons in these networks stop their unique pitter-patter and start firing in unison. Their tight synchronous activity – with groups of neurons flicking on and off together – looks a lot like a seizure. </p>
<p class="TXI">Physiologists have known for centuries that certain substances act as convulsants – that is, they induce seizures. With the increased understanding of neurotransmitters that came in the mid-twentieth century, it became clear that many of these drugs interfered with inhibition. Bicuculline, for example, is found in plants across North America and stops GABA from attaching to its receptor. Thujone, present in low doses in absinthe, prevents GABA receptors from letting chloride ions in. Whatever the mechanism, in the end, these drugs are throwing the balance off in the brain, putting inhibitory influences at a disadvantage. Using his bird’s-eye view of the brain’s behaviour, Brunel could see how changing the brain’s parameters – through drugs or otherwise – moved it into different states. </p>
<p class="TXI">Travelling to the other end of Brunel’s map reveals yet another pattern of activity. In this realm, inhibition rules over excitation. If the external input remains at medium strength, the neurons remain noisy here. Move up or down, however, and two similar, but <a id="page_144"></a>different, behaviours appear. With both high and low external input, the neurons show some cohesion. If you added up how many neurons were active at any given time, you’d see waves of activity: brief periods of more-than-average firing followed by less. But unlike the military precision of the seizure state, networks here are more like a percussion section made up of six-year-olds: there may be some organisation, but not everybody is playing together all the time. In fact, an individual neuron in these networks only participates in every third or fourth wave – and even then their timing isn’t always perfect. In this way, these states are both oscillating and noisy. </p>
<p class="TXI">The feature that differentiates behaviour in the top right corner of the map from that in the bottom right is the <span class="italic">frequency</span> of that oscillation. Drive the network with strong external inputs and its average activity will move up and down quickly – as fast as 180 times per second. The strong input drives the excitatory cells that drive the inhibitory cells to shut them down; then the inhibitory cells shut themselves down and the whole thing repeats. Reduce the network’s input and it oscillates more slowly, around 20 times a second. These slow oscillations occur because the external input to the network is so weak, and inhibition is so strong, that many neurons just don’t get enough input to fire. The ones that are firing, however, use their connections to slowly rev the network back up. If too many inhibitory cells get activated, though, the network becomes quiet again. </p>
<p class="TXI">Despite their superficial similarity to a seizure, these messy oscillations aren’t actually debilitating. In fact, scientists have observed oscillations in all different parts <a id="page_145"></a>of the brain under all different conditions. Groups of neurons in the visual cortex, for example, can oscillate at a swift 60 times a second. The hippocampus (the memory-processing machine from last chapter) sometimes oscillates quickly and other times slowly. The olfactory bulb, where odours are processed, generates waves that range from once per second – aligning with inhalation – to a hundred times. Oscillations can be found everywhere if you care to look.</p>
<p class="TXI">Mathematicians are happy to see oscillations. This is because, to a mathematician, oscillations are approachable. Chaos and randomness are a challenge to capture with equations, but perfect periodicity is easy and it’s elegant. Over millennia, mathematicians have developed the equipment not just to describe oscillations, but to predict how they’ll interact and to spot them in signals that – to the untrained eye – may not look like oscillations at all. </p>
<p class="TXI">Nancy Kopell is a mathematician, or at least she used to be. Like her mother and sister before her, Kopell majored in mathematics as an undergraduate. She then got a PhD<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> in it from the University of California, Berkeley, in 1967, and became a professor of mathematics <a id="page_146"></a>at Northeastern University in Boston. But after many years crossing back and forth between the border of mathematics and biology – taking problems from the latter to inspire ideas for the former – she started to feel more settled in the land of biology. As Kopell wrote in an autobiography: ‘My perspective started to shift, and I found myself at least as interested in the physiological phenomena as the mathematics problems that they generated. I didn’t stop thinking mathematically, but problems interested me less if I didn’t see the relevance to specific biological networks.’ Many of the biological networks that interested her were neural ones and throughout her career she’s studied all manner of oscillations in the brain.</p>
<p class="TXI">High-frequency oscillations are referred to by neuroscientists as ‘gamma’ waves. The reason for this is that Hans Berger, the inventor of the original EEG machine, called the big slow waves he could see by eye on his shoddy equipment ‘alpha’ waves and everything else ‘beta’; the scientists who came after him simply followed suit, giving new frequencies they found new Greek letters. Gamma waves, while fast, are usually small – or ‘low amplitude’ in technical terms. Their presence, detectable by a modern EEG or an electrode in the brain, is associated with an alert and attentive mind.</p>
<p class="TXI">In 2005, Kopell and colleagues came up with an explanation for how gamma oscillations could help the brain focus. Their theory stems from the idea that neurons representing the information you’re paying attention to should get a head start in the oscillation. Consider trying to listen to a phone call in the middle of a noisy room. Here, the signal you are paying attention <a id="page_147"></a>to – the voice on the other end of the line – is competing against all the distracting sounds in the room. In Kopell’s model, the voice is represented by one group of excitatory cells and the background chatter by another. Both of these groups send connections to a common pool of inhibitory neurons and both get connections back from it in return. </p>
<p class="TXI">Importantly, the neurons representing the voice – because they are the object of attention – get a little more input than the ‘background’ neurons. This means they’ll fire first and more vigorously. If these ‘voice’ neurons fire in unison, they will – through their connections to the inhibitory cells – cause a big, sharp increase in inhibitory cell-firing. This wave of inhibition will then shut down the cells representing both the voice and the background noise. Because of this, the background neurons never get the chance to fire and therefore can’t interfere with the sound of the voice. It’s as though the voice neurons, by being the first to fire, are pushing themselves through a door and then slamming it shut on the background neurons. And as long as the voice neurons keep getting a little extra input, this process will repeat over and over – creating an oscillation. The background neurons will be forced to remain silent each time. This leaves just the clean sound of the voice in the phone as the only remaining signal. </p>
<p class="TXI">Beyond just this role in attention, neuroscientists have devised countless other ways in which oscillations could help the brain. These include uses in navigation, memory and movement. Oscillations are also supposed to make communication between brain areas better and help organise neurons into separately functioning <a id="page_148"></a>groups. On top of this, theories abound for how oscillations go wrong in diseases like schizophrenia, bipolar disorder and autism.</p>
<p class="TXI">The omnipresence of oscillations may make it seem like their importance would go unquestioned, but this is far from the case. While several different roles for oscillations have been devised, many scientists remain sceptical. </p>
<p class="TXI">Part of the concern comes from the very first step: how oscillations are measured. Instead of recording from many neurons at once, many researchers interested in oscillations use an indirect measure that comes from the fluid that surrounds neurons. Specifically, when neurons are getting a lot of input, the composition of ions in this fluid changes and this can be used as a proxy for how active the population is. But the relationship between ion flows in this fluid and the real activity of neurons is complicated and not completely understood. This makes it hard to know if observed oscillations are actually happening. </p>
<p class="TXI">Scientists can also be swayed by the tools available to them. EEG has been around for a century and it makes the spotting of oscillations easy, even in human subjects; experiments can be performed in an afternoon on willing (usually undergraduate) research participants. As mentioned, a similar ease and familiarity applies to the mathematical tools of analysing oscillations. This may make researchers more likely to seek out these brainwaves, even in cases where they might not provide the best answers. To paraphrase the old adage, when a hammer is the easiest tool you have to use, you start looking for nails.</p> <a id="page_149"></a>
<p class="TXI">Another issue is impact, especially when it comes to fast oscillations like gamma. If one brain state has stronger gamma waves than another, it means more neurons are firing as part of a wave in that state rather than sporadically on their own. But when these waves come so quickly, being part of one may only make a neuron fire a few milliseconds before or after it otherwise would have. Could that kind of temporal precision really matter? Or is all that matters the total number of spikes produced? Many elegant hypotheses about how oscillations can help haven’t been directly tested – and can be quite hard to test – so answers are unknown. </p>
<p class="TXI">As neuroscientist Chris Moore said in a 2019 <span class="italic">Science Daily</span> interview: ‘Gamma rhythms have been a huge topic of debate … Some greatly respected neuroscientists view gamma rhythms as the magic, unifying clock that aligns signals across brain areas. There are other equally respected neuroscientists that, colourfully, view gamma rhythms as the exhaust fumes of computation: They show up when the engine is running but they’re absolutely not important.’ </p>
<p class="TXI">Exhaust fumes may be produced when a car moves, but they’re not directly what’s making it go. Similarly, networks of neurons may produce oscillations when performing computations, but whether those oscillations are what’s doing the computing remains to be seen. </p>
<p class="TXI">As has been shown, the interaction between excitatory and inhibitory cells can create a zoo of different firing patterns. Putting these two forces in conflict has both benefits and risks. It grants the network the ability to respond with lightning speed and to generate the smooth <a id="page_150"></a>rhythms needed for sleep. At the same time, it places the brain dangerously close to seizures and creates literal chaos. Making sense of such a multifaceted system can be a challenge. Luckily a diversity of mathematical methods – those developed for physics, meteorology and understanding oscillations – have helped tame the wild nature of neural firing.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-1" id="fn-1">1</a> ﻿Historically, this set of techniques went under the more obvious name of ﻿‘﻿self-consistent field theory﻿’﻿, but it﻿’﻿s now known as ﻿‘﻿mean-field theory﻿’﻿. The trick behind the mean-field approach is that you don﻿’﻿t need to provide an equation for each and every interacting particle in your system. Instead, you can study a ﻿‘﻿representative﻿’﻿ particle that receives its own output as input. That makes studying self-consistency a lot easier.﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-2" id="fn-2">2</a> ﻿In popular culture it﻿’﻿s better known as the ﻿‘﻿butterfly effect﻿’﻿, the idea that something as insignificant as a butterfly flapping its wings can change the whole course of history. ﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-3" id="fn-3">3</a> ﻿Again, the population will still produce the same amount of spikes ﻿<span class="italic">on average</span>﻿ in response to a given input. It﻿’﻿s just how those spikes are distributed across time and neurons that varies. If your neurons were truly following no rules for how they responded to inputs, you wouldn﻿’﻿t be able to be reading this right now. ﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-4" id="fn-4">4</a> ﻿Brunel, perhaps unsurprisingly at this point, started as a physicist. He learned about neuroscience during his PhD in the early 1990s, when a course exposed him to this new trend of applying the tools of physics to the brain.﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-5" id="fn-5">5</a> ﻿Kopell﻿’﻿s reasons for going to graduate school were somewhat unusual: ﻿‘﻿I had not entered college thinking about going to graduate school. But when my senior year arrived, I was not married and had nothing specific I wanted to do, so graduate school seemed like a good option.﻿’﻿ But the sexism she encountered there was perhaps more expected: ﻿‘﻿It was the unspoken but very widespread assumption that women in math were like dancing bears ﻿–﻿ perhaps they could do it, but not very well, and the attempt was an amusing spectacle.﻿’﻿﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<meta charset="utf-8"/>
<title>Contents</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="FMT" id="re_contents">Contents</p>
<p class="TOC-CH"><a href="chapter1.xhtml#chapter1" id="re_chapter1">Chapter 1:  Spherical Cows</a></p>
<p class="TOC-CH"><a href="chapter2.xhtml#chapter2" id="re_chapter2">Chapter 2:  How Neurons Get Their Spike</a></p>
<p class="TOC-CH"><a href="chapter3.xhtml#chapter3" id="re_chapter3">Chapter 3:  Learning to Compute</a></p>
<p class="TOC-CH"><a href="chapter4.xhtml#chapter4" id="re_chapter4">Chapter 4:  Making and Maintaining Memories</a></p>
<p class="TOC-CH"><a href="chapter5.xhtml#chapter5" id="re_chapter5">Chapter 5:  Excitation and Inhibition</a></p>
<p class="TOC-CH"><a href="chapter6.xhtml#chapter6" id="re_chapter6">Chapter 6:  Stages of Sight</a></p>
<p class="TOC-CH"><a href="chapter7.xhtml#chapter7" id="re_chapter7">Chapter 7:  Cracking the Neural Code</a></p>
<p class="TOC-CH"><a href="chapter8.xhtml#chapter8" id="re_chapter8">Chapter 8:  Movement in Low Dimensions</a></p>
<p class="TOC-CH"><a href="chapter9.xhtml#chapter9" id="re_chapter9">Chapter 9:  From Structure to Function</a></p>
<p class="TOC-CH"><a href="chapter10.xhtml#chapter10" id="re_chapter10">Chapter 10:  Making Rational Decisions</a></p>
<p class="TOC-CH"><a href="chapter11.xhtml#chapter11" id="re_chapter11">Chapter 11:  How Rewards Guide Actions</a></p>
<p class="TOC-CH"><a href="chapter12.xhtml#chapter12" id="re_chapter12">Chapter 12:  Grand Unified Theories of the Brain</a></p>
<p class="TOC-CH1"><a href="Mathematical.xhtml#Mathematical" id="re_Mathematical">Mathematical Appendix</a></p>
<p class="TOC-CH"><a href="ack.xhtml#ack" id="re_ack">Acknowledgements</a></p>
<p class="TOC-CH"><a href="bib.xhtml#bib" id="re_bib">Bibliography</a></p>
<p class="TOC-CH"><a href="index.xhtml#index" id="re_index">Index</a></p>
</body>
</html>