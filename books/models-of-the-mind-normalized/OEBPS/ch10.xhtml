<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 10</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter10"><a href="contents.xhtml#re_chapter10">CHAPTER TEN</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter10">Making Rational Decisions</a><a id="page_279"></a></p>
<p class="H1" id="b-9781472966445-ch1077-sec11">
<span class="bold">
<span>Probability and Bayes’ rule</span>
</span></p>
<p class="TXT">When Hermann von Helmholtz was a small child in early nineteenth-century Prussia, he walked through his hometown of Potsdam with his mother. Passing a stand containing small dolls aligned in a row, he asked her to reach out and get one for him. His mother did not oblige, however, though not out of neglect or discipline. Rather, she couldn’t reach for the dolls because there were none. What the young Helmholtz was experiencing was an illusion; the ‘dolls’ he saw near him were actually <span class="italic">people</span> far away, at the top of the town’s church tower. ‘The circumstances were impressed on my memory,’ Helmholtz later wrote, ‘because it was by this mistake that I learned to understand the law of foreshortening in perspective.’</p>
<p class="TXI">Helmholtz went on to become a prominent physician, physiologist and physicist. One of his greatest contri­butions was the design of the ophthalmoscope, a tool to look inside the eye that is still used by doctors to this day. He also furthered understanding of colour vision with his work on the ‘trichromatic theory’ – the idea that three different cell types in the eye each respond to different wavelengths of light – through which he deduced that colour-blind patients must lack one of these cell types. Outside the eye, Helmholtz published a <a id="page_280"></a>volume on acoustics, the experience of tones, how sound is conducted through the ear and the way it excites the nerves. By turning his characteristic thoughtfulness and precision to the study of the sense organs, Helmholtz repeatedly illuminated the physical mechanisms by which information from the world enters the mind. </p>
<p class="TXI">But the deeper question of how the mind uses that information always lingered with him. Inheriting a keen interest in philosophy from his father, Helmholtz’s worldview was impacted in several ways by the work of German philosopher Immanuel Kant. In Kant’s philosophy, the ‘Ding an sich’, or ‘thing-in-itself’, refers to the real objects out in the world – objects that can’t be experienced directly, but only through the impressions they make on our sense organs. But if two different conditions in the world – for example, a nearby doll or faraway person – can produce the same pattern of light hitting the eye, how does the mind decide which is the correct one to perceive? How, Helmholtz wanted to know, can perception form out of ambiguous or uncertain inputs? </p>
<p class="TXI">Ruminating on this question, Helmholtz concluded that a large amount of processing must go on between the point at which sensory information comes in and the moment it becomes a conscious experience. The result of this processing, he wrote, is ‘equivalent to a <span class="italic">conclusion</span>, to the extent that the observed action on our senses enables us to form an idea as to the possible cause of this action’. This idea became known as ‘unconscious inference’ because the objects in the world must be inferred by their effects on the sense organs. Taking further inspiration from Kant, Helmholtz proposed that this inference <a id="page_281"></a>proceeded by interpreting the current sensory input in light of pre-existing knowledge about the world. In particular, just as his mistake with the dolls taught him about perspective, Helmholtz believed that experiences in the past can influence perceptions in the present.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> </p>
<p class="TXI">Despite being one of the most mathematically adept physiologists of all time, Helmholtz never defined unconscious inference mathematically. His ideas on the topic, while thorough, remained qualitative and mostly speculative. They were also rejected. Scientists at the time felt that the notion of ‘unconscious inference’ was a contradiction in terms. Inference, or decision-making, was a conscious process by default; it simply couldn’t be occurring beneath the surface. </p>
<p class="TXI">But Helmholtz would be vindicated, nearly 100 years after his death, by psychologists using mathematics originally developed more than 50 years before his birth. Unconscious inference – dressed in the equations of probability – would come to encapsulate the basic mechanisms of how humans perceive, decide and act. </p>
<p class="center">* * *</p>
<p class="TXT">It’s not uncommon for mathematical topics – even some of the most abstract – to have their origins in very practical professions. The tools of geometry arose from building and land surveying; ancient astronomers contributed to the concept of zero becoming commonplace; and the field of probability was born out of gambling.</p> <a id="page_282"></a>
<p class="TXI">Girolamo Cardano was an Italian physician but, not unlike many educated men of the sixteenth century, he felt comfortable dabbling in a variety of subjects. According to his own count, he wrote well over a hundred books – most of them lost to time – with titles as far-ranging as <span class="italic">On the Seven Planets</span>, <span class="italic">On the Immortality of the Soul</span> and <span class="italic">On the Urine</span>. Regarding one of his books that would remain for posterity, Cardano wrote: ‘The book <span class="italic">On Games of Chance</span> I also wrote; why should not a man who is a gambler, a dicer, and at the same time an author, write a book on gaming?’ And a gambler Cardano was; the book reads more like a gaming manual borne from personal experience than a textbook. Yet it was, for its time, the most thorough treatment of the rules of probability available.</p>
<p class="TXI">Cardano focuses most of his mathematics on the casting of dice. He is quick to acknowledge that any of the six sides of a die is as likely as the others to show up, but that in practice they won’t always be found equally: ‘In six casts each point should turn up once; but since some will be repeated, it follows that others will not turn up.’ After working through examples of what to expect when rolling one, two or three dice, he concludes: ‘There is one general rule, namely, that we should consider the whole circuit, and the number of those casts which represents in how many ways the favourable result can occur, and compare that number to the remainder of the circuit.’ In other words, the probability that a certain result will happen can be calculated as the number of outcomes that lead to that result divided by the total number of possible outcomes.</p> <a id="page_283"></a>
<p class="TXI">Take, for example, the rolling of two dice. If the rolling of one die has six possible outcomes, the rolling of two dice has 6 x 6 = 36 possible outcomes. If we say that our desired result is that, after rolling the two dice, their faces sum to three, then there are two possible outcomes that lead to that result: 1) the first die shows one and the second shows two; or 2) the first die shows two and the second shows one. The probability that we get our desired result is thus 2/36 or 1/18. </p>
<p class="TXI">According to Cardano: ‘Gambling is nothing but fraud and number and luck.’ So, in addition to discussing the numbers, he made sure to devote more than two chapters to the topic of fraud. Much of the focus was on how to notice a cheat: ‘The die may be dishonest either because it has been rounded off, or because it is too narrow 
(a fault which is plainly visible).’ The book also gave tips on how to handle a cheat when you spot one: ‘When you suspect fraud, play for small stakes, have spectators.’ Though it should be noted that Cardano’s autobiography offers a rather different take on how to react. Under the chapter entitled ‘Perils, Accidents and Persistent Treacheries’ he recalls a time when he noticed a man’s cards were marked and ‘I impetuously slashed his face with my poniard, though not deeply’. </p>
<p class="TXI">Importantly, Cardano made clear that most of his calculations of probabilities held only if the dice in use were honest, not if he were playing ‘in the house of a professional cheat’ (as he described the incident above). In that case, the probabilities would need to be ‘made so much the larger or smaller in the proportion to the departure from true equality’.</p><a id="page_284"></a>
<p class="TXI">Accounting for different probabilities under different conditions – such as a cheating player – would later become known as <span class="italic">conditional probability</span>. Conditional probability can be thought of as a simple if-then statement. If you’re given the fact that X is true, then what’s the chance that Y is too? For example, <span class="italic">given</span> that the die is fair, the probability that a roll of it will produce a two is 1/6. Alternatively, the probability would be, say, 1/3 <span class="italic">given</span> that you’re playing with a cheat who has altered the die to prefer twos. The probability of an event thus depends on the circumstances it is <span class="italic">conditioned upon</span>. </p>
<p class="TXI">One topic that flummoxed mathematicians for centuries after Cardano was the question of <span class="italic">inverse probability</span>. Standard probability may be able to say how different dice create different chances, but the goal of inverse probability was to go the other way – to reverse the reasoning and find the cause behind the effects.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> For example, if Cardano didn’t know if he was in a game with a cheat or not, he could observe the rolls of the die to try to determine if it was biased. If one too many twos came up, he may have suspected that something was amiss (though hopefully he would have kept his poniard to himself).</p>
<p class="TXI">French mathematician Pierre-Simon Laplace worked on the issue of inverse probability intermittently over 40 years of his career. The culmination came in 1812 with the publication of <span class="italic">Théorie Analytique des Probabilités</span>. Here, <a id="page_285"></a>Laplace demonstrates a simple rule that would come to be one of the most important and influential findings in mathematics. </p>
<p class="TXI">The rule says that if you want to know the probability that the die is weighted, you have to combine two different terms. The first is the probability that the rolls you’ve seen could come from a weighted die and the second is the probability of the die being weighted in the first place. More technically, this is usually stated as: the probability of your hypothesis (‘the die is weighted’) given your evidence (the rolls you’ve seen) is proportional to the probability of your evidence given your hypothesis (the odds you’d see those rolls if the die were weighted) times the probability of your hypothesis (how likely is the die to be weighted to begin with) (see Figure 22).</p>
<p class="TXI">Let’s say the die has come up ‘two’ three times in a row and you want to know if you’re being taken for a ride. With a fair die, the probability of that streak is 1/6 x 1/6 x 1/6 = 1/216. This would be called the probability of the evidence <span class="italic">given</span> the belief that the die is fair. On the other hand, the die may be weighted to roll a two, say, one-third of the time. The probability of the evidence <a id="page_286"></a>
<span class="italic">given</span> the hypothesis the die is weighted this way would be 1/3 x 1/3 x 1/3 = 1/27. Comparing these numbers, it’s clear that three twos in a row is much more probable with a weighted die than with a fair one; it seems you may be playing with a cheat.</p>
<p class="TXI">But these numbers are insufficient. To draw a proper conclusion, the rule says we need to combine this with more information. Specifically, we need to multiply these numbers by the probability, in general, of the die being weighted or not. </p>
<p class="TXI">Let’s say in this case, your gambling partner is your closest friend of many years. You’d put the chance that they are using a weighted die at only 1 in 100. Multiplying the probability of getting three twos when using a weighted die times the low probability that the die is weighted, we get 1/27 x 1/100 = 1/2,700 or 0.00037. Doing this for the other hypothesis – that the die is unweighted – we get 1/216 x 99/100 = 0.0045. The second number being larger than the first, you’d be fair in concluding that your friend is not, in fact, a fraud. </p>
<p class="TXI">What this example shows is the power of the <span class="italic">prior</span>. The ‘prior’ is the name given to the probability of the hypothesis – in this case the probability your friend has altered the die. Running through the same equations, but assuming you are playing with a stranger who is as likely to cheat as not (that is, the probability of cheating is 0.5), the outcome is different: 0.019 versus 0.0023 in favour of a weighted die. In this way, a strong prior can be a deciding factor. </p>
<p class="TXI">The other term – the probability of the rolls given the hypothesis – is called the ‘likelihood’. It indicates how likely you’d be to see what you’ve seen if your hypothesis <a id="page_287"></a>about the world were true. Its role in inverse probability reflects the fact that, to determine the cause of any effect, one must first know the likely effects of each cause.</p>
<p class="TXI">Both the likelihood and the prior on their own are incomplete. They represent different sources of knowledge: the evidence you have here and now versus an understanding accumulated over time. When they agree, the outcome is easy. Otherwise, they exert their influence in proportion to their certainty. In the absence of clear prior knowledge, the likelihood dominates the decision. When the pull from the prior is strong, it can make you hardly believe your own eyes. In the presence of a strong prior, extraordinary claims can only be believed with extraordinary evidence.</p>
<p class="TXI">‘When you hear hoof beats, think of horses, not zebras’ is a bit of advice frequently given to medical students. It’s meant to remind them that, of two diseases with similar symptoms, the more common one should be their first guess. It is also an excellent example of the rule of inverse probability in action. Whether you’re in the presence of a horse or of a zebra, you’ve got a similar chance of hearing hoof beats; in technical terms, the likelihoods in the two cases are the same. Given such ambiguous evidence, the decision falls into the hands of the prior and, in this case, prior knowledge says horses are more common and therefore the best guess.</p>
<p class="image-fig" id="fig22.jpg">
<img alt="" src="Images/chapter-10-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 22</span>
</span></p>
<p class="TXI">In the 200 years since the publication of his work, in papers, in textbooks and on classroom chalkboards, the equation for inverse probability that Laplace wrote down has been referred to as ‘Bayes’ rule’. Thomas Bayes was a Presbyterian minister in eighteenth-century England. Also an amateur mathematician, Bayes did <a id="page_288"></a>work on the problem of inverse probability and he was able to solve a specific version of it. But all his thoughtfulness and calculations never quite got him to the form of Bayes’ rule we know today. What’s more, Bayes himself never published the work. An essay containing his thoughts on ‘a problem in the doctrine of chances’ was eventually sent to the Royal Society by a friend of his, another minister named Richard Price, in 1763, two years after Bayes’ death. Price put substantial work into turning Bayes’ notes into a proper essay; he wrote an introduction motivating the problem and added an extensive technical appendix (unfortunately all this effort did not prevent the essay from being referred to as ‘one of the more difficult works to read in the history of statistics’<sup><a href="#fn-3" id="fnt-3">3</a>
</sup>). Despite Laplace being alive at the time Bayes’ essay was published, he did not seem to be aware of it until after he had made substantial progress on his own.</p>
<p class="TXI">It could thus be said that the Reverend Bayes doesn’t quite deserve the empire that’s been posthumously gifted to him. It’s not clear he would’ve wanted it anyway. Bayes’ rule has not always fared well among scientists and philosophers. Much like Helmholtz’s work on unconscious inference, the equation has variably been underutilised and misunderstood. This was, initially, due to the difficulty of applying it. Laplace himself was able to use the rule on some problems of measurement in astronomy and also to <a id="page_289"></a>support the long-running hypothesis that slightly more male than female babies are born on average. But, depending on the problem in question, applying Bayes’ rule could involve some complex calculus, making it an onerous approach before the modern computer was around to help. </p>
<p class="TXI">But the real struggle for Bayes’ rule came later – and ran deeper. While the validity of Laplace’s equation was unquestioned, how to <span class="italic">interpret</span> that equation occupied and divided statisticians for decades. According to philosopher of science Donald Gillies: ‘The dispute between the Bayesians and the anti-Bayesians has been one of the major intellectual controversies of the twentieth century.’ The biggest target in the crosshairs of the anti-Bayesians was the prior. Where – they wanted to know – does this information come from? In theory, it is world knowledge. In practice, it is someone’s knowledge. As the giant of twentieth-century statistics Ronald Fisher said, the assumptions that go into choosing the prior are ‘entirely arbitrary, nor has any method been put forward by which such assumptions can be made even with consistent uniqueness’. Without providing an unbiased, repeatable procedure for reaching a conclusion, Bayes’ rule was no rule at all. Because of this, the method was cast aside, branded – in a way that would be sure to scare off serious scientists – as ‘subjective’.</p>
<p class="TXI">Conceptual concerns have a habit of fading when exposed to the light of practical proof, however. And in the latter part of the twentieth century, Bayes’ rule was proving its worth. Actuaries, for example, came to realise that their rates were better calculated using the principles of inverse probability. In epidemiology, Bayes helped sort <a id="page_290"></a>out the connection between smoking and lung cancer. And in the fight against the Nazis, code-breaker Alan Turing turned to Bayesian principles to uncover messages written with the ‘unbreakable’ Enigma machine. Bayes’ rule was emerging as a tamer of uncertainty anywhere that it reared its head. Practically speaking, the prior proved only a minor problem. It could be initialised with an educated guess and updated in light of new evidence (or, barring any knowledge at all, each hypothesis is simply given an equal chance). With its repeated success in spite of an active movement against it, Bayes’ rule certainly earns the title bestowed on it by Sharon McGrayne’s book, <span class="italic">The Theory That Would Not Die</span>.</p>
<p class="center">* * *</p>
<p class="TXT">When Bayes’ rule entered psychology it was not with a bang, but with a battering. No single publication carried it in. Rather, starting with the field of decision theory in the 1960s, multiple different lines of research employed it and explored it, until eventually the idea of the brain as Bayesian bloomed at the turn of the twenty-first century. </p>
<p class="TXI">Some of the early work on Bayesian principles in the brain came from an unlikely place: space. On its mission to get space travel off the ground, the National Aeronautics and Space Administration (NASA) knew that it would have to engineer more than just flight suits and jet engines. It also investigated the ‘human factors’ of flying – such as how pilots read flight equipment, sense their environment and interact with controls. Researching this problem in 1972, aeronautical engineer <a id="page_291"></a>Renwick Curry wrote one of the earliest papers placing human perception in Bayesian terms. Specifically, he used Bayes’ rule to explain patterns in how humans perceive motion. Academic boundaries being what they are, however, few psychologists heard about it.</p>
<p class="TXI">Economics offered another way for Bayes to creep in. Ever eager to capture human behaviour in compact mathematical form, economists turned to Bayes’ rule as early as the 1980s. In ‘Are individuals Bayesian decision-makers?’, written by William Viscusi in 1985, workers were shown to either over-or underestimate the riskiness of specific jobs because they relied on their prior beliefs about how risky jobs are in general.</p>
<p class="TXI">Psychologists also spotted Bayes on the horizon via one of their former sources of inspiration. As we saw in <a href="chapter3.xhtml#chapter3">Chapter 3</a>, the study of the brain has been influenced by the field of formal logic. By the end of the twentieth century, in many ways, probability was the new logic – an improved way to assess how humans think. Instead of the harsh true-false dichotomy of Boolean logic, probability offers shades of grey. In this way, it aligns better with our own intuitions about our beliefs. As Laplace himself wrote: ‘Probability theory is nothing but common sense reduced to calculation.’ </p>
<p class="TXI">Of course, probability is a bit better than that because the rules are mathematically worked out to be the <span class="italic">best</span> form of common sense – and Bayes’ rule in particular is a prescription for how best to reason. </p>
<p class="TXI">It was on these grounds that John Anderson formally debuted a Bayesian approach to psychology, under a method he referred to as ‘rational analysis’. It was an idea that came to him in 1987 while he was in Australia, on <a id="page_292"></a>sabbatical from his job as a professor of psychology and computer science at Carnegie Mellon University. Rational analysis, according to Anderson, stems from the belief that ‘there is a reason for the way the mind is’. Specifically, it posits that an understanding of how the mind works will best grow out of an understanding of where it came from. When it comes to Bayes, the reasoning starts with the fact that humans live in a messy, uncertain world. Yet – Anderson argues – humans have evolved within this world to behave as rationally as possible. Bayes’ rule is a description of how to reason rationally under conditions of uncertainty. Therefore, humans should be using Bayes’ rule. Put simply, if evolution has done its job, we should see Bayes’ rule in the brain.</p>
<p class="TXI">The details of just how the rule will be applied and to what problems depend on more specific features of the environment. As an example, Anderson offers a Bayesian theory of memory recall. It says that the probability of a particular memory being useful in a particular situation is found by combining: 1) how likely you’d be to find yourself in that situation if that memory were useful; with 2) a prior that assumes more recent memories are more likely to be useful. This choice of prior is meant to reflect the fact that humans come from a world where information has a shelf life; therefore, more recent memories are more likely to be of value. </p>
<p class="TXI">Importantly, under the rational analysis framework, ‘rational’ can be far from perfect. Memory, for example, certainly can fail us. But, according to this viewpoint, if we forget a fact from primary school 20 years on, we are not being irrational. Given the limited capacity of <a id="page_293"></a>memory and the ever-changing world in which we live, it makes perfect sense to let old and little-used information go. In this way, the prior in a Bayesian model can be thought of as storing a shortcut. It’s an encoding of the basic stats of the world that can make decision-making faster, easier and – in most cases – more accurate. If, however, we find ourselves in a world that deviates from the one we’ve evolved and developed in, our priors can be misleading. ‘Think of horses’ is only good advice in a place with more horses than zebras.</p>
<p class="center">* * *</p>
<p class="TXT">In early 1993, a group of researchers met at the Chatham Bars Inn in Chatham, Massachusetts. The group included psychologists David Knill (a professor at the University of Pennsylvania who served as organiser) and Whitman Richards (a professor at MIT who was part of the first crop of PhD students in the Department of Psychology there in the 1960s). Also at the meeting were scientists trained in physiology and neuroscience like Heinrich Bülthoff, whose work was on the visual system of fruit flies; as well as engineers and mathematicians such as Alan Yuille, a student of Stephen Hawking.</p>
<p class="TXI">On the agenda for this eclectic bunch was a hunt for a new formal theory of perception – ideally one that could capture the complexities of the senses while also offering new, testable hypotheses. A complexity of particular concern was how the senses appear to be affected by more than just what meets the eye, ear or nose. That is, incoming sensory information combines with a rich set of background knowledge before <a id="page_294"></a>perception is complete. According to Knill, no theory at the time was able to say precisely ‘how prior knowledge should be brought to bear upon the interpretation of sensory data’.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p>
<p class="TXI">The meeting birthed a book, published in 1996, the title of which reveals the solution the attendees settled on: <span class="italic">Perception as Bayesian Inference</span>. The seeds for this idea had, as we’ve seen, been scattered around for some time, growing in different ways in different fields. This was an opportunity to pull them together. The book presents a unified and clear approach to the Bayesian study of perception, focusing mainly on the sense of vision. Its success spawned countless research papers in the years that followed. If Anderson’s work on ‘rational analysis’ put Bayes on psychology’s map, this book gave it its own country.</p>
<p class="TXI">To understand the basics of Bayesian perception, consider an example. Light reflects off a flower and hits the eye. The wavelength of the light is around 670 nanometres (nm). It’s the task of the brain to figure out, given the wavelength it’s receiving, what the ‘thing-in-itself’ is, or what is really going on in the world. In Bayesian terms, this would be the probability of the hypothesis that a certain flower is present given that a wavelength of 670nm is hitting the eye. </p>
<p class="TXI">Bayes’ rule tells us what to do. First, we need to find out how likely we are to see that wavelength under different conditions. The likelihood of seeing 670nm light if the flower is blue and illuminated by white light <a id="page_295"></a>is very low (blue light falls between 450 and 480nm). The likelihood of seeing 670nm light if the flower is red and illuminated by white light is quite high; 670nm is right in the middle of the red spectrum. However, the probability of seeing 670nm light if the flower is <span class="italic">white</span> and illuminated by <span class="italic">red</span> light is also quite high. Since both of these scenarios are just as likely to produce 670nm light, if we stop here, we may be quite unsure of which one is the better interpretation.</p>
<p class="TXI">But as good Bayesians, we remember the importance of the prior. The probability of a world illuminated by red light is, by most measures, quite small. White light, however, is a very common sight. The scenarios above that assume white light are thus much more probable. Multiplying the prior probability of different scenarios times the probability of seeing 670nm light in that scenario, we see that only one scores high on both of these measures. We therefore conclude that in front of us there is a red flower, illuminated by regular white light. </p>
<p class="TXI">Of course, ‘we’ don’t actually conclude that. This process, just as Helmholtz anticipated, happens unconsciously. The odds are weighed out of our sight and we know only the end result. It is, in this way, a never-ending procedure to produce perception – an underground production line in the mind. At each moment, probabilities are calculated and compared, each perception a bit of computation according to Bayes’ rule. </p>
<p class="TXI">With all the work that goes into perception, it’s no surprise that the brain can sometimes come up with odd results. In 2002, a team of researchers out of the US and Israel catalogued a series of common illusions that people fall prey to when trying to estimate the <a id="page_296"></a>movement of an object. It included the fact that the shape of an object influences the direction we think it is moving in, that two items moving in different directions may appear as one and that dimmer objects appear to move more slowly. </p>
<p class="TXI">This may seem simply like a list of our failings, but the researchers found that all of these lapses could be explained by a simple Bayesian model. Particularly, these habits fall out of the calculation if we assume a specific prior: that motion is more likely to be slow than fast. Take, for example, the last illusion. When an object is hard to see, the evidence it provides about its movement is weak. In the absence of evidence, Bayes’ rule relies on the prior – and the prior says things move slowly. This bit of mathematics may explain why drivers have a tendency to speed up in the fog – with weak information about their own movement, they assume their speed is too slow. Importantly, the Bayesian approach recasts these tricks of the mind as traits of a rational calculation. It shows how some mistakes are actually reasonable guesses in an uncertain world. </p>
<p class="TXI">There is, however, another part to the process of perception. So far, we’ve simply assumed that the percept we experience should be the one with the highest probability. That’s a reasonable choice, but it is a choice nonetheless, and a different one could be made.</p>
<p class="image-fig" id="fig23.jpg">
<img alt="" src="Images/chapter-10-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 23</span>
</span></p>
<p class="TXI">Consider the Necker cube. This famous illusion (see Figure 23) admits more than one interpretation: the lower square could be seen as coming forwards, like a box faced slightly downwards, or it could be behind the plane of the page, suggesting a box tilted upwards. Both boxes are equally likely to produce this pattern of lines, so the <a id="page_297"></a>decision about which is the true state would be strongly influenced by the prior. Let’s say, downwards-tilted boxes are a bit more likely in general. So, after applying Bayes’ rule, the probability of there being a downwards box when we see these lines is 0.51 and an upwards box 0.49. Taking the standard approach to mapping this to perception, we’d say the larger of the two probabilities wins – we see it as a downwards box, end of story. </p>
<p class="TXI">On the other hand, rather than choosing one interpretation and sticking to it, the brain could choose to alternate between the two. The box could appear downwards at one moment and upwards the next, switching back and forth repeatedly. In this case, the probabilities tell us not which interpretation to stick with, but rather <span class="italic">the amount of time</span> to spend in each. </p>
<p class="TXI">This switching is exactly what researchers at the University of Rochester (including David Knill) saw in an experiment in 2011. The experimenters overlaid two visual patterns such that it was unclear whether the first was on top of the second or vice versa – that is, the image could be interpreted two different ways. Asking people to indicate when their perception of the image <a id="page_298"></a>switched from one view to the other, they could determine the amount of time spent seeing each. Assuming that either pattern was equally likely to be on top (that is, the prior probabilities are the same), Bayes’ rule says people should spend 50 per cent of their time seeing it each way. And this is exactly what they found. But to really test the predictive powers of Bayes’ rule, the scientists needed to move away from the 50-50 scenario. To do this, they manipulated the image to make one pattern appear slightly more on top than the other. This altered the likelihood – that is, the probability of seeing this image given that one or the other pattern truly was on top. The more they altered the image in this direction, the more time viewers spent seeing the preferred pattern on top – exactly in accordance with Bayes’ rule. </p>
<p class="TXI">As this study shows, a set of probabilities can be mapped to a perception in interesting ways – a mapping known by scientists as the decision function. Bayes’ rule itself doesn’t tell us what decision function to use; it only provides the probabilities. Perception could be collapsed to the interpretation with the highest probability, or it could not. Perception could be a sampling from different interpretations over time in accordance with their probabilities, or it could not. Overall, perception could be the result of any complex combination of probabilities. The output of Bayes’ rule therefore provides a rich representation of sensory information, one that the brain can use in any way that seems most reasonable. In this way, probabilities mean possibilities. </p>
<p class="TXI">Another benefit of thinking of the mind as dealing in probabilities is that it opens the door to quantifying a potentially elusive concept: confidence. Confidence is <a id="page_299"></a>intuitively tied to evidence and certainty. When walking around in a dark room, where visual evidence is weak, we move slowly because we aren’t confident we won’t bump into a wall or table. In a brightly lit room, however, the strong influence of the clear visual evidence removes such doubts. The Bayesian confidence hypothesis formalises this intuition by saying that how confident a person is in their interpretation of the world is directly related to the probability of that interpretation given the evidence – that is, the output of Bayes’ rule. In the dark room where evidence is low, so is the probability of any given interpretation of the room and, therefore, so is confidence. </p>
<p class="TXI">Researchers from the UK tested just how well this Bayesian hypothesis matches the data in 2015. To do so, they asked people to look for a particular pattern in two different images that were quickly flashed one after the other. The subjects then reported which of the two images had the pattern and, importantly, how confident they were in their decision. The decisions and confidence of the humans were compared to the predictions of the Bayesian model and to predictions from two simpler mathematical models. Bayes’ rule provided the better match for the majority of the data, supporting the Bayesian confidence hypothesis. </p>
<p class="center">* * *</p>
<p class="TXT">‘In the laboratory, we like to simplify the enormous task of understanding how the brain works,’ said Dora Angelaki in an interview in 2014. ‘Traditionally, neuroscience has studied one sensory system at a time. But in the real world, this is not what happens.’</p> <a id="page_300"></a>
<p class="TXI">Angelaki, originally from Crete, is a professor of Neuroscience at New York University. She credits her background in electrical engineering for her desire to seek out the underlying principles of how things work. As part of her research, she is correcting neuroscience’s bias towards simplicity by studying how the senses interact.</p>
<p class="TXI">The particular senses Angelaki seeks to combine are visual and vestibular. The vestibular system provides the little-known sixth sense of balance. Tucked deep in the ear, it is composed of a set of tiny tubes and stone-filled sacs. Through the sloshing of fluids in the tubes and the movement of the stones, the system offers – like the liquid in a level – a measurement of the head’s tilt and acceleration. This system works in tandem with the visual system to provide an overall sense of place, orientation and movement. When these two systems are out of whack, unpleasant sensations like motion sickness can occur. </p>
<p class="TXI">In her effort to understand the vestibular system, Angelaki has borrowed methods from an uncommon source: pilot training. Subjects in her experiments are strapped into a seat on a moving platform like the kind used in flight simulators. The platform can give them brief bursts of acceleration in different directions. At the same time a screen in front of them gives a visual sense of movement in the form of dots of light flowing past them – a visual not unlike that of jumping to lightspeed in <span class="italic">Star Wars</span>. While pilot training generally keeps the physical and visual motion aligned, Angelaki uses this set-up to see what the brain does when they disagree.</p>
<p class="TXI">Bayes’ rule offers a guess about that. Treating the visual and vestibular inputs as separate streams of evidence <a id="page_301"></a>about the same external world, the mathematics of probability provides a simple means for how to combine them. Instead of a single likelihood term – like in the standard Bayes’ rule – the two likelihoods (one from each sense) are multiplied together. Let’s say your task is to determine if you’re moving to the left or to the right. To calculate the probability that you’re actually moving to the right – given some vestibular and visual inputs – you’d multiply the likelihood that you’d see that visual input if you were moving to the right <span class="italic">times</span> the likelihood that you’d receive that vestibular input if you were moving to the right. To complete the process, this value then goes on to be multiplied by the prior probability of rightwards movement. The same can be done for leftwards movement, and the two are compared.</p>
<p class="TXI">Just as a rumour transforms into a fact as you hear it from many different people, in Bayes’ rule getting the same information from multiple senses strengthens the belief in that information. When the moving platform and the display screen are both consistent with rightwards movement, both visual and vestibular likelihoods will be high and thus so will the outcome of their multiplication. This contributes to a confident conclusion of rightwards movement. If they’re put at odds – the platform moves right while the dots say left – then the vestibular likelihood would still say the probability of rightwards movement is high, but the visual one would say it’s low. Multiplying these leads to a middling result and only moderate confidence one way or the other. </p>
<p class="TXI">But, as with rumours, the reliability of the source matters. In her experiments, Angelaki can reduce the confidence subjects have in one or the other sensory <a id="page_302"></a>inputs. To make the visual inputs less reliable, she simply makes them messier. That is, rather than all the dots moving together to give a strong sense of directed motion, some dots move randomly. The more dots that are random, the less reliable the visual information becomes. </p>
<p class="TXI">Looking at how that plays out with probabilities, we see that Bayes’ rule naturally titrates how much to rely on a source based on how reliable it is. If the dots were moving completely randomly, the visual input would provide no information about movement direction. In this case, the likelihood of the visual input given rightwards movement would be equal to the likelihood of it given leftwards movement. With equal likelihoods on both sides, the visual input wouldn’t weigh the decision either way. The tiebreaker would come from the vestibular inputs (and the prior). If, instead, 90 per cent of the dots moved randomly and 10 per cent indicated rightwards movement, then the likelihood of the visual input in support of rightwards movement would be slightly higher than that of leftwards. Now the visual input does weigh the decision – but only slightly. As the visual input becomes more reliable, its say in the decision grows. In this way, Bayes’ rule automatically puts more stock in a source in proportion to how certain it is.</p>
<p class="TXI">Investigating the conclusions people come to about their movement in these experiments, Angelaki and her lab have shown once again that, for the most part, humans behave as good Bayesians. When the visual evidence is weak, they depend more on their vestibular system. One caveat is that, while they use the visual information more as it becomes more reliable, they still don’t use it quite as much as Bayes’ rule would predict. That is, the vestibular input is consistently <a id="page_303"></a>overemphasised – an effect found in monkeys as well. This could be a result of the fact the visual input is always a bit ambiguous: seeing dots moving past could be an effect of your own movement, or it could just be the dots moving. So, the vestibular input is, in general, a more trustworthy source and therefore worthy of more weight. </p>
<p class="center">* * *</p>
<p class="TXT">Once the Bayesian approach to perception was unleashed, it quickly spread to all corners of psychology. Like a Magic Eye illusion, staring at any data long enough can make the structure of Bayes’ rule pop out of it. As a result, priors and likelihoods abound in the study of the mind. </p>
<p class="TXI">As we’ve already seen, Bayes’ rule has been evoked to explain motion perception, the switching of ambiguous illusions like the Necker cube, confidence and the combination of vision and vestibular inputs. It’s also been adapted to account for how we can be tricked by ventriloquists, our sense of the passing of time and our ability to spot anomalies. It can even be stretched and expanded to cover such tasks as motor skill learning, language understanding and our ability to generalise. Such a unifying framework to describe so much of mental activity seems an unmitigated success. Indeed, according to philosopher of mind Michael Rescorla, the Bayesian approach is ‘our best current science of perception’. </p>
<p class="TXI">Yet, not all psychologists can be counted as devoted disciples of the Reverend Bayes. </p>
<p class="TXI">To some, a theory that explains everything is at risk of explaining nothing at all. The flip side of the flexibility <a id="page_304"></a>of the Bayesian approach is that it can also be accused of having too many <span class="italic">free parameters</span>. The free parameters of a model are all its movable parts – all the choices that the researcher can make when using it. The same way that, if given enough strokes even the worst golfer could eventually get the ball in the cup, if given enough free parameters any model can fit any data. If a finding from a new experiment conflicts with an old one, for example, an over-parameterised model can easily twist its way around to encompass them both. If getting the model to fit the data is as easy as a bank making change, its success isn’t very interesting. A model that can say anything can never be wrong. As psychologists Jeffrey Bowers and Colin Davis wrote in their 2012 critique of the Bayesian approach: ‘This ability to describe the data accurately comes at the cost of falsifiability.’</p>
<p class="TXI">There are indeed many ways to squeeze parts of perception into a Bayesian package. Take, for example, the likelihood calculation. Computing a quantity like ‘the likelihood of seeing 670nm light given the presence of a red flower’ requires some knowledge and assumptions about how light reflects off different materials and how the eye absorbs it. Without a perfect understanding of the physical world, the model-maker must put in some of their own assumptions here. They could, therefore, wiggle these assumptions around a bit to match the data. Another source of choice is the decision function. As we saw earlier, the output of Bayes’ rule can be mapped to the perception and decision of an animal any number of ways. This option, too, has the power to make any action look Bayesian in theory. And then, of course, there are those pesky priors.</p><a id="page_305"></a>
<p class="TXI">Just as they gave pause to statisticians in the twentieth century, priors have proven a challenge to psychologists of the twenty-first. If the assumption of a certain prior – for example, that motion is likely to be slow – helps explain a psychological phenomena, that can be taken as good evidence that the brain really uses that prior. But what if a different phenomena is best explained by a different prior – say, one that assumes motion is fast? Should it be assumed that the priors in our mind are constant across time and task? Or are they flexible and fluid? And how can we know?</p>
<p class="TXI">As a result of these concerns, some researchers have embarked on an exploration into the properties of priors. French cognitive scientist Pascal Mamassian has worked to investigate a particularly common one: the assumption that light comes from above. For more than two centuries, experiments and illusions have found that humans keep this implicit belief about the source of illumination in mind as they make sense of shadows in a scene. It’s a sensible guess, given the location of our dominant light source, the sun. More recently, experiments have revised this finding slightly and found that humans actually assume that light comes from above <span class="italic">and slightly to the left</span>. Mamassian conducted tests revealing this bias in the lab, but he also found a more creative way to interrogate it. Analysing 659 paintings from the Louvre museum in Paris, he found that in a full 84 per cent of portraits and 67 per cent of non-portrait paintings, the light source was indeed biased towards the left side. Artists may have come to prefer this setting exactly because it aligns with our intuitions, creating a more pleasant and interpretable painting.</p><a id="page_306"></a>
<p class="TXI">Another open question about priors is their origin. Priors can serve as an efficient way to imprint facts about the world on our minds; but are these facts gifted to us from previous generations through our genes, or do we develop them in our own lifetimes? To test this, a study in 1970 raised chickens in an environment wherein all light came from below. If the prior assumption that light comes from overhead is learned in their lifetimes, these birds wouldn’t have it. How the animals interacted with visual stimuli, however, showed they did still assume light should be from above. This points in favour of an inherited prior. </p>
<p class="TXI">Humans, of course, aren’t chickens, and the development of our nervous system may allow for a bit more flexibility. Investigating the prior biases of children of various ages, psychologist James Stone found in 2010 that children as young as four did show a bias towards assuming overhead light, though it was weaker than that in adults. This bias grows steadily over the years to reach adult strength, suggesting that a partially innate prior may be fine-tuned by experience. Further in support of this flexibility, a team from the UK and Germany showed in 2004 that our grip on where light must be coming from can be loosened. Through training, participants were able to shift their prior beliefs about the source of the light by several degrees. </p>
<p class="TXI">Picking a particular prior and poking at it from different directions through a multitude of experiments helps verify it as a resilient and reliable effect. Each prior this is done for becomes less of a free parameter in the model and more fixed.</p> <a id="page_307"></a>
<p class="TXI">Another question supporters of the Bayesian brain hypothesis need to address is ‘how?’</p>
<p class="TXI">While there are reasons to believe the brain <span class="italic">should</span> use Bayes’ rule, and there is evidence that it <span class="italic">does</span>, the question of how this plays out in neurons remains a lively area of research.</p>
<p class="TXI">When it comes to priors, scientists are looking for what cupboard in the brain stores these bits of background knowledge and how they get mixed into the neural decision-making process. One hypothesis is that it’s a simple numbers game. If a group of neurons are charged with representing something about the world – say, where in the environment a sound is coming from – each neuron may have its own preferred location. This means it responds the most when sound is coming from there. If the brain determines where the sound is by adding up the activity of all the neurons that prefer the same location, locations with more neurons will have an advantage. So, if the prior says that sound is more likely to come from central locations than the periphery, this can be implemented by simply increasing the number of neurons that prefer the centre. As it turns out, neuroscientists Brian Fischer and Jose Luis Peña found this exact scheme in the brains of owls in 2011. Identifying neural signatures of priors this way can give insights to where they come from and how they work.</p>
<p class="TXI">Theorists are building – and experimentalists are testing – many more hypotheses about how Bayes’ rule plays out in the brain. There are a multitude of ways that neurons can conspire to combine likelihoods and priors. <a id="page_308"></a>These different hypotheses should not be considered in competition, nor should any single winner expect to be crowned at the end. Rather, while Bayes’ rule can be one-size-fits-all for capturing the outputs of perception, the physical underpinnings of this rule may come in many different shapes and styles.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-1" id="fn-1">1</a> ﻿In this way Helmholtz deviated from Kant, who believed that much of this world knowledge was innate rather than learned.﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-2" id="fn-2">2</a> ﻿Speaking about probability in terms of cause and effect was not uncommon at the time. But it is not, in general, a wise thing to do. The probability that you﻿’﻿ll be holding an umbrella given that the person next to you on the street is may be high, but the one is not the cause of the other. ﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-3" id="fn-3">3</a> ﻿The statistician and historian responsible for this assessment, Stephen Stigler, is also known for ﻿‘﻿Stigler﻿’﻿s law of eponymy﻿’﻿, which claims that a scientific law is never named after its true originator. The sociologist Robert Merton is believed to be the originator of this law. ﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-4" id="fn-4">4</a> ﻿Some of the existing theories of the time that failed to do this include the models of the visual system discussed in ﻿﻿Chapter 6﻿﻿.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 10</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter10"><a href="contents.xhtml#re_chapter10">CHAPTER TEN</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter10">Making Rational Decisions</a><a id="page_279"></a></p>
<p class="H1" id="b-9781472966445-ch1077-sec11">
<span class="bold">
<span>Probability and Bayes’ rule</span>
</span></p>
<p class="TXT">When Hermann von Helmholtz was a small child in early nineteenth-century Prussia, he walked through his hometown of Potsdam with his mother. Passing a stand containing small dolls aligned in a row, he asked her to reach out and get one for him. His mother did not oblige, however, though not out of neglect or discipline. Rather, she couldn’t reach for the dolls because there were none. What the young Helmholtz was experiencing was an illusion; the ‘dolls’ he saw near him were actually <span class="italic">people</span> far away, at the top of the town’s church tower. ‘The circumstances were impressed on my memory,’ Helmholtz later wrote, ‘because it was by this mistake that I learned to understand the law of foreshortening in perspective.’</p>
<p class="TXI">Helmholtz went on to become a prominent physician, physiologist and physicist. One of his greatest contri­butions was the design of the ophthalmoscope, a tool to look inside the eye that is still used by doctors to this day. He also furthered understanding of colour vision with his work on the ‘trichromatic theory’ – the idea that three different cell types in the eye each respond to different wavelengths of light – through which he deduced that colour-blind patients must lack one of these cell types. Outside the eye, Helmholtz published a <a id="page_280"></a>volume on acoustics, the experience of tones, how sound is conducted through the ear and the way it excites the nerves. By turning his characteristic thoughtfulness and precision to the study of the sense organs, Helmholtz repeatedly illuminated the physical mechanisms by which information from the world enters the mind. </p>
<p class="TXI">But the deeper question of how the mind uses that information always lingered with him. Inheriting a keen interest in philosophy from his father, Helmholtz’s worldview was impacted in several ways by the work of German philosopher Immanuel Kant. In Kant’s philosophy, the ‘Ding an sich’, or ‘thing-in-itself’, refers to the real objects out in the world – objects that can’t be experienced directly, but only through the impressions they make on our sense organs. But if two different conditions in the world – for example, a nearby doll or faraway person – can produce the same pattern of light hitting the eye, how does the mind decide which is the correct one to perceive? How, Helmholtz wanted to know, can perception form out of ambiguous or uncertain inputs? </p>
<p class="TXI">Ruminating on this question, Helmholtz concluded that a large amount of processing must go on between the point at which sensory information comes in and the moment it becomes a conscious experience. The result of this processing, he wrote, is ‘equivalent to a <span class="italic">conclusion</span>, to the extent that the observed action on our senses enables us to form an idea as to the possible cause of this action’. This idea became known as ‘unconscious inference’ because the objects in the world must be inferred by their effects on the sense organs. Taking further inspiration from Kant, Helmholtz proposed that this inference <a id="page_281"></a>proceeded by interpreting the current sensory input in light of pre-existing knowledge about the world. In particular, just as his mistake with the dolls taught him about perspective, Helmholtz believed that experiences in the past can influence perceptions in the present.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> </p>
<p class="TXI">Despite being one of the most mathematically adept physiologists of all time, Helmholtz never defined unconscious inference mathematically. His ideas on the topic, while thorough, remained qualitative and mostly speculative. They were also rejected. Scientists at the time felt that the notion of ‘unconscious inference’ was a contradiction in terms. Inference, or decision-making, was a conscious process by default; it simply couldn’t be occurring beneath the surface. </p>
<p class="TXI">But Helmholtz would be vindicated, nearly 100 years after his death, by psychologists using mathematics originally developed more than 50 years before his birth. Unconscious inference – dressed in the equations of probability – would come to encapsulate the basic mechanisms of how humans perceive, decide and act. </p>
<p class="center">* * *</p>
<p class="TXT">It’s not uncommon for mathematical topics – even some of the most abstract – to have their origins in very practical professions. The tools of geometry arose from building and land surveying; ancient astronomers contributed to the concept of zero becoming commonplace; and the field of probability was born out of gambling.</p> <a id="page_282"></a>
<p class="TXI">Girolamo Cardano was an Italian physician but, not unlike many educated men of the sixteenth century, he felt comfortable dabbling in a variety of subjects. According to his own count, he wrote well over a hundred books – most of them lost to time – with titles as far-ranging as <span class="italic">On the Seven Planets</span>, <span class="italic">On the Immortality of the Soul</span> and <span class="italic">On the Urine</span>. Regarding one of his books that would remain for posterity, Cardano wrote: ‘The book <span class="italic">On Games of Chance</span> I also wrote; why should not a man who is a gambler, a dicer, and at the same time an author, write a book on gaming?’ And a gambler Cardano was; the book reads more like a gaming manual borne from personal experience than a textbook. Yet it was, for its time, the most thorough treatment of the rules of probability available.</p>
<p class="TXI">Cardano focuses most of his mathematics on the casting of dice. He is quick to acknowledge that any of the six sides of a die is as likely as the others to show up, but that in practice they won’t always be found equally: ‘In six casts each point should turn up once; but since some will be repeated, it follows that others will not turn up.’ After working through examples of what to expect when rolling one, two or three dice, he concludes: ‘There is one general rule, namely, that we should consider the whole circuit, and the number of those casts which represents in how many ways the favourable result can occur, and compare that number to the remainder of the circuit.’ In other words, the probability that a certain result will happen can be calculated as the number of outcomes that lead to that result divided by the total number of possible outcomes.</p> <a id="page_283"></a>
<p class="TXI">Take, for example, the rolling of two dice. If the rolling of one die has six possible outcomes, the rolling of two dice has 6 x 6 = 36 possible outcomes. If we say that our desired result is that, after rolling the two dice, their faces sum to three, then there are two possible outcomes that lead to that result: 1) the first die shows one and the second shows two; or 2) the first die shows two and the second shows one. The probability that we get our desired result is thus 2/36 or 1/18. </p>
<p class="TXI">According to Cardano: ‘Gambling is nothing but fraud and number and luck.’ So, in addition to discussing the numbers, he made sure to devote more than two chapters to the topic of fraud. Much of the focus was on how to notice a cheat: ‘The die may be dishonest either because it has been rounded off, or because it is too narrow 
(a fault which is plainly visible).’ The book also gave tips on how to handle a cheat when you spot one: ‘When you suspect fraud, play for small stakes, have spectators.’ Though it should be noted that Cardano’s autobiography offers a rather different take on how to react. Under the chapter entitled ‘Perils, Accidents and Persistent Treacheries’ he recalls a time when he noticed a man’s cards were marked and ‘I impetuously slashed his face with my poniard, though not deeply’. </p>
<p class="TXI">Importantly, Cardano made clear that most of his calculations of probabilities held only if the dice in use were honest, not if he were playing ‘in the house of a professional cheat’ (as he described the incident above). In that case, the probabilities would need to be ‘made so much the larger or smaller in the proportion to the departure from true equality’.</p><a id="page_284"></a>
<p class="TXI">Accounting for different probabilities under different conditions – such as a cheating player – would later become known as <span class="italic">conditional probability</span>. Conditional probability can be thought of as a simple if-then statement. If you’re given the fact that X is true, then what’s the chance that Y is too? For example, <span class="italic">given</span> that the die is fair, the probability that a roll of it will produce a two is 1/6. Alternatively, the probability would be, say, 1/3 <span class="italic">given</span> that you’re playing with a cheat who has altered the die to prefer twos. The probability of an event thus depends on the circumstances it is <span class="italic">conditioned upon</span>. </p>
<p class="TXI">One topic that flummoxed mathematicians for centuries after Cardano was the question of <span class="italic">inverse probability</span>. Standard probability may be able to say how different dice create different chances, but the goal of inverse probability was to go the other way – to reverse the reasoning and find the cause behind the effects.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> For example, if Cardano didn’t know if he was in a game with a cheat or not, he could observe the rolls of the die to try to determine if it was biased. If one too many twos came up, he may have suspected that something was amiss (though hopefully he would have kept his poniard to himself).</p>
<p class="TXI">French mathematician Pierre-Simon Laplace worked on the issue of inverse probability intermittently over 40 years of his career. The culmination came in 1812 with the publication of <span class="italic">Théorie Analytique des Probabilités</span>. Here, <a id="page_285"></a>Laplace demonstrates a simple rule that would come to be one of the most important and influential findings in mathematics. </p>
<p class="TXI">The rule says that if you want to know the probability that the die is weighted, you have to combine two different terms. The first is the probability that the rolls you’ve seen could come from a weighted die and the second is the probability of the die being weighted in the first place. More technically, this is usually stated as: the probability of your hypothesis (‘the die is weighted’) given your evidence (the rolls you’ve seen) is proportional to the probability of your evidence given your hypothesis (the odds you’d see those rolls if the die were weighted) times the probability of your hypothesis (how likely is the die to be weighted to begin with) (see Figure 22).</p>
<p class="TXI">Let’s say the die has come up ‘two’ three times in a row and you want to know if you’re being taken for a ride. With a fair die, the probability of that streak is 1/6 x 1/6 x 1/6 = 1/216. This would be called the probability of the evidence <span class="italic">given</span> the belief that the die is fair. On the other hand, the die may be weighted to roll a two, say, one-third of the time. The probability of the evidence <a id="page_286"></a>
<span class="italic">given</span> the hypothesis the die is weighted this way would be 1/3 x 1/3 x 1/3 = 1/27. Comparing these numbers, it’s clear that three twos in a row is much more probable with a weighted die than with a fair one; it seems you may be playing with a cheat.</p>
<p class="TXI">But these numbers are insufficient. To draw a proper conclusion, the rule says we need to combine this with more information. Specifically, we need to multiply these numbers by the probability, in general, of the die being weighted or not. </p>
<p class="TXI">Let’s say in this case, your gambling partner is your closest friend of many years. You’d put the chance that they are using a weighted die at only 1 in 100. Multiplying the probability of getting three twos when using a weighted die times the low probability that the die is weighted, we get 1/27 x 1/100 = 1/2,700 or 0.00037. Doing this for the other hypothesis – that the die is unweighted – we get 1/216 x 99/100 = 0.0045. The second number being larger than the first, you’d be fair in concluding that your friend is not, in fact, a fraud. </p>
<p class="TXI">What this example shows is the power of the <span class="italic">prior</span>. The ‘prior’ is the name given to the probability of the hypothesis – in this case the probability your friend has altered the die. Running through the same equations, but assuming you are playing with a stranger who is as likely to cheat as not (that is, the probability of cheating is 0.5), the outcome is different: 0.019 versus 0.0023 in favour of a weighted die. In this way, a strong prior can be a deciding factor. </p>
<p class="TXI">The other term – the probability of the rolls given the hypothesis – is called the ‘likelihood’. It indicates how likely you’d be to see what you’ve seen if your hypothesis <a id="page_287"></a>about the world were true. Its role in inverse probability reflects the fact that, to determine the cause of any effect, one must first know the likely effects of each cause.</p>
<p class="TXI">Both the likelihood and the prior on their own are incomplete. They represent different sources of knowledge: the evidence you have here and now versus an understanding accumulated over time. When they agree, the outcome is easy. Otherwise, they exert their influence in proportion to their certainty. In the absence of clear prior knowledge, the likelihood dominates the decision. When the pull from the prior is strong, it can make you hardly believe your own eyes. In the presence of a strong prior, extraordinary claims can only be believed with extraordinary evidence.</p>
<p class="TXI">‘When you hear hoof beats, think of horses, not zebras’ is a bit of advice frequently given to medical students. It’s meant to remind them that, of two diseases with similar symptoms, the more common one should be their first guess. It is also an excellent example of the rule of inverse probability in action. Whether you’re in the presence of a horse or of a zebra, you’ve got a similar chance of hearing hoof beats; in technical terms, the likelihoods in the two cases are the same. Given such ambiguous evidence, the decision falls into the hands of the prior and, in this case, prior knowledge says horses are more common and therefore the best guess.</p>
<p class="image-fig" id="fig22.jpg">
<img alt="" src="Images/chapter-10-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 22</span>
</span></p>
<p class="TXI">In the 200 years since the publication of his work, in papers, in textbooks and on classroom chalkboards, the equation for inverse probability that Laplace wrote down has been referred to as ‘Bayes’ rule’. Thomas Bayes was a Presbyterian minister in eighteenth-century England. Also an amateur mathematician, Bayes did <a id="page_288"></a>work on the problem of inverse probability and he was able to solve a specific version of it. But all his thoughtfulness and calculations never quite got him to the form of Bayes’ rule we know today. What’s more, Bayes himself never published the work. An essay containing his thoughts on ‘a problem in the doctrine of chances’ was eventually sent to the Royal Society by a friend of his, another minister named Richard Price, in 1763, two years after Bayes’ death. Price put substantial work into turning Bayes’ notes into a proper essay; he wrote an introduction motivating the problem and added an extensive technical appendix (unfortunately all this effort did not prevent the essay from being referred to as ‘one of the more difficult works to read in the history of statistics’<sup><a href="#fn-3" id="fnt-3">3</a>
</sup>). Despite Laplace being alive at the time Bayes’ essay was published, he did not seem to be aware of it until after he had made substantial progress on his own.</p>
<p class="TXI">It could thus be said that the Reverend Bayes doesn’t quite deserve the empire that’s been posthumously gifted to him. It’s not clear he would’ve wanted it anyway. Bayes’ rule has not always fared well among scientists and philosophers. Much like Helmholtz’s work on unconscious inference, the equation has variably been underutilised and misunderstood. This was, initially, due to the difficulty of applying it. Laplace himself was able to use the rule on some problems of measurement in astronomy and also to <a id="page_289"></a>support the long-running hypothesis that slightly more male than female babies are born on average. But, depending on the problem in question, applying Bayes’ rule could involve some complex calculus, making it an onerous approach before the modern computer was around to help. </p>
<p class="TXI">But the real struggle for Bayes’ rule came later – and ran deeper. While the validity of Laplace’s equation was unquestioned, how to <span class="italic">interpret</span> that equation occupied and divided statisticians for decades. According to philosopher of science Donald Gillies: ‘The dispute between the Bayesians and the anti-Bayesians has been one of the major intellectual controversies of the twentieth century.’ The biggest target in the crosshairs of the anti-Bayesians was the prior. Where – they wanted to know – does this information come from? In theory, it is world knowledge. In practice, it is someone’s knowledge. As the giant of twentieth-century statistics Ronald Fisher said, the assumptions that go into choosing the prior are ‘entirely arbitrary, nor has any method been put forward by which such assumptions can be made even with consistent uniqueness’. Without providing an unbiased, repeatable procedure for reaching a conclusion, Bayes’ rule was no rule at all. Because of this, the method was cast aside, branded – in a way that would be sure to scare off serious scientists – as ‘subjective’.</p>
<p class="TXI">Conceptual concerns have a habit of fading when exposed to the light of practical proof, however. And in the latter part of the twentieth century, Bayes’ rule was proving its worth. Actuaries, for example, came to realise that their rates were better calculated using the principles of inverse probability. In epidemiology, Bayes helped sort <a id="page_290"></a>out the connection between smoking and lung cancer. And in the fight against the Nazis, code-breaker Alan Turing turned to Bayesian principles to uncover messages written with the ‘unbreakable’ Enigma machine. Bayes’ rule was emerging as a tamer of uncertainty anywhere that it reared its head. Practically speaking, the prior proved only a minor problem. It could be initialised with an educated guess and updated in light of new evidence (or, barring any knowledge at all, each hypothesis is simply given an equal chance). With its repeated success in spite of an active movement against it, Bayes’ rule certainly earns the title bestowed on it by Sharon McGrayne’s book, <span class="italic">The Theory That Would Not Die</span>.</p>
<p class="center">* * *</p>
<p class="TXT">When Bayes’ rule entered psychology it was not with a bang, but with a battering. No single publication carried it in. Rather, starting with the field of decision theory in the 1960s, multiple different lines of research employed it and explored it, until eventually the idea of the brain as Bayesian bloomed at the turn of the twenty-first century. </p>
<p class="TXI">Some of the early work on Bayesian principles in the brain came from an unlikely place: space. On its mission to get space travel off the ground, the National Aeronautics and Space Administration (NASA) knew that it would have to engineer more than just flight suits and jet engines. It also investigated the ‘human factors’ of flying – such as how pilots read flight equipment, sense their environment and interact with controls. Researching this problem in 1972, aeronautical engineer <a id="page_291"></a>Renwick Curry wrote one of the earliest papers placing human perception in Bayesian terms. Specifically, he used Bayes’ rule to explain patterns in how humans perceive motion. Academic boundaries being what they are, however, few psychologists heard about it.</p>
<p class="TXI">Economics offered another way for Bayes to creep in. Ever eager to capture human behaviour in compact mathematical form, economists turned to Bayes’ rule as early as the 1980s. In ‘Are individuals Bayesian decision-makers?’, written by William Viscusi in 1985, workers were shown to either over-or underestimate the riskiness of specific jobs because they relied on their prior beliefs about how risky jobs are in general.</p>
<p class="TXI">Psychologists also spotted Bayes on the horizon via one of their former sources of inspiration. As we saw in <a href="chapter3.xhtml#chapter3">Chapter 3</a>, the study of the brain has been influenced by the field of formal logic. By the end of the twentieth century, in many ways, probability was the new logic – an improved way to assess how humans think. Instead of the harsh true-false dichotomy of Boolean logic, probability offers shades of grey. In this way, it aligns better with our own intuitions about our beliefs. As Laplace himself wrote: ‘Probability theory is nothing but common sense reduced to calculation.’ </p>
<p class="TXI">Of course, probability is a bit better than that because the rules are mathematically worked out to be the <span class="italic">best</span> form of common sense – and Bayes’ rule in particular is a prescription for how best to reason. </p>
<p class="TXI">It was on these grounds that John Anderson formally debuted a Bayesian approach to psychology, under a method he referred to as ‘rational analysis’. It was an idea that came to him in 1987 while he was in Australia, on <a id="page_292"></a>sabbatical from his job as a professor of psychology and computer science at Carnegie Mellon University. Rational analysis, according to Anderson, stems from the belief that ‘there is a reason for the way the mind is’. Specifically, it posits that an understanding of how the mind works will best grow out of an understanding of where it came from. When it comes to Bayes, the reasoning starts with the fact that humans live in a messy, uncertain world. Yet – Anderson argues – humans have evolved within this world to behave as rationally as possible. Bayes’ rule is a description of how to reason rationally under conditions of uncertainty. Therefore, humans should be using Bayes’ rule. Put simply, if evolution has done its job, we should see Bayes’ rule in the brain.</p>
<p class="TXI">The details of just how the rule will be applied and to what problems depend on more specific features of the environment. As an example, Anderson offers a Bayesian theory of memory recall. It says that the probability of a particular memory being useful in a particular situation is found by combining: 1) how likely you’d be to find yourself in that situation if that memory were useful; with 2) a prior that assumes more recent memories are more likely to be useful. This choice of prior is meant to reflect the fact that humans come from a world where information has a shelf life; therefore, more recent memories are more likely to be of value. </p>
<p class="TXI">Importantly, under the rational analysis framework, ‘rational’ can be far from perfect. Memory, for example, certainly can fail us. But, according to this viewpoint, if we forget a fact from primary school 20 years on, we are not being irrational. Given the limited capacity of <a id="page_293"></a>memory and the ever-changing world in which we live, it makes perfect sense to let old and little-used information go. In this way, the prior in a Bayesian model can be thought of as storing a shortcut. It’s an encoding of the basic stats of the world that can make decision-making faster, easier and – in most cases – more accurate. If, however, we find ourselves in a world that deviates from the one we’ve evolved and developed in, our priors can be misleading. ‘Think of horses’ is only good advice in a place with more horses than zebras.</p>
<p class="center">* * *</p>
<p class="TXT">In early 1993, a group of researchers met at the Chatham Bars Inn in Chatham, Massachusetts. The group included psychologists David Knill (a professor at the University of Pennsylvania who served as organiser) and Whitman Richards (a professor at MIT who was part of the first crop of PhD students in the Department of Psychology there in the 1960s). Also at the meeting were scientists trained in physiology and neuroscience like Heinrich Bülthoff, whose work was on the visual system of fruit flies; as well as engineers and mathematicians such as Alan Yuille, a student of Stephen Hawking.</p>
<p class="TXI">On the agenda for this eclectic bunch was a hunt for a new formal theory of perception – ideally one that could capture the complexities of the senses while also offering new, testable hypotheses. A complexity of particular concern was how the senses appear to be affected by more than just what meets the eye, ear or nose. That is, incoming sensory information combines with a rich set of background knowledge before <a id="page_294"></a>perception is complete. According to Knill, no theory at the time was able to say precisely ‘how prior knowledge should be brought to bear upon the interpretation of sensory data’.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p>
<p class="TXI">The meeting birthed a book, published in 1996, the title of which reveals the solution the attendees settled on: <span class="italic">Perception as Bayesian Inference</span>. The seeds for this idea had, as we’ve seen, been scattered around for some time, growing in different ways in different fields. This was an opportunity to pull them together. The book presents a unified and clear approach to the Bayesian study of perception, focusing mainly on the sense of vision. Its success spawned countless research papers in the years that followed. If Anderson’s work on ‘rational analysis’ put Bayes on psychology’s map, this book gave it its own country.</p>
<p class="TXI">To understand the basics of Bayesian perception, consider an example. Light reflects off a flower and hits the eye. The wavelength of the light is around 670 nanometres (nm). It’s the task of the brain to figure out, given the wavelength it’s receiving, what the ‘thing-in-itself’ is, or what is really going on in the world. In Bayesian terms, this would be the probability of the hypothesis that a certain flower is present given that a wavelength of 670nm is hitting the eye. </p>
<p class="TXI">Bayes’ rule tells us what to do. First, we need to find out how likely we are to see that wavelength under different conditions. The likelihood of seeing 670nm light if the flower is blue and illuminated by white light <a id="page_295"></a>is very low (blue light falls between 450 and 480nm). The likelihood of seeing 670nm light if the flower is red and illuminated by white light is quite high; 670nm is right in the middle of the red spectrum. However, the probability of seeing 670nm light if the flower is <span class="italic">white</span> and illuminated by <span class="italic">red</span> light is also quite high. Since both of these scenarios are just as likely to produce 670nm light, if we stop here, we may be quite unsure of which one is the better interpretation.</p>
<p class="TXI">But as good Bayesians, we remember the importance of the prior. The probability of a world illuminated by red light is, by most measures, quite small. White light, however, is a very common sight. The scenarios above that assume white light are thus much more probable. Multiplying the prior probability of different scenarios times the probability of seeing 670nm light in that scenario, we see that only one scores high on both of these measures. We therefore conclude that in front of us there is a red flower, illuminated by regular white light. </p>
<p class="TXI">Of course, ‘we’ don’t actually conclude that. This process, just as Helmholtz anticipated, happens unconsciously. The odds are weighed out of our sight and we know only the end result. It is, in this way, a never-ending procedure to produce perception – an underground production line in the mind. At each moment, probabilities are calculated and compared, each perception a bit of computation according to Bayes’ rule. </p>
<p class="TXI">With all the work that goes into perception, it’s no surprise that the brain can sometimes come up with odd results. In 2002, a team of researchers out of the US and Israel catalogued a series of common illusions that people fall prey to when trying to estimate the <a id="page_296"></a>movement of an object. It included the fact that the shape of an object influences the direction we think it is moving in, that two items moving in different directions may appear as one and that dimmer objects appear to move more slowly. </p>
<p class="TXI">This may seem simply like a list of our failings, but the researchers found that all of these lapses could be explained by a simple Bayesian model. Particularly, these habits fall out of the calculation if we assume a specific prior: that motion is more likely to be slow than fast. Take, for example, the last illusion. When an object is hard to see, the evidence it provides about its movement is weak. In the absence of evidence, Bayes’ rule relies on the prior – and the prior says things move slowly. This bit of mathematics may explain why drivers have a tendency to speed up in the fog – with weak information about their own movement, they assume their speed is too slow. Importantly, the Bayesian approach recasts these tricks of the mind as traits of a rational calculation. It shows how some mistakes are actually reasonable guesses in an uncertain world. </p>
<p class="TXI">There is, however, another part to the process of perception. So far, we’ve simply assumed that the percept we experience should be the one with the highest probability. That’s a reasonable choice, but it is a choice nonetheless, and a different one could be made.</p>
<p class="image-fig" id="fig23.jpg">
<img alt="" src="Images/chapter-10-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 23</span>
</span></p>
<p class="TXI">Consider the Necker cube. This famous illusion (see Figure 23) admits more than one interpretation: the lower square could be seen as coming forwards, like a box faced slightly downwards, or it could be behind the plane of the page, suggesting a box tilted upwards. Both boxes are equally likely to produce this pattern of lines, so the <a id="page_297"></a>decision about which is the true state would be strongly influenced by the prior. Let’s say, downwards-tilted boxes are a bit more likely in general. So, after applying Bayes’ rule, the probability of there being a downwards box when we see these lines is 0.51 and an upwards box 0.49. Taking the standard approach to mapping this to perception, we’d say the larger of the two probabilities wins – we see it as a downwards box, end of story. </p>
<p class="TXI">On the other hand, rather than choosing one interpretation and sticking to it, the brain could choose to alternate between the two. The box could appear downwards at one moment and upwards the next, switching back and forth repeatedly. In this case, the probabilities tell us not which interpretation to stick with, but rather <span class="italic">the amount of time</span> to spend in each. </p>
<p class="TXI">This switching is exactly what researchers at the University of Rochester (including David Knill) saw in an experiment in 2011. The experimenters overlaid two visual patterns such that it was unclear whether the first was on top of the second or vice versa – that is, the image could be interpreted two different ways. Asking people to indicate when their perception of the image <a id="page_298"></a>switched from one view to the other, they could determine the amount of time spent seeing each. Assuming that either pattern was equally likely to be on top (that is, the prior probabilities are the same), Bayes’ rule says people should spend 50 per cent of their time seeing it each way. And this is exactly what they found. But to really test the predictive powers of Bayes’ rule, the scientists needed to move away from the 50-50 scenario. To do this, they manipulated the image to make one pattern appear slightly more on top than the other. This altered the likelihood – that is, the probability of seeing this image given that one or the other pattern truly was on top. The more they altered the image in this direction, the more time viewers spent seeing the preferred pattern on top – exactly in accordance with Bayes’ rule. </p>
<p class="TXI">As this study shows, a set of probabilities can be mapped to a perception in interesting ways – a mapping known by scientists as the decision function. Bayes’ rule itself doesn’t tell us what decision function to use; it only provides the probabilities. Perception could be collapsed to the interpretation with the highest probability, or it could not. Perception could be a sampling from different interpretations over time in accordance with their probabilities, or it could not. Overall, perception could be the result of any complex combination of probabilities. The output of Bayes’ rule therefore provides a rich representation of sensory information, one that the brain can use in any way that seems most reasonable. In this way, probabilities mean possibilities. </p>
<p class="TXI">Another benefit of thinking of the mind as dealing in probabilities is that it opens the door to quantifying a potentially elusive concept: confidence. Confidence is <a id="page_299"></a>intuitively tied to evidence and certainty. When walking around in a dark room, where visual evidence is weak, we move slowly because we aren’t confident we won’t bump into a wall or table. In a brightly lit room, however, the strong influence of the clear visual evidence removes such doubts. The Bayesian confidence hypothesis formalises this intuition by saying that how confident a person is in their interpretation of the world is directly related to the probability of that interpretation given the evidence – that is, the output of Bayes’ rule. In the dark room where evidence is low, so is the probability of any given interpretation of the room and, therefore, so is confidence. </p>
<p class="TXI">Researchers from the UK tested just how well this Bayesian hypothesis matches the data in 2015. To do so, they asked people to look for a particular pattern in two different images that were quickly flashed one after the other. The subjects then reported which of the two images had the pattern and, importantly, how confident they were in their decision. The decisions and confidence of the humans were compared to the predictions of the Bayesian model and to predictions from two simpler mathematical models. Bayes’ rule provided the better match for the majority of the data, supporting the Bayesian confidence hypothesis. </p>
<p class="center">* * *</p>
<p class="TXT">‘In the laboratory, we like to simplify the enormous task of understanding how the brain works,’ said Dora Angelaki in an interview in 2014. ‘Traditionally, neuroscience has studied one sensory system at a time. But in the real world, this is not what happens.’</p> <a id="page_300"></a>
<p class="TXI">Angelaki, originally from Crete, is a professor of Neuroscience at New York University. She credits her background in electrical engineering for her desire to seek out the underlying principles of how things work. As part of her research, she is correcting neuroscience’s bias towards simplicity by studying how the senses interact.</p>
<p class="TXI">The particular senses Angelaki seeks to combine are visual and vestibular. The vestibular system provides the little-known sixth sense of balance. Tucked deep in the ear, it is composed of a set of tiny tubes and stone-filled sacs. Through the sloshing of fluids in the tubes and the movement of the stones, the system offers – like the liquid in a level – a measurement of the head’s tilt and acceleration. This system works in tandem with the visual system to provide an overall sense of place, orientation and movement. When these two systems are out of whack, unpleasant sensations like motion sickness can occur. </p>
<p class="TXI">In her effort to understand the vestibular system, Angelaki has borrowed methods from an uncommon source: pilot training. Subjects in her experiments are strapped into a seat on a moving platform like the kind used in flight simulators. The platform can give them brief bursts of acceleration in different directions. At the same time a screen in front of them gives a visual sense of movement in the form of dots of light flowing past them – a visual not unlike that of jumping to lightspeed in <span class="italic">Star Wars</span>. While pilot training generally keeps the physical and visual motion aligned, Angelaki uses this set-up to see what the brain does when they disagree.</p>
<p class="TXI">Bayes’ rule offers a guess about that. Treating the visual and vestibular inputs as separate streams of evidence <a id="page_301"></a>about the same external world, the mathematics of probability provides a simple means for how to combine them. Instead of a single likelihood term – like in the standard Bayes’ rule – the two likelihoods (one from each sense) are multiplied together. Let’s say your task is to determine if you’re moving to the left or to the right. To calculate the probability that you’re actually moving to the right – given some vestibular and visual inputs – you’d multiply the likelihood that you’d see that visual input if you were moving to the right <span class="italic">times</span> the likelihood that you’d receive that vestibular input if you were moving to the right. To complete the process, this value then goes on to be multiplied by the prior probability of rightwards movement. The same can be done for leftwards movement, and the two are compared.</p>
<p class="TXI">Just as a rumour transforms into a fact as you hear it from many different people, in Bayes’ rule getting the same information from multiple senses strengthens the belief in that information. When the moving platform and the display screen are both consistent with rightwards movement, both visual and vestibular likelihoods will be high and thus so will the outcome of their multiplication. This contributes to a confident conclusion of rightwards movement. If they’re put at odds – the platform moves right while the dots say left – then the vestibular likelihood would still say the probability of rightwards movement is high, but the visual one would say it’s low. Multiplying these leads to a middling result and only moderate confidence one way or the other. </p>
<p class="TXI">But, as with rumours, the reliability of the source matters. In her experiments, Angelaki can reduce the confidence subjects have in one or the other sensory <a id="page_302"></a>inputs. To make the visual inputs less reliable, she simply makes them messier. That is, rather than all the dots moving together to give a strong sense of directed motion, some dots move randomly. The more dots that are random, the less reliable the visual information becomes. </p>
<p class="TXI">Looking at how that plays out with probabilities, we see that Bayes’ rule naturally titrates how much to rely on a source based on how reliable it is. If the dots were moving completely randomly, the visual input would provide no information about movement direction. In this case, the likelihood of the visual input given rightwards movement would be equal to the likelihood of it given leftwards movement. With equal likelihoods on both sides, the visual input wouldn’t weigh the decision either way. The tiebreaker would come from the vestibular inputs (and the prior). If, instead, 90 per cent of the dots moved randomly and 10 per cent indicated rightwards movement, then the likelihood of the visual input in support of rightwards movement would be slightly higher than that of leftwards. Now the visual input does weigh the decision – but only slightly. As the visual input becomes more reliable, its say in the decision grows. In this way, Bayes’ rule automatically puts more stock in a source in proportion to how certain it is.</p>
<p class="TXI">Investigating the conclusions people come to about their movement in these experiments, Angelaki and her lab have shown once again that, for the most part, humans behave as good Bayesians. When the visual evidence is weak, they depend more on their vestibular system. One caveat is that, while they use the visual information more as it becomes more reliable, they still don’t use it quite as much as Bayes’ rule would predict. That is, the vestibular input is consistently <a id="page_303"></a>overemphasised – an effect found in monkeys as well. This could be a result of the fact the visual input is always a bit ambiguous: seeing dots moving past could be an effect of your own movement, or it could just be the dots moving. So, the vestibular input is, in general, a more trustworthy source and therefore worthy of more weight. </p>
<p class="center">* * *</p>
<p class="TXT">Once the Bayesian approach to perception was unleashed, it quickly spread to all corners of psychology. Like a Magic Eye illusion, staring at any data long enough can make the structure of Bayes’ rule pop out of it. As a result, priors and likelihoods abound in the study of the mind. </p>
<p class="TXI">As we’ve already seen, Bayes’ rule has been evoked to explain motion perception, the switching of ambiguous illusions like the Necker cube, confidence and the combination of vision and vestibular inputs. It’s also been adapted to account for how we can be tricked by ventriloquists, our sense of the passing of time and our ability to spot anomalies. It can even be stretched and expanded to cover such tasks as motor skill learning, language understanding and our ability to generalise. Such a unifying framework to describe so much of mental activity seems an unmitigated success. Indeed, according to philosopher of mind Michael Rescorla, the Bayesian approach is ‘our best current science of perception’. </p>
<p class="TXI">Yet, not all psychologists can be counted as devoted disciples of the Reverend Bayes. </p>
<p class="TXI">To some, a theory that explains everything is at risk of explaining nothing at all. The flip side of the flexibility <a id="page_304"></a>of the Bayesian approach is that it can also be accused of having too many <span class="italic">free parameters</span>. The free parameters of a model are all its movable parts – all the choices that the researcher can make when using it. The same way that, if given enough strokes even the worst golfer could eventually get the ball in the cup, if given enough free parameters any model can fit any data. If a finding from a new experiment conflicts with an old one, for example, an over-parameterised model can easily twist its way around to encompass them both. If getting the model to fit the data is as easy as a bank making change, its success isn’t very interesting. A model that can say anything can never be wrong. As psychologists Jeffrey Bowers and Colin Davis wrote in their 2012 critique of the Bayesian approach: ‘This ability to describe the data accurately comes at the cost of falsifiability.’</p>
<p class="TXI">There are indeed many ways to squeeze parts of perception into a Bayesian package. Take, for example, the likelihood calculation. Computing a quantity like ‘the likelihood of seeing 670nm light given the presence of a red flower’ requires some knowledge and assumptions about how light reflects off different materials and how the eye absorbs it. Without a perfect understanding of the physical world, the model-maker must put in some of their own assumptions here. They could, therefore, wiggle these assumptions around a bit to match the data. Another source of choice is the decision function. As we saw earlier, the output of Bayes’ rule can be mapped to the perception and decision of an animal any number of ways. This option, too, has the power to make any action look Bayesian in theory. And then, of course, there are those pesky priors.</p><a id="page_305"></a>
<p class="TXI">Just as they gave pause to statisticians in the twentieth century, priors have proven a challenge to psychologists of the twenty-first. If the assumption of a certain prior – for example, that motion is likely to be slow – helps explain a psychological phenomena, that can be taken as good evidence that the brain really uses that prior. But what if a different phenomena is best explained by a different prior – say, one that assumes motion is fast? Should it be assumed that the priors in our mind are constant across time and task? Or are they flexible and fluid? And how can we know?</p>
<p class="TXI">As a result of these concerns, some researchers have embarked on an exploration into the properties of priors. French cognitive scientist Pascal Mamassian has worked to investigate a particularly common one: the assumption that light comes from above. For more than two centuries, experiments and illusions have found that humans keep this implicit belief about the source of illumination in mind as they make sense of shadows in a scene. It’s a sensible guess, given the location of our dominant light source, the sun. More recently, experiments have revised this finding slightly and found that humans actually assume that light comes from above <span class="italic">and slightly to the left</span>. Mamassian conducted tests revealing this bias in the lab, but he also found a more creative way to interrogate it. Analysing 659 paintings from the Louvre museum in Paris, he found that in a full 84 per cent of portraits and 67 per cent of non-portrait paintings, the light source was indeed biased towards the left side. Artists may have come to prefer this setting exactly because it aligns with our intuitions, creating a more pleasant and interpretable painting.</p><a id="page_306"></a>
<p class="TXI">Another open question about priors is their origin. Priors can serve as an efficient way to imprint facts about the world on our minds; but are these facts gifted to us from previous generations through our genes, or do we develop them in our own lifetimes? To test this, a study in 1970 raised chickens in an environment wherein all light came from below. If the prior assumption that light comes from overhead is learned in their lifetimes, these birds wouldn’t have it. How the animals interacted with visual stimuli, however, showed they did still assume light should be from above. This points in favour of an inherited prior. </p>
<p class="TXI">Humans, of course, aren’t chickens, and the development of our nervous system may allow for a bit more flexibility. Investigating the prior biases of children of various ages, psychologist James Stone found in 2010 that children as young as four did show a bias towards assuming overhead light, though it was weaker than that in adults. This bias grows steadily over the years to reach adult strength, suggesting that a partially innate prior may be fine-tuned by experience. Further in support of this flexibility, a team from the UK and Germany showed in 2004 that our grip on where light must be coming from can be loosened. Through training, participants were able to shift their prior beliefs about the source of the light by several degrees. </p>
<p class="TXI">Picking a particular prior and poking at it from different directions through a multitude of experiments helps verify it as a resilient and reliable effect. Each prior this is done for becomes less of a free parameter in the model and more fixed.</p> <a id="page_307"></a>
<p class="TXI">Another question supporters of the Bayesian brain hypothesis need to address is ‘how?’</p>
<p class="TXI">While there are reasons to believe the brain <span class="italic">should</span> use Bayes’ rule, and there is evidence that it <span class="italic">does</span>, the question of how this plays out in neurons remains a lively area of research.</p>
<p class="TXI">When it comes to priors, scientists are looking for what cupboard in the brain stores these bits of background knowledge and how they get mixed into the neural decision-making process. One hypothesis is that it’s a simple numbers game. If a group of neurons are charged with representing something about the world – say, where in the environment a sound is coming from – each neuron may have its own preferred location. This means it responds the most when sound is coming from there. If the brain determines where the sound is by adding up the activity of all the neurons that prefer the same location, locations with more neurons will have an advantage. So, if the prior says that sound is more likely to come from central locations than the periphery, this can be implemented by simply increasing the number of neurons that prefer the centre. As it turns out, neuroscientists Brian Fischer and Jose Luis Peña found this exact scheme in the brains of owls in 2011. Identifying neural signatures of priors this way can give insights to where they come from and how they work.</p>
<p class="TXI">Theorists are building – and experimentalists are testing – many more hypotheses about how Bayes’ rule plays out in the brain. There are a multitude of ways that neurons can conspire to combine likelihoods and priors. <a id="page_308"></a>These different hypotheses should not be considered in competition, nor should any single winner expect to be crowned at the end. Rather, while Bayes’ rule can be one-size-fits-all for capturing the outputs of perception, the physical underpinnings of this rule may come in many different shapes and styles.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-1" id="fn-1">1</a> ﻿In this way Helmholtz deviated from Kant, who believed that much of this world knowledge was innate rather than learned.﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-2" id="fn-2">2</a> ﻿Speaking about probability in terms of cause and effect was not uncommon at the time. But it is not, in general, a wise thing to do. The probability that you﻿’﻿ll be holding an umbrella given that the person next to you on the street is may be high, but the one is not the cause of the other. ﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-3" id="fn-3">3</a> ﻿The statistician and historian responsible for this assessment, Stephen Stigler, is also known for ﻿‘﻿Stigler﻿’﻿s law of eponymy﻿’﻿, which claims that a scientific law is never named after its true originator. The sociologist Robert Merton is believed to be the originator of this law. ﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-4" id="fn-4">4</a> ﻿Some of the existing theories of the time that failed to do this include the models of the visual system discussed in ﻿﻿Chapter 6﻿﻿.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 3</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter3"><a href="contents.xhtml#re_chapter3">CHAPTER THREE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter3">Learning to Compute</a><a id="page_49"></a></p>
<p class="H1" id="b-9781472966445-ch222-sec3">
<span class="bold">
<span>McCulloch-Pitts, the Perceptron and 
artificial neural networks</span>
</span></p>
<p class="TXT">Cambridge University mathematician Bertrand Russell spent 10 years at the beginning of the twentieth century toiling towards a monumental goal: to identify the philosophical roots from which all of mathematics stems. Undertaken in collaboration with his former teacher Alfred Whitehead, this ambitious project produced a book, the <span class="italic">Principia Mathematica</span>, which was delivered to the publishers overdue and over budget. The authors themselves had to chip in towards the publishing costs just to get it done and they didn’t see any royalties for 40 years. </p>
<p class="TXI">But the financial hurdle was perhaps the smallest one to overcome in getting this opus finished. Russell had to fight against his own agitation with the scholarly material. According to his autobiography, he spent days staring at a blank sheet of paper and evenings contemplating jumping in front of a train. Work on the book also coincided with the dissolution of Russell’s marriage and a strain on his relationship with Whitehead – who, according to Russell, was fighting his own mental and marital battles at the time. The book was even physically demanding: Russell spent 12 hours a day at a desk writing out the intricate symbolism needed <a id="page_50"></a>to convey his complex mathematical ideas and when the time came to bring the manuscript to the publisher it was too large for him to carry. Despite it all, Russell and Whitehead eventually completed and published the text they hoped would tame the seemingly wild state of mathematics. </p>
<p class="TXI">The conceit of the <span class="italic">Principia</span> was that all of mathematics could be reduced to logic. In other words, Russell and Whitehead believed that a handful of basic statements, known as ‘expressions’, could be combined in just the right way to generate all the formalisms, claims and findings of mathematicians. These expressions didn’t stem from any observations of the real world. Rather, they were meant to be universal. For example, the expression: if X is true, then the statement ‘X is true or Y is true’ is also true. Such expressions are made up of propositions: fundamental units of logic that can be either true or false, written as letters like X or Y. These propositions are strung together with ‘Boolean’ operators<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> such as ‘and’, ‘or’ and ‘not’. </p>
<p class="TXI">In the first volume of the <span class="italic">Principia</span>, Russell and Whitehead provided fewer than two dozen of these abstract expressions. From these humble seeds, they built mathematics. They were even able to triumphantly conclude – after scores of symbol-filled pages – that 1+1=2. </p>
<p class="TXI">Russell and Whitehead’s demonstration that the full grandeur of mathematics could be captured with the simple <a id="page_51"></a>rules of logic<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> had immense philosophical implications as it provided proof of the power of logic. What’s more, it meant that a subsequent finding, made by a different pair of men some 30 years later, would have immense implications of its own. This finding said that neurons, simply via the nature of their anatomy and physiology, were implementing the rules of logic. It revolutionised the study of the brain and of intelligence itself. </p>
<p class="center">* * *</p>
<p class="TXT">When Detroit native Walter Pitts was just 12 years old, he was invited by Russell to join him as a graduate student at Cambridge University. The young boy had, the story goes, encountered a copy of the <span class="italic">Principia</span> after running into a library to avoid bullies. As he read, Pitts found what he believed to be errors in the work. So, he sent his notes on the subject off to Russell, who, presumably not knowing the boy’s age, then offered him the position. Pitts didn’t accept it. But a few years later, when Russell was visiting the University of Chicago, Pitts went to sit in on his lectures. Having fled an abusive family home to come to Chicago, Pitts decided not to return. He remained in the city, homeless. </p>
<p class="TXI">Luckily, the University of Chicago had another world-famous logician for Pitts to criticise – Rudolf Carnap. Again, Pitts wrote up notes – this time identifying issues in Carnap’s recent book <span class="italic">The Logical Syntax of Language</span> – and delivered them to Carnap’s office at the University of Chicago. Pitts didn’t stick around long enough to hear his <a id="page_52"></a>reaction, but Carnap, impressed, eventually chased down Pitts, whom he referred to as ‘that newsboy who understood logic’. On this occasion, the philosopher he critiqued actually did get Pitts to work with him. Though he never officially enrolled, Pitts functioned effectively as a graduate student for Carnap and fraternised with a group of scholars who were interested in the mathematics of biology.</p>
<p class="TXI">Warren McCulloch’s interest in philosophy took a more traditional form. Born in New Jersey, he studied the subject (along with psychology) at Yale and read many of the greats. He was most enamoured with Immanuel Kant and Gottfried Leibniz (whose ideas were very influential for Russell), and he read the <span class="italic">Principia</span> at the age of 25. But, despite the beard upon his long face, McCulloch was not a philosopher – he was a physiologist. He attended medical school in Manhattan and then went on to observe the panoply of ways in which the brain can break as a neurology intern at Bellevue Hospital and at the Rockland State Hospital psychiatric facility. In 1941, he joined the University of Illinois at Chicago as the director of the laboratory for basic research in the department of psychiatry. </p>
<p class="TXI">As with all great origin stories, there are conflicting accounts of how McCulloch and Pitts met. One claims that it happened when McCulloch spoke in front of a research group Pitts was a part of. Another story is that Carnap introduced them. Finally, a contemporary of the two men, Jerome Lettvin, claimed he introduced them and that all three bonded over a mutual love of Leibniz. In any case, by 1942, the 43-year-old McCulloch and his wife had taken the 18-year-old Pitts into their home, <a id="page_53"></a>and the two men were spending evenings drinking whisky and discussing logic. </p>
<p class="TXI">The wall between ‘mind’ and ‘body’ was strong among scientists in the early twentieth century. The mind was considered internal and intangible; the body, including the brain, was physical. Researchers on either side of this wall toiled diligently, but separately, at their own problems. Biologists, as we saw in the last chapter, were working hard to uncover the physical machinery of neurons: using pipettes and electrodes and chemicals to sort out what causes a spike and how. Psychiatrists, on the other hand, were attempting to uncover the machinery of the mind through lengthy sessions of Freudian psychoanalysis. Few on either side would attempt a glance over the wall at the other. They spoke separate languages and worked towards different goals. For most practitioners, the question of how neural building blocks could create the structure of the mind was not just unanswered, it was unasked. </p>
<p class="TXI">But McCulloch, from as early on as his time at medical school, had immersed himself in a crowd of scientists who did care about this question and allowed him the space to think about it. Eventually, through his physiological observations, he came up with a hunch. He saw in the emerging concepts of neuroscience a possible mapping to the notions of logic and computation he so adored in philosophy. To think of the brain as a computing device following the rules of logic – rather than just a bag of proteins and chemicals – would open the door to understanding thought in terms of neural activity.</p> <a id="page_54"></a>
<p class="TXI">Analytical skill, however, was not where McCulloch excelled. Some who knew him say he was too much of a romantic to be held down by such details. So, despite years of toying with these ideas in his mind and in conversation (even as a Bellevue intern he was accused of ‘trying to write an equation for the working of the brain’), McCulloch struggled with several technical issues of how to enact them. Pitts, however, was comparably unfazed by the analytical. As soon as he spoke with him about it, Pitts saw what approaches were needed to formally realise McCulloch’s intuitions. Not long after they met, one of the most influential papers on computation was written.</p>
<p class="TXI">‘A logical calculus of the ideas immanent in nervous activity’ was published in 1943. The paper is 17 pages long with many equations, only three references (one of which is to the <span class="italic">Principia</span>) and a single figure consisting of little neural circuits drawn by McCulloch’s daughter.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> </p>
<p class="TXI">The paper begins by reviewing the biology of neurons that was known at the time: neurons have cell bodies and axons; two neurons connect when the axon of the first meets the body of the second; through this connection one neuron provides input to the other; a certain amount of input is needed for a neuron to fire; a cell either fires a spike or it doesn’t – no half spikes or in-between spikes; and the input from some neurons – inhibitory neurons – has the power to prevent a cell from spiking.</p><a id="page_55"></a>
<p class="TXI">McCulloch and Pitts go on to explain how these biological details are congruent with Boolean logic. The core of their claim is that the activity state of each neuron – either firing or not – is like the truth value of a proposition – either true or false. In their own words, they ‘conceive of the response of any neuron as factually equivalent to a proposition which proposed its adequate stimulus’. </p>
<p class="TXI">By ‘its adequate stimulus’ they are referring to something about the world. Imagine a neuron in the visual cortex whose activity represents the statement ‘the current visual stimulus looks like a duck’. If that neuron is firing, that statement is true; if the neuron is not firing, it is false. Now imagine another neuron, in the auditory cortex, that represents the statement ‘the current auditory stimulus is quacking like a duck’. Again, if this neuron is firing, that statement is true, otherwise it is false. </p>
<p class="TXI">Now we can use the connections between neurons to enact Boolean operations. For example, by giving a third neuron inputs from both of these neurons, we could implement the rule ‘if it looks like a duck <span class="italic">and</span> it quacks like a duck, it’s a duck’. All we have to do is build the third neuron such that it will only fire if both of its input neurons are firing. That way, both ‘looks like a duck’ and ‘quacks like a duck’ have to be true in order for the conclusion represented by the third neuron (‘it’s a duck’) to be true. </p>
<p class="TXI">This describes the simple circuit needed to implement the Boolean operation ‘and’. McCulloch and Pitts in their paper show how to implement many others. To implement ‘or’ is very similar, however the strength of the connections from each neuron must be so strong <a id="page_56"></a>that one input alone is enough to make the output neuron fire. In this case, the ‘is a duck’ neuron would fire if the ‘looks like a duck’ neuron <span class="italic">or</span> the ‘quacks like a duck’ neuron (or both) were firing. The authors even show how to string together multiple Boolean operations. For example, to implement a statement like ‘X and not Y’, the neuron representing X connects to an output neuron with a strength enough to make it fire. But the neuron representing Y <span class="italic">inhibits</span> the output neuron, meaning it prevents it from firing. This way, the output neuron will only fire if the X-representing neuron <span class="italic">is</span> firing and the Y-representing neuron is <span class="italic">not</span> (see Figure 4). </p>
<p class="TXI">These circuits, which are meant to represent what networks of real neurons can do, became known as <span class="italic">artificial</span> neural networks. </p>
<p class="image-fig" id="fig4.jpg">
<img alt="" src="Images/chapter-01-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 4</span>
</span></p>
<p class="TXI">The ability to spot logic at play in the interactions of neurons came from McCulloch’s discerning eye. As a physiologist, he knew that neurons were more complex than his simple drawings and equations suggested. They had membranes, ion channels and forking paths of <a id="page_57"></a>dendrites. But the theory didn’t need their full complexity. So, like an impressionist painter using only the necessary strokes, he intentionally highlighted only the elements of neural activity required for the story he wanted to tell. In doing so, he demonstrated the artistry inherent to model-building; it is a subjective and creative process to decide which facts belong in the foreground. </p>
<p class="TXI">The radical story that McCulloch and Pitts told with their model – that neurons were performing a logical calculus – was the first attempt to use the principles of computation to turn the mind–body problem into a mind–body connection. Networks of neurons were now imbued with all the power of a formal logical system. Like a chain of falling dominoes, once certain truth values entered into a neural population (say, via sensory organs), a cascade of interactions could deduce the truth value of new and different statements. This meant a population of neurons could carry out endless computations: interpreting sensory inputs, developing conclusions, forming plans, reasoning through arguments, performing calculations and so on. </p>
<p class="TXI">With this step in their research, McCulloch and Pitts advanced the study of human thought and, at the same time, kicked it off its throne. The ‘mind’ lost its status as mysterious and ethereal once it was brought down to solid ground – that is, once its grand abilities were reduced to the firing of neurons. To adapt a quote from Lettvin, the brain could now be thought of as ‘a machine, meaty and miraculous, but still a machine’. More boldly still, McCulloch’s student Michael Arbib later remarked that the work ‘killed dualism’.</p><a id="page_58"></a>
<p class="TXI">Russell was known to lament that, despite the 20 years put into it and the impact it had on logicians and philosophers, the <span class="italic">Principia</span> had little effect on practising mathematicians. Its new take on the foundations of mathematics simply didn’t seem to mean much to those doing mathematics; it didn’t change their day-to-day work. The same could be said of McCulloch and Pitts’ discovery for neuroscientists of the time. Biologists, physiologists, anatomists – the scientists doing the labour of physically mining neurons for the details of their workings – didn’t take much from the theory. This was in part because it wasn’t obvious what experiments should follow from it. But it may also have stemmed from the very technical notation in the paper and its less-than-inviting writing style. In a review on nerve conduction written three years later, the author refers to the McCulloch-Pitts paper as ‘not for the lay reader’ and remarks that if this style of work is to be useful, it’s necessary for ‘physiologists to familiarise themselves with mathematical technology, or for mathematicians to elaborate at least their conclusions in a less formidable language’. The wall between mind and body may have come down, but the one between biologist and mathematician stood strong. </p>
<p class="TXI">There was a separate group of people – a group with the requisite technical know-how – who did take an interest in the logical calculus of neurons. In the post-war era, a series of meetings hosted by the philanthropic Macy Foundation brought together biologists and technologists, many of whom wished to use biological findings to build brain-like machines. McCulloch was an organiser of these meetings, and fellow attendees <a id="page_59"></a>included the ‘father of cybernetics’ Norbert Wiener and John von Neumann, the inventor of the modern computer architecture, who was directly inspired in its design by the McCulloch-Pitts neurons. As Lettvin described it 40 years later: ‘The whole field of neurology and neurobiology ignored the structure, the message and the form of McCulloch and Pitts’ theory. Instead, those who were inspired by it were those who were destined to become the aficionados of a new venture, now called Artificial Intelligence.’</p>
<p class="center">* * *</p>
<p class="EXTF">The <span class="italic">Navy last week demonstrated the embryo of an electronic computer named the Perceptron which, when completed in about a year, is expected to be the first non-living mechanism able to 
‘perceive, recognise, and identify its surroundings without human training or control.’ [...]</span> </p>
<p class="EXT-L">
<span class="italic">“‘Dr. Frank Rosenblatt, research psychologist at the Cornell Aeronautical Laboratory, Inc., Buffalo, NY, designer of the ­Perceptron, conducted the demonstration. The machine, he said, would be the first electronic device to think as the human brain. Like humans, Perceptron will make mistakes at first, ‘but it will grow wiser as it gains experience’, he said.</span></p>
<p class="TXT">This summary, from an article entitled ‘Electronic “brain” teaches itself’, appeared in the 13 July 1958 edition of the <span class="italic">New York Times</span>, opposite a letter to the editor about the ongoing debate on whether smoking causes cancer. Frank Rosenblatt, the 30-year-old architect of the project, was reaching beyond his training in experimental psychology to build a computer meant to rival the most advanced technology at the time.</p><a id="page_60"></a>
<p class="TXI">The computer in question was taller than the engineers who operated it and about twice as long. It was covered on either end in various control panels and readout mechanisms. Rosenblatt requested the services of three ‘professional people’ and an associated technical staff for 18 months to build it, and the estimated cost was $100,000 (around $870,000 today). The word ‘perceptron’, defined by Rosenblatt, is a generic term for a certain class of devices that can ‘recognise similarities or identities between patterns of optical, electrical or tonal information’. The Perceptron – the computer that was built in 1958 – was thus technically a subclass known as a ‘photoperceptron’ because it took as its input the output of a camera mounted on a tripod at one end of the machine. </p>
<p class="TXI">The Perceptron was, just like the models introduced in the McCulloch-Pitts paper, an artificial neural network. It was a simplified replica of what real neurons do and how they connect to each other. But rather than remaining a mathematical construct that exists only as the ink of equations on a page, the Perceptron was physically realised. The camera provided 400 inputs to this network in the form of a 20x20 grid of light sensors. Wires then randomly connected the output of these sensors to 1,000 ‘association units’ – small electrical circuits that summed up their inputs and switched to ‘on’ or ‘off’ as a result, just like a neuron. The output of these association units became the input to the ‘response units’, which themselves could be ‘on’ or ‘off’. The number of response units was equal to the number of mutually exclusive categories to which an image could belong. So, if the Navy wanted to use the Perceptron, say, to <a id="page_61"></a>determine if a jet was present in an image or not, there would be two response units: one for jet and one for no jet. At the end of the machine opposite the camera was a set of light bulbs that let the engineer know which of the response units was active – that is, which category the input belonged to. </p>
<p class="TXI">Implementing an artificial neural network this way was large and cumbersome, full of switches, plugboards and gas tubes. The same network made up of real neurons would be smaller than a grain of sea salt. But achieving this physical implementation was important. It meant that theories of how neurons compute could actually be tested in the real world on real data. Whereas the McCulloch-Pitts work was about proving a point in theory, the Perceptron put it into practice. </p>
<p class="TXI">Another important difference between the Perceptron and the McCulloch-Pitts network was that, as Rosenblatt told the <span class="italic">New York Times</span>, the Perceptron learns. In the McCulloch and Pitts paper, the authors make no reference to how the connectivity between the neurons comes to be. It is simply defined according to what logical function the network needs to carry out and it stays that way. For the Perceptron to learn, however, it must modify its connections.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> In fact, the Perceptron derives all its functionality from changing its connection strengths until they are just right. </p>
<p class="TXI">The type of learning the Perceptron engages in is known as ‘supervised’ learning. By providing pairs of inputs and outputs – say, a series of pictures and whether <a id="page_62"></a>they each contain a jet or not – the Perceptron learns to make this decision on its own. It does so by changing the strength of the connections – also known as the ‘weights’ – between the association units and the readouts. </p>
<p class="TXI">Specifically, when an image is provided to the network, it activates units first in the input layer, then in the association layer, and finally in the readout layer, indicating the network’s decision. If the network gets the classification wrong, the weights change according to these rules:</p>
<p class="NLF">1. If a readout unit is ‘off’ when it should be ‘on’, the connections from the ‘on’ association units to that readout unit are <span class="italic">strengthened</span>.</p>
<p class="NLL">2. If a readout unit is ‘on’ when it should be ‘off’, the connections from the ‘on’ association units to that readout unit are <span class="italic">weakened</span>.</p>
<p class="TXT">By following these rules, the network will start to correctly associate images with the category they belong to. If the network can learn to do this well, it will stop making errors and the weights will stop changing. </p>
<p class="TXI">This procedure for learning was, in many ways, the most remarkable part of the Perceptron. It was the conceptual key that could open all doors. Rather than needing to tell a computer exactly how to solve a problem, you need only show it some examples of that problem solved. This had the potential to revolutionise computing and Rosenblatt was not shy in saying so. He told the <span class="italic">New York Times</span> that Perceptrons would ‘be able to recognise people and call out their names’ and ‘to hear speech in one language and instantly translate it to <a id="page_63"></a>speech or writing in another language’. He also added that ‘it would be possible to build Perceptrons that could reproduce themselves on an assembly line and which would be “conscious” of their existence’. This was a bold statement, to say the least, and not everyone was happy with Rosenblatt’s public bravado. But the spirit of the claim – that a computer that could learn would expedite the solving of almost any problem – rang true.</p>
<p class="TXI">The power of learning, however, came with a price. Letting the system decide its own connectivity effectively divorced these connections from the concept of Boolean operators. The network <span class="italic">could</span> learn the connectivity that McCulloch and Pitts had identified as required for ‘and’, ‘or’, <span class="italic">etc.</span> But there was no requirement that it does, nor any need to understand the system in this light. Furthermore, while the association units in the Perceptron machine were designed to be only ‘on’ or ‘off’, the learning rule doesn’t actually require that they be this way. In fact, the activity level of these artificial neurons could be any positive number and the rule would still work.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> This makes the system more flexible, but without a binary ‘on’-‘off’ response it makes it harder to map the activity of these units to the binary truth values of propositions. Compared with the crisp and clear logic of the McCulloch-Pitts networks, the Perceptron was an uninterpretable mess. But it worked. Interpretability was sacrificed for ability.</p> <a id="page_64"></a>
<p class="TXI">The Perceptron machine and its associated learning procedure became a popular object of study in the burgeoning field of artificial intelligence. When it made the transition from a specific physical object (the Perceptron) to an abstract mathematical concept (the perceptron algorithm) the separate input and association layers were done away with. Instead, input units representing incoming data connected directly to the readout units and, through learning, these connections changed to make the network better at its task. How and what the perceptron in this simplified form could learn was studied from every angle. Researchers explored its workings mathematically using pen and paper, or physically by building their own perceptron machines, or – when digital computers finally became available – electronically by simulating it.</p>
<p class="TXI">The perceptron generated hope that humans could build machines that learn like we do; in this way it put the prospect of artificial intelligence within 
reach. Simultaneously, it provided a new way of understanding our own intelligence. It showed that artificial neural networks could compute without abiding by the strict rules of logic. If the perceptron could perceive without the use of propositions or operators, it follows that each neuron and connection in the brain needn’t have a clear role in terms of Boolean logic either. Instead, the brain could be working in a sloppier way, wherein, like the perceptron, the function of a network is distributed across its neurons and emerges out of the connections between them. This new approach to the study of the brain became known as ‘connectionism’.</p> <a id="page_65"></a>
<p class="TXI">The work of McCulloch and Pitts was an important stepping stone. As the first demonstration of how networks of neurons could think, it was responsible for getting neuroscience away from the shores of pure biology and into the sea of computation. This fact, rather than the veracity of its claims, is what earns it its place in history. The intellectual ancestor of McCulloch and Pitts’ work, the <span class="italic">Principia Mathematica</span>, could be said to have suffered a similar fate. In 1931, German mathematician Kurt Gödel published ‘On formally undecidable propositions of <span class="italic">Principia Mathematica</span> and related systems’. This paper took the <span class="italic">Principia Mathematica</span> as a starting point to show why its very goal – to explain all of mathematics from simple premises – was impossible to achieve. Russell and Whitehead had not, in fact, done what they believed they did.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> Gödel’s findings became known as the ‘incompleteness theorem’ and had a revolutionary effect on mathematics and philosophy. An effect that stemmed, in part, from Russell and Whitehead’s failed attempt.</p>
<p class="TXI">Russell and McCulloch were able to take the failings of their respective works in their stride. Pitts, on the other hand, was made of finer cloth. The realisation that the brain was not enacting the beautiful rules of logic tore him apart.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> This, along with pre-existing mental struggles and the end of a relationship with an important mentor, drove him to drink and experiment with other <a id="page_66"></a>drugs. He became erratic and delirious; he burned his work and withdrew from his friends. He died from the impacts of liver disease in 1969 – the same year McCulloch died. McCulloch was 70; Pitts was 46. </p>
<p class="TXI">* * *</p>
<p class="image-fig" id="fig5.jpg">
<img alt="" src="Images/chapter-01-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 5</span>
</span></p>
<p class="TXI">The cerebellum is a forest. Folded up neatly near where the spinal cord enters the skull, this bit of the brain is thick with different types of neurons, like different species of trees, all living in chaotic harmony (see Figure 5). The Purkinje cells are large, easily identified and heavily branched: from the body of these cells, dendrites stretch up and away, like a thousand alien hands raised in prayer. The granule cells are numerous and small – with cell bodies less than half the size of the Purkinje’s – but their reach is far. Their axons initially grow upwards, in parallel with the Purkinje cells’ dendrites. They then make a sharp right turn to run directly through the branches of the Purkinje cells, like power lines through treetops. This is where the granule cells make contact with the Purkinje cells: each Purkinje cell gets input from hundreds of <a id="page_67"></a>thousands of granule cells. Climbing fibres are axons that follow a longer path on their way to the Purkinje cells. These axons come from cells in a different brain region – the inferior olive – from which they navigate all the way to the bottom of the Purkinje cell bodies and creep up around them. Winding their way around the base of the Purkinje cell dendrites like ivy, the climbing fibres form connections. Unlike the granule cells, only a single climbing fibre targets each Purkinje cell. In the cerebellar landscape, Purkinje cells are thus central. They have scores of granule cells imposing on them from the top and a small yet precise set of climbing fibres closing in on them from the bottom. </p>
<p class="TXI">In its twisty, organic way, the circuitry of the cerebellum possesses an organisation and precision unbefitting of biology. It was in this biological wiring that James Albus, a PhD student in electrical engineering working at NASA, saw the principles of the perceptron at play. </p>
<p class="TXI">The cerebellum plays a crucial role in motor control; it helps with balance, coordination, reflexes and more. One of the most widely studied of its abilities is eye-blink conditioning. This is a trained reflex that can be found in everyday life. For example, if a determined parent or roommate tries to get you out of bed in the morning by pulling open the curtains, you’ll instinctively close your eyes in response to the sunlight. After a few days of this, simply the sound of the curtains being opened may be enough to make you blink in anticipation. </p>
<p class="TXI">In the lab, this process is studied in rabbits and the intruding sunlight is replaced by a small puff of air on the eye (just annoying enough to ensure they’ll want to avoid it). After several trials of playing a sound (such as a <a id="page_68"></a>short blip of a pure tone) and following it by this little air puff, the rabbit eventually learns to close its eyes immediately upon hearing the tone. Play the animal a new sound (for example, a clapping noise) that hasn’t been paired with air puffs and it won’t blink. This makes eye-blink conditioning a simple classification task: the rabbit has to decide if a sound it hears is indicative of an upcoming air puff (in which case the eyes should close) or if it is a neutral noise (in which case they can remain open). Disrupt the cerebellum and rabbits can’t learn this task. </p>
<p class="TXI">The Purkinje cells have the power to close the eyes. Specifically, a dip in their normally high firing rate will, via connections the Purkinje cells send out of this area, cause the eyes to shut. Based on this anatomy, Albus saw their place as the readout – that is, they indicate the outcome of the classification. </p>
<p class="TXI">The perceptron learns via supervision: it needs inputs and labels for those inputs to know when it has erred. Albus saw these two functions in the two different types of connections on to Purkinje cells. The granule cells pass along sensory signals; specifically, different granule cells fire depending on which sound is being played. The climbing fibres tell the cerebellum about the air puff; they fire when this annoyance is felt. Importantly, this means the climbing fibres signal an error. They indicate that the animal made a mistake in not closing its eyes when it should have. </p>
<p class="TXI">To prevent this error, the connections from the granule cells to the Purkinje cells need to change. In particular, Albus anticipated that any granule cells that were active before the climbing fibre was active (<span class="italic">i.e.</span>, before an error), <a id="page_69"></a>should weaken their connection to the Purkinje cell. That way, the next time those granule cells fire – <span class="italic">i.e.</span>, the next time the same sound is played – they <span class="italic">won’t</span> cause firing in the Purkinje cells. And that dip in Purkinje cell firing <span class="italic">will</span> cause the eyes to close. Through this changing of connection strengths, the animal learns from its past mistakes and avoids future air puffs to the eye.</p>
<p class="TXI">In this way, the Purkinje cell acts like a president advised by a cabinet of granule cell advisors. At first the Purkinje cell listens to all of them. But if it’s clear that some are providing bad advice – that is, their input is followed by negative news delivered by the climbing fibre – their influence over the Purkinje cell will fade. And the Purkinje cell will act better in the future. It is a process that directly mirrors the perceptron learning rule.</p>
<p class="TXI">When Albus proposed this mapping between the perceptron and the cerebellum in 1971,<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> his prediction about how the connections between granule cells and Purkinje cells should change was just that – a prediction. No one had directly observed this kind of learning in the cerebellum. But by the mid-1980s, evidence had piled up in Albus’ favour. It became clear that the strength of the connection between a granule cell and a Purkinje cell does decrease after an error. The particular molecular mechanisms of this process have even been revealed. We now know that granule cell inputs cause a receptor in <a id="page_70"></a>the membrane of the Purkinje cell to respond, effectively tagging which granule cell inputs were active at a given time. If a climbing fibre input comes later (during an air puff), it causes calcium to flood into the Purkinje cell. The presence of this calcium signals to all the tagged connections to decrease their strength. Patients with fragile X syndrome – a genetic disorder that leads to intellectual disabilities – appear to be missing a protein that regulates this connection from the granule cells on to the Purkinje cell. As a result, they have trouble learning tasks like eye-blink conditioning. </p>
<p class="TXI">The perceptron, with its explicit rules of how learning should proceed in a neural network, offered clear testable ideas for neuroscientists to hunt for – and find – in the brain. In doing so, it was able to connect science across scales. The smallest physical detail – calcium ions moving through the inside of a neuron, for example – inherits a much larger meaning in light of its role in computation. </p>
<p class="center">* * *</p>
<p class="TXT">The reign of the perceptron was cut short in 1969. And with a twist of Shakespearean irony, it was its namesake that killed it. </p>
<p class="TXI">
<span class="italic">Perceptrons</span> was written by Marvin Minsky and Seymour Papert, both mathematicians at the Massachusetts Institute of Technology. The book was subtitled <span class="italic">An Introduction to Computational Geometry</span> and had a simple abstract design on the cover. Minsky and Papert were drawn to write about the topic of perceptrons out of appreciation for Rosenblatt’s invention and a desire to explore it further. <a id="page_71"></a>In fact, Minsky and Papert met at a conference where they were presenting similar results from their explorations into how the perceptron learns. </p>
<p class="TXI">Papert was a native of South Africa with full cheeks, a healthy beard and not one, but <span class="italic">two</span>, PhDs in mathematics. He had a lifelong interest in education and how it could be transformed by computing. Minsky was less than a year older than Papert, with sharper features and large glasses. A New York native, he attended the Bronx High School of Science with Frank Rosenblatt; he was also mentored by McCulloch and Pitts. </p>
<p class="TXI">Minsky and Papert shared with McCulloch and Pitts the compulsive desire to formalise thinking. True advances in understanding computation, they believed, came from mathematical derivations. All the empirical success of the perceptron – whatever computing it was able to carry out or categories it was able to learn – meant next to nothing without a mathematical understanding of why and how it worked. </p>
<p class="TXI">At this time, the perceptron was attracting a lot of attention – and money – from the artificial intelligence community. But it wasn’t attracting the kind of mathematical scrutiny Minsky and Papert yearned for. The two were thus explicitly motivated to write their book by a desire to increase the rigour around the study of perceptrons – but also, as Papert later acknowledged, by some desire to decrease the reverence for them.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup></p>
<p class="TXI">The pages of <span class="italic">Perceptrons</span> are comprised mainly of proofs, theorems and derivations. Each contributes to a <a id="page_72"></a>story about the perceptron: defining what it is, what it can do and how it learns. Yet from the publication of these some 200 pages – a thorough exploration of the ins and outs of the perceptron’s workings – the message the community received was largely about its limitations. This is because Minsky and Papert had shown, conclusively, that certain simple computations were impossible for the perceptron to do. </p>
<p class="TXI">Consider a perceptron that has just two inputs, and each input can be ‘on’ or ‘off’. We want the perceptron to report if the two inputs are the same: to respond yes (<span class="italic">i.e.</span>, have its readout unit be on) if both inputs are on <span class="italic">or</span> if both inputs are off. But if one is on and the other is off, the readout unit should be off. Like sorting socks out of the laundry, the perceptron should only respond when it sees a matching pair.</p>
<p class="TXI">To make sure the readout doesn’t fire when only one input is on, the weights from each input need to be sufficiently low. They could, for example, each be half the amount needed to make the readout turn on. This way, when both are on, the readout <span class="italic">will</span> fire and it won’t fire when only one input is on. In this setup the readout is responding correctly for three of the four possible input conditions. But in the condition where both inputs are off, the readout will be off – an incorrect classification. </p>
<p class="TXI">As it turns out, no matter how much we fiddle with connection strengths, there is no way to satisfy all the needs of the classification at once. The perceptron simply cannot do it. And the problem with that is that no good model of the brain – or promising artificial intelligence – should fail at a task as simple as deciding if two things are the same or not.</p> <a id="page_73"></a>
<p class="TXI">Albus, whose paper on the cerebellum was published in 1971, knew of the limitations of the perceptron and knew that, despite these limitations, it was still powerful enough to be a model of the eye-blink conditioning task. But a model of the whole human brain, as Rosenblatt promised? Not possible.</p>
<p class="TXI">The portrait that Minsky and Papert painted forced researchers to see the perceptron’s powers clearly. Prior to the book, researchers were able to explore what the perceptron could do blindly, with the hope that the limits of its abilities were still far off, if they existed at all. Once the contours were put in stark relief, however, there was no denying that these boundaries existed, and that they existed much closer than expected. In truth, all this amounted to was an understanding of the perceptron – exactly what Minsky and Papert set out to do. But the end of ignorance around the perceptron meant the end of excitement around it as well. As Papert put it: ‘Being understood can be a fate as bad as death.’</p>
<p class="TXI">The period that followed the publication of <span class="italic">Perceptrons</span> is known as the ‘dark ages’ of connectionism. It was marked by significant decreases in funding to the research programmes that had grown out of Rosenblatt’s initial work. The neural network approach to building artificial intelligence was snuffed out. All the excessive promises, hopes and hype had to be retracted. Rosenblatt himself died tragically in a boating accident two years after the book was published and the field he helped build remained dormant for more than 10 years.</p>
<p class="TXI">But if the hype around the perceptron was excessive and ill informed, so too was the backlash against it. The limitations in Minsky and Papert’s book were true: the <a id="page_74"></a>perceptron in the form they were studying it was incapable of many things. But it didn’t need to keep that form. The same-or-not problem, for example, could be easily solved by adding an additional layer of neurons between the input and the readout. This layer could be composed of two neurons, one with weights that make it fire when both inputs are on and the other with weights that make it fire when both inputs are off. Now the readout neuron, which gets its input from these middle neurons, just needs to be active if one of the middle neurons is active. </p>
<p class="TXI">‘Multi-layer perceptrons’, as these new neural architectures were called, had the potential to bring connectionism back from the dead.<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> But before a full resurrection was possible, one problem had to be solved: learning. The original perceptron algorithm provided the recipe for setting the connections between the input neurons and the readout neurons – that is, the learning rule was designed for a two-layered network. If the new breed of neural networks was going to have three, four, five or more layers, how should the connections between all those layers be set? (see Figure 6) Despite all the good features of the perceptron learning rule – its simplicity, the proof that it could work, the fact that it had been spotted in the wild of the cerebellum – it was unable to answer this question. Knowing that a multi-layer perceptron <span class="italic">could</span> solve more complex problems was not enough to deliver <a id="page_75"></a>on the grand promises of connectionism. What was needed was for it to <span class="italic">learn</span> to solve those problems.</p>
<p class="center">* * *</p>
<p class="TXT">The Easter Sunday of the connectionist revival story came in 1986. The paper ‘Learning representations by back-propagating errors’, written by two cognitive scientists from the University of California San Diego, David Rumelhart and Ronald Williams, and a computer scientist from Carnegie Mellon, Geoffrey Hinton, was published on 9 October in the journal <span class="italic">Nature</span>. It presented a solution to the exact problem the field had: how to train multi-layer artificial neural networks. The learning algorithm in the paper, called ‘backpropagation’, became widely used by the community at the time. And it remains to this day the dominant way in which artificial neural networks are trained to do interesting tasks.</p>
<p class="image-fig" id="fig6.jpg">
<img alt="" src="Images/chapter-01-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 6</span>
</span></p>
<p class="TXI">The original perceptron’s learning rule works because, with only two layers, it’s easy to see how to fix what’s gone wrong. If a readout neuron is off when it should be on, connections going from the input layer to that neuron <a id="page_76"></a>need to get stronger and vice versa. The relationship between these connections and the readout is thus clear. The backpropagation algorithm has a more difficult problem to solve. In a network with many layers between the input and readout, the relationships between all these connections and the readout aren’t as clear. Now instead of a president and his or her advisors, we have a president, their advisors, and the employees of those advisors. The amount of trust an advisor has in any given employee – <span class="italic">i.e.</span>, the strength of the connection from that employee to the advisor – will certainly have an impact, ultimately, on what the president does. But this impact is harder to directly see and harder to fix if the president feels something is going wrong. </p>
<p class="TXI">What was needed was an explicit way to calculate how any connection in the network would impact the readout layer. As it turns out, mathematics offers a neat way to do this. Consider an artificial neural network with three layers: input, middle and readout. How do the connections from the input to the middle layer impact the readout? We know the activity of the middle layer is a result of the activities of the input neurons and the weights of their connections to the middle layer. With this knowledge, writing an equation for how these weights affect the activity at the middle layer is straightforward. We also know that the readout neurons follow the same rule: their activity is determined by the activities of the middle neurons and the weights connecting the middle neurons to the readout. Therefore, an equation describing how these weights impact the readout is also easy to get. The only thing left to do is find a way to string these two equations together. That <a id="page_77"></a>way we’ll have an equation that tells us directly how the connections from the input to the middle layer impact the readout. </p>
<p class="TXI">When forming a train in the game of dominoes, the number on the end of one tile needs to match the number on the start of another in order for them to connect. The same is true for stitching together these equations. Here, the common term that connects the two equations is the activity of the middle layer: this activity both determines the activity of the readout and is determined by the input-to-middle connections. After joining these equations via the middle layer activity, the impact of the input-to-middle layer connections on the readout can be calculated directly. And this makes it easy to figure out how those connections should change when the readout is wrong. In calculus, this linking together of relationships is known as the ‘chain rule’ and it is the core of the backpropagation algorithm. </p>
<p class="TXI">The chain rule was discovered over 200 years ago by none other than the idol of McCulloch and Pitts, philosopher and polymath Gottfried Leibniz. Given how useful the rule is, its application to the training of multi-layer neural networks was no surprise. In fact, the backpropagation algorithm appears to have been invented at least three separate times before 1986. But the 1986 paper was part of a perfect storm that ensured its findings would spread far and wide. The first reason for this was the content of the paper itself. Not only did it show that neural networks could be trained this way, it also analysed the workings of networks trained on several cognitive tasks, such as understanding relations on a family tree. Another component of the success was the <a id="page_78"></a>increase in computational power that came in the 1980s; this was important for making the training of multi-layer neural networks practically possible for researchers. Finally, the same year the paper was published, one of its authors, Rumelhart, also published a book on connectionism that included the backpropagation algorithm. That book – written with a different Carnegie Mellon professor, James McClelland – went on to sell an estimated 40,000 copies by the mid-1990s. Its title, <span class="italic">Parallel Distributed Processing</span>, lent its name to the entire research agenda of building artificial neural networks in the late 1980s and early 1990s. </p>
<p class="TXI">For somewhat similar reasons, the story of artificial neural networks took an even more dramatic turn roughly a decade into the new millennium. The heaps of data accumulated in the internet age united with the computational power of the twenty-first century to supercharge progress in this field. Networks with more and more layers could suddenly be trained on more and more complex tasks. Such scaled-up models – referred to as ‘deep neural networks’ – are currently transforming artificial intelligence and neuroscience alike. </p>
<p class="TXI">The deep neural networks of today are based on the same basic understanding of neurons as those of McCulloch and Pitts. Beyond that base inspiration, though, they don’t aim to directly replicate the human brain. They aren’t trying to mimic its structure or anatomy, for example.<sup><a href="#fn-11" id="fnt-11">11</a>
</sup> But they do aim to mimic human behaviour and they’re getting quite good at it. When <a id="page_79"></a>Google’s popular language translation service started using a deep neural network approach in 2016, it reduced translation errors by 50 per cent. YouTube also uses deep neural networks to help its recommendation algorithm better understand what videos people want to see. And when Apple’s voice assistant Siri responds to a command, it is a deep neural network that is doing the listening and the speaking. </p>
<p class="TXI">In total, deep neural networks can now be trained to find objects in images, play games, understand preferences, translate between different languages, turn speech into written words and turn written words into speech. Not unlike the original Perceptron machine, the computers these networks run on fill up rooms. They’re located in server centres across the globe, where they hum away processing the world’s image, text and audio data. Rosenblatt may have been pleased to see that some of his grand promises to the <span class="italic">New York Times</span> were indeed fulfilled. They just required a scale nearly a thousand times what he had available at the time. </p>
<p class="TXI">The backpropagation algorithm was necessary to boost artificial neural networks to the point where they could reach near-human levels of performance on some tasks. As a learning rule for neural networks, it really works. Unfortunately, that doesn’t mean it works like the brain. While the perceptron learning rule was something that could be seen at play between real neurons, the backpropagation algorithm is not. It was designed as a mathematical tool to make artificial neural networks work, not a model of how the brain learns (and its inventors were very clear on that from the start). The reason for this is that real neurons can typically only <a id="page_80"></a>know about the activity of the neurons they’re connected to – not about the activity of the neurons those neurons connect to and so on and so on. For this reason, there is no obvious way for real neurons to implement the chain rule. They must be doing something different. </p>
<p class="TXI">For some researchers – particularly researchers in the field of artificial intelligence – the artificial nature of backpropagation is no problem. Their goal is to build computers that can think, by whatever means necessary. But for other scientists – neuroscientists in particular – finding the learning algorithm of the brain is paramount. We know the brain is good at getting better; we see it when we learn a musical instrument, how to drive or how to read a new language. The question is how.</p>
<p class="TXI">Because backpropagation is what we know works, some neuroscientists are starting there. They’re checking for signs that the brain is doing something <span class="italic">like</span> backpropagation, even if it can’t do it exactly. Inspiration comes from the success story of finding a perceptron at work in the cerebellum. There, clues were present in the anatomy; the different placement of the climbing fibres and granule cells pointed to a different role for each. Other brain areas display patterns of connectivity which may hint at how they are learning. For example, in the neocortex, some neurons have dendrites that stretch out way above them. Faraway regions of the brain send inputs to these dendrites. Do they carry with them information about how these neurons have impacted those that come after them in the brain’s neural network? Could this information be used to change the strength of the network’s connections? Both neuroscientists and artificial intelligence researchers hold out hope that the <a id="page_81"></a>brain’s version of backpropagation will be found and that, when it is, it can be copied to create algorithms that learn even better and faster than today’s artificial neural networks. </p>
<p class="TXI">In their hunt to understand how the mind learns from supervision, modern researchers are doing just what McCulloch did. They’re looking at the piles of facts we have about the biology of the brain and trying to see in it a computational structure. Today, they are guided in their search by the workings of artificial systems. Tomorrow, the findings from biology will again guide the building of artificial intelligence. This back-and-forth defines the symbiotic relationship between these two fields. Researchers looking to build artificial neural networks can take inspiration from the patterns found in biological ones, while neuroscientists can look to the study of artificial intelligence to identify the computational role of biological details. In this way, artificial neural networks keep the study of the mind and the brain connected.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-1" id="fn-1">1</a> ﻿Named after the English mathematician George Boole. While they used his ideas, Russell and Whitehead didn﻿’﻿t actually use the term ﻿‘﻿Boolean﻿’﻿, as it wasn﻿’﻿t coined until 1913.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-2" id="fn-2">2</a> ﻿At least that﻿’﻿s what it looked like at the time ﻿…﻿ More on this later.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-3" id="fn-3">3</a> ﻿The use of the word ﻿‘﻿circuit﻿’﻿ here differs from that in the last chapter. In addition to its meaning as an electrical circuit, neuroscientists also use the word to refer to a group of neurons connected in a specific way. ﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-4" id="fn-4">4</a> ﻿More on how learning ﻿–﻿ and memory ﻿–﻿ relies on a change in connections in the next chapter.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-5" id="fn-5">5</a> ﻿This can be thought of as representing the ﻿<span class="italic">rate</span>﻿ of spiking of a neuron, rather than if the neuron is emitting a spike or not. Using this type of artificial neuron only requires a small modification to the learning procedure.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-6" id="fn-6">6</a> ﻿The cracks in the ﻿<span class="italic">Principia</span>﻿﻿’﻿s foundation were noticeable even when it was published. Some of the ﻿‘﻿basic﻿’﻿ premises it had to assume were not really very basic and were hard to justify.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-7" id="fn-7">7</a> ﻿This realisation came even more directly from a study on the frog brain that Pitts was involved with. More on that study in ﻿﻿Chapter 6﻿﻿.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-8" id="fn-8">8</a> ﻿The mapping is sometimes referred to as the ﻿‘﻿Marr-Albus-Ito﻿’﻿ theory of motor learning, named also after David Marr and Masao Ito, who both put forth similar models of how the cerebellum learns. ﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-9" id="fn-9">9</a> ﻿The particular words Papert used to describe his feelings about Perceptron-mania at the time were ﻿‘﻿hostility﻿’﻿ and ﻿‘﻿annoyance﻿’﻿.﻿</p>
<p class="FN2"><a href="chapter3.xhtml#fnt-10" id="fn-10">10</a> ﻿Technically they weren﻿’﻿t ﻿‘﻿new﻿’﻿. Minsky and Papert do reference multi-layer perceptrons in their book. However, they were dismissive about the potential powers of these devices and, unfortunately for science, did not encourage their further study. ﻿</p>
<p class="FN2"><a href="chapter3.xhtml#fnt-11" id="fn-11">11</a> ﻿With the exception of deep neural networks that are built to understand images, which we will hear all about in ﻿﻿Chapter 6﻿﻿.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<meta charset="utf-8"/>
<title>Contents</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="FMT" id="re_contents">Contents</p>
<p class="TOC-CH"><a href="chapter1.xhtml#chapter1" id="re_chapter1">Chapter 1:  Spherical Cows</a></p>
<p class="TOC-CH"><a href="chapter2.xhtml#chapter2" id="re_chapter2">Chapter 2:  How Neurons Get Their Spike</a></p>
<p class="TOC-CH"><a href="chapter3.xhtml#chapter3" id="re_chapter3">Chapter 3:  Learning to Compute</a></p>
<p class="TOC-CH"><a href="chapter4.xhtml#chapter4" id="re_chapter4">Chapter 4:  Making and Maintaining Memories</a></p>
<p class="TOC-CH"><a href="chapter5.xhtml#chapter5" id="re_chapter5">Chapter 5:  Excitation and Inhibition</a></p>
<p class="TOC-CH"><a href="chapter6.xhtml#chapter6" id="re_chapter6">Chapter 6:  Stages of Sight</a></p>
<p class="TOC-CH"><a href="chapter7.xhtml#chapter7" id="re_chapter7">Chapter 7:  Cracking the Neural Code</a></p>
<p class="TOC-CH"><a href="chapter8.xhtml#chapter8" id="re_chapter8">Chapter 8:  Movement in Low Dimensions</a></p>
<p class="TOC-CH"><a href="chapter9.xhtml#chapter9" id="re_chapter9">Chapter 9:  From Structure to Function</a></p>
<p class="TOC-CH"><a href="chapter10.xhtml#chapter10" id="re_chapter10">Chapter 10:  Making Rational Decisions</a></p>
<p class="TOC-CH"><a href="chapter11.xhtml#chapter11" id="re_chapter11">Chapter 11:  How Rewards Guide Actions</a></p>
<p class="TOC-CH"><a href="chapter12.xhtml#chapter12" id="re_chapter12">Chapter 12:  Grand Unified Theories of the Brain</a></p>
<p class="TOC-CH1"><a href="Mathematical.xhtml#Mathematical" id="re_Mathematical">Mathematical Appendix</a></p>
<p class="TOC-CH"><a href="ack.xhtml#ack" id="re_ack">Acknowledgements</a></p>
<p class="TOC-CH"><a href="bib.xhtml#bib" id="re_bib">Bibliography</a></p>
<p class="TOC-CH"><a href="index.xhtml#index" id="re_index">Index</a></p>
</body>
</html>