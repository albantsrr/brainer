<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 11</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter11"><a href="contents.xhtml#re_chapter11">CHAPTER ELEVEN</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter11">How Rewards Guide Actions</a><a id="page_309"></a></p>
<p class="H1" id="b-9781472966445-ch1182-sec12">
<span class="bold">
<span>Temporal difference and reinforcement learning</span>
</span></p>
<p class="TXT">For much of his life as a scientist, Ivan Petrovich Pavlov had one passion: digestion. He started his academic work in 1870 with a thesis on pancreatic nerves. For 10 years as a professor of pharmacology in St Petersburg, he devised ways of measuring gastric juices in animals as they went about their life to show how secretions from different organs change in response to food or starvation. And by 1904, he was granted the Nobel prize ‘in recognition of his work on the physiology of digestion through which knowledge on vital aspects of the subject has been transformed and enlarged’. </p>
<p class="TXI">It is surprising, then, given all of his success in studying the gut, that Pavlov would go down in history as one of the most influential figures in psychology.</p>
<p class="TXI">Pavlov’s transition to studying the mind was, in a way, accidental. In an experiment designed to measure how dogs salivate in response to different foods, he noticed their mouths watering before the food even arrived – all it took was the sound of the footsteps of the assistant bringing in the bowls. This was not completely unusual. Much of Pavlov’s previous work looked at how the digestive system is influenced by the nervous system, but these were usually more obvious interactions such as the smell of food impacting stomach secretions – interactions <a id="page_310"></a>that were plausibly thought to be innate to the animal. Drooling at the sound of footsteps isn’t a response hardwired into genes. It has to be learned. </p>
<p class="TXI">Pavlov was a strict and unforgiving scientist. When public shootings related to the Russian Revolution caused a colleague to be late to a meeting, Pavlov replied: ‘What difference does a revolution make when you have experiments to do in the laboratory?’ But this intensity could lend itself to meticulous work, and when he decided to follow up on these salivation observations he did so thoroughly and exhaustively. </p>
<p class="TXI">Pavlov would repeatedly present a dog with a neutral cue – like the ticking of a metronome or the sound of a buzzer (but not a bell, as is commonly thought; Pavlov relied only on stimuli that could be precisely controlled). He followed this neutral cue with food. After these pairings he would observe how much the dogs salivated in response to the cue alone. He wrote, in characteristic detail: ‘When the sounds from a beating metronome are allowed to fall upon the ear, a salivary secretion begins after nine seconds, and in the course of 45 seconds 11 drops have been secreted.’</p>
<p class="TXI">Varying the specifics of this procedure, Pavlov catalogued many features of the learning process. He asked questions like: ‘How many pairings of cue then food does it take to reliably learn?’ (around 20); ‘Does the timing between the cue and the food matter?’ (yes, the cue has to start before the food arrives but not too much before); ‘Does the cue need to be neutral?’ (no, the animals could learn to salivate in response to slightly negative cues, such as the application of a skin irritant); and many more.</p><a id="page_311"></a>
<p class="TXI">This process – repeatedly pairing an upcoming reward with something usually unrelated to it until the two become linked – is known as classical or (unsurprisingly) ‘Pavlovian’ conditioning and it became a staple in early psychology research. Reviewers of Pavlov’s 1927 book outlining his methodology and results described his work as ‘of vital interest to all who study the mind and the brain’ and ‘remarkable both from the point of view of the exactness of his methods and the scientific insight shown in the sweeping character of his conclusions’. </p>
<p class="TXI">Pavlov’s work eventually fed into one of the biggest movements in twentieth-century science: behaviourism. According to behaviourism, psychology should not be defined as the study of the mind, but rather, as the study of behaviour. Behaviourists therefore prefer descriptions of observable external activity to any theorising about internal mental activity like thoughts, beliefs or emotions. To them, the behaviour of humans and animals can be understood as an elaborate set of reflexes – that is, mappings between inputs from the world to outputs produced by the animal. Conditioning experiments like Pavlov’s offered a clean way of quantifying these inputs and outputs, feeding into the behaviourism frenzy.</p>
<p class="TXI">After the publication of his book, therefore, many scientists were eager to replicate and build off Pavlov’s work. American psychologist B. F. Skinner, for example, learned about Pavlov through a book review written by famed sci-fi author H. G. Wells. Reading this article piqued Skinner’s interest in psychology and set him on the path to becoming a leading figure of the behaviourist <a id="page_312"></a>movement, conducting countless precise examinations of behaviour in rats, pigeons and humans.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">When any field of science amasses enough quantitative data, it eventually turns to mathematical modelling to make sense of it. Models find structure in piles of numbers; they can stitch together disparate findings and show how they arise from a unified process. In the decades after Pavlov, the amount of data being generated from behavioural experiments on learning made it ready for modelling. As William Estes, a prominent American psychologist working on the mathematics of learning, wrote in 1950, conditioning data ‘are sufficiently orderly and reproducible to support exact quantitative predictions of behaviour’. </p>
<p class="TXI">Another paper, published in 1951, agreed: ‘Among the branches of psychology, few are as rich as learning in quantity and variety of available data necessary for model building.’ This paper, ‘A mathematical model for simple learning’, was written by Robert Bush and Frederick Mosteller at the Laboratory of Social Relations at Harvard University. Bush was a physicist-turned-psychologist and Mosteller a statistician. Together, influenced by the work of Estes, they laid out a formula for learning associations between cues and rewards that would be the starting point for a series of increasingly elaborate models. Through the decades, the learning that these models capture became known as ‘reinforcement <a id="page_313"></a>learning’. Reinforcement learning is an explanation for how complex behaviour arises when simple rewards and punishments are the only learning signals. It is, in many ways, the art of learning what to do without being told.</p>
<p class="center">* * *</p>
<p class="TXT">In their model, Bush and Mosteller focused on a specific measure of the learned association between the cue and reward: the probability of response. For Pavlov’s dogs, this is the probability of salivating in response to the buzzer. Bush and Mosteller used a simple equation to explain how that probability changes each time the reward is – or isn’t – given after the cue.</p>
<p class="TXI">Say you start with any random dog off the street (it is, in fact, rumoured that Pavlov got his subjects by stealing them off the streets). The probability that this dog will salivate at the sound of a buzzer starts at zero; it has no reason to suspect that the buzzer means food. Now you press the buzzer and then give the dog a piece of meat. According to the Bush-Mosteller model, after this encounter, the probability that the dog will salivate in response to the buzzer increases (see Figure 24). The exact amount that it increases depends on a parameter in the formula called the learning rate. Learning rates control the speed of the whole process. If the learning rate is very high, a single pairing could be enough to solidify the buzzer-food relationship in the dog’s mind. At more reasonable rates, however, the probability of salivating remains low after the first pairing – maybe it goes to 10 per cent – and raises each time the buzzer is followed by food.</p><a id="page_314"></a>
<p class="TXI">Regardless of the value of the learning rate, however, the second time the buzzer is followed by food, the probability of salivating increases <span class="italic">less</span> than it did after the first time. So, if it went from 0 to 10 per cent after the first pairing, it would increase only another nine percentage points, to 19 per cent, after the second pairing. And only by about eight percentage points after the third. This reflects that, in the Bush-Mosteller model (and in the dogs), the change to the probability with each pairing depends on the value of the probability itself. In other words, learning depends on what is already learned. </p>
<p class="TXI">This is, from a certain angle, intuitive. Nothing new is learned from seeing the sun rise every day. To the extent that we believe something will happen, its actual happening has little effect on us. Anticipated rewards are no different. We don’t update our opinion of our boss, for example, if we get the same holiday bonus we’ve received for the past five years. And the dogs only update their response to the buzzer to the extent that the food that follows differs from what they expect. The power to change expectations comes only from violating them.</p>
<p class="image-fig" id="fig24.jpg">
<img alt="" src="Images/chapter-11-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 24</span>
</span></p><a id="page_315"></a>
<p class="TXI">This violation can be for better or worse. To the dog, the first piece of post-‘buzz’ meat is a lovely surprise and one that has a big impact on its expectations. After repeated pairings, though, expectations shift and slobbering at the sound of the buzzer becomes second nature. At this point, the most impactful thing that could happen would be to hear the buzzer and <span class="italic">not</span> receive food. Such a deprivation would lead to a large decrease in the probability of future salivation – a decrease as large as the increase that occurred with the first pairing. This inverse side of reward-based learning, wherein the animal learns to <span class="italic">dis</span>associate a cue from a reward, is called extinction. With each presentation of the cue without the expected reward, the extinction process unbuilds the association, eventually extinguishing the learned response entirely. Bush and Mosteller made a point of showing that their model captures this process to a tee as well. </p>
<p class="TXI">At the same time that Bush and Mosteller were turning salivation information into equations, another man on the opposite side of the country was working to apply mathematics to some of the trickiest problems in business and industry. The deep and important connections between these works wouldn’t be realised for decades to come.</p>
<p class="center">* * *</p>
<p class="TXT">The RAND Corporation is an American think tank founded in 1948. A non-profit offshoot of the Douglas Aircraft Company, its central aim was to extend the collaboration between science and the military that blossomed out of necessity during the Second World War. The corporation’s name is appropriately generic (RAND literally stands for Research ANd Development) for the <a id="page_316"></a>range of research projects it pursues. Over the years, RAND employees have made significant contributions to the fields of space exploration, economics, computing and even foreign relations.</p>
<p class="TXI">Richard Bellman worked at RAND as a research mathematician from 1952 to 1965. An admirer of the subject as early as his teenage years, Bellman’s path to becoming a mathematician was repeatedly interrupted by the Second World War. First, to lend support to the war effort, he left his postgraduate training at Johns Hopkins University to teach military electronics at the University of Wisconsin. He later moved to Princeton University, where he taught in the Army Specialized Training Program and worked on his own studies. He would eventually complete his PhD at Princeton, but not before he was drafted to work in Los Alamos as a theoretical physicist for the Manhattan Project. The intrusions didn’t seem to impact his career prospects much. He became a tenured professor at Stanford University just three years after the war at the age of only 28. </p>
<p class="TXI">Leaving the academic world for RAND at 32 was, in Bellman’s words, the difference between being ‘a traditional intellectual, or a modern intellectual using the results of my research for the problems of contemporary society’. At RAND his mathematical skill was applied to real-world problems. Problems like scheduling medical patients, organising production lines, devising long-term investment strategies or determining the purchasing plan for department stores. Bellman didn’t have to set foot in a hospital or on a factory floor to help with these issues, however. All of these problems – and a great many more – are huddled under one abstract mathematical umbrella. <a id="page_317"></a>And, in the eyes of a mathematician, to be able to solve any of them is to solve them all.</p>
<p class="TXI">What these problems have in common is they are all ‘sequential decision processes’. In a sequential decision process, there is something to be maximised: patients seen, items produced, money made, orders shipped. And there are different steps that can be taken to do that. The goal is to determine which set of steps should be taken. How can the maximum be reached? What is the best way to climb the mountain?</p>
<p class="TXI">Without much previous work to draw on in this field, Bellman turned to a tried-and-true strategy in mathematics: he formalised an intuition.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> The mathematical conclusion this led him to is now known as the Bellman equation and the simple intuition it captures is that the best plan of action is the one in which all the steps are the best possible ones to take. Obvious though it may seem, when written in mathematics even banal statements can have power. </p>
<p class="TXI">To see how Bellman made use of this intuition, we have to understand how he framed the problem. Bellman first set out to define how good a plan was in terms of how much reward – be that money, widgets, shipments, <span class="italic">etc.</span> – it is likely to accrue. Let’s say you have a five-step plan. The total reward is the sum of the reward you get at each of those five steps. But after you’ve taken the first step, you now have a four-step plan. So, we could instead say that the total reward from the original five-step plan is the reward you get from taking the first step plus the <a id="page_318"></a>total reward from the four-step plan. And the total reward from the four-step plan is just the reward from taking <span class="italic">its</span> first step plus the reward from the resulting three-step plan. And so on, and so on.</p>
<p class="TXI">By defining the reward of one plan in terms of the reward of another, Bellman made his definition <span class="italic">recursive</span>. A recursive process is one that contains itself. Consider, for example, alphabetisation. If you want to alphabetise a list of names, you’d start by sorting all the names according to their first letter. After that, you’d need to apply that <span class="italic">same</span> sorting process again on all the names starting with the same letter to sort them according to their second letter, and so on. This makes alphabetisation recursive. </p>
<p class="TXI">Recursion is a common trick in mathematics and computer science in part because recursive definitions are flexible; they can be made as long or as short as needed. The formula for calculating a plan’s total reward, for example, can just as easily be applied to a five-step plan as a 500-step one. Recursion is also a conceptually simple way to accomplish something potentially difficult. Like the turns of a spiral staircase, each step in a recursive definition is familiar but not identical, and we need only follow them down one by one to the end.</p>
<p class="TXI">Bellman’s framing contains two further insights that helped make his strategy effective for deploying on real-world problems. The first was to incorporate the very relatable fact that a reward you get immediately is worth more than a reward you get later. He did this by introducing a <span class="italic">discounting factor</span> into his recursive definition. So, whereas in the initial formula the reward <a id="page_319"></a>from a five-step plan was equal to the reward from the first step plus the full reward from the four-step plan, an equation with discounting would say it is equal to the reward from the first step plus, maybe, 80 per cent of the reward from the four-step plan. Discounting is a way to weigh immediate gratification against delayed; it is 
‘a bird in the hand is worth two in the bush’ codified into mathematics. </p>
<p class="TXI">The second insight was more conceptual and more radical. It was a switch from focusing on rewards to focusing on <span class="italic">values</span>. </p>
<p class="TXI">To understand this switch, let’s consider the owner of a small business – a very small business. Angela is a busker on the New York City subway system. She knows she can play her electric violin for 20 minutes at certain subway stations before being chased away by the authorities, at which point she’s not allowed to return. Different stations, however, have different payouts. Tourist areas can be very lucrative whereas commuter stops for native New Yorkers yield far fewer donations. She’s leaving her house on Greenpoint Avenue in Brooklyn and wants to end up near a friend’s place in Bleecker Street. What path should she take to make the most money on the way to her destination? </p>
<p class="TXI">So far, we’ve noticed that, after starting from one position and taking a step in a plan, we find ourselves in circumstances broadly similar to how we began – except we are starting from a different position and have a different plan. In sequential decision-making, the different positions we can move through are called states and the steps in a plan are frequently referred to as actions. In Angela’s case, the states are the different <a id="page_320"></a>subway stations she can be at. Each time Angela takes an action (for example, from station A to station B), she finds herself in a new state (station B) that both yields some reward (the amount of donations her playing gets) and provides her with a new set of possible actions (other stations to go to). In this way, states define what actions are available (you can’t go straight from Greenpoint Avenue to Times Square, for example) and actions determine what the next states are. </p>
<p class="TXI">This interplay – wherein the actions taken as part of a plan affect what actions will be available in the future – is part of what makes sequential decision processes so difficult. What Bellman did was to take this constellation of states, actions and rewards, and turn it on its head. Rather than talk about the reward expected from a series of actions, he focused on the value that any given state has.</p>
<p class="TXI">Value, as used colloquially, is a nebulous concept. It elicits ideas about money and worth, but also deeper notions of meaning and utility that can be hard to pin down. The Bellman equation, however, defines value precisely. Using the same recursive structure introduced earlier, Bellman defined the value of a state as the reward you get in that state plus the discounted value of the next state. You’ll notice, in this definition there is no explicit concept of a plan; value is defined by other values. </p>
<p class="TXI">Yet, this equation does rely on knowledge of the next state. Without a plan to say what action is taken, how do we know what the next state will be? This is where the original intuition – the idea that the best plan is made up of the best actions – comes into play. To calculate the value at the next state, you simply assume that the best <a id="page_321"></a>possible action is taken. And the best possible action is the one that leads to the state with the highest value! When wrapped up in the language of value, the plan itself fades away. </p>
<p class="TXI">So how does this help Angela? Given a map of possible subway stations (see Figure 25) and the associated donations she expects to get from each, we can calculate a ‘value function’. A value function is simply the value associated with each state (in this case, each station). We can calculate this by starting at the end and working backwards. Once Angela reaches Bleecker Street, she will go straight to her friend’s house and not do any busking, so the reward she will get at her final destination is $0. Because there are no further states from this point, the value of Bleecker Street is also zero. Backing up from here, the values of Union Square and 34th Street can be calculated in terms of the reward expected there and the value of Bleecker Street. This process continues until the value for each station is calculated.</p>
<p class="image-fig" id="fig25.jpg">
<img alt="" src="Images/chapter-11-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 25</span>
</span></p>
<p class="TXI">With these values in hand, Angela can now plan her journey. Starting out from Greenpoint Avenue, she can take the train to either Court Square or Metropolitan Avenue. Which should she choose? Looking just at the possible rewards from each, Metropolitan Avenue seems the better choice, as it offers $10 versus Court Square’s $5. But looking at the value function, Court Square is the correct choice. This is because the value function cares about what states you can get yourself into in the future – and from Court Square Angela can go straight to the jackpot, Times Square. Angela could also go to Queen’s Plaza from Court Square, but that isn’t relevant here, because the value function assumes Angela is smart. It assumes that from Court Square she would go to <a id="page_322"></a>Times Square because Times Square is the better choice. All in all, following the value function would take Angela through Court Square to Times Square then to 34th Street and finally on to her destination at Bleecker Street. In total, she will have earned $65 – the most that any path on this map could offer.</p>
<p class="TXI">Bellman’s move to focus on the value function was important because it corrected a flaw in the original framing of the problem. We started out by trying to calculate the total reward we could get from a given plan. But when solving a sequential decision process, we <a id="page_323"></a>aren’t given a plan. In fact, a plan is exactly what we’re trying to find! Once we know the value function, though, the plan is simple: follow it. Like breadcrumbs left on a forest path, the value function tells you where to go. Anyone looking for the most reward needs only to greedily seek the next state with the highest value. All actions can be chosen based on this simple rule.</p>
<p class="TXI">Some interesting things happen as a result of the discounting that is part of the definition of value. For example, look at the options Angela has from Times Square. She can either go to 34th Street, get $20 and then end at Bleecker Street or she can go to 14th Street, get $8, then go to Union Square and get $12 and finally end at Bleecker Street. Both routes earn her $20 in total. But the value of 34th Street is 20 whereas the value of 14th Street is 17.6 (calculated as 8 + 0.8 x 12), indicating that 34th Street is the better option. This demonstrates how discounting future rewards can lead to plans with fewer steps; if there is only so much reward to get, it’s best to get it quicker rather than slower. Discounting also means that even big rewards will be ignored if they are too distant. If a train station in New Jersey could garner Angela $75, it may still not influence her choice as she leaves her house. The impact of a reward on a value function is like the ripple from a stone dropped in water. It’s felt strongest in the nearby states, but its power is diluted the farther away you go.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup></p> <a id="page_324"></a>
<p class="TXI">This technical definition of value – based on states and recursion and discounting factors – may seem a far cry from the word we use in everyday language. But those colloquial connotations are very much present in this equation. Why do we value money? Not because there is actually much pleasure to be had from the paper or the coin itself, but because of the future we can envision once we have that paper or coin. Money is worth only what it can get us later and what we can get later is baked into Bellman’s definition of value. </p>
<p class="TXI">Bellman’s work to frame sequential decision processes this way truly did allow him to become the ‘modern intellectual’ he aimed to be when moving to RAND. In the years after his first publications describing this solution, countless companies and government entities began to apply it out in the world. By the 1970s, Bellman’s ideas had been deployed on problems as diverse as sewer system design, airline scheduling and even the running of research departments at large companies such as Monsanto. The technique went under the name ‘dynamic programming’, a rather bland phrase Bellman actually coined with the aim of keeping some of the mathematics-phobic higher-ups in the military out of his hair. ‘The 1950s were not good years for mathematical research,’ Bellman wrote in his autobiography. ‘The RAND Corporation was employed by the Air Force, and the Air Force had [Charles] Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. … Thus, I thought dynamic programming was a good name. It was something not even a <a id="page_325"></a>Congressman could object to. So I used it as an umbrella for my activities.’ </p>
<p class="TXI">In applying the method in each of these settings, engineers had to find a way to calculate the value function. In some cases, like the subway example given above, the landscape of the problem is simple enough that the calculation is straightforward. But simple problems are rarely realistic. The real world has a large number of potential states; these states can connect to each other in complex or even uncertain ways; and they can do so through many possible actions. Much effort was put into finding the value function in these trickier situations. Yet even with clever techniques, the application of dynamic programming usually pushed at the edge of computing power at the time. The calculation of the value function was always a bottleneck in the process. And without a way to find the value function, the full potential of Bellman’s contributions would remain unreached. </p>
<p class="center">* * *</p>
<p class="TXT">There is an irony to Pavlov’s legacy. Its immediate effect was to set off behaviourism, a movement with religious-like dedication to ignoring the mind and focusing only on directly measurable behaviour. Yet the lineage of mathematical models it spawned found success in the other direction, by going increasingly inside the mind; to capture reinforcement learning in equations required the use of terms representing hidden mental concepts.</p>
<p class="TXI">One of the well-known extensions to the Bush-Mosteller model came 20 years later, in 1972, and was developed by another duo, Yale psychologists Robert <a id="page_326"></a>Rescorla and Allan Wagner. Rescorla and Wagner generalised the Bush-Mosteller model, making it applicable to a wider range of experimental settings and able to capture more findings. The first alteration they made was to the very measure the model was trying to explain. </p>
<p class="TXI">Bush and Mosteller’s ‘probability of response’ was too specific and too limited. Rescorla and Wagner instead aimed to capture a more abstract value which they referred to as ‘associative strength’. This strength of the association between a cue and a reward is something that would exist in the mind of the participant, making it not directly measurable, but different experiments could try to read it out in different ways. This could include measuring a probability of response, like the probability of salivating, but also other measures, such as the amount of salivation, or behaviours like barking or movement. In this way, Rescorla and Wagner subsumed the Bush-Mosteller model into a broader framework. </p>
<p class="TXI">The Rescorla-Wagner model also expanded to incorporate a known feature of conditioning experiments referred to as ‘blocking’. Blocking occurs when an initial cue is paired with a reward, and then a second cue is also given along with the first and both are paired with the reward. So, for example, after a dog had learned to associate the sound of the buzzer with food, the experimenter would then flash a light at the same time as the buzzer and then give the food. In Bush and Mosteller’s model, the cues were treated completely separately. So, if the light and buzzer were paired with food enough times, the dog should come to associate the light with food the same way it had learned the association with the buzzer. Then we would expect that presenting the light alone <a id="page_327"></a>would cause the dog to salivate. This is not, in fact, what happens; the dogs don’t salivate in response to the light alone. The presence of the buzzer <span class="italic">blocks</span> the ability of the light to be associated with the food. </p>
<p class="TXI">This provides further proof that learning is driven by errors. In particular, errors about predicted reward. When the animal hears the buzzer, it knows food is coming. So, when the food arrives there is no error in its prediction of that reward. As we saw before, that means it doesn’t update its beliefs about the buzzer. But it also means it doesn’t update its beliefs about anything else either. Whether there was a light on at the same time as the buzzer or not is irrelevant. The light has no bearing on the predicted reward, the received reward or the difference between the two, which defines the prediction error – and without an error, everything is stuck as it is. Prediction error is the grease that oils the wheels of learning. </p>
<p class="TXI">Rescorla and Wagner thus made their update in associative strength between one cue and a reward dependent not just on that cue’s current associative strength, but on the sum of the associative strengths of all cues present. If one of these associative strengths is high (for example if the buzzer is present), then the presence of the reward wouldn’t change any of them (the association with the light is never learned). This summing across multiple cues is also something that would need to be done internal to the animal, further reflecting the rejection of behaviourism and a move to the mind. </p>
<p class="TXI">But the watershed moment in reinforcement learning came in the mid-1980s from the work of a ponytail-wielding Canadian computer scientist named Richard <a id="page_328"></a>Sutton and his PhD adviser Andrew Barto. Sutton was educated in both psychology and computer science, and Barto spent a lot of his time reading the psychology literature. This proved a powerful combination as the work they did together pulled from and gave back to both fields. </p>
<p class="TXI">Sutton’s work removed the final tangible element of the model: the reward itself. Up until this point, the moment of learning centred around the time that 
a reward was given or denied. If you catch a whiff of the smoke from a blown-out candle and are then handed a piece of birthday cake, the association between the two strengthens. But a candle extinguished at the end of 
a religious ceremony likely doesn’t come with cake and so the association weakens. In either case, though, the cake itself is the important variable. Its presence or absence is key. Anything can serve as a cue, but the reward must be a primal one – food, water, sex. But once we come to associate smoke with birthday cake, we may notice some other regularities. For example, the smoke is usually preceded by singing and the singing may be preceded by people putting on silly hats. None of these things are rewards in and of themselves (especially the singing, at most parties), but they form a chain that attaches each in some degree to the primary reward. Knowing this information can be useful: if we want cake, being on the lookout for silly hats may help.</p>
<p class="TXI">Rescorla and Wagner had no way of allowing for this backing up of associations – no way, essentially, for a 
cue associated with a reward in one circumstance to 
play the role of a reward in another. But Sutton did. In 
Sutton’s algorithm – known as ‘temporal difference <a id="page_329"></a>learning’ – beliefs are updated in response to any violation of expectations. Walking through your office hallway to your desk, for example, expectations about reward may be pretty low. But when you hear your co-workers in the conference room start the first verse of ‘Happy Birthday’, a violation has occurred. Beliefs must be updated; you are now in a state where reward is on the horizon. This is where temporal difference learning occurs. You may choose to enter the conference room, finish the song, smell the candles and eat the cake. In doing those actions, no further violations will have occurred – and therefore, no further learning. It is thus not the receipt of the reward itself that causes any changes. The only learning that happened was in that hallway, many steps away from the reward. </p>
<p class="TXI">What exactly is being learned here, though? What mental concept is it that got updated in the hallway? It’s not the association of a cue with a reward – not directly at least. Instead it’s more of a signal telling you the path to reward, should you follow the right steps in between. </p>
<p class="TXI">This may sound familiar because what temporal difference learning helps you learn is a value function. At each moment in time, according to this framing, we have expectations – essentially a sense of how far we are from reward – that define the value of the state we are in. As time passes or we take actions in the world we may find ourselves in new states, which have their own associated values. If we’ve correctly anticipated the value of those new states, then all’s well. But if the value of the current state is different from what we predicted it would be when we were in the state before, we’ve got an error. And errors induce learning. In particular, if the value of <a id="page_330"></a>the current state is more or less than we expected it to be when we were in the previous state, we <span class="italic">change the value of the previous state</span>. That is, we take the surprise that occurred now and use it to change our belief about the past. That way, the next time we find ourselves in that previous state, we will better be able to predict the future. </p>
<p class="TXI">Consider driving to an amusement park. Here the value of your location is measured in terms of how far you are from this rewarding destination. As you leave your house, you expect to arrive in 40 minutes. You drive straight for five minutes and get on a highway. You now expect to arrive in 35 minutes. After 15 minutes on the highway, you take an exit. Your estimated time of arrival is now 20 minutes. But, when you get off that exit and turn on to a side street you hit a traffic jam. As you sit in your hardly moving car, you know you won’t be at the park for another 30 minutes. Your expected arrival time has now jumped up by 10 minutes – a significant error. </p>
<p class="TXI">What should be learned from this error? If you had an accurate view of the world, you would’ve anticipated 30 more minutes of driving at the moment you took the exit. So, temporal difference learning says you should <span class="italic">update the value of the state associated with that exit</span>. That is, you use the information received at one state (a traffic jam on the side street) to update your beliefs about the value of the state before (the exit). And this may mean that the next time you drive to this amusement park, you will avoid that exit and choose another instead. But it doesn’t take arriving at the amusement park 10 minutes late to learn from that mistake; the expectation of that happening at the sight of the traffic was enough.</p><a id="page_331"></a>
<p class="TXI">What Sutton’s algorithm shows is that by mere exploration – simple trial and error – humans, animals or even an artificial intelligence can eventually learn the correct value function for the states they’re exploring. All it takes is updating expectations when expectations change – ‘learning a guess from a guess’ as Sutton describes it. </p>
<p class="TXI">As an extension of Bellman’s work on dynamic programming, temporal difference learning had the potential to solve real-world problems. Its simple moment-by-moment learning rule made it attractive from a computing perspective: it didn’t demand as much memory as programmes that needed to store the entire set of actions that preceded a reward before learning from it. It also worked. One of the biggest displays of its power was TD-Gammon, a computer program trained via temporal difference learning to play the board game backgammon. Board games are particularly useful tests of reinforcement learning because rewards frequently only come at the very end of a game, in the form of a win or loss. Using such a coarse and distant signal to guide strategy at the very first move is a challenge, but one that temporal difference learning could meet. Built in 1992 by Gerald Tesauro, a scientist at IBM, TD-Gammon played hundreds of thousands of games against itself, eventually reaching the level of an intermediate player without ever taking instructions from a human. Because it learned in isolation, it also developed strategies not tried by humans (who are generally influenced by each other’s gameplay to stick within a certain set of moves). In the end, TD-Gammon’s unusual moves actually influenced theory and understanding about the game of backgammon itself.</p><a id="page_332"></a>
<p class="TXI">In 2013 another application of temporal difference learning made headlines, this time applied to video games. Scientists at the artificial intelligence research company DeepMind built a computer program that taught itself to play multiple games from the 1970s arcade system Atari. This artificial gamer got the full Atari experience. The only inputs to the algorithm were the pixels on the screen – it was given no special knowledge that some of those pixels may represent spaceships or ping-pong bats or submarines. The actions it was allowed to take included the standard buttons such as up, down, left, right, A, B; and the reward for the model came in terms of the score provided by the game it was playing. As this burdens the algorithm with a more challenging task than backgammon – which at least had the concepts of pieces and locations baked into the inputs to the model – the researchers combined temporal difference learning with deep neural networks (a method we encountered in <a href="chapter3.xhtml#chapter3">Chapter 3</a>).<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> One version of this deep neural network had around 20,000 artificial neurons and, after weeks of learning, reached human-level performance on 29 out of 49 games tested. Because this Atari algorithm also learned asocially, it ended up with some interesting quirks, including discovering a clever trick for tunnelling through a wall in the brick-clearing game <span class="italic">Breakout</span>. </p>
<p class="TXI">While games are a flashy and fun way to demonstrate the power of this approach, its application didn’t stop there. After Google acquired DeepMind in 2014, it set reinforcement-learning algorithms to the task of <a id="page_333"></a>minimising energy use in its massive data centres. The result was a 40 per cent decrease in energy used to cool the centres and likely hundreds of millions in savings over the years as a result. With a single-minded focus on achieving the goal at hand, reinforcement learning algorithms find creative and efficient solutions to hard problems. These alien minds can thus help devise plans humans would’ve never thought of. </p>
<p class="TXI">The paths of sequential decision-making and Pavlovian conditioning represent a victory of convergent scientific evolution. The trajectories of Bellman and Pavlov start with separate and substantial problems, each seething with their own demanding details. How should a hospital schedule its nurses and doctors to serve the most patients? What causes a dog to salivate when the sound of a buzzer hits its ears? These questions are seemingly worlds apart. But by peeling away the weight of the specifics – leaving only the bare bones of the problem to remain – their interlocking nature becomes clear. This is one of the roles of mathematics: to put questions disconnected in the physical world into the same conceptual space wherein their underlying similarities can shine through.</p>
<p class="TXI">The story of reinforcement learning is thus one of successful interdisciplinary interaction. It shows that psychology and engineering and computer science can work together to make progress on hard problems. It demonstrates how mathematics can be used to understand, and replicate, the ability of animals and humans to learn from their surroundings. The story would be a remarkable one as it is, if it ended there. But it doesn’t end there.</p>
<p class="center">* * *</p><a id="page_334"></a>
<p class="TXT-con">Octopamine is a molecule found in the nervous systems of many insects, molluscs and worms. It is so named because of its initial discovery in the salivary glands of the octopus in 1948. In the brain of a bee, octopamine is released upon run-ins with nectar. In the early 1990s, Terry Sejnowski, a professor at the Salk Institute in San Diego, California, and two of his lab members, Read Montague and Peter Dayan, were thinking about octopamine. In particular, they built a model – a computer simulation of bee behaviour – that was centred on the neuron in the bee brain that releases octopamine. A bee’s choices about what flowers to land on or avoid, they proposed, could be explained by a Rescorla-Wagner model of learning and a neural circuit including the octopamine neuron could be the hardware that implements it. But as they worked out this octopamine puzzle, the team heard of another study, conducted some 6,000 miles away by a German professor named Wolfram Schultz, on octopamine’s chemical cousin dopamine.</p>
<p class="TXI">You may be familiar with dopamine. It has a bit of a reputation in popular culture. Countless news articles refer to it as something like ‘our brain’s pleasure and reward-related chemical’ or talk about how everyday activities like eating a cupcake cause ‘a surge of the reward chemical dopamine to hit the decision-making area of the brain’. It’s branded as the pleasure molecule and it’s not uncommon for products to be pedalled under its powerful name. Pop stars have named albums and songs after it. ‘Dopamine diets’ claim (without evidence) to provide foods that boost dopamine while keeping you slim. And the tech start-up Dopamine Labs promised to increase user engagement in phone apps by <a id="page_335"></a>doling out squirts of the neurotransmitter. This poor celebrity chemical has also been badly maligned – referred to as the source of all addictions and maladaptive behaviours. Online communities like The Dopamine Project have cropped up aiming to provide ‘better living through dopamine awareness’. And some Silicon Valley dwellers have even attempted ‘dopamine fasts’ as a respite from constant over-stimulation.</p>
<p class="TXI">While it is true that a release of dopamine can accompany rewards, that is far from the whole story. What Schultz’s study in particular showed was a case in which the neurons that dole out dopamine were <span class="italic">silent</span> when a reward was given.</p>
<p class="TXI">Specifically, Schultz trained monkeys to reach their arm out in front of them in order to receive some juice.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> During this training process, he recorded the activity of a population of dopamine-releasing neurons tucked into the underside of the brain. Schultz observed that at the end of training – when the animals knew they would get some juice by making a reach – these neurons showed no response at all to the juice reward being delivered. </p>
<p class="TXI">When Schultz first published these results he didn’t have a clear explanation for why the dopamine neurons were behaving this way, but the members of the Sejnowski lab did. And they reached out to Schultz to embark on a collaboration that would test the hypothesis that dopamine neurons encode the prediction errors necessary for temporal difference learning. It would be <a id="page_336"></a>the start of what Sejnowski referred to as ‘one of the most exciting scientific periods of my life’. </p>
<p class="TXI">Dayan and Montague worked to reanalyse Schultz’s data through the lens of learning algorithms. They focused on the simplest of Schultz’s experiments, which consisted of a light at the desired reach location turning on and, if the animal reached to it, a drop of juice was delivered half a second later. What they wanted to know was how the response of the dopamine neurons changed as the animal came to learn this association. But they were also interested in a particular circumstance after learning: what happens when the juice doesn’t follow the light. If the animals learned the light–juice association, they would know to expect it and if the juice didn’t show up that would be a significant prediction error. Did the dopamine neurons reflect that? </p>
<p class="TXI">The neurons that release dopamine tend to fire around five spikes per second when nothing much is going on. At the start of the learning process, right after the animal got what seemed like a surprise shot of juice after making an arm movement, that rate jumped up briefly to about 20 spikes per second. The light that came before the movement, however, elicited nothing. But after enough pairings, once the animal came to understand how the light and the reach and the juice were all related, that pattern shifted. The dopamine neurons stopped responding to the juice. This is a change perfectly in line with the notion that they signal prediction error, because once the animal can correctly predict the juice there is no more error. And they started responding to the light. Why? Because the light had become associated with the reward but – crucially – they had no idea when it would <a id="page_337"></a>come on. When it did arrive it was an error. Specifically, it’s an error in the predicted value of the state of the animal. Sitting in the experimental chair going about its life, the monkey expects the next moment to be more or less similar to the current one. When the light turns on, that expectation is violated. Like hearing the first few bars of ‘Happy Birthday’ in your office hallway, it’s a pleasant surprise, but a surprise nonetheless.</p>
<p class="TXI">The final analysis – done while sporadically omitting the juice after the reach – was to see how <span class="italic">unpleasant</span> surprises were encoded. If dopamine is encoding errors, it should indicate when things are worse than expected as well. And with the juice absent, the neurons did just that. They had a dip in their firing right at the time the juice would’ve been delivered. Specifically, the neurons would go from five to 20 spikes per second in response to the light; then as the animal reached out its arm they’d return back to five. But, about half a second after the reach, when it was clear there was no juice coming, they’d shut off completely. An expectation had been violated and the dopamine neurons were letting it be known.</p>
<p class="TXI">This study showed that the firing of dopamine neurons can signal the errors – both positive and negative – about predicted values that are needed for learning. It was thus an important point in shifting the understanding of dopamine from a pleasure molecule to a pedagogical one.</p>
<p class="TXI">If the point of encoding the error is to learn from it, though, where does that learning happen? It turns out that’s a bit hard to pin down because these dopamine-releasing neurons release dopamine in many corners of the brain; their projections burrow through the brain like pipework, touching regions near and far. Still, <a id="page_338"></a>a location that seems particularly important is the striatum. The striatum is a group of neurons that serves as the primary input to a collection of brain areas involved in guiding movement and actions. Neurons in the striatum contribute to the production of behaviour by associating sensory inputs with actions or actions with other actions.</p>
<p class="TXI">As we saw in <a href="chapter4.xhtml#chapter4">Chapter 4</a>, Hebbian learning is an easy way for associations between ideas to become encoded in the connections between neurons. Under Hebbian rules, if one neuron regularly fires before another, the weight of the connection from the first to the second is strengthened. In reinforcement learning, however, we need more than just to know that two events happened close in time. We need to know how those events relate to reward. Specifically, we only want to update the strength between a cue and an action (for example, seeing a light and reaching for it) if that pairing turns out to be associated with reward. </p>
<p class="TXI">So, the neurons in the striatum don’t follow basic Hebbian learning. Instead they follow a modified form wherein the firing of one neuron before another only strengthens their connection if it happens <span class="italic">in the presence of dopamine</span>. Dopamine – which encodes the error signal needed for updating values – is thus also required for the physical changes needed for updating that occur at the synapse. In this way, dopamine truly does act as a lubricant for learning. </p>
<p class="TXI">Having the language of temporal difference learning in which to talk about the functioning of the brain has altered the conversation on clinical topics such as addiction. One theory, put forth in 2004 by neuroscientist <a id="page_339"></a>David Redish, tries to explain the addictive properties of drugs like amphetamine and cocaine in terms of the effects they have on dopamine release. It posits that these drugs cause a release of dopamine that is independent of the true prediction error. Specifically, by overdriving the dopamine neurons, these drugs send the false signal to the rest of the brain that the drug experience is always better than expected. This errant error signal still drives learning, pushing the estimated value of states associated with drug use higher and higher. Deforming the value function in this way is guaranteed to have detrimental effects on behaviour like the ones seen in addiction.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup></p>
<p class="center">* * *</p>
<p class="TXT">David Marr was a British neuroscientist with a background in mathematics. His book, <span class="italic">Vision</span>: ﻿A <span class="italic">Computational Investigation into the Human Representation and Processing of Visual Information</span> was published in 1982, two years after his death. In the first chapter, he lays out the components needed for a successful analysis of a neural system. According to Marr, to understand any bit of the brain we should be able to explain it on each of three levels: computational, algorithmic and implementational. The computational level asks what is the overall purpose of this system, that is, what is it trying to do? The algorithmic level asks how, <span class="italic">i.e.</span>, through what steps, does it achieve this <a id="page_340"></a>goal. And finally, the implementational level asks specifically what bits of the system – what neurons, neurotransmitters, <span class="italic">etc.</span> – carry out these steps.</p>
<p class="TXI">An explanation encompassing all of Marr’s levels is an aspiration towards which many neuroscientists strive. The systems that carry out reinforcement learning are a rare case where they can come within striking distance of this high bar. At the computational level reinforcement learning has a simple answer: maximise reward. This is what Bellman recognised as the goal of sequential decision processes and what following the value function should get you. But how do we learn the value function? That’s where temporal difference learning comes in. The work of Bush, Mosteller, Resorla, Wagner and Sutton all turned stacks of data from conditioning experiments into strings of symbols that could describe the algorithm needed to do the learning part of reinforcement learning. On the implementation level, dopamine neurons take on the task of calculating prediction error and the signals they send to other brain areas control the associations learned there. In this way, a satisfying understanding of a fundamental ability – to learn from rewards – was achieved by tunnelling towards the topic from many different angles.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-1" id="fn-1">1</a> ﻿The kind of conditioning Skinner is most associated with is known as ﻿‘﻿operant conditioning﻿’﻿, which involves performing an action before getting a reward. The line between operant and Pavlovian conditioning is sometimes sharp, sometimes blurred and information in this chapter will at times relate to both.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-2" id="fn-2">2</a> ﻿Interestingly, Bellman was aware of Bush and Mosteller﻿’﻿s publications, but his work on these problems was developed independently of that.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-3" id="fn-3">3</a> ﻿Because it controls the balance between caring about now versus the future, the strength of discounting can have sizable impacts on value and therefore on which actions are chosen. Scientists have posited that disorders such as addiction or ADHD can be understood in terms of inappropriate reward discounting. More on addiction later.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-4" id="fn-4">4</a> ﻿Specifically, they used a deep convolutional neural network which, as we saw in ﻿﻿Chapter 6﻿﻿, is used to model the visual system. ﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-5" id="fn-5">5</a> ﻿This is actually an example of the ﻿‘﻿operant﻿’﻿ form of conditioning mentioned earlier, because the animals need to make a reach to receive their reward.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-6" id="fn-6">6</a> ﻿This theory can explain several aspects of addiction, but one of its big predictions failed. If these drugs lead to non-stop prediction error, then the blocking phenomena described previously shouldn﻿’﻿t be seen when drugs are used as reward. An experiment in rats indicated, however, that blocking still does occur.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 11</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter11"><a href="contents.xhtml#re_chapter11">CHAPTER ELEVEN</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter11">How Rewards Guide Actions</a><a id="page_309"></a></p>
<p class="H1" id="b-9781472966445-ch1182-sec12">
<span class="bold">
<span>Temporal difference and reinforcement learning</span>
</span></p>
<p class="TXT">For much of his life as a scientist, Ivan Petrovich Pavlov had one passion: digestion. He started his academic work in 1870 with a thesis on pancreatic nerves. For 10 years as a professor of pharmacology in St Petersburg, he devised ways of measuring gastric juices in animals as they went about their life to show how secretions from different organs change in response to food or starvation. And by 1904, he was granted the Nobel prize ‘in recognition of his work on the physiology of digestion through which knowledge on vital aspects of the subject has been transformed and enlarged’. </p>
<p class="TXI">It is surprising, then, given all of his success in studying the gut, that Pavlov would go down in history as one of the most influential figures in psychology.</p>
<p class="TXI">Pavlov’s transition to studying the mind was, in a way, accidental. In an experiment designed to measure how dogs salivate in response to different foods, he noticed their mouths watering before the food even arrived – all it took was the sound of the footsteps of the assistant bringing in the bowls. This was not completely unusual. Much of Pavlov’s previous work looked at how the digestive system is influenced by the nervous system, but these were usually more obvious interactions such as the smell of food impacting stomach secretions – interactions <a id="page_310"></a>that were plausibly thought to be innate to the animal. Drooling at the sound of footsteps isn’t a response hardwired into genes. It has to be learned. </p>
<p class="TXI">Pavlov was a strict and unforgiving scientist. When public shootings related to the Russian Revolution caused a colleague to be late to a meeting, Pavlov replied: ‘What difference does a revolution make when you have experiments to do in the laboratory?’ But this intensity could lend itself to meticulous work, and when he decided to follow up on these salivation observations he did so thoroughly and exhaustively. </p>
<p class="TXI">Pavlov would repeatedly present a dog with a neutral cue – like the ticking of a metronome or the sound of a buzzer (but not a bell, as is commonly thought; Pavlov relied only on stimuli that could be precisely controlled). He followed this neutral cue with food. After these pairings he would observe how much the dogs salivated in response to the cue alone. He wrote, in characteristic detail: ‘When the sounds from a beating metronome are allowed to fall upon the ear, a salivary secretion begins after nine seconds, and in the course of 45 seconds 11 drops have been secreted.’</p>
<p class="TXI">Varying the specifics of this procedure, Pavlov catalogued many features of the learning process. He asked questions like: ‘How many pairings of cue then food does it take to reliably learn?’ (around 20); ‘Does the timing between the cue and the food matter?’ (yes, the cue has to start before the food arrives but not too much before); ‘Does the cue need to be neutral?’ (no, the animals could learn to salivate in response to slightly negative cues, such as the application of a skin irritant); and many more.</p><a id="page_311"></a>
<p class="TXI">This process – repeatedly pairing an upcoming reward with something usually unrelated to it until the two become linked – is known as classical or (unsurprisingly) ‘Pavlovian’ conditioning and it became a staple in early psychology research. Reviewers of Pavlov’s 1927 book outlining his methodology and results described his work as ‘of vital interest to all who study the mind and the brain’ and ‘remarkable both from the point of view of the exactness of his methods and the scientific insight shown in the sweeping character of his conclusions’. </p>
<p class="TXI">Pavlov’s work eventually fed into one of the biggest movements in twentieth-century science: behaviourism. According to behaviourism, psychology should not be defined as the study of the mind, but rather, as the study of behaviour. Behaviourists therefore prefer descriptions of observable external activity to any theorising about internal mental activity like thoughts, beliefs or emotions. To them, the behaviour of humans and animals can be understood as an elaborate set of reflexes – that is, mappings between inputs from the world to outputs produced by the animal. Conditioning experiments like Pavlov’s offered a clean way of quantifying these inputs and outputs, feeding into the behaviourism frenzy.</p>
<p class="TXI">After the publication of his book, therefore, many scientists were eager to replicate and build off Pavlov’s work. American psychologist B. F. Skinner, for example, learned about Pavlov through a book review written by famed sci-fi author H. G. Wells. Reading this article piqued Skinner’s interest in psychology and set him on the path to becoming a leading figure of the behaviourist <a id="page_312"></a>movement, conducting countless precise examinations of behaviour in rats, pigeons and humans.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">When any field of science amasses enough quantitative data, it eventually turns to mathematical modelling to make sense of it. Models find structure in piles of numbers; they can stitch together disparate findings and show how they arise from a unified process. In the decades after Pavlov, the amount of data being generated from behavioural experiments on learning made it ready for modelling. As William Estes, a prominent American psychologist working on the mathematics of learning, wrote in 1950, conditioning data ‘are sufficiently orderly and reproducible to support exact quantitative predictions of behaviour’. </p>
<p class="TXI">Another paper, published in 1951, agreed: ‘Among the branches of psychology, few are as rich as learning in quantity and variety of available data necessary for model building.’ This paper, ‘A mathematical model for simple learning’, was written by Robert Bush and Frederick Mosteller at the Laboratory of Social Relations at Harvard University. Bush was a physicist-turned-psychologist and Mosteller a statistician. Together, influenced by the work of Estes, they laid out a formula for learning associations between cues and rewards that would be the starting point for a series of increasingly elaborate models. Through the decades, the learning that these models capture became known as ‘reinforcement <a id="page_313"></a>learning’. Reinforcement learning is an explanation for how complex behaviour arises when simple rewards and punishments are the only learning signals. It is, in many ways, the art of learning what to do without being told.</p>
<p class="center">* * *</p>
<p class="TXT">In their model, Bush and Mosteller focused on a specific measure of the learned association between the cue and reward: the probability of response. For Pavlov’s dogs, this is the probability of salivating in response to the buzzer. Bush and Mosteller used a simple equation to explain how that probability changes each time the reward is – or isn’t – given after the cue.</p>
<p class="TXI">Say you start with any random dog off the street (it is, in fact, rumoured that Pavlov got his subjects by stealing them off the streets). The probability that this dog will salivate at the sound of a buzzer starts at zero; it has no reason to suspect that the buzzer means food. Now you press the buzzer and then give the dog a piece of meat. According to the Bush-Mosteller model, after this encounter, the probability that the dog will salivate in response to the buzzer increases (see Figure 24). The exact amount that it increases depends on a parameter in the formula called the learning rate. Learning rates control the speed of the whole process. If the learning rate is very high, a single pairing could be enough to solidify the buzzer-food relationship in the dog’s mind. At more reasonable rates, however, the probability of salivating remains low after the first pairing – maybe it goes to 10 per cent – and raises each time the buzzer is followed by food.</p><a id="page_314"></a>
<p class="TXI">Regardless of the value of the learning rate, however, the second time the buzzer is followed by food, the probability of salivating increases <span class="italic">less</span> than it did after the first time. So, if it went from 0 to 10 per cent after the first pairing, it would increase only another nine percentage points, to 19 per cent, after the second pairing. And only by about eight percentage points after the third. This reflects that, in the Bush-Mosteller model (and in the dogs), the change to the probability with each pairing depends on the value of the probability itself. In other words, learning depends on what is already learned. </p>
<p class="TXI">This is, from a certain angle, intuitive. Nothing new is learned from seeing the sun rise every day. To the extent that we believe something will happen, its actual happening has little effect on us. Anticipated rewards are no different. We don’t update our opinion of our boss, for example, if we get the same holiday bonus we’ve received for the past five years. And the dogs only update their response to the buzzer to the extent that the food that follows differs from what they expect. The power to change expectations comes only from violating them.</p>
<p class="image-fig" id="fig24.jpg">
<img alt="" src="Images/chapter-11-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 24</span>
</span></p><a id="page_315"></a>
<p class="TXI">This violation can be for better or worse. To the dog, the first piece of post-‘buzz’ meat is a lovely surprise and one that has a big impact on its expectations. After repeated pairings, though, expectations shift and slobbering at the sound of the buzzer becomes second nature. At this point, the most impactful thing that could happen would be to hear the buzzer and <span class="italic">not</span> receive food. Such a deprivation would lead to a large decrease in the probability of future salivation – a decrease as large as the increase that occurred with the first pairing. This inverse side of reward-based learning, wherein the animal learns to <span class="italic">dis</span>associate a cue from a reward, is called extinction. With each presentation of the cue without the expected reward, the extinction process unbuilds the association, eventually extinguishing the learned response entirely. Bush and Mosteller made a point of showing that their model captures this process to a tee as well. </p>
<p class="TXI">At the same time that Bush and Mosteller were turning salivation information into equations, another man on the opposite side of the country was working to apply mathematics to some of the trickiest problems in business and industry. The deep and important connections between these works wouldn’t be realised for decades to come.</p>
<p class="center">* * *</p>
<p class="TXT">The RAND Corporation is an American think tank founded in 1948. A non-profit offshoot of the Douglas Aircraft Company, its central aim was to extend the collaboration between science and the military that blossomed out of necessity during the Second World War. The corporation’s name is appropriately generic (RAND literally stands for Research ANd Development) for the <a id="page_316"></a>range of research projects it pursues. Over the years, RAND employees have made significant contributions to the fields of space exploration, economics, computing and even foreign relations.</p>
<p class="TXI">Richard Bellman worked at RAND as a research mathematician from 1952 to 1965. An admirer of the subject as early as his teenage years, Bellman’s path to becoming a mathematician was repeatedly interrupted by the Second World War. First, to lend support to the war effort, he left his postgraduate training at Johns Hopkins University to teach military electronics at the University of Wisconsin. He later moved to Princeton University, where he taught in the Army Specialized Training Program and worked on his own studies. He would eventually complete his PhD at Princeton, but not before he was drafted to work in Los Alamos as a theoretical physicist for the Manhattan Project. The intrusions didn’t seem to impact his career prospects much. He became a tenured professor at Stanford University just three years after the war at the age of only 28. </p>
<p class="TXI">Leaving the academic world for RAND at 32 was, in Bellman’s words, the difference between being ‘a traditional intellectual, or a modern intellectual using the results of my research for the problems of contemporary society’. At RAND his mathematical skill was applied to real-world problems. Problems like scheduling medical patients, organising production lines, devising long-term investment strategies or determining the purchasing plan for department stores. Bellman didn’t have to set foot in a hospital or on a factory floor to help with these issues, however. All of these problems – and a great many more – are huddled under one abstract mathematical umbrella. <a id="page_317"></a>And, in the eyes of a mathematician, to be able to solve any of them is to solve them all.</p>
<p class="TXI">What these problems have in common is they are all ‘sequential decision processes’. In a sequential decision process, there is something to be maximised: patients seen, items produced, money made, orders shipped. And there are different steps that can be taken to do that. The goal is to determine which set of steps should be taken. How can the maximum be reached? What is the best way to climb the mountain?</p>
<p class="TXI">Without much previous work to draw on in this field, Bellman turned to a tried-and-true strategy in mathematics: he formalised an intuition.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> The mathematical conclusion this led him to is now known as the Bellman equation and the simple intuition it captures is that the best plan of action is the one in which all the steps are the best possible ones to take. Obvious though it may seem, when written in mathematics even banal statements can have power. </p>
<p class="TXI">To see how Bellman made use of this intuition, we have to understand how he framed the problem. Bellman first set out to define how good a plan was in terms of how much reward – be that money, widgets, shipments, <span class="italic">etc.</span> – it is likely to accrue. Let’s say you have a five-step plan. The total reward is the sum of the reward you get at each of those five steps. But after you’ve taken the first step, you now have a four-step plan. So, we could instead say that the total reward from the original five-step plan is the reward you get from taking the first step plus the <a id="page_318"></a>total reward from the four-step plan. And the total reward from the four-step plan is just the reward from taking <span class="italic">its</span> first step plus the reward from the resulting three-step plan. And so on, and so on.</p>
<p class="TXI">By defining the reward of one plan in terms of the reward of another, Bellman made his definition <span class="italic">recursive</span>. A recursive process is one that contains itself. Consider, for example, alphabetisation. If you want to alphabetise a list of names, you’d start by sorting all the names according to their first letter. After that, you’d need to apply that <span class="italic">same</span> sorting process again on all the names starting with the same letter to sort them according to their second letter, and so on. This makes alphabetisation recursive. </p>
<p class="TXI">Recursion is a common trick in mathematics and computer science in part because recursive definitions are flexible; they can be made as long or as short as needed. The formula for calculating a plan’s total reward, for example, can just as easily be applied to a five-step plan as a 500-step one. Recursion is also a conceptually simple way to accomplish something potentially difficult. Like the turns of a spiral staircase, each step in a recursive definition is familiar but not identical, and we need only follow them down one by one to the end.</p>
<p class="TXI">Bellman’s framing contains two further insights that helped make his strategy effective for deploying on real-world problems. The first was to incorporate the very relatable fact that a reward you get immediately is worth more than a reward you get later. He did this by introducing a <span class="italic">discounting factor</span> into his recursive definition. So, whereas in the initial formula the reward <a id="page_319"></a>from a five-step plan was equal to the reward from the first step plus the full reward from the four-step plan, an equation with discounting would say it is equal to the reward from the first step plus, maybe, 80 per cent of the reward from the four-step plan. Discounting is a way to weigh immediate gratification against delayed; it is 
‘a bird in the hand is worth two in the bush’ codified into mathematics. </p>
<p class="TXI">The second insight was more conceptual and more radical. It was a switch from focusing on rewards to focusing on <span class="italic">values</span>. </p>
<p class="TXI">To understand this switch, let’s consider the owner of a small business – a very small business. Angela is a busker on the New York City subway system. She knows she can play her electric violin for 20 minutes at certain subway stations before being chased away by the authorities, at which point she’s not allowed to return. Different stations, however, have different payouts. Tourist areas can be very lucrative whereas commuter stops for native New Yorkers yield far fewer donations. She’s leaving her house on Greenpoint Avenue in Brooklyn and wants to end up near a friend’s place in Bleecker Street. What path should she take to make the most money on the way to her destination? </p>
<p class="TXI">So far, we’ve noticed that, after starting from one position and taking a step in a plan, we find ourselves in circumstances broadly similar to how we began – except we are starting from a different position and have a different plan. In sequential decision-making, the different positions we can move through are called states and the steps in a plan are frequently referred to as actions. In Angela’s case, the states are the different <a id="page_320"></a>subway stations she can be at. Each time Angela takes an action (for example, from station A to station B), she finds herself in a new state (station B) that both yields some reward (the amount of donations her playing gets) and provides her with a new set of possible actions (other stations to go to). In this way, states define what actions are available (you can’t go straight from Greenpoint Avenue to Times Square, for example) and actions determine what the next states are. </p>
<p class="TXI">This interplay – wherein the actions taken as part of a plan affect what actions will be available in the future – is part of what makes sequential decision processes so difficult. What Bellman did was to take this constellation of states, actions and rewards, and turn it on its head. Rather than talk about the reward expected from a series of actions, he focused on the value that any given state has.</p>
<p class="TXI">Value, as used colloquially, is a nebulous concept. It elicits ideas about money and worth, but also deeper notions of meaning and utility that can be hard to pin down. The Bellman equation, however, defines value precisely. Using the same recursive structure introduced earlier, Bellman defined the value of a state as the reward you get in that state plus the discounted value of the next state. You’ll notice, in this definition there is no explicit concept of a plan; value is defined by other values. </p>
<p class="TXI">Yet, this equation does rely on knowledge of the next state. Without a plan to say what action is taken, how do we know what the next state will be? This is where the original intuition – the idea that the best plan is made up of the best actions – comes into play. To calculate the value at the next state, you simply assume that the best <a id="page_321"></a>possible action is taken. And the best possible action is the one that leads to the state with the highest value! When wrapped up in the language of value, the plan itself fades away. </p>
<p class="TXI">So how does this help Angela? Given a map of possible subway stations (see Figure 25) and the associated donations she expects to get from each, we can calculate a ‘value function’. A value function is simply the value associated with each state (in this case, each station). We can calculate this by starting at the end and working backwards. Once Angela reaches Bleecker Street, she will go straight to her friend’s house and not do any busking, so the reward she will get at her final destination is $0. Because there are no further states from this point, the value of Bleecker Street is also zero. Backing up from here, the values of Union Square and 34th Street can be calculated in terms of the reward expected there and the value of Bleecker Street. This process continues until the value for each station is calculated.</p>
<p class="image-fig" id="fig25.jpg">
<img alt="" src="Images/chapter-11-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 25</span>
</span></p>
<p class="TXI">With these values in hand, Angela can now plan her journey. Starting out from Greenpoint Avenue, she can take the train to either Court Square or Metropolitan Avenue. Which should she choose? Looking just at the possible rewards from each, Metropolitan Avenue seems the better choice, as it offers $10 versus Court Square’s $5. But looking at the value function, Court Square is the correct choice. This is because the value function cares about what states you can get yourself into in the future – and from Court Square Angela can go straight to the jackpot, Times Square. Angela could also go to Queen’s Plaza from Court Square, but that isn’t relevant here, because the value function assumes Angela is smart. It assumes that from Court Square she would go to <a id="page_322"></a>Times Square because Times Square is the better choice. All in all, following the value function would take Angela through Court Square to Times Square then to 34th Street and finally on to her destination at Bleecker Street. In total, she will have earned $65 – the most that any path on this map could offer.</p>
<p class="TXI">Bellman’s move to focus on the value function was important because it corrected a flaw in the original framing of the problem. We started out by trying to calculate the total reward we could get from a given plan. But when solving a sequential decision process, we <a id="page_323"></a>aren’t given a plan. In fact, a plan is exactly what we’re trying to find! Once we know the value function, though, the plan is simple: follow it. Like breadcrumbs left on a forest path, the value function tells you where to go. Anyone looking for the most reward needs only to greedily seek the next state with the highest value. All actions can be chosen based on this simple rule.</p>
<p class="TXI">Some interesting things happen as a result of the discounting that is part of the definition of value. For example, look at the options Angela has from Times Square. She can either go to 34th Street, get $20 and then end at Bleecker Street or she can go to 14th Street, get $8, then go to Union Square and get $12 and finally end at Bleecker Street. Both routes earn her $20 in total. But the value of 34th Street is 20 whereas the value of 14th Street is 17.6 (calculated as 8 + 0.8 x 12), indicating that 34th Street is the better option. This demonstrates how discounting future rewards can lead to plans with fewer steps; if there is only so much reward to get, it’s best to get it quicker rather than slower. Discounting also means that even big rewards will be ignored if they are too distant. If a train station in New Jersey could garner Angela $75, it may still not influence her choice as she leaves her house. The impact of a reward on a value function is like the ripple from a stone dropped in water. It’s felt strongest in the nearby states, but its power is diluted the farther away you go.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup></p> <a id="page_324"></a>
<p class="TXI">This technical definition of value – based on states and recursion and discounting factors – may seem a far cry from the word we use in everyday language. But those colloquial connotations are very much present in this equation. Why do we value money? Not because there is actually much pleasure to be had from the paper or the coin itself, but because of the future we can envision once we have that paper or coin. Money is worth only what it can get us later and what we can get later is baked into Bellman’s definition of value. </p>
<p class="TXI">Bellman’s work to frame sequential decision processes this way truly did allow him to become the ‘modern intellectual’ he aimed to be when moving to RAND. In the years after his first publications describing this solution, countless companies and government entities began to apply it out in the world. By the 1970s, Bellman’s ideas had been deployed on problems as diverse as sewer system design, airline scheduling and even the running of research departments at large companies such as Monsanto. The technique went under the name ‘dynamic programming’, a rather bland phrase Bellman actually coined with the aim of keeping some of the mathematics-phobic higher-ups in the military out of his hair. ‘The 1950s were not good years for mathematical research,’ Bellman wrote in his autobiography. ‘The RAND Corporation was employed by the Air Force, and the Air Force had [Charles] Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. … Thus, I thought dynamic programming was a good name. It was something not even a <a id="page_325"></a>Congressman could object to. So I used it as an umbrella for my activities.’ </p>
<p class="TXI">In applying the method in each of these settings, engineers had to find a way to calculate the value function. In some cases, like the subway example given above, the landscape of the problem is simple enough that the calculation is straightforward. But simple problems are rarely realistic. The real world has a large number of potential states; these states can connect to each other in complex or even uncertain ways; and they can do so through many possible actions. Much effort was put into finding the value function in these trickier situations. Yet even with clever techniques, the application of dynamic programming usually pushed at the edge of computing power at the time. The calculation of the value function was always a bottleneck in the process. And without a way to find the value function, the full potential of Bellman’s contributions would remain unreached. </p>
<p class="center">* * *</p>
<p class="TXT">There is an irony to Pavlov’s legacy. Its immediate effect was to set off behaviourism, a movement with religious-like dedication to ignoring the mind and focusing only on directly measurable behaviour. Yet the lineage of mathematical models it spawned found success in the other direction, by going increasingly inside the mind; to capture reinforcement learning in equations required the use of terms representing hidden mental concepts.</p>
<p class="TXI">One of the well-known extensions to the Bush-Mosteller model came 20 years later, in 1972, and was developed by another duo, Yale psychologists Robert <a id="page_326"></a>Rescorla and Allan Wagner. Rescorla and Wagner generalised the Bush-Mosteller model, making it applicable to a wider range of experimental settings and able to capture more findings. The first alteration they made was to the very measure the model was trying to explain. </p>
<p class="TXI">Bush and Mosteller’s ‘probability of response’ was too specific and too limited. Rescorla and Wagner instead aimed to capture a more abstract value which they referred to as ‘associative strength’. This strength of the association between a cue and a reward is something that would exist in the mind of the participant, making it not directly measurable, but different experiments could try to read it out in different ways. This could include measuring a probability of response, like the probability of salivating, but also other measures, such as the amount of salivation, or behaviours like barking or movement. In this way, Rescorla and Wagner subsumed the Bush-Mosteller model into a broader framework. </p>
<p class="TXI">The Rescorla-Wagner model also expanded to incorporate a known feature of conditioning experiments referred to as ‘blocking’. Blocking occurs when an initial cue is paired with a reward, and then a second cue is also given along with the first and both are paired with the reward. So, for example, after a dog had learned to associate the sound of the buzzer with food, the experimenter would then flash a light at the same time as the buzzer and then give the food. In Bush and Mosteller’s model, the cues were treated completely separately. So, if the light and buzzer were paired with food enough times, the dog should come to associate the light with food the same way it had learned the association with the buzzer. Then we would expect that presenting the light alone <a id="page_327"></a>would cause the dog to salivate. This is not, in fact, what happens; the dogs don’t salivate in response to the light alone. The presence of the buzzer <span class="italic">blocks</span> the ability of the light to be associated with the food. </p>
<p class="TXI">This provides further proof that learning is driven by errors. In particular, errors about predicted reward. When the animal hears the buzzer, it knows food is coming. So, when the food arrives there is no error in its prediction of that reward. As we saw before, that means it doesn’t update its beliefs about the buzzer. But it also means it doesn’t update its beliefs about anything else either. Whether there was a light on at the same time as the buzzer or not is irrelevant. The light has no bearing on the predicted reward, the received reward or the difference between the two, which defines the prediction error – and without an error, everything is stuck as it is. Prediction error is the grease that oils the wheels of learning. </p>
<p class="TXI">Rescorla and Wagner thus made their update in associative strength between one cue and a reward dependent not just on that cue’s current associative strength, but on the sum of the associative strengths of all cues present. If one of these associative strengths is high (for example if the buzzer is present), then the presence of the reward wouldn’t change any of them (the association with the light is never learned). This summing across multiple cues is also something that would need to be done internal to the animal, further reflecting the rejection of behaviourism and a move to the mind. </p>
<p class="TXI">But the watershed moment in reinforcement learning came in the mid-1980s from the work of a ponytail-wielding Canadian computer scientist named Richard <a id="page_328"></a>Sutton and his PhD adviser Andrew Barto. Sutton was educated in both psychology and computer science, and Barto spent a lot of his time reading the psychology literature. This proved a powerful combination as the work they did together pulled from and gave back to both fields. </p>
<p class="TXI">Sutton’s work removed the final tangible element of the model: the reward itself. Up until this point, the moment of learning centred around the time that 
a reward was given or denied. If you catch a whiff of the smoke from a blown-out candle and are then handed a piece of birthday cake, the association between the two strengthens. But a candle extinguished at the end of 
a religious ceremony likely doesn’t come with cake and so the association weakens. In either case, though, the cake itself is the important variable. Its presence or absence is key. Anything can serve as a cue, but the reward must be a primal one – food, water, sex. But once we come to associate smoke with birthday cake, we may notice some other regularities. For example, the smoke is usually preceded by singing and the singing may be preceded by people putting on silly hats. None of these things are rewards in and of themselves (especially the singing, at most parties), but they form a chain that attaches each in some degree to the primary reward. Knowing this information can be useful: if we want cake, being on the lookout for silly hats may help.</p>
<p class="TXI">Rescorla and Wagner had no way of allowing for this backing up of associations – no way, essentially, for a 
cue associated with a reward in one circumstance to 
play the role of a reward in another. But Sutton did. In 
Sutton’s algorithm – known as ‘temporal difference <a id="page_329"></a>learning’ – beliefs are updated in response to any violation of expectations. Walking through your office hallway to your desk, for example, expectations about reward may be pretty low. But when you hear your co-workers in the conference room start the first verse of ‘Happy Birthday’, a violation has occurred. Beliefs must be updated; you are now in a state where reward is on the horizon. This is where temporal difference learning occurs. You may choose to enter the conference room, finish the song, smell the candles and eat the cake. In doing those actions, no further violations will have occurred – and therefore, no further learning. It is thus not the receipt of the reward itself that causes any changes. The only learning that happened was in that hallway, many steps away from the reward. </p>
<p class="TXI">What exactly is being learned here, though? What mental concept is it that got updated in the hallway? It’s not the association of a cue with a reward – not directly at least. Instead it’s more of a signal telling you the path to reward, should you follow the right steps in between. </p>
<p class="TXI">This may sound familiar because what temporal difference learning helps you learn is a value function. At each moment in time, according to this framing, we have expectations – essentially a sense of how far we are from reward – that define the value of the state we are in. As time passes or we take actions in the world we may find ourselves in new states, which have their own associated values. If we’ve correctly anticipated the value of those new states, then all’s well. But if the value of the current state is different from what we predicted it would be when we were in the state before, we’ve got an error. And errors induce learning. In particular, if the value of <a id="page_330"></a>the current state is more or less than we expected it to be when we were in the previous state, we <span class="italic">change the value of the previous state</span>. That is, we take the surprise that occurred now and use it to change our belief about the past. That way, the next time we find ourselves in that previous state, we will better be able to predict the future. </p>
<p class="TXI">Consider driving to an amusement park. Here the value of your location is measured in terms of how far you are from this rewarding destination. As you leave your house, you expect to arrive in 40 minutes. You drive straight for five minutes and get on a highway. You now expect to arrive in 35 minutes. After 15 minutes on the highway, you take an exit. Your estimated time of arrival is now 20 minutes. But, when you get off that exit and turn on to a side street you hit a traffic jam. As you sit in your hardly moving car, you know you won’t be at the park for another 30 minutes. Your expected arrival time has now jumped up by 10 minutes – a significant error. </p>
<p class="TXI">What should be learned from this error? If you had an accurate view of the world, you would’ve anticipated 30 more minutes of driving at the moment you took the exit. So, temporal difference learning says you should <span class="italic">update the value of the state associated with that exit</span>. That is, you use the information received at one state (a traffic jam on the side street) to update your beliefs about the value of the state before (the exit). And this may mean that the next time you drive to this amusement park, you will avoid that exit and choose another instead. But it doesn’t take arriving at the amusement park 10 minutes late to learn from that mistake; the expectation of that happening at the sight of the traffic was enough.</p><a id="page_331"></a>
<p class="TXI">What Sutton’s algorithm shows is that by mere exploration – simple trial and error – humans, animals or even an artificial intelligence can eventually learn the correct value function for the states they’re exploring. All it takes is updating expectations when expectations change – ‘learning a guess from a guess’ as Sutton describes it. </p>
<p class="TXI">As an extension of Bellman’s work on dynamic programming, temporal difference learning had the potential to solve real-world problems. Its simple moment-by-moment learning rule made it attractive from a computing perspective: it didn’t demand as much memory as programmes that needed to store the entire set of actions that preceded a reward before learning from it. It also worked. One of the biggest displays of its power was TD-Gammon, a computer program trained via temporal difference learning to play the board game backgammon. Board games are particularly useful tests of reinforcement learning because rewards frequently only come at the very end of a game, in the form of a win or loss. Using such a coarse and distant signal to guide strategy at the very first move is a challenge, but one that temporal difference learning could meet. Built in 1992 by Gerald Tesauro, a scientist at IBM, TD-Gammon played hundreds of thousands of games against itself, eventually reaching the level of an intermediate player without ever taking instructions from a human. Because it learned in isolation, it also developed strategies not tried by humans (who are generally influenced by each other’s gameplay to stick within a certain set of moves). In the end, TD-Gammon’s unusual moves actually influenced theory and understanding about the game of backgammon itself.</p><a id="page_332"></a>
<p class="TXI">In 2013 another application of temporal difference learning made headlines, this time applied to video games. Scientists at the artificial intelligence research company DeepMind built a computer program that taught itself to play multiple games from the 1970s arcade system Atari. This artificial gamer got the full Atari experience. The only inputs to the algorithm were the pixels on the screen – it was given no special knowledge that some of those pixels may represent spaceships or ping-pong bats or submarines. The actions it was allowed to take included the standard buttons such as up, down, left, right, A, B; and the reward for the model came in terms of the score provided by the game it was playing. As this burdens the algorithm with a more challenging task than backgammon – which at least had the concepts of pieces and locations baked into the inputs to the model – the researchers combined temporal difference learning with deep neural networks (a method we encountered in <a href="chapter3.xhtml#chapter3">Chapter 3</a>).<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> One version of this deep neural network had around 20,000 artificial neurons and, after weeks of learning, reached human-level performance on 29 out of 49 games tested. Because this Atari algorithm also learned asocially, it ended up with some interesting quirks, including discovering a clever trick for tunnelling through a wall in the brick-clearing game <span class="italic">Breakout</span>. </p>
<p class="TXI">While games are a flashy and fun way to demonstrate the power of this approach, its application didn’t stop there. After Google acquired DeepMind in 2014, it set reinforcement-learning algorithms to the task of <a id="page_333"></a>minimising energy use in its massive data centres. The result was a 40 per cent decrease in energy used to cool the centres and likely hundreds of millions in savings over the years as a result. With a single-minded focus on achieving the goal at hand, reinforcement learning algorithms find creative and efficient solutions to hard problems. These alien minds can thus help devise plans humans would’ve never thought of. </p>
<p class="TXI">The paths of sequential decision-making and Pavlovian conditioning represent a victory of convergent scientific evolution. The trajectories of Bellman and Pavlov start with separate and substantial problems, each seething with their own demanding details. How should a hospital schedule its nurses and doctors to serve the most patients? What causes a dog to salivate when the sound of a buzzer hits its ears? These questions are seemingly worlds apart. But by peeling away the weight of the specifics – leaving only the bare bones of the problem to remain – their interlocking nature becomes clear. This is one of the roles of mathematics: to put questions disconnected in the physical world into the same conceptual space wherein their underlying similarities can shine through.</p>
<p class="TXI">The story of reinforcement learning is thus one of successful interdisciplinary interaction. It shows that psychology and engineering and computer science can work together to make progress on hard problems. It demonstrates how mathematics can be used to understand, and replicate, the ability of animals and humans to learn from their surroundings. The story would be a remarkable one as it is, if it ended there. But it doesn’t end there.</p>
<p class="center">* * *</p><a id="page_334"></a>
<p class="TXT-con">Octopamine is a molecule found in the nervous systems of many insects, molluscs and worms. It is so named because of its initial discovery in the salivary glands of the octopus in 1948. In the brain of a bee, octopamine is released upon run-ins with nectar. In the early 1990s, Terry Sejnowski, a professor at the Salk Institute in San Diego, California, and two of his lab members, Read Montague and Peter Dayan, were thinking about octopamine. In particular, they built a model – a computer simulation of bee behaviour – that was centred on the neuron in the bee brain that releases octopamine. A bee’s choices about what flowers to land on or avoid, they proposed, could be explained by a Rescorla-Wagner model of learning and a neural circuit including the octopamine neuron could be the hardware that implements it. But as they worked out this octopamine puzzle, the team heard of another study, conducted some 6,000 miles away by a German professor named Wolfram Schultz, on octopamine’s chemical cousin dopamine.</p>
<p class="TXI">You may be familiar with dopamine. It has a bit of a reputation in popular culture. Countless news articles refer to it as something like ‘our brain’s pleasure and reward-related chemical’ or talk about how everyday activities like eating a cupcake cause ‘a surge of the reward chemical dopamine to hit the decision-making area of the brain’. It’s branded as the pleasure molecule and it’s not uncommon for products to be pedalled under its powerful name. Pop stars have named albums and songs after it. ‘Dopamine diets’ claim (without evidence) to provide foods that boost dopamine while keeping you slim. And the tech start-up Dopamine Labs promised to increase user engagement in phone apps by <a id="page_335"></a>doling out squirts of the neurotransmitter. This poor celebrity chemical has also been badly maligned – referred to as the source of all addictions and maladaptive behaviours. Online communities like The Dopamine Project have cropped up aiming to provide ‘better living through dopamine awareness’. And some Silicon Valley dwellers have even attempted ‘dopamine fasts’ as a respite from constant over-stimulation.</p>
<p class="TXI">While it is true that a release of dopamine can accompany rewards, that is far from the whole story. What Schultz’s study in particular showed was a case in which the neurons that dole out dopamine were <span class="italic">silent</span> when a reward was given.</p>
<p class="TXI">Specifically, Schultz trained monkeys to reach their arm out in front of them in order to receive some juice.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> During this training process, he recorded the activity of a population of dopamine-releasing neurons tucked into the underside of the brain. Schultz observed that at the end of training – when the animals knew they would get some juice by making a reach – these neurons showed no response at all to the juice reward being delivered. </p>
<p class="TXI">When Schultz first published these results he didn’t have a clear explanation for why the dopamine neurons were behaving this way, but the members of the Sejnowski lab did. And they reached out to Schultz to embark on a collaboration that would test the hypothesis that dopamine neurons encode the prediction errors necessary for temporal difference learning. It would be <a id="page_336"></a>the start of what Sejnowski referred to as ‘one of the most exciting scientific periods of my life’. </p>
<p class="TXI">Dayan and Montague worked to reanalyse Schultz’s data through the lens of learning algorithms. They focused on the simplest of Schultz’s experiments, which consisted of a light at the desired reach location turning on and, if the animal reached to it, a drop of juice was delivered half a second later. What they wanted to know was how the response of the dopamine neurons changed as the animal came to learn this association. But they were also interested in a particular circumstance after learning: what happens when the juice doesn’t follow the light. If the animals learned the light–juice association, they would know to expect it and if the juice didn’t show up that would be a significant prediction error. Did the dopamine neurons reflect that? </p>
<p class="TXI">The neurons that release dopamine tend to fire around five spikes per second when nothing much is going on. At the start of the learning process, right after the animal got what seemed like a surprise shot of juice after making an arm movement, that rate jumped up briefly to about 20 spikes per second. The light that came before the movement, however, elicited nothing. But after enough pairings, once the animal came to understand how the light and the reach and the juice were all related, that pattern shifted. The dopamine neurons stopped responding to the juice. This is a change perfectly in line with the notion that they signal prediction error, because once the animal can correctly predict the juice there is no more error. And they started responding to the light. Why? Because the light had become associated with the reward but – crucially – they had no idea when it would <a id="page_337"></a>come on. When it did arrive it was an error. Specifically, it’s an error in the predicted value of the state of the animal. Sitting in the experimental chair going about its life, the monkey expects the next moment to be more or less similar to the current one. When the light turns on, that expectation is violated. Like hearing the first few bars of ‘Happy Birthday’ in your office hallway, it’s a pleasant surprise, but a surprise nonetheless.</p>
<p class="TXI">The final analysis – done while sporadically omitting the juice after the reach – was to see how <span class="italic">unpleasant</span> surprises were encoded. If dopamine is encoding errors, it should indicate when things are worse than expected as well. And with the juice absent, the neurons did just that. They had a dip in their firing right at the time the juice would’ve been delivered. Specifically, the neurons would go from five to 20 spikes per second in response to the light; then as the animal reached out its arm they’d return back to five. But, about half a second after the reach, when it was clear there was no juice coming, they’d shut off completely. An expectation had been violated and the dopamine neurons were letting it be known.</p>
<p class="TXI">This study showed that the firing of dopamine neurons can signal the errors – both positive and negative – about predicted values that are needed for learning. It was thus an important point in shifting the understanding of dopamine from a pleasure molecule to a pedagogical one.</p>
<p class="TXI">If the point of encoding the error is to learn from it, though, where does that learning happen? It turns out that’s a bit hard to pin down because these dopamine-releasing neurons release dopamine in many corners of the brain; their projections burrow through the brain like pipework, touching regions near and far. Still, <a id="page_338"></a>a location that seems particularly important is the striatum. The striatum is a group of neurons that serves as the primary input to a collection of brain areas involved in guiding movement and actions. Neurons in the striatum contribute to the production of behaviour by associating sensory inputs with actions or actions with other actions.</p>
<p class="TXI">As we saw in <a href="chapter4.xhtml#chapter4">Chapter 4</a>, Hebbian learning is an easy way for associations between ideas to become encoded in the connections between neurons. Under Hebbian rules, if one neuron regularly fires before another, the weight of the connection from the first to the second is strengthened. In reinforcement learning, however, we need more than just to know that two events happened close in time. We need to know how those events relate to reward. Specifically, we only want to update the strength between a cue and an action (for example, seeing a light and reaching for it) if that pairing turns out to be associated with reward. </p>
<p class="TXI">So, the neurons in the striatum don’t follow basic Hebbian learning. Instead they follow a modified form wherein the firing of one neuron before another only strengthens their connection if it happens <span class="italic">in the presence of dopamine</span>. Dopamine – which encodes the error signal needed for updating values – is thus also required for the physical changes needed for updating that occur at the synapse. In this way, dopamine truly does act as a lubricant for learning. </p>
<p class="TXI">Having the language of temporal difference learning in which to talk about the functioning of the brain has altered the conversation on clinical topics such as addiction. One theory, put forth in 2004 by neuroscientist <a id="page_339"></a>David Redish, tries to explain the addictive properties of drugs like amphetamine and cocaine in terms of the effects they have on dopamine release. It posits that these drugs cause a release of dopamine that is independent of the true prediction error. Specifically, by overdriving the dopamine neurons, these drugs send the false signal to the rest of the brain that the drug experience is always better than expected. This errant error signal still drives learning, pushing the estimated value of states associated with drug use higher and higher. Deforming the value function in this way is guaranteed to have detrimental effects on behaviour like the ones seen in addiction.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup></p>
<p class="center">* * *</p>
<p class="TXT">David Marr was a British neuroscientist with a background in mathematics. His book, <span class="italic">Vision</span>: ﻿A <span class="italic">Computational Investigation into the Human Representation and Processing of Visual Information</span> was published in 1982, two years after his death. In the first chapter, he lays out the components needed for a successful analysis of a neural system. According to Marr, to understand any bit of the brain we should be able to explain it on each of three levels: computational, algorithmic and implementational. The computational level asks what is the overall purpose of this system, that is, what is it trying to do? The algorithmic level asks how, <span class="italic">i.e.</span>, through what steps, does it achieve this <a id="page_340"></a>goal. And finally, the implementational level asks specifically what bits of the system – what neurons, neurotransmitters, <span class="italic">etc.</span> – carry out these steps.</p>
<p class="TXI">An explanation encompassing all of Marr’s levels is an aspiration towards which many neuroscientists strive. The systems that carry out reinforcement learning are a rare case where they can come within striking distance of this high bar. At the computational level reinforcement learning has a simple answer: maximise reward. This is what Bellman recognised as the goal of sequential decision processes and what following the value function should get you. But how do we learn the value function? That’s where temporal difference learning comes in. The work of Bush, Mosteller, Resorla, Wagner and Sutton all turned stacks of data from conditioning experiments into strings of symbols that could describe the algorithm needed to do the learning part of reinforcement learning. On the implementation level, dopamine neurons take on the task of calculating prediction error and the signals they send to other brain areas control the associations learned there. In this way, a satisfying understanding of a fundamental ability – to learn from rewards – was achieved by tunnelling towards the topic from many different angles.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-1" id="fn-1">1</a> ﻿The kind of conditioning Skinner is most associated with is known as ﻿‘﻿operant conditioning﻿’﻿, which involves performing an action before getting a reward. The line between operant and Pavlovian conditioning is sometimes sharp, sometimes blurred and information in this chapter will at times relate to both.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-2" id="fn-2">2</a> ﻿Interestingly, Bellman was aware of Bush and Mosteller﻿’﻿s publications, but his work on these problems was developed independently of that.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-3" id="fn-3">3</a> ﻿Because it controls the balance between caring about now versus the future, the strength of discounting can have sizable impacts on value and therefore on which actions are chosen. Scientists have posited that disorders such as addiction or ADHD can be understood in terms of inappropriate reward discounting. More on addiction later.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-4" id="fn-4">4</a> ﻿Specifically, they used a deep convolutional neural network which, as we saw in ﻿﻿Chapter 6﻿﻿, is used to model the visual system. ﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-5" id="fn-5">5</a> ﻿This is actually an example of the ﻿‘﻿operant﻿’﻿ form of conditioning mentioned earlier, because the animals need to make a reach to receive their reward.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-6" id="fn-6">6</a> ﻿This theory can explain several aspects of addiction, but one of its big predictions failed. If these drugs lead to non-stop prediction error, then the blocking phenomena described previously shouldn﻿’﻿t be seen when drugs are used as reward. An experiment in rats indicated, however, that blocking still does occur.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 3</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter3"><a href="contents.xhtml#re_chapter3">CHAPTER THREE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter3">Learning to Compute</a><a id="page_49"></a></p>
<p class="H1" id="b-9781472966445-ch222-sec3">
<span class="bold">
<span>McCulloch-Pitts, the Perceptron and 
artificial neural networks</span>
</span></p>
<p class="TXT">Cambridge University mathematician Bertrand Russell spent 10 years at the beginning of the twentieth century toiling towards a monumental goal: to identify the philosophical roots from which all of mathematics stems. Undertaken in collaboration with his former teacher Alfred Whitehead, this ambitious project produced a book, the <span class="italic">Principia Mathematica</span>, which was delivered to the publishers overdue and over budget. The authors themselves had to chip in towards the publishing costs just to get it done and they didn’t see any royalties for 40 years. </p>
<p class="TXI">But the financial hurdle was perhaps the smallest one to overcome in getting this opus finished. Russell had to fight against his own agitation with the scholarly material. According to his autobiography, he spent days staring at a blank sheet of paper and evenings contemplating jumping in front of a train. Work on the book also coincided with the dissolution of Russell’s marriage and a strain on his relationship with Whitehead – who, according to Russell, was fighting his own mental and marital battles at the time. The book was even physically demanding: Russell spent 12 hours a day at a desk writing out the intricate symbolism needed <a id="page_50"></a>to convey his complex mathematical ideas and when the time came to bring the manuscript to the publisher it was too large for him to carry. Despite it all, Russell and Whitehead eventually completed and published the text they hoped would tame the seemingly wild state of mathematics. </p>
<p class="TXI">The conceit of the <span class="italic">Principia</span> was that all of mathematics could be reduced to logic. In other words, Russell and Whitehead believed that a handful of basic statements, known as ‘expressions’, could be combined in just the right way to generate all the formalisms, claims and findings of mathematicians. These expressions didn’t stem from any observations of the real world. Rather, they were meant to be universal. For example, the expression: if X is true, then the statement ‘X is true or Y is true’ is also true. Such expressions are made up of propositions: fundamental units of logic that can be either true or false, written as letters like X or Y. These propositions are strung together with ‘Boolean’ operators<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> such as ‘and’, ‘or’ and ‘not’. </p>
<p class="TXI">In the first volume of the <span class="italic">Principia</span>, Russell and Whitehead provided fewer than two dozen of these abstract expressions. From these humble seeds, they built mathematics. They were even able to triumphantly conclude – after scores of symbol-filled pages – that 1+1=2. </p>
<p class="TXI">Russell and Whitehead’s demonstration that the full grandeur of mathematics could be captured with the simple <a id="page_51"></a>rules of logic<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> had immense philosophical implications as it provided proof of the power of logic. What’s more, it meant that a subsequent finding, made by a different pair of men some 30 years later, would have immense implications of its own. This finding said that neurons, simply via the nature of their anatomy and physiology, were implementing the rules of logic. It revolutionised the study of the brain and of intelligence itself. </p>
<p class="center">* * *</p>
<p class="TXT">When Detroit native Walter Pitts was just 12 years old, he was invited by Russell to join him as a graduate student at Cambridge University. The young boy had, the story goes, encountered a copy of the <span class="italic">Principia</span> after running into a library to avoid bullies. As he read, Pitts found what he believed to be errors in the work. So, he sent his notes on the subject off to Russell, who, presumably not knowing the boy’s age, then offered him the position. Pitts didn’t accept it. But a few years later, when Russell was visiting the University of Chicago, Pitts went to sit in on his lectures. Having fled an abusive family home to come to Chicago, Pitts decided not to return. He remained in the city, homeless. </p>
<p class="TXI">Luckily, the University of Chicago had another world-famous logician for Pitts to criticise – Rudolf Carnap. Again, Pitts wrote up notes – this time identifying issues in Carnap’s recent book <span class="italic">The Logical Syntax of Language</span> – and delivered them to Carnap’s office at the University of Chicago. Pitts didn’t stick around long enough to hear his <a id="page_52"></a>reaction, but Carnap, impressed, eventually chased down Pitts, whom he referred to as ‘that newsboy who understood logic’. On this occasion, the philosopher he critiqued actually did get Pitts to work with him. Though he never officially enrolled, Pitts functioned effectively as a graduate student for Carnap and fraternised with a group of scholars who were interested in the mathematics of biology.</p>
<p class="TXI">Warren McCulloch’s interest in philosophy took a more traditional form. Born in New Jersey, he studied the subject (along with psychology) at Yale and read many of the greats. He was most enamoured with Immanuel Kant and Gottfried Leibniz (whose ideas were very influential for Russell), and he read the <span class="italic">Principia</span> at the age of 25. But, despite the beard upon his long face, McCulloch was not a philosopher – he was a physiologist. He attended medical school in Manhattan and then went on to observe the panoply of ways in which the brain can break as a neurology intern at Bellevue Hospital and at the Rockland State Hospital psychiatric facility. In 1941, he joined the University of Illinois at Chicago as the director of the laboratory for basic research in the department of psychiatry. </p>
<p class="TXI">As with all great origin stories, there are conflicting accounts of how McCulloch and Pitts met. One claims that it happened when McCulloch spoke in front of a research group Pitts was a part of. Another story is that Carnap introduced them. Finally, a contemporary of the two men, Jerome Lettvin, claimed he introduced them and that all three bonded over a mutual love of Leibniz. In any case, by 1942, the 43-year-old McCulloch and his wife had taken the 18-year-old Pitts into their home, <a id="page_53"></a>and the two men were spending evenings drinking whisky and discussing logic. </p>
<p class="TXI">The wall between ‘mind’ and ‘body’ was strong among scientists in the early twentieth century. The mind was considered internal and intangible; the body, including the brain, was physical. Researchers on either side of this wall toiled diligently, but separately, at their own problems. Biologists, as we saw in the last chapter, were working hard to uncover the physical machinery of neurons: using pipettes and electrodes and chemicals to sort out what causes a spike and how. Psychiatrists, on the other hand, were attempting to uncover the machinery of the mind through lengthy sessions of Freudian psychoanalysis. Few on either side would attempt a glance over the wall at the other. They spoke separate languages and worked towards different goals. For most practitioners, the question of how neural building blocks could create the structure of the mind was not just unanswered, it was unasked. </p>
<p class="TXI">But McCulloch, from as early on as his time at medical school, had immersed himself in a crowd of scientists who did care about this question and allowed him the space to think about it. Eventually, through his physiological observations, he came up with a hunch. He saw in the emerging concepts of neuroscience a possible mapping to the notions of logic and computation he so adored in philosophy. To think of the brain as a computing device following the rules of logic – rather than just a bag of proteins and chemicals – would open the door to understanding thought in terms of neural activity.</p> <a id="page_54"></a>
<p class="TXI">Analytical skill, however, was not where McCulloch excelled. Some who knew him say he was too much of a romantic to be held down by such details. So, despite years of toying with these ideas in his mind and in conversation (even as a Bellevue intern he was accused of ‘trying to write an equation for the working of the brain’), McCulloch struggled with several technical issues of how to enact them. Pitts, however, was comparably unfazed by the analytical. As soon as he spoke with him about it, Pitts saw what approaches were needed to formally realise McCulloch’s intuitions. Not long after they met, one of the most influential papers on computation was written.</p>
<p class="TXI">‘A logical calculus of the ideas immanent in nervous activity’ was published in 1943. The paper is 17 pages long with many equations, only three references (one of which is to the <span class="italic">Principia</span>) and a single figure consisting of little neural circuits drawn by McCulloch’s daughter.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> </p>
<p class="TXI">The paper begins by reviewing the biology of neurons that was known at the time: neurons have cell bodies and axons; two neurons connect when the axon of the first meets the body of the second; through this connection one neuron provides input to the other; a certain amount of input is needed for a neuron to fire; a cell either fires a spike or it doesn’t – no half spikes or in-between spikes; and the input from some neurons – inhibitory neurons – has the power to prevent a cell from spiking.</p><a id="page_55"></a>
<p class="TXI">McCulloch and Pitts go on to explain how these biological details are congruent with Boolean logic. The core of their claim is that the activity state of each neuron – either firing or not – is like the truth value of a proposition – either true or false. In their own words, they ‘conceive of the response of any neuron as factually equivalent to a proposition which proposed its adequate stimulus’. </p>
<p class="TXI">By ‘its adequate stimulus’ they are referring to something about the world. Imagine a neuron in the visual cortex whose activity represents the statement ‘the current visual stimulus looks like a duck’. If that neuron is firing, that statement is true; if the neuron is not firing, it is false. Now imagine another neuron, in the auditory cortex, that represents the statement ‘the current auditory stimulus is quacking like a duck’. Again, if this neuron is firing, that statement is true, otherwise it is false. </p>
<p class="TXI">Now we can use the connections between neurons to enact Boolean operations. For example, by giving a third neuron inputs from both of these neurons, we could implement the rule ‘if it looks like a duck <span class="italic">and</span> it quacks like a duck, it’s a duck’. All we have to do is build the third neuron such that it will only fire if both of its input neurons are firing. That way, both ‘looks like a duck’ and ‘quacks like a duck’ have to be true in order for the conclusion represented by the third neuron (‘it’s a duck’) to be true. </p>
<p class="TXI">This describes the simple circuit needed to implement the Boolean operation ‘and’. McCulloch and Pitts in their paper show how to implement many others. To implement ‘or’ is very similar, however the strength of the connections from each neuron must be so strong <a id="page_56"></a>that one input alone is enough to make the output neuron fire. In this case, the ‘is a duck’ neuron would fire if the ‘looks like a duck’ neuron <span class="italic">or</span> the ‘quacks like a duck’ neuron (or both) were firing. The authors even show how to string together multiple Boolean operations. For example, to implement a statement like ‘X and not Y’, the neuron representing X connects to an output neuron with a strength enough to make it fire. But the neuron representing Y <span class="italic">inhibits</span> the output neuron, meaning it prevents it from firing. This way, the output neuron will only fire if the X-representing neuron <span class="italic">is</span> firing and the Y-representing neuron is <span class="italic">not</span> (see Figure 4). </p>
<p class="TXI">These circuits, which are meant to represent what networks of real neurons can do, became known as <span class="italic">artificial</span> neural networks. </p>
<p class="image-fig" id="fig4.jpg">
<img alt="" src="Images/chapter-01-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 4</span>
</span></p>
<p class="TXI">The ability to spot logic at play in the interactions of neurons came from McCulloch’s discerning eye. As a physiologist, he knew that neurons were more complex than his simple drawings and equations suggested. They had membranes, ion channels and forking paths of <a id="page_57"></a>dendrites. But the theory didn’t need their full complexity. So, like an impressionist painter using only the necessary strokes, he intentionally highlighted only the elements of neural activity required for the story he wanted to tell. In doing so, he demonstrated the artistry inherent to model-building; it is a subjective and creative process to decide which facts belong in the foreground. </p>
<p class="TXI">The radical story that McCulloch and Pitts told with their model – that neurons were performing a logical calculus – was the first attempt to use the principles of computation to turn the mind–body problem into a mind–body connection. Networks of neurons were now imbued with all the power of a formal logical system. Like a chain of falling dominoes, once certain truth values entered into a neural population (say, via sensory organs), a cascade of interactions could deduce the truth value of new and different statements. This meant a population of neurons could carry out endless computations: interpreting sensory inputs, developing conclusions, forming plans, reasoning through arguments, performing calculations and so on. </p>
<p class="TXI">With this step in their research, McCulloch and Pitts advanced the study of human thought and, at the same time, kicked it off its throne. The ‘mind’ lost its status as mysterious and ethereal once it was brought down to solid ground – that is, once its grand abilities were reduced to the firing of neurons. To adapt a quote from Lettvin, the brain could now be thought of as ‘a machine, meaty and miraculous, but still a machine’. More boldly still, McCulloch’s student Michael Arbib later remarked that the work ‘killed dualism’.</p><a id="page_58"></a>
<p class="TXI">Russell was known to lament that, despite the 20 years put into it and the impact it had on logicians and philosophers, the <span class="italic">Principia</span> had little effect on practising mathematicians. Its new take on the foundations of mathematics simply didn’t seem to mean much to those doing mathematics; it didn’t change their day-to-day work. The same could be said of McCulloch and Pitts’ discovery for neuroscientists of the time. Biologists, physiologists, anatomists – the scientists doing the labour of physically mining neurons for the details of their workings – didn’t take much from the theory. This was in part because it wasn’t obvious what experiments should follow from it. But it may also have stemmed from the very technical notation in the paper and its less-than-inviting writing style. In a review on nerve conduction written three years later, the author refers to the McCulloch-Pitts paper as ‘not for the lay reader’ and remarks that if this style of work is to be useful, it’s necessary for ‘physiologists to familiarise themselves with mathematical technology, or for mathematicians to elaborate at least their conclusions in a less formidable language’. The wall between mind and body may have come down, but the one between biologist and mathematician stood strong. </p>
<p class="TXI">There was a separate group of people – a group with the requisite technical know-how – who did take an interest in the logical calculus of neurons. In the post-war era, a series of meetings hosted by the philanthropic Macy Foundation brought together biologists and technologists, many of whom wished to use biological findings to build brain-like machines. McCulloch was an organiser of these meetings, and fellow attendees <a id="page_59"></a>included the ‘father of cybernetics’ Norbert Wiener and John von Neumann, the inventor of the modern computer architecture, who was directly inspired in its design by the McCulloch-Pitts neurons. As Lettvin described it 40 years later: ‘The whole field of neurology and neurobiology ignored the structure, the message and the form of McCulloch and Pitts’ theory. Instead, those who were inspired by it were those who were destined to become the aficionados of a new venture, now called Artificial Intelligence.’</p>
<p class="center">* * *</p>
<p class="EXTF">The <span class="italic">Navy last week demonstrated the embryo of an electronic computer named the Perceptron which, when completed in about a year, is expected to be the first non-living mechanism able to 
‘perceive, recognise, and identify its surroundings without human training or control.’ [...]</span> </p>
<p class="EXT-L">
<span class="italic">“‘Dr. Frank Rosenblatt, research psychologist at the Cornell Aeronautical Laboratory, Inc., Buffalo, NY, designer of the ­Perceptron, conducted the demonstration. The machine, he said, would be the first electronic device to think as the human brain. Like humans, Perceptron will make mistakes at first, ‘but it will grow wiser as it gains experience’, he said.</span></p>
<p class="TXT">This summary, from an article entitled ‘Electronic “brain” teaches itself’, appeared in the 13 July 1958 edition of the <span class="italic">New York Times</span>, opposite a letter to the editor about the ongoing debate on whether smoking causes cancer. Frank Rosenblatt, the 30-year-old architect of the project, was reaching beyond his training in experimental psychology to build a computer meant to rival the most advanced technology at the time.</p><a id="page_60"></a>
<p class="TXI">The computer in question was taller than the engineers who operated it and about twice as long. It was covered on either end in various control panels and readout mechanisms. Rosenblatt requested the services of three ‘professional people’ and an associated technical staff for 18 months to build it, and the estimated cost was $100,000 (around $870,000 today). The word ‘perceptron’, defined by Rosenblatt, is a generic term for a certain class of devices that can ‘recognise similarities or identities between patterns of optical, electrical or tonal information’. The Perceptron – the computer that was built in 1958 – was thus technically a subclass known as a ‘photoperceptron’ because it took as its input the output of a camera mounted on a tripod at one end of the machine. </p>
<p class="TXI">The Perceptron was, just like the models introduced in the McCulloch-Pitts paper, an artificial neural network. It was a simplified replica of what real neurons do and how they connect to each other. But rather than remaining a mathematical construct that exists only as the ink of equations on a page, the Perceptron was physically realised. The camera provided 400 inputs to this network in the form of a 20x20 grid of light sensors. Wires then randomly connected the output of these sensors to 1,000 ‘association units’ – small electrical circuits that summed up their inputs and switched to ‘on’ or ‘off’ as a result, just like a neuron. The output of these association units became the input to the ‘response units’, which themselves could be ‘on’ or ‘off’. The number of response units was equal to the number of mutually exclusive categories to which an image could belong. So, if the Navy wanted to use the Perceptron, say, to <a id="page_61"></a>determine if a jet was present in an image or not, there would be two response units: one for jet and one for no jet. At the end of the machine opposite the camera was a set of light bulbs that let the engineer know which of the response units was active – that is, which category the input belonged to. </p>
<p class="TXI">Implementing an artificial neural network this way was large and cumbersome, full of switches, plugboards and gas tubes. The same network made up of real neurons would be smaller than a grain of sea salt. But achieving this physical implementation was important. It meant that theories of how neurons compute could actually be tested in the real world on real data. Whereas the McCulloch-Pitts work was about proving a point in theory, the Perceptron put it into practice. </p>
<p class="TXI">Another important difference between the Perceptron and the McCulloch-Pitts network was that, as Rosenblatt told the <span class="italic">New York Times</span>, the Perceptron learns. In the McCulloch and Pitts paper, the authors make no reference to how the connectivity between the neurons comes to be. It is simply defined according to what logical function the network needs to carry out and it stays that way. For the Perceptron to learn, however, it must modify its connections.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> In fact, the Perceptron derives all its functionality from changing its connection strengths until they are just right. </p>
<p class="TXI">The type of learning the Perceptron engages in is known as ‘supervised’ learning. By providing pairs of inputs and outputs – say, a series of pictures and whether <a id="page_62"></a>they each contain a jet or not – the Perceptron learns to make this decision on its own. It does so by changing the strength of the connections – also known as the ‘weights’ – between the association units and the readouts. </p>
<p class="TXI">Specifically, when an image is provided to the network, it activates units first in the input layer, then in the association layer, and finally in the readout layer, indicating the network’s decision. If the network gets the classification wrong, the weights change according to these rules:</p>
<p class="NLF">1. If a readout unit is ‘off’ when it should be ‘on’, the connections from the ‘on’ association units to that readout unit are <span class="italic">strengthened</span>.</p>
<p class="NLL">2. If a readout unit is ‘on’ when it should be ‘off’, the connections from the ‘on’ association units to that readout unit are <span class="italic">weakened</span>.</p>
<p class="TXT">By following these rules, the network will start to correctly associate images with the category they belong to. If the network can learn to do this well, it will stop making errors and the weights will stop changing. </p>
<p class="TXI">This procedure for learning was, in many ways, the most remarkable part of the Perceptron. It was the conceptual key that could open all doors. Rather than needing to tell a computer exactly how to solve a problem, you need only show it some examples of that problem solved. This had the potential to revolutionise computing and Rosenblatt was not shy in saying so. He told the <span class="italic">New York Times</span> that Perceptrons would ‘be able to recognise people and call out their names’ and ‘to hear speech in one language and instantly translate it to <a id="page_63"></a>speech or writing in another language’. He also added that ‘it would be possible to build Perceptrons that could reproduce themselves on an assembly line and which would be “conscious” of their existence’. This was a bold statement, to say the least, and not everyone was happy with Rosenblatt’s public bravado. But the spirit of the claim – that a computer that could learn would expedite the solving of almost any problem – rang true.</p>
<p class="TXI">The power of learning, however, came with a price. Letting the system decide its own connectivity effectively divorced these connections from the concept of Boolean operators. The network <span class="italic">could</span> learn the connectivity that McCulloch and Pitts had identified as required for ‘and’, ‘or’, <span class="italic">etc.</span> But there was no requirement that it does, nor any need to understand the system in this light. Furthermore, while the association units in the Perceptron machine were designed to be only ‘on’ or ‘off’, the learning rule doesn’t actually require that they be this way. In fact, the activity level of these artificial neurons could be any positive number and the rule would still work.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> This makes the system more flexible, but without a binary ‘on’-‘off’ response it makes it harder to map the activity of these units to the binary truth values of propositions. Compared with the crisp and clear logic of the McCulloch-Pitts networks, the Perceptron was an uninterpretable mess. But it worked. Interpretability was sacrificed for ability.</p> <a id="page_64"></a>
<p class="TXI">The Perceptron machine and its associated learning procedure became a popular object of study in the burgeoning field of artificial intelligence. When it made the transition from a specific physical object (the Perceptron) to an abstract mathematical concept (the perceptron algorithm) the separate input and association layers were done away with. Instead, input units representing incoming data connected directly to the readout units and, through learning, these connections changed to make the network better at its task. How and what the perceptron in this simplified form could learn was studied from every angle. Researchers explored its workings mathematically using pen and paper, or physically by building their own perceptron machines, or – when digital computers finally became available – electronically by simulating it.</p>
<p class="TXI">The perceptron generated hope that humans could build machines that learn like we do; in this way it put the prospect of artificial intelligence within 
reach. Simultaneously, it provided a new way of understanding our own intelligence. It showed that artificial neural networks could compute without abiding by the strict rules of logic. If the perceptron could perceive without the use of propositions or operators, it follows that each neuron and connection in the brain needn’t have a clear role in terms of Boolean logic either. Instead, the brain could be working in a sloppier way, wherein, like the perceptron, the function of a network is distributed across its neurons and emerges out of the connections between them. This new approach to the study of the brain became known as ‘connectionism’.</p> <a id="page_65"></a>
<p class="TXI">The work of McCulloch and Pitts was an important stepping stone. As the first demonstration of how networks of neurons could think, it was responsible for getting neuroscience away from the shores of pure biology and into the sea of computation. This fact, rather than the veracity of its claims, is what earns it its place in history. The intellectual ancestor of McCulloch and Pitts’ work, the <span class="italic">Principia Mathematica</span>, could be said to have suffered a similar fate. In 1931, German mathematician Kurt Gödel published ‘On formally undecidable propositions of <span class="italic">Principia Mathematica</span> and related systems’. This paper took the <span class="italic">Principia Mathematica</span> as a starting point to show why its very goal – to explain all of mathematics from simple premises – was impossible to achieve. Russell and Whitehead had not, in fact, done what they believed they did.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> Gödel’s findings became known as the ‘incompleteness theorem’ and had a revolutionary effect on mathematics and philosophy. An effect that stemmed, in part, from Russell and Whitehead’s failed attempt.</p>
<p class="TXI">Russell and McCulloch were able to take the failings of their respective works in their stride. Pitts, on the other hand, was made of finer cloth. The realisation that the brain was not enacting the beautiful rules of logic tore him apart.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> This, along with pre-existing mental struggles and the end of a relationship with an important mentor, drove him to drink and experiment with other <a id="page_66"></a>drugs. He became erratic and delirious; he burned his work and withdrew from his friends. He died from the impacts of liver disease in 1969 – the same year McCulloch died. McCulloch was 70; Pitts was 46. </p>
<p class="TXI">* * *</p>
<p class="image-fig" id="fig5.jpg">
<img alt="" src="Images/chapter-01-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 5</span>
</span></p>
<p class="TXI">The cerebellum is a forest. Folded up neatly near where the spinal cord enters the skull, this bit of the brain is thick with different types of neurons, like different species of trees, all living in chaotic harmony (see Figure 5). The Purkinje cells are large, easily identified and heavily branched: from the body of these cells, dendrites stretch up and away, like a thousand alien hands raised in prayer. The granule cells are numerous and small – with cell bodies less than half the size of the Purkinje’s – but their reach is far. Their axons initially grow upwards, in parallel with the Purkinje cells’ dendrites. They then make a sharp right turn to run directly through the branches of the Purkinje cells, like power lines through treetops. This is where the granule cells make contact with the Purkinje cells: each Purkinje cell gets input from hundreds of <a id="page_67"></a>thousands of granule cells. Climbing fibres are axons that follow a longer path on their way to the Purkinje cells. These axons come from cells in a different brain region – the inferior olive – from which they navigate all the way to the bottom of the Purkinje cell bodies and creep up around them. Winding their way around the base of the Purkinje cell dendrites like ivy, the climbing fibres form connections. Unlike the granule cells, only a single climbing fibre targets each Purkinje cell. In the cerebellar landscape, Purkinje cells are thus central. They have scores of granule cells imposing on them from the top and a small yet precise set of climbing fibres closing in on them from the bottom. </p>
<p class="TXI">In its twisty, organic way, the circuitry of the cerebellum possesses an organisation and precision unbefitting of biology. It was in this biological wiring that James Albus, a PhD student in electrical engineering working at NASA, saw the principles of the perceptron at play. </p>
<p class="TXI">The cerebellum plays a crucial role in motor control; it helps with balance, coordination, reflexes and more. One of the most widely studied of its abilities is eye-blink conditioning. This is a trained reflex that can be found in everyday life. For example, if a determined parent or roommate tries to get you out of bed in the morning by pulling open the curtains, you’ll instinctively close your eyes in response to the sunlight. After a few days of this, simply the sound of the curtains being opened may be enough to make you blink in anticipation. </p>
<p class="TXI">In the lab, this process is studied in rabbits and the intruding sunlight is replaced by a small puff of air on the eye (just annoying enough to ensure they’ll want to avoid it). After several trials of playing a sound (such as a <a id="page_68"></a>short blip of a pure tone) and following it by this little air puff, the rabbit eventually learns to close its eyes immediately upon hearing the tone. Play the animal a new sound (for example, a clapping noise) that hasn’t been paired with air puffs and it won’t blink. This makes eye-blink conditioning a simple classification task: the rabbit has to decide if a sound it hears is indicative of an upcoming air puff (in which case the eyes should close) or if it is a neutral noise (in which case they can remain open). Disrupt the cerebellum and rabbits can’t learn this task. </p>
<p class="TXI">The Purkinje cells have the power to close the eyes. Specifically, a dip in their normally high firing rate will, via connections the Purkinje cells send out of this area, cause the eyes to shut. Based on this anatomy, Albus saw their place as the readout – that is, they indicate the outcome of the classification. </p>
<p class="TXI">The perceptron learns via supervision: it needs inputs and labels for those inputs to know when it has erred. Albus saw these two functions in the two different types of connections on to Purkinje cells. The granule cells pass along sensory signals; specifically, different granule cells fire depending on which sound is being played. The climbing fibres tell the cerebellum about the air puff; they fire when this annoyance is felt. Importantly, this means the climbing fibres signal an error. They indicate that the animal made a mistake in not closing its eyes when it should have. </p>
<p class="TXI">To prevent this error, the connections from the granule cells to the Purkinje cells need to change. In particular, Albus anticipated that any granule cells that were active before the climbing fibre was active (<span class="italic">i.e.</span>, before an error), <a id="page_69"></a>should weaken their connection to the Purkinje cell. That way, the next time those granule cells fire – <span class="italic">i.e.</span>, the next time the same sound is played – they <span class="italic">won’t</span> cause firing in the Purkinje cells. And that dip in Purkinje cell firing <span class="italic">will</span> cause the eyes to close. Through this changing of connection strengths, the animal learns from its past mistakes and avoids future air puffs to the eye.</p>
<p class="TXI">In this way, the Purkinje cell acts like a president advised by a cabinet of granule cell advisors. At first the Purkinje cell listens to all of them. But if it’s clear that some are providing bad advice – that is, their input is followed by negative news delivered by the climbing fibre – their influence over the Purkinje cell will fade. And the Purkinje cell will act better in the future. It is a process that directly mirrors the perceptron learning rule.</p>
<p class="TXI">When Albus proposed this mapping between the perceptron and the cerebellum in 1971,<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> his prediction about how the connections between granule cells and Purkinje cells should change was just that – a prediction. No one had directly observed this kind of learning in the cerebellum. But by the mid-1980s, evidence had piled up in Albus’ favour. It became clear that the strength of the connection between a granule cell and a Purkinje cell does decrease after an error. The particular molecular mechanisms of this process have even been revealed. We now know that granule cell inputs cause a receptor in <a id="page_70"></a>the membrane of the Purkinje cell to respond, effectively tagging which granule cell inputs were active at a given time. If a climbing fibre input comes later (during an air puff), it causes calcium to flood into the Purkinje cell. The presence of this calcium signals to all the tagged connections to decrease their strength. Patients with fragile X syndrome – a genetic disorder that leads to intellectual disabilities – appear to be missing a protein that regulates this connection from the granule cells on to the Purkinje cell. As a result, they have trouble learning tasks like eye-blink conditioning. </p>
<p class="TXI">The perceptron, with its explicit rules of how learning should proceed in a neural network, offered clear testable ideas for neuroscientists to hunt for – and find – in the brain. In doing so, it was able to connect science across scales. The smallest physical detail – calcium ions moving through the inside of a neuron, for example – inherits a much larger meaning in light of its role in computation. </p>
<p class="center">* * *</p>
<p class="TXT">The reign of the perceptron was cut short in 1969. And with a twist of Shakespearean irony, it was its namesake that killed it. </p>
<p class="TXI">
<span class="italic">Perceptrons</span> was written by Marvin Minsky and Seymour Papert, both mathematicians at the Massachusetts Institute of Technology. The book was subtitled <span class="italic">An Introduction to Computational Geometry</span> and had a simple abstract design on the cover. Minsky and Papert were drawn to write about the topic of perceptrons out of appreciation for Rosenblatt’s invention and a desire to explore it further. <a id="page_71"></a>In fact, Minsky and Papert met at a conference where they were presenting similar results from their explorations into how the perceptron learns. </p>
<p class="TXI">Papert was a native of South Africa with full cheeks, a healthy beard and not one, but <span class="italic">two</span>, PhDs in mathematics. He had a lifelong interest in education and how it could be transformed by computing. Minsky was less than a year older than Papert, with sharper features and large glasses. A New York native, he attended the Bronx High School of Science with Frank Rosenblatt; he was also mentored by McCulloch and Pitts. </p>
<p class="TXI">Minsky and Papert shared with McCulloch and Pitts the compulsive desire to formalise thinking. True advances in understanding computation, they believed, came from mathematical derivations. All the empirical success of the perceptron – whatever computing it was able to carry out or categories it was able to learn – meant next to nothing without a mathematical understanding of why and how it worked. </p>
<p class="TXI">At this time, the perceptron was attracting a lot of attention – and money – from the artificial intelligence community. But it wasn’t attracting the kind of mathematical scrutiny Minsky and Papert yearned for. The two were thus explicitly motivated to write their book by a desire to increase the rigour around the study of perceptrons – but also, as Papert later acknowledged, by some desire to decrease the reverence for them.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup></p>
<p class="TXI">The pages of <span class="italic">Perceptrons</span> are comprised mainly of proofs, theorems and derivations. Each contributes to a <a id="page_72"></a>story about the perceptron: defining what it is, what it can do and how it learns. Yet from the publication of these some 200 pages – a thorough exploration of the ins and outs of the perceptron’s workings – the message the community received was largely about its limitations. This is because Minsky and Papert had shown, conclusively, that certain simple computations were impossible for the perceptron to do. </p>
<p class="TXI">Consider a perceptron that has just two inputs, and each input can be ‘on’ or ‘off’. We want the perceptron to report if the two inputs are the same: to respond yes (<span class="italic">i.e.</span>, have its readout unit be on) if both inputs are on <span class="italic">or</span> if both inputs are off. But if one is on and the other is off, the readout unit should be off. Like sorting socks out of the laundry, the perceptron should only respond when it sees a matching pair.</p>
<p class="TXI">To make sure the readout doesn’t fire when only one input is on, the weights from each input need to be sufficiently low. They could, for example, each be half the amount needed to make the readout turn on. This way, when both are on, the readout <span class="italic">will</span> fire and it won’t fire when only one input is on. In this setup the readout is responding correctly for three of the four possible input conditions. But in the condition where both inputs are off, the readout will be off – an incorrect classification. </p>
<p class="TXI">As it turns out, no matter how much we fiddle with connection strengths, there is no way to satisfy all the needs of the classification at once. The perceptron simply cannot do it. And the problem with that is that no good model of the brain – or promising artificial intelligence – should fail at a task as simple as deciding if two things are the same or not.</p> <a id="page_73"></a>
<p class="TXI">Albus, whose paper on the cerebellum was published in 1971, knew of the limitations of the perceptron and knew that, despite these limitations, it was still powerful enough to be a model of the eye-blink conditioning task. But a model of the whole human brain, as Rosenblatt promised? Not possible.</p>
<p class="TXI">The portrait that Minsky and Papert painted forced researchers to see the perceptron’s powers clearly. Prior to the book, researchers were able to explore what the perceptron could do blindly, with the hope that the limits of its abilities were still far off, if they existed at all. Once the contours were put in stark relief, however, there was no denying that these boundaries existed, and that they existed much closer than expected. In truth, all this amounted to was an understanding of the perceptron – exactly what Minsky and Papert set out to do. But the end of ignorance around the perceptron meant the end of excitement around it as well. As Papert put it: ‘Being understood can be a fate as bad as death.’</p>
<p class="TXI">The period that followed the publication of <span class="italic">Perceptrons</span> is known as the ‘dark ages’ of connectionism. It was marked by significant decreases in funding to the research programmes that had grown out of Rosenblatt’s initial work. The neural network approach to building artificial intelligence was snuffed out. All the excessive promises, hopes and hype had to be retracted. Rosenblatt himself died tragically in a boating accident two years after the book was published and the field he helped build remained dormant for more than 10 years.</p>
<p class="TXI">But if the hype around the perceptron was excessive and ill informed, so too was the backlash against it. The limitations in Minsky and Papert’s book were true: the <a id="page_74"></a>perceptron in the form they were studying it was incapable of many things. But it didn’t need to keep that form. The same-or-not problem, for example, could be easily solved by adding an additional layer of neurons between the input and the readout. This layer could be composed of two neurons, one with weights that make it fire when both inputs are on and the other with weights that make it fire when both inputs are off. Now the readout neuron, which gets its input from these middle neurons, just needs to be active if one of the middle neurons is active. </p>
<p class="TXI">‘Multi-layer perceptrons’, as these new neural architectures were called, had the potential to bring connectionism back from the dead.<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> But before a full resurrection was possible, one problem had to be solved: learning. The original perceptron algorithm provided the recipe for setting the connections between the input neurons and the readout neurons – that is, the learning rule was designed for a two-layered network. If the new breed of neural networks was going to have three, four, five or more layers, how should the connections between all those layers be set? (see Figure 6) Despite all the good features of the perceptron learning rule – its simplicity, the proof that it could work, the fact that it had been spotted in the wild of the cerebellum – it was unable to answer this question. Knowing that a multi-layer perceptron <span class="italic">could</span> solve more complex problems was not enough to deliver <a id="page_75"></a>on the grand promises of connectionism. What was needed was for it to <span class="italic">learn</span> to solve those problems.</p>
<p class="center">* * *</p>
<p class="TXT">The Easter Sunday of the connectionist revival story came in 1986. The paper ‘Learning representations by back-propagating errors’, written by two cognitive scientists from the University of California San Diego, David Rumelhart and Ronald Williams, and a computer scientist from Carnegie Mellon, Geoffrey Hinton, was published on 9 October in the journal <span class="italic">Nature</span>. It presented a solution to the exact problem the field had: how to train multi-layer artificial neural networks. The learning algorithm in the paper, called ‘backpropagation’, became widely used by the community at the time. And it remains to this day the dominant way in which artificial neural networks are trained to do interesting tasks.</p>
<p class="image-fig" id="fig6.jpg">
<img alt="" src="Images/chapter-01-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 6</span>
</span></p>
<p class="TXI">The original perceptron’s learning rule works because, with only two layers, it’s easy to see how to fix what’s gone wrong. If a readout neuron is off when it should be on, connections going from the input layer to that neuron <a id="page_76"></a>need to get stronger and vice versa. The relationship between these connections and the readout is thus clear. The backpropagation algorithm has a more difficult problem to solve. In a network with many layers between the input and readout, the relationships between all these connections and the readout aren’t as clear. Now instead of a president and his or her advisors, we have a president, their advisors, and the employees of those advisors. The amount of trust an advisor has in any given employee – <span class="italic">i.e.</span>, the strength of the connection from that employee to the advisor – will certainly have an impact, ultimately, on what the president does. But this impact is harder to directly see and harder to fix if the president feels something is going wrong. </p>
<p class="TXI">What was needed was an explicit way to calculate how any connection in the network would impact the readout layer. As it turns out, mathematics offers a neat way to do this. Consider an artificial neural network with three layers: input, middle and readout. How do the connections from the input to the middle layer impact the readout? We know the activity of the middle layer is a result of the activities of the input neurons and the weights of their connections to the middle layer. With this knowledge, writing an equation for how these weights affect the activity at the middle layer is straightforward. We also know that the readout neurons follow the same rule: their activity is determined by the activities of the middle neurons and the weights connecting the middle neurons to the readout. Therefore, an equation describing how these weights impact the readout is also easy to get. The only thing left to do is find a way to string these two equations together. That <a id="page_77"></a>way we’ll have an equation that tells us directly how the connections from the input to the middle layer impact the readout. </p>
<p class="TXI">When forming a train in the game of dominoes, the number on the end of one tile needs to match the number on the start of another in order for them to connect. The same is true for stitching together these equations. Here, the common term that connects the two equations is the activity of the middle layer: this activity both determines the activity of the readout and is determined by the input-to-middle connections. After joining these equations via the middle layer activity, the impact of the input-to-middle layer connections on the readout can be calculated directly. And this makes it easy to figure out how those connections should change when the readout is wrong. In calculus, this linking together of relationships is known as the ‘chain rule’ and it is the core of the backpropagation algorithm. </p>
<p class="TXI">The chain rule was discovered over 200 years ago by none other than the idol of McCulloch and Pitts, philosopher and polymath Gottfried Leibniz. Given how useful the rule is, its application to the training of multi-layer neural networks was no surprise. In fact, the backpropagation algorithm appears to have been invented at least three separate times before 1986. But the 1986 paper was part of a perfect storm that ensured its findings would spread far and wide. The first reason for this was the content of the paper itself. Not only did it show that neural networks could be trained this way, it also analysed the workings of networks trained on several cognitive tasks, such as understanding relations on a family tree. Another component of the success was the <a id="page_78"></a>increase in computational power that came in the 1980s; this was important for making the training of multi-layer neural networks practically possible for researchers. Finally, the same year the paper was published, one of its authors, Rumelhart, also published a book on connectionism that included the backpropagation algorithm. That book – written with a different Carnegie Mellon professor, James McClelland – went on to sell an estimated 40,000 copies by the mid-1990s. Its title, <span class="italic">Parallel Distributed Processing</span>, lent its name to the entire research agenda of building artificial neural networks in the late 1980s and early 1990s. </p>
<p class="TXI">For somewhat similar reasons, the story of artificial neural networks took an even more dramatic turn roughly a decade into the new millennium. The heaps of data accumulated in the internet age united with the computational power of the twenty-first century to supercharge progress in this field. Networks with more and more layers could suddenly be trained on more and more complex tasks. Such scaled-up models – referred to as ‘deep neural networks’ – are currently transforming artificial intelligence and neuroscience alike. </p>
<p class="TXI">The deep neural networks of today are based on the same basic understanding of neurons as those of McCulloch and Pitts. Beyond that base inspiration, though, they don’t aim to directly replicate the human brain. They aren’t trying to mimic its structure or anatomy, for example.<sup><a href="#fn-11" id="fnt-11">11</a>
</sup> But they do aim to mimic human behaviour and they’re getting quite good at it. When <a id="page_79"></a>Google’s popular language translation service started using a deep neural network approach in 2016, it reduced translation errors by 50 per cent. YouTube also uses deep neural networks to help its recommendation algorithm better understand what videos people want to see. And when Apple’s voice assistant Siri responds to a command, it is a deep neural network that is doing the listening and the speaking. </p>
<p class="TXI">In total, deep neural networks can now be trained to find objects in images, play games, understand preferences, translate between different languages, turn speech into written words and turn written words into speech. Not unlike the original Perceptron machine, the computers these networks run on fill up rooms. They’re located in server centres across the globe, where they hum away processing the world’s image, text and audio data. Rosenblatt may have been pleased to see that some of his grand promises to the <span class="italic">New York Times</span> were indeed fulfilled. They just required a scale nearly a thousand times what he had available at the time. </p>
<p class="TXI">The backpropagation algorithm was necessary to boost artificial neural networks to the point where they could reach near-human levels of performance on some tasks. As a learning rule for neural networks, it really works. Unfortunately, that doesn’t mean it works like the brain. While the perceptron learning rule was something that could be seen at play between real neurons, the backpropagation algorithm is not. It was designed as a mathematical tool to make artificial neural networks work, not a model of how the brain learns (and its inventors were very clear on that from the start). The reason for this is that real neurons can typically only <a id="page_80"></a>know about the activity of the neurons they’re connected to – not about the activity of the neurons those neurons connect to and so on and so on. For this reason, there is no obvious way for real neurons to implement the chain rule. They must be doing something different. </p>
<p class="TXI">For some researchers – particularly researchers in the field of artificial intelligence – the artificial nature of backpropagation is no problem. Their goal is to build computers that can think, by whatever means necessary. But for other scientists – neuroscientists in particular – finding the learning algorithm of the brain is paramount. We know the brain is good at getting better; we see it when we learn a musical instrument, how to drive or how to read a new language. The question is how.</p>
<p class="TXI">Because backpropagation is what we know works, some neuroscientists are starting there. They’re checking for signs that the brain is doing something <span class="italic">like</span> backpropagation, even if it can’t do it exactly. Inspiration comes from the success story of finding a perceptron at work in the cerebellum. There, clues were present in the anatomy; the different placement of the climbing fibres and granule cells pointed to a different role for each. Other brain areas display patterns of connectivity which may hint at how they are learning. For example, in the neocortex, some neurons have dendrites that stretch out way above them. Faraway regions of the brain send inputs to these dendrites. Do they carry with them information about how these neurons have impacted those that come after them in the brain’s neural network? Could this information be used to change the strength of the network’s connections? Both neuroscientists and artificial intelligence researchers hold out hope that the <a id="page_81"></a>brain’s version of backpropagation will be found and that, when it is, it can be copied to create algorithms that learn even better and faster than today’s artificial neural networks. </p>
<p class="TXI">In their hunt to understand how the mind learns from supervision, modern researchers are doing just what McCulloch did. They’re looking at the piles of facts we have about the biology of the brain and trying to see in it a computational structure. Today, they are guided in their search by the workings of artificial systems. Tomorrow, the findings from biology will again guide the building of artificial intelligence. This back-and-forth defines the symbiotic relationship between these two fields. Researchers looking to build artificial neural networks can take inspiration from the patterns found in biological ones, while neuroscientists can look to the study of artificial intelligence to identify the computational role of biological details. In this way, artificial neural networks keep the study of the mind and the brain connected.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-1" id="fn-1">1</a> ﻿Named after the English mathematician George Boole. While they used his ideas, Russell and Whitehead didn﻿’﻿t actually use the term ﻿‘﻿Boolean﻿’﻿, as it wasn﻿’﻿t coined until 1913.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-2" id="fn-2">2</a> ﻿At least that﻿’﻿s what it looked like at the time ﻿…﻿ More on this later.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-3" id="fn-3">3</a> ﻿The use of the word ﻿‘﻿circuit﻿’﻿ here differs from that in the last chapter. In addition to its meaning as an electrical circuit, neuroscientists also use the word to refer to a group of neurons connected in a specific way. ﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-4" id="fn-4">4</a> ﻿More on how learning ﻿–﻿ and memory ﻿–﻿ relies on a change in connections in the next chapter.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-5" id="fn-5">5</a> ﻿This can be thought of as representing the ﻿<span class="italic">rate</span>﻿ of spiking of a neuron, rather than if the neuron is emitting a spike or not. Using this type of artificial neuron only requires a small modification to the learning procedure.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-6" id="fn-6">6</a> ﻿The cracks in the ﻿<span class="italic">Principia</span>﻿﻿’﻿s foundation were noticeable even when it was published. Some of the ﻿‘﻿basic﻿’﻿ premises it had to assume were not really very basic and were hard to justify.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-7" id="fn-7">7</a> ﻿This realisation came even more directly from a study on the frog brain that Pitts was involved with. More on that study in ﻿﻿Chapter 6﻿﻿.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-8" id="fn-8">8</a> ﻿The mapping is sometimes referred to as the ﻿‘﻿Marr-Albus-Ito﻿’﻿ theory of motor learning, named also after David Marr and Masao Ito, who both put forth similar models of how the cerebellum learns. ﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-9" id="fn-9">9</a> ﻿The particular words Papert used to describe his feelings about Perceptron-mania at the time were ﻿‘﻿hostility﻿’﻿ and ﻿‘﻿annoyance﻿’﻿.﻿</p>
<p class="FN2"><a href="chapter3.xhtml#fnt-10" id="fn-10">10</a> ﻿Technically they weren﻿’﻿t ﻿‘﻿new﻿’﻿. Minsky and Papert do reference multi-layer perceptrons in their book. However, they were dismissive about the potential powers of these devices and, unfortunately for science, did not encourage their further study. ﻿</p>
<p class="FN2"><a href="chapter3.xhtml#fnt-11" id="fn-11">11</a> ﻿With the exception of deep neural networks that are built to understand images, which we will hear all about in ﻿﻿Chapter 6﻿﻿.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 4</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter4"><a href="contents.xhtml#re_chapter4">CHAPTER FOUR</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter4">Making and Maintaining Memories</a><a id="page_83"></a></p>
<p class="H1" id="b-9781472966445-ch354-sec4">
<span class="bold">
<span>The Hopfield network and attractors</span>
</span></p>
<p class="TXT">A block of iron at 770°C (1,418°F) is a sturdy grey mesh. Each of its trillions of atoms serves as a single brick in the endless parallel walls and ceilings of its crystalline structure. It is a paragon of orderliness. In opposition to their organised structural arrangement, however, the magnetic arrangement of these atoms is a mess. </p>
<p class="TXI">Each iron atom forms a dipole – a miniature magnet with one positive and one negative end. Heat unsteadies these atoms, flipping the direction of their poles around at random. On the micro-level this means many tiny magnets each exerting a force in its own direction. But as these forces work against each other, their net effect becomes negligible. When you zoom out, this mass of mini-magnets has no magnetism at all.</p>
<p class="TXI">As the temperature dips below 770°C, however, something changes. The direction of an individual atom is less likely to switch. With its dipole set in place, the atom starts to exert a constant pressure on its neighbours. This indicates to them which direction they too should be facing. Atoms with different directions vie for influence over the local group until eventually everyone falls in line, one way or the other. With all the small dipoles aligned, the net force is strong. The previously inert block of iron becomes a powerful magnet.</p> <a id="page_84"></a>
<p class="TXI">Philip Warren Anderson, an American physicist who won a Nobel Prize working on such phenomena, wrote in a now-famous essay entitled ‘More is different’ that ‘the behaviour of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles’. That is, the collective action of many small particles – organised only through their local interactions – can produce a function not directly possible in any of them alone. Physicists have formalised these interactions as equations and successfully used them to explain the behaviour of metals, gases and ice. </p>
<p class="TXI">In the late 1970s, a colleague of Anderson’s, John J. Hopfield, saw in these mathematical models of magnetism a structure akin to that of the brain. Hopfield used this insight to bring under mathematical control a long-lasting mystery: the question of how neurons make and maintain memories.</p>
<p class="center">* * *</p>
<p class="TXT">Richard Semon was wrong.</p>
<p class="TXI">A German biologist working at the turn of the twentieth century, Semon wrote two lengthy books on the science of memory. They were filled with detailed descriptions of experimental results, theories and a vocabulary for describing memory’s impact on ‘organic tissue’. Semon’s work was insightful, honest and clear – but it contained a major flaw. Just as French naturalist Jean-Baptiste Lamarck believed (in contrast to our current understanding of evolution) that traits acquired by an animal in its lifetime could be passed to its offspring, <a id="page_85"></a>Semon proposed that <span class="italic">memories</span> acquired by an animal could be passed down. That is, he believed that an organism’s learned responses to its own environment would arise without instruction in its offspring. As a result of this mistaken intuition, much of Semon’s otherwise valuable work was slowly cast aside and forgotten.</p>
<p class="TXI">Being wrong about memory isn’t unusual. Philosopher René Descartes, for example, thought memories were activated by a small gland directing the flow of ‘animal spirits’. What’s unique about Semon is that, despite the flaw in his work that sentenced him to historical obscurity, one of his contributions remained influential long enough to spawn an entire body of research. This small artefact of his efforts is the ‘engram’ – a word coined by Semon in <span class="italic">The</span>
<span class="italic">Mneme</span> in 1904, and subsequently learned by millions of students of psychology and neuroscience.</p>
<p class="TXI">At the time Semon was writing, memory had only recently come under scientific scrutiny – and most of the results were purely about memorisation skills, not about biology. For example, people would be trained to memorise pairs of nonsense words (such as ‘wsp’ and ‘niq’) and then were tested on their ability to retrieve the second word when prompted with the first. This type of memory, known as <span class="italic">associative</span> memory, would become a target of research for decades to come. But Semon was interested in more than just behaviour; he wanted to know what changes in an animal’s physiology could support such associative memories.</p>
<p class="TXI">Led by scant experimental data, he broke up the process of creating and recovering memories into multiple components. Finding common words too <a id="page_86"></a>vague and overloaded, he created novel terms for these divisions of labour. The word that would become so influential, the engram, was defined as ‘the enduring though primarily latent modification in the irritable substance produced by a stimulus’. Or, to put it more plainly: the physical changes in the brain that happen when a memory is formed. Another term, ‘ecphory’, was assigned to the ‘influences which awake the mnemic trace or engram out of its latent state into one of manifested activity’. This distinction between engram and ecphory (or between the processes that lay a memory and those that retrieve it) was one of the many conceptual advances that Semon’s work provided. Despite the fact that his name and most of his language have disappeared from the literature, many of Semon’s conceptual insights were correct and they form the core of how memories are modelled today.</p>
<p class="TXI">In 1950, American psychologist Karl Lashley published ‘In search of the engram’, a paper that solidified the legacy of the word. It also set a rather dismal tone for the field. The paper was so titled because the search was all Lashley felt he had accomplished in 30 years of experiments. Lashley’s experiments involved training animals to make an association (for example, to react in a specific way when shown a circle versus an ‘X’) or learn a task, such as how to run through a particular maze. He would then surgically remove specific brain areas or connection pathways and observe how behaviour was impacted post-operatively. Lashley couldn’t find any area or pattern of lesions that reliably interfered with memory. He concluded that memories must thus somehow be distributed equally across the <a id="page_87"></a>brain, rather than in any single area. But based on some calculations about how many neurons could be used for a memory and the number of pathways between them, he was uncertain about how this was possible. His landmark article thus reads as something of a white flag, a surrendering of any attempt to draw conclusions about the location of memory in the face of a mass of inconsistent data. The physical nature of memory remained to Lashley as vexing as ever.</p>
<p class="TXI">At the same time, however, a former student of Lashley’s was developing his own theories on learning and memory. </p>
<p class="TXI">Donald Hebb, a Canadian psychologist whose early work as a school teacher grew his interest in the mind, was intent on making psychology a biological science. In his 1949 book, <span class="italic">The Organization of Behavior</span>, he describes the task of a psychologist as ‘reducing the vagaries of human thought to a mechanical process of cause and effect’. And in that book, he lays down the mechanical process he believed to be behind memory formation.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> Overcoming the limited, and sometimes misleading, physiological data available at the time, Hebb came to this principle about the physical underpinnings of learning largely through intuition. Yet it would go on to have huge empirical success. The principle, now known as Hebbian learning, is succinctly described by the phrase ‘neurons that fire together wire together’.</p> <a id="page_88"></a>
<p class="TXI">Hebbian learning describes what happens at the small junction between two neurons where one can send a signal to the other, a space called the synapse. Suppose there are two neurons, A and B. The axon from neuron A makes a synaptic connection on to the dendrite or cell body of neuron B (making it the ‘pre-synaptic’ neuron, and neuron B the ‘post-synaptic’ neuron, see Figure 7). In Hebbian learning, if neuron A repeatedly fires before neuron B, the connection from A to B will strengthen. A stronger connection means that the next time A fires it will be more effective in causing B to fire. In this way, activity determines connectivity and connectivity determines activity.</p>
<p class="image-fig" id="fig7.jpg">
<img alt="" src="Images/chapter-04-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 7</span>
</span></p>
<p class="TXI">Hebb’s approach, with its focus on the synapse, situates the engram as both local and global: local because a memory’s imprint occurs at the small gap where one neuron meets another, but global because these changes may be happening at synapses all across the brain. It also makes memory the natural consequence of experience: with pliable synapses, any activation of the brain has the potential to leave a trace. </p>
<p class="TXI">Lashley, a dutiful scientist intent on following the facts, accepted that the engram must be distributed based on his own experiments. But he found no satisfaction in <a id="page_89"></a>Hebb’s solution, which – though an enticing and elegant theory – was based more on speculation than on hard evidence. He turned down Hebb’s offer to be a co-author on the work.</p>
<p class="TXI">Lashley may not have supported Hebb’s ideas, but since the publication of his book countless experiments have. Sea slugs – foot-long slimy brown invertebrates with only about 20,000 neurons – became a creature of much study in this area, due to their ability to learn a very basic association. These shell-less slugs have a gill on their backs that, if threatened, can be quickly retracted for safe keeping. In the lab, a short electric shock will cause the gill to withdraw. If such a shock is repeatedly preceded by a harmless light touch, the slug will eventually start withdrawing in response to the touch alone, demonstrating an association between the touch and what’s expected to come next. It is the marine critter equivalent of learning to pair ‘wsp’ with ‘niq’. This association was shown, in line with Hebb’s theory of learning, to be mediated by a strengthening of the connections between the neurons that represent the touch and those that lead to the gill’s response. The change in behaviour was forged through a changing of connections. </p>
<p class="TXI">Hebbian learning has not just been observed; it’s been controlled as well. In 1999, Princeton researchers showed that genetically modifying proteins in the cell membrane that contribute to synaptic changes can control a mouse’s capacity for learning. Increasing the function of these receptors enhances the ability of mice to remember objects they’ve been shown before. Interfering with these proteins impairs it.</p> <a id="page_90"></a>
<p class="TXI">It is now established science that experience leads to the activation of neurons and that activating neurons can alter the connections between them. This story is accepted as at least a partial answer to the question of the engram. But, as Semon describes it, the engram itself is only part of the story of memory. Memory also requires <span class="italic">re</span>membering. How can this way of depositing memories allow for long-term storage and recall? </p>
<p class="center">* * *</p>
<p class="TXT">It was no real surprise that John J. Hopfield became a physicist. Born in 1933 to John Hopfield Sr, a man who made a name for himself in ultraviolet spectroscopy, and Helen Hopfield, who studied atmospheric electromagnetic radiation, Hopfield Jr grew up in a household where physics was as much a philosophy as it was a science. ‘Physics was a point of view that the world around us is, with effort, ingenuity and adequate resources, understandable in a predictive and reasonably quantitative fashion,’ Hopfield wrote in an autobiography. ‘Being a physicist is a dedication to a quest for this kind of understanding.’ And a physicist is what he would be.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> </p>
<p class="TXI">Hopfield, a tall and lanky man with an engaging smile, earned his PhD in 1958 from Cornell University. He further emulated his father by receiving a Guggenheim fellowship, using it to study at the Cavendish Laboratory <a id="page_91"></a>at Cambridge. But even by this stage, Hopfield’s enthusiasm for the subject of his PhD – condensed matter physics – was waning. ‘In 1968, I had run out of problems … to which my particular talents seemed useful,’ he later wrote.</p>
<p class="TXI">Hopfield’s gateway from physics to biology was hemoglobin, a molecule that both serves a crucial biological function as the carrier of oxygen in blood and could be studied with many of the techniques of experimental physics at the time. Hopfield worked on hemoglobin’s structure for several years at Bell Labs, but he found his real calling in biology after being invited to a seminar series on neuroscience in Boston in the late 1970s. There he encountered a variegated group of clinicians and neuroscientists, gathered together to address the deep question of how the mind emerges from the brain. Hopfield was captivated. </p>
<p class="TXI">Mathematically minded as he was, though, Hopfield was dismayed by the qualitative approach to the brain he saw on display. He was concerned that, despite their obvious talents in biology, these researchers, ‘would never possibly solve the problem because the solution can be expressed only in an appropriate mathematical language and structure’.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> This was a language that physicists had. Hopfield therefore made a point of using his physicist’s skillset even as he embarked on a study of memory. In his eyes, certain physicists of the time who made the leap <a id="page_92"></a>to biology had immigrated fully, taking on the questions, culture and vocabulary of their new land. He wanted to firmly retain his citizenship as a physicist.</p>
<p class="TXI">In 1982 Hopfield published ‘Neural networks and physical systems with emergent collective computational abilities’, which laid out the description and results of what is now known as the Hopfield network. This was Hopfield’s first paper on the topic; he was only dipping his toe into the field of neuroscience and yet it made quite the splash.</p>
<p class="image-fig" id="fig8.jpg">
<img alt="" src="Images/chapter-04-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 8</span>
</span></p>
<p class="TXI">The Hopfield network (see Figure 8) is a mathematical model of neurons that can implement what Hopfield described as ‘content-addressable memory’. This term, coming from computer science, refers to the notion that a full memory can be retrieved from just a small component of it. The network that Hopfield designed for this task is simply composed. It is made only of binary neurons (like the McCulloch-Pitts neurons introduced in the last chapter), which can be either ‘on’ or ‘off’. It is therefore the interactions between these neurons from which the intriguing behaviours of this network emerge. </p>
<p class="TXI">The Hopfield network is <span class="italic">recurrent</span>, meaning that each neuron’s activity is determined by that of any of the <a id="page_93"></a>others in the network. Therefore, each neuron’s activity serves as both input and output to its neighbours. Specifically, each input a neuron receives from another neuron is multiplied by a particular number – a synaptic weight. These weighted inputs are then added together and compared to a threshold: if the sum is greater than (or equal to) the threshold, the neuron’s activity level is 1 (‘on’), otherwise it’s 0 (‘off’). This output then feeds into the input calculations of the other neurons in the network, whose outputs feed back into more input calculations and so on and so on.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p>
<p class="TXI">Like bodies in a mosh pit, the components of a recurrent system push and pull on each other, with the state of a unit at any given moment determined by those that surround it. The neurons in a Hopfield network are thus just like the atoms of iron constantly influencing each other through their magnetic interactions. The effects of this incessant interaction can be myriad and complex. To predict the patterns these interlocking parts will generate is essentially impossible without the precision of a mathematical model. Hopfield was intimately familiar with these models and their ability to show how local interactions lead to the emergence of global behaviour. </p>
<p class="TXI">Hopfield found that if the weights between the neurons in his network are just right the network as a <a id="page_94"></a>whole can implement associative memory. To understand this, we must first define what counts as a memory in this abstract model. Imagine that each neuron in a Hopfield network represents a single object: neuron A is a rocking chair, neuron B is a bike, neuron C is an elephant, and so on. To represent a particular memory, say that of your childhood bedroom, the neurons that represent all the objects in that room – the bed, your toys, photographs on the wall – should be ‘on’; while those that represent objects not in that room – the moon, a city bus, kitchen knives – should be ‘off’. The network as a whole is then in the ‘your childhood bedroom’ activity state. A different activity state – with different sets of neurons ‘on’ or ‘off’ – would represent a different memory. </p>
<p class="TXI">In associative memory, a small input to the network reactivates an entire memory state. For example, seeing a picture of yourself on your childhood bed may activate some of the neurons that represent your bedroom: the bed neurons and pillow neurons, and so on. In the Hopfield network, the connections between these neurons and the ones that represent other parts of the bedroom – the curtains and your toys and your desk – cause these other neurons to become active, recreating the full bedroom experience. Negatively weighted connections between the bedroom neurons and those that represent, say, a local park, ensure that the bedroom memory is not infiltrated by other items. That way you don’t end up remembering a swing set next to your closet. </p>
<p class="TXI">As some neurons turn on and others off, it is their interactivity that brings the full memory into stark relief. <a id="page_95"></a>The heavy-lifting of memory is thus done by the synapses. It is the strength of these connections that carries out the formidable yet delicate task of memory retrieval. </p>
<p class="TXI">In the language of physics, a fully retrieved memory is an example of an <span class="italic">attractor</span>. An attractor is, in short, a popular pattern of activity. It is one that other patterns of activity will evolve towards, just as water is pulled down a drain. A memory is an attractor because the activation of a few of the neurons that form the memory will drive the network to fill in the rest. Once a network is in an attractor state, it remains there with the neurons fixed in their ‘on’ or ‘off’ positions. Always fond of describing things in terms of energy, physicists consider attractors ‘low energy’ states. They’re a comfortable position for a system to be in; that is what makes them attractive and stable. </p>
<p class="TXI">Imagine a trampoline with a person standing on it. A ball placed anywhere on the trampoline will roll towards the person and stay there. The ball being in the divot created by the person is thus an attractor state for this system. If two people of the same size were standing opposite each other on the trampoline, the system would have two attractors. The ball would roll towards whomever it was initially closest to, but all roads would still lead to an attractor. Memory systems wouldn’t be of much use if they could only store one memory, so it is important that the Hopfield network can sustain multiple attractors. The same way the ball is compelled towards the nearest low point on the trampoline, initial neural activity states evolve towards the nearest, most similar memory (see Figure 9). The initial states that lead to a specific memory attractor – for example, the picture <a id="page_96"></a>of your childhood bed that reignites a memory of the whole room or a trip to a beach that ignites the memory of a childhood holiday – are said to be in that memory’s ‘basin of attraction’.</p>
<p class="TXI">
<span class="italic">The Pleasures of Memory</span> is a 1792 poem by Samuel Rogers. Reflecting on the universal journey on which memory can take the mind, he wrote:</p>
<p class="EXTF">
<span class="italic">Lulled in the countless chambers of the brain,</span></p>
<p class="EXTM">
<span class="italic">Our thoughts are linked by many a hidden chain.</span></p>
<p class="EXTM">
<span class="italic">Awake but one, and lo, what myriads rise!</span></p>
<p class="EXTL">
<span class="italic">Each stamps its image as the other flies!</span></p>
<p class="TXT">Rogers’ ‘hidden chain’ can be found in the pattern of weights that reignite a memory in the Hopfield network. <a id="page_97"></a>Indeed, the attractor model aligns with much of our intuition about memory. It implicitly addresses the time it takes for memories to be restored, as the network needs time to activate the right neurons. Attractors can also be slightly displaced in the network, creating memories that are mostly correct, with a detail or two changed. And memories that are too similar may simply merge into one. While collapsing memory to a series of zeros and ones may seem an affront to the richness of our experience of it, it is the condensation of this seemingly ineffable process that puts an understanding of it within reach.</p>
<p class="image-fig" id="fig9.jpg">
<img alt="" src="Images/chapter-04-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 9</span>
</span></p>
<p class="TXI">In the Hopfield network, how robustly neurons are connected with each other defines which patterns of neural activity form a memory. The engram is therefore in the weights – but how does it get there? How can an experience create just the right weights to make a memory? Hebb tells us that memories should come out of a strengthening of the connections between neurons that have similar activity – and in the Hopfield network that is just how it’s done.</p>
<p class="TXI">The Hopfield network encodes a set of memories through a simple procedure. For every experience in which two neurons are either both active or inactive, the connection between them is strengthened. In this way, the neurons that fire together come to be wired together. On the other hand, for every pattern where one neuron is active and the other is inactive, the connection is weakened.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> After this learning <a id="page_98"></a>procedure, neurons that are commonly co-active in memories will have a strong positive connection, those that have opposite activity patterns will have strong negative connections and others will fall somewhere in between. This is just the connectivity needed to form attractors.</p>
<p class="TXI">Attractors are not trivial phenomena. After all, if all the neurons in a network are constantly sending and receiving inputs, why should we assume their activity would ever settle into a memory state, let alone the <span class="italic">right</span> memory state? So, to be certain that the right attractors would form in these networks, Hopfield had to make a pretty strange assumption: weights in the Hopfield network are <span class="italic">symmetric</span>. That means the strength of the connection from neuron A to neuron B is always the same as the strength from B to A. Enforcing this rule offered a mathematical guarantee of attractors. The problem is that the odds of finding a population of neurons like this in the brain are dismal to say the least. It would require that each axon going out of one cell and forming a synapse with another be matched exactly by that same cell sending its axon back, connecting to the first cell with the same strength. Biology simply isn’t that clean.</p>
<p class="TXI">This illuminates the ever-present tension in the mathematical approach to biology. The physicist’s perspective, which depends on an almost irrational degree of simplification, is at constant odds with the biology, full as it is of messy, inconvenient details. In this case, the details of the maths demanded symmetric weights in order to make any definitive statement about attractors and thus to make progress on modelling the <a id="page_99"></a>process of memory. A biologist would likely have dismissed the assumption outright.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup></p>
<p class="TXI">Hopfield, with one foot on either side of the mathematics–biology divide, knew to appreciate the perspective of the neuroscientists. To ease their concerns, he showed in his original paper that – even though it couldn’t be guaranteed mathematically – networks that allowed asymmetric weights still seemed able to learn and sustain attractors relatively well. </p>
<p class="TXI">The Hopfield network thus offered a proof of concept that Hebb’s ideas about learning could actually work. Beyond that, it offered a chance to study memory mathematically – to quantify it. For example, precisely how many memories can a network hold? This is a question that can only be asked with a precise model of memory in mind. In the simplest version of the Hopfield network, the number of memories depends on the number of neurons in the network. A network with 1,000 neurons, for example, can store about 140 memories; 2,000 neurons can store 280; 10,000 can store 1,400 and so on. If the number of memories remains less than about 14 per cent the number of neurons, each memory will be restored with minimal error. Adding more memories, however, will be like the final addition to a house of cards that causes it to cave in. When pushed past its capacity, the Hopfield network collapses: inputs <a id="page_100"></a>go towards meaningless attractors and no memories are successfully recovered. It’s a phenomenon given the appropriately dramatic name ‘blackout catastrophe’.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup></p>
<p class="TXI">Precision cannot be evaded; once this estimate of memory capacity is found, it’s reasonable to start asking if it aligns with the number of memories we know to be stored by the brain. A landmark study in 1973 showed that people who had been shown more than 10,000 images (each only once and for only a brief period of time) were quite capable of recognising them later on. The 10 million neurons in the perirhinal cortex – a brain region implicated in visual memory – could store this amount of images, but it wouldn’t leave much space for anything else. Therefore, there seemed to be a problem with Hebbian learning.</p>
<p class="TXI">This problem becomes less problematic, however, when we realise that recognition is not recall. That is, a feeling of familiarity when seeing an image can happen without the ability to regenerate that image from scratch. The Hopfield network is remarkable for being capable of the latter, more difficult task – it fully completes a memory from a partial bit of it. But the former task is still important. Thanks to researchers working at the University of Bristol, it’s now known that recognition can also be performed by a network that uses Hebbian learning. These networks, when assessed on their ability to label an input as novel or familiar, have a significantly <a id="page_101"></a>higher capacity: 1,000 neurons can now recognise as many as 23,000 images. Just as Semon so presciently identified, this is an example of an issue that arises from relying on common language to parcel up the functions of the brain. What feels like simply ‘memory’ to us crumbles when pierced by the scrutiny of science and mathematics into a smattering of different skills.</p>
<p class="center">* * *</p>
<p class="TXT">When, in 1953, American doctor William Scoville removed the hippocampus from each side of 27-year-old Henry Molaison’s brain, he thought he was helping prevent Molaison’s seizures. What Scoville didn’t know was the incredible impact this procedure would have on the science of memory. Molaison (more famously known as ‘H. M.’ in scientific papers to hide his identity until his death in 2008) did find some relief from his seizures after the procedure, but he never formed another conscious memory again. Molaison’s subsequent and permanent amnesia initiated a course of research that centred the hippocampus – a curved finger-length structure deep in the brain – as a hub in the memory-formation system. Evading Lashley’s troubled search, this is a location that does play a special role in storing memories.</p>
<p class="TXI">Current theories of hippocampal function go as follows: information about the world first reaches the hippocampus at the dentate gyrus, a region that runs along the bottom edge of the hippocampus. Here, the representation is primed and prepped to be in a form more amenable to memory storage. The dentate gyrus then sends connections on to where attractors are <a id="page_102"></a>believed to form, an area called CA3; CA3 has extensive recurrent connections that make it a prime substrate for Hopfield network-like effects. This area then sends output to another region called CA1, which acts as a relay station; it sends the remembered information back to the rest of the brain (see Figure 10).</p>
<p class="image-fig" id="fig10.jpg">
<img alt="" src="Images/chapter-04-image-04.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 10</span>
</span></p>
<p class="TXI">What’s interesting about this final step – and what may have muddied Lashley’s original findings – is that these projections out to different areas of the brain are believed to facilitate the <span class="italic">copying</span> of memories. In this way, CA3 acts as a buffer, or warehouse, holding on to memories until they can be transferred to other brain areas. It does so by reactivating the memory in those areas. The hippocampus thus helps the rest of the brain memorise things using the same strategy you’d use to study for a test: repetition. By repeatedly reactivating the same group of neurons elsewhere in the brain, the hippocampus gives those neurons the chance to undergo Hebbian learning themselves. Eventually, their own weights have changed enough for the memory to be safely stored there.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> With <a id="page_103"></a>his hippocampus gone, Molaison had no warehouse for his experiences, no way to replay his memories back to his brain. </p>
<p class="TXI">With knowledge of this memory storehouse in the brain, researchers can look into how it works. Particularly, they can look for attractors in it.</p>
<p class="TXI">In 2005, scientists at University College London recorded the activity of hippocampal cells in rats. The rats got used to being in two different enclosures – a circular one and a square one. Their hippocampal neurons showed one pattern of activity when they were in the circle and a different pattern when they were in the square. The test for attractors came when an animal was placed into a new ‘squircle’ environment, the shape of which was somewhere in between a circle and a square. The researchers found that if the environment was more square-like the neural activity went to the pattern associated with the square environment; more circle-like and it went to that of the circle. Crucially, there were no intermediate representations in response to intermediate environments, only all circle or all square. This makes the memories of the circle and square environments attractors. An initial input that isn’t exactly one or the other is unstable; it gets inescapably driven towards the nearest established memory. </p>
<p class="TXI">The Hopfield network made manifest the theories of Hebb and showed how attractors – normally studied in physics – could explain the mysteries of memory. Yet Hopfield knew the limitations of bringing mathematics to real brains in real laboratories. He described his own model as a ‘mere parody of the complexities of neurobiology’. Indeed, as the creation of a physicist, it <a id="page_104"></a>lacks all the gooey richness of biology. But as a parody capable of powerful computations, it has offered many insights as well – insights that didn’t end with simple storage and recall. </p>
<p class="center">* * *</p>
<p class="TXT">You’re eating dinner in your kitchen when your roommate comes home. When you see them, you remember that last night you finished a book they had lent you and you want to return it before they leave for a trip the next day. So, you put down your food, head out of the kitchen and go down the hallway. You walk up the stairs, turn, enter your room and think: ‘Wait, what am I doing here?’ </p>
<p class="TXI">The sensation is a common one. So much so it’s been given a name: ‘destinesia’ or amnesia about why you’ve gone to where you are. It’s a failure of what’s called ‘working memory’, the ability to hold an idea in mind, even just for the 10 seconds it takes to walk from room to room. Working memory is crucial for just about all aspects of cognition: it’s hard to make a decision or work through a plan if you keep forgetting what you’re thinking about.</p>
<p class="TXI">Psychologists have been studying working memory for decades. The term itself was first coined in the 1960 book <span class="italic">Plans and the Structure of Behavior</span> written by George A. Miller and fellow scientists working at the Center for Advanced Study in the Behavioral Sciences in California. But the concept was explored well before that. Indeed, Miller himself wrote one of the most influential papers on the topic four years previously, in 1956. Perhaps <a id="page_105"></a>anticipating its fame, Miller gave the paper the cheeky title ‘The magical number seven, plus or minus two’. What that magical number refers to is the number of items humans can hold in their working memory at any one time. </p>
<p class="TXI">An example of how to assess this is to 1) show a participant several coloured squares on a screen; 2) ask them to wait for some time between several seconds to minutes; 3) then show them a second set of coloured squares. The task of the subject is to indicate if the colours of the second set are the same as the colours of the first. People can do well on this task if the number of squares shown remains small, achieving nearly 100 per cent accuracy if only one square is shown. Adding in more squares makes the performance drop and drop, until past seven, it’s almost no different to random guessing. Whether seven really is a special value when it comes to this kind of working memory capacity is up for debate; some studies find lower limits, some higher. However, there’s no doubt that Miller’s paper made an impact and psychologists have worked to characterise nearly every aspect of working memory since, from what can be held in it to how long it can last. </p>
<p class="TXI">But the question remains of how the brain actually does this: where are working memories stored and in what way? A tried-and-tested method for answering such questions – lesion experiments – pointed to the prefrontal cortex, a large part of the brain just behind the forehead. Whether it was humans with unfortunate injuries or laboratory animals with the area removed, it was clear that damaging the prefrontal cortex reduced working memory substantially. Without it, animals can <a id="page_106"></a>hardly hold on to an idea for more than a second or two. Thoughts and experiences pass through their minds like water through cupped hands. </p>
<p class="TXI">With an ‘X’ marking the spot, neuroscientists then began to dig. Dropping an electrode into the prefrontal cortex of monkeys, in 1971 researchers at the University of California in Los Angeles eavesdropped on the neurons there. The scientists, Joaquin Fuster and Garrett Alexander, did this while the animals performed a task similar to the colour memory test. These tests are known as ‘delayed response’ tasks because they include a delay period wherein the important information is absent from the screen and must thus be held in memory. The question was: what are neurons in the prefrontal cortex doing during this delay?</p>
<p class="TXI">Most of the brain areas responsible for vision have a stereotyped response to this kind of task: the neurons respond strongly when the patterns on the screen initially show up and then again when they reappear after the delay, but during the delay period – when no visual inputs are actually entering the brain – these areas are mostly quiet. Out of sight really does mean out of mind for these neurons. What Fuster and Alexander found, however, was that cells in the prefrontal cortex were different. The neurons there that responded to the visual patterns kept firing even after the patterns disappeared; that is, they maintained their activity during the delay period. A physical signature of working memory at work!</p>
<p class="TXI">Countless experiments since have replicated these findings, showing maintained activity during delay periods under many different circumstances, both in <a id="page_107"></a>the prefrontal cortex and beyond. Experiments have also hinted that when these firing patterns are out of whack, working memory goes awry. In some experiments, for example, applying a brief electrical stimulation during the delay period can disrupt the ongoing activity and this leads to a dip in performance on delayed response tasks. </p>
<p class="TXI">What is so special about these neurons that they can do this? Why can they hold on to information and maintain their firing for seconds to minutes, when other neurons let theirs go? For this kind of sustained output, neurons usually need sustained input. But if delay activity occurs without any external input from an image then that sustained input must come from neighbouring neurons. Thus, delay activity can only be generated by a network of neurons working together, the connections between them conspiring to keep the activity alive. This is where the idea of attractors comes back into play. </p>
<p class="TXI">So far we’ve looked at attractors in Hopfield networks, which show how input cues reignite a memory. It may not be clear how this helps with working memory. After all, working memory is all about what happens after that ignition; after you stand up to get your roommate’s book, how do you keep that goal in mind? As it turns out, however, an attractor is exactly what’s needed in this situation, because an attractor stays put.</p>
<p class="TXI">Attractors are defined by derivatives. If we know the inputs a neuron gets and the weights those inputs are multiplied by, we can write down an equation – a derivative – describing how the activity of that neuron will change over time as a result of those inputs. If this derivative is zero that means there is no change in the <a id="page_108"></a>activity of the neuron over time; it just keeps firing at the same, constant rate. Recall that, because this neuron is part of a recurrent network, it not only gets input but also serves as input to other neurons. So, its activity goes into calculating the derivative of a neighbouring neuron. If none of the inputs to this neighbouring neuron are changing – that is, their derivatives are all zero as well – then it too will have a zero derivative and will keep firing at the same rate. When a network is in an attractor state, the derivative of each and every neuron in that network is zero.</p>
<p class="TXI">And that is how, if the connections between neurons are just right, memories started at one point in time can last for much longer. All the cells can maintain their firing rate because all the cells around them are doing the same. Nothing changes if nothing changes. </p>
<p class="TXI">The problem is that things do change. When you leave the kitchen and walk to your bedroom, you encounter all kinds of things – your shoes in the hallway, the bathroom you meant to clean, the sight of rain on the window – that could cause changes in the input to the neurons that are trying to hold on to the memory. And those changes could push the neurons out of the attractor state representing the book and off to somewhere else entirely. For working memory to function, the network needs to be good at resisting the influence of such distractors. A run-of-the-mill attractor can resist distracting input to an extent. Recall the trampoline example. If the person standing on the trampoline gave a little nudge to the ball, it would likely roll just out of its divot and then back in. With only a small perturbation the memory stays intact, but give the <a id="page_109"></a>ball a heartier kick and who knows where it will end up? Good memory should be robust to such distractions – so what could make a network good at holding on to memories? </p>
<p class="TXI">The dance between data and theory is a complex one, with no clear lead or follow. Sometimes mathematical models are developed just to fit a certain dataset. Other times the details from data are absent or ignored and theorists do as their name suggests: theorise about how a system <span class="italic">could</span> work before knowing how it does. When it comes to building a robust network for working memory, scientists in the 1990s went in the latter direction. They came up with what’s known as the ‘ring network’, a hand-designed model of a neural circuit that would be ideal for the robust maintenance of working memories. </p>
<p class="TXI">Unlike Hopfield networks, ring networks are well described by their name: they are composed of several neurons arranged in a ring, with each neuron connecting only to those near to it. Like Hopfield networks, these models have attractor states – activity patterns that are self-sustaining and can represent memories. But the attractor states in a ring model are different to those in a Hopfield network. Attractors in a Hopfield model are <span class="italic">discrete</span>. This means that each attractor state – the one for your childhood bedroom, the one for your childhood holiday, the one for your current bedroom – is entirely isolated from the rest. There is no smooth way to transition between these different memories, regardless of how similar they are; you have to completely leave one attractor state to get to another. Attractors in a ring network, on the other hand, are <span class="italic">continuous</span>. With continuous attractors, transitioning between similar <a id="page_110"></a>memories is easy. Rather than being thought of as a trampoline with people standing at different points, models with continuous attractor states are more like the gutter of a bowling lane: once the ball gets into the gutter it can’t easily get out, but it can move smoothly within it.</p>
<p class="TXI">Networks with continuous attractor states like the ring model are helpful for a variety of reasons and chief among them is the type of errors they make. It may seem silly to praise a memory system for its errors – wouldn’t we prefer no errors at all? – but if we assume that no network can have perfect memory, then the quality of the errors becomes very important. A ring network allows for small, sensible errors. </p>
<p class="TXI">Consider the example of the working memory test where subjects had to keep in mind the colour of shapes on a screen. Colours map well to ring networks because, as you’ll recall from art class, colours lie on a wheel. So, imagine a network of neurons arranged in a ring, with each neuron representing a slightly different colour. At one side of the ring are red-representing neurons, next to them are orange, then yellow and green; this brings us to the side opposite the red, where there are the blue-representing neurons, which lead to the violet ones and back to red. </p>
<p class="TXI">In this task, when a shape is seen, it creates activity in the neurons that represent its colour, while the other neurons remain silent. This creates a little ‘bump’ of activity on the ring, centred on the remembered colour. If any distracting input comes in while the subject tries to hold on to this colour memory – from other random sights in the room, for example – it may push or pull the activity bump away from the desired colour. But – and this is the crucial <a id="page_111"></a>point – it will only be able to push it to a very nearby place on the ring. So red may become red-orange or green may become teal. But the memory of red would be very unlikely to become green. Or, for that matter, to become no colour at all; that is, there will always be a bump <span class="italic">somewhere</span> on the ring. These properties are all a direct result of the gutter-like nature of a continuous attractor – it has low resistance for moving between nearby states, but high resistance to perturbations otherwise.</p>
<p class="TXI">Another benefit of the ring network is that it can be used to do things. The ‘working’ in working memory is meant to counter the notion that memory is just about passively maintaining information. Rather, holding ideas in working memory lets us combine them with other information and come to new conclusions. An excellent example of this is the head direction system in rats, which also served as the inspiration for early ring network models. </p>
<p class="TXI">Rats (along with many other animals) have an internal compass: a set of neurons that keep track of the direction the animal is facing at all times. If the animal turns to face a new direction, the activity of these cells changes to reflect that change. Even if the rat sits still in a silent darkened room, these neurons continue to fire, holding on to the information about its direction. In 1995, a team from Bruce McNaughton’s lab at the University of Arizona and, separately, Kechen Zhang of the University of California, San Diego, posited that this set of cells could be well described by a ring network. Direction being one of those concepts that maps well to a circle, a bump of activity on the ring would be used to store the direction the animal was facing (See Figure 11).</p> <a id="page_112"></a>
<p class="TXI">But not only could a ring network explain how knowledge of head direction was maintained over time, it also served as a model of how the stored direction could change when the animal did. Head direction cells receive input from other neurons, such as those from the visual system and the vestibular system (which keeps track of bodily motion). If these inputs are hooked up to the ring network just right, they can push the bump of activity along to a new place on the ring. If the vestibular system says the body is now moving leftwards, for example, the bump gets pushed to the left. In this way, movement along the ring doesn’t create errors in memory, but rather updates the memory based on new information. ‘Working’ memory earns its name.</p>
<p class="image-fig" id="fig11.jpg">
<img alt="" src="Images/chapter-04-image-05.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 11</span>
</span></p>
<p class="TXI">Ring networks are a lovely solution to the complex problem of how to create robust and functional working memory systems. They are also beautiful mathematical objects. They display the desirable properties of simplicity and symmetry. They’re precise and finely tuned, elegant even.</p> <a id="page_113"></a>
<p class="TXI">As such, they are completely unrealistic. Because to the biologist, of course, ‘finely tuned’ are dirty words. Anything that requires delicate planning and pristine conditions to operate well won’t survive the chaos that is brain development and activity. Many of the desirable properties of ring networks only occur under very particular assumptions about the connectivity between neurons, assumptions that just don’t seem very realistic. So, despite all their desirable theoretical properties and useful abilities, the chances of seeing a ring network in the brain seemed slim. </p>
<p class="TXI">The discovery made in a research centre just outside Washington DC in 2015 was therefore all the more exciting. </p>
<p class="TXI">Janelia Research Campus is a world-class research facility hidden away in the idyllic former farmland of Ashburn, Virginia. Vivek Jayaraman has been at Janelia since 2006. He and his team of about half a dozen people work to understand navigation in <span class="italic">Drosophila melanogaster</span>, a species of fruit fly commonly studied in neuroscience. On a par with a grain of rice, the size of these animals is both a blessing and a curse. While they can be difficult to get hold of, these tiny flies only have around 135,000 neurons, roughly 0.2 per cent the amount of another popular lab animal, the mouse. On top of that, a lot is known about these neurons. Many of them are easily categorised based on the genes they express, and their numbers and locations are very similar across individuals. </p>
<p class="TXI">Like rodents, flies also have a system for keeping track of head direction. For the fly, these head direction neurons are located in a region known as the ellipsoid <a id="page_114"></a>body. The ellipsoid body is centrally placed in the fly brain and it has a unique shape: it has a hole in the middle with cells arranged all around that hole, forming a doughnut made of neurons – or in other words, a ring.</p>
<p class="TXI">Neurons arranged in a ring, however, do not necessarily make a ring network. So, what the Jayaraman lab set out to do was investigate whether this group of neurons that <span class="italic">looked</span> like a ring network, actually <span class="italic">behaved</span> like one. To do this, they put a special dye in the ellipsoid body neurons, one that makes them light up green when they’re active. They then had the fly walk around, while they filmed the neurons. If you were to look at these neurons on a screen, as the fly heads forwards, you’d see a flickering of little green points at one location on an otherwise black screen. Should the fly choose to make a turn, that flickering patch would swing around to a new location. Over time, as the fly moves and the green patch on the screen moves with it, the points that have lit up form a clear ring structure, matching the underlying shape of the ellipsoid body. If you turn the lights off in the room so the fly has no ability to see which way it’s facing, the green flicker still remains at the same location on that ring – a clear sign that the memory of heading direction is being maintained. </p>
<p class="TXI">In addition to observing the activity on the ring, the experimenters also manipulated it in order to probe the extremes of its behaviour. A true ring network can only support one ‘bump’ of activity; that is, only neurons at one location on the ring can be active at a given time. So, the researchers artificially stimulated neurons on the side of the ring opposite to those already active. This strong stimulation of the opposing neurons caused the original bump to shut down, and the bump at the new location was maintained, even after the stimulation was <a id="page_115"></a>turned off. Through these experiments, it became clear that the ellipsoid body was no imposter, but a vivid example of a theory come to life. </p>
<p class="TXI">This finding – a ring network in the literal, visible shape of a ring – feels a bit like nature winking at us. William Skaggs and the other authors of one of the original papers proposing the ring network explicitly doubted the possibility of such a finding: ‘For expository purposes it is helpful to think of the network as a set of circular layers; this does not reflect the anatomical organisation of the corresponding cells in the brain.’ Most theorists working on ring network models assumed that they’d be embedded in some larger, messier network of neurons. And that is bound to be the case for most systems in most species. This anomalously pristine example likely arises from a very precisely controlled genetic programme. Others will be much harder to spot. </p>
<p class="TXI">Even if we usually can’t see them directly, we can make predictions about behaviours we’d expect to see if the brain is using continuous attractors. In 1991, pioneering working memory researcher Patricia Goldman-Rakic found that blocking the function of the neuromodulator dopamine made it harder for monkeys to remember the location of items. Dopamine is known to alter the flow of ions into and out of a cell. In 2000, researchers at the Salk Institute in California showed how mimicking the presence of dopamine in a model with a continuous attractor enhanced the model’s memory.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup> It stabilised the activity of the neurons encoding the memory, making <a id="page_116"></a>them more resistant to irrelevant inputs. Because dopamine is associated with reward,<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> this model also predicts that under conditions where a person is anticipating a large reward their working memory will be better – and that is exactly what has been found. When people are promised more reward in turn for remembering something, their working memory is better. Here, the concept of an attractor works as the thread that stitches chemical changes together with cognitive ones. It links ions to experiences. </p>
<p class="TXI">Attractors are omnipresent in the physical world. They arise from local interactions between the parts of a system. Whether those parts are atoms in a metal, planets in a solar system or even people in a community, they will be compelled towards an attractor state and, without major disruptions, will stay there. Applying these concepts to the neurons that form a memory connects dots across biology and psychology. On one side, Hopfield networks link the formation and retrieval of memories to the way in which connections between neurons change. On the other, structures like ring networks underlie how ideas are held in the mind. In one simple framework we capture how memories are recorded, retained and reactivated.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-1" id="fn-1">1</a> ﻿Jerzy Konorski, a Polish neurophysiologist, published a book with very similar ideas the year before Hebb did. In fact, Konorski anticipated several important findings in neuroscience and psychology. However, the global East﻿–﻿West divide at the time isolated his contributions. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-2" id="fn-2">2</a> ﻿When Hopfield wrote on an undergraduate admission form that he intended to study ﻿‘﻿physics or chemistry﻿’﻿, his university advisor ﻿–﻿ a colleague of his father﻿’﻿s ﻿–﻿ crossed out the latter option saying, 
﻿‘﻿I don﻿’﻿t believe we need to consider chemistry.﻿’﻿ ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-3" id="fn-3">3</a> ﻿Hopfield﻿’﻿s attitude was not unique. The 1980s found many physicists, bored with their own field, looking at the brain and thinking, ﻿‘﻿I could solve that.﻿’﻿ After Hopfield﻿’﻿s success, this population increased even more.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-4" id="fn-4">4</a> ﻿While the calculation of an individual neuron﻿’﻿s activity in terms of inputs and weights is the same as described for the perceptron in the last chapter, the perceptron is a ﻿<span class="italic">feedforward</span>﻿ (not recurrent) network. Recurrence means that the connections can form loops: neuron A connects to neuron B, which connects back to neuron A, for example. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-5" id="fn-5">5</a> ﻿This second part ﻿–﻿ the idea that connection strength should ﻿<span class="italic">decrease</span>﻿ if a pre-synaptic neuron is highly active while the post-synaptic neuron remains quiet ﻿–﻿ was not part of Hebb﻿’﻿s original sketch, but it has since been borne out by experiments.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-6" id="fn-6">6</a> ﻿In fact, when Hopfield presented an early version of this work to a group of neuroscientists, one attendee commented that ﻿‘﻿it was a beautiful talk but unfortunately had nothing to do with neurobiology﻿’﻿. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-7" id="fn-7">7</a> ﻿You may know some people with stories of their own ﻿‘﻿blackout catastrophe﻿’﻿ after a night of drinking. However, the exact type of memory failure seen in Hopfield networks is not actually believed to occur in humans.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-8" id="fn-8">8</a> ﻿This process is believed to occur while you sleep.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-9" id="fn-9">9</a> ﻿This model was composed of the Hodgkin-Huxley style of neurons described in ﻿﻿Chapter 2﻿﻿, which makes incorporating dopamine﻿’﻿s effects on ion flows easy.﻿</p>
<p class="FN2"><a href="chapter4.xhtml#fnt-10" id="fn-10">10</a> ﻿Lots more on this in ﻿﻿Chapter 11﻿﻿!﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<meta charset="utf-8"/>
<title>Contents</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="FMT" id="re_contents">Contents</p>
<p class="TOC-CH"><a href="chapter1.xhtml#chapter1" id="re_chapter1">Chapter 1:  Spherical Cows</a></p>
<p class="TOC-CH"><a href="chapter2.xhtml#chapter2" id="re_chapter2">Chapter 2:  How Neurons Get Their Spike</a></p>
<p class="TOC-CH"><a href="chapter3.xhtml#chapter3" id="re_chapter3">Chapter 3:  Learning to Compute</a></p>
<p class="TOC-CH"><a href="chapter4.xhtml#chapter4" id="re_chapter4">Chapter 4:  Making and Maintaining Memories</a></p>
<p class="TOC-CH"><a href="chapter5.xhtml#chapter5" id="re_chapter5">Chapter 5:  Excitation and Inhibition</a></p>
<p class="TOC-CH"><a href="chapter6.xhtml#chapter6" id="re_chapter6">Chapter 6:  Stages of Sight</a></p>
<p class="TOC-CH"><a href="chapter7.xhtml#chapter7" id="re_chapter7">Chapter 7:  Cracking the Neural Code</a></p>
<p class="TOC-CH"><a href="chapter8.xhtml#chapter8" id="re_chapter8">Chapter 8:  Movement in Low Dimensions</a></p>
<p class="TOC-CH"><a href="chapter9.xhtml#chapter9" id="re_chapter9">Chapter 9:  From Structure to Function</a></p>
<p class="TOC-CH"><a href="chapter10.xhtml#chapter10" id="re_chapter10">Chapter 10:  Making Rational Decisions</a></p>
<p class="TOC-CH"><a href="chapter11.xhtml#chapter11" id="re_chapter11">Chapter 11:  How Rewards Guide Actions</a></p>
<p class="TOC-CH"><a href="chapter12.xhtml#chapter12" id="re_chapter12">Chapter 12:  Grand Unified Theories of the Brain</a></p>
<p class="TOC-CH1"><a href="Mathematical.xhtml#Mathematical" id="re_Mathematical">Mathematical Appendix</a></p>
<p class="TOC-CH"><a href="ack.xhtml#ack" id="re_ack">Acknowledgements</a></p>
<p class="TOC-CH"><a href="bib.xhtml#bib" id="re_bib">Bibliography</a></p>
<p class="TOC-CH"><a href="index.xhtml#index" id="re_index">Index</a></p>
</body>
</html>