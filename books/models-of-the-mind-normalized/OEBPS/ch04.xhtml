<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 4</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter4"><a href="contents.xhtml#re_chapter4">CHAPTER FOUR</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter4">Making and Maintaining Memories</a><a id="page_83"></a></p>
<p class="H1" id="b-9781472966445-ch354-sec4">
<span class="bold">
<span>The Hopfield network and attractors</span>
</span></p>
<p class="TXT">A block of iron at 770°C (1,418°F) is a sturdy grey mesh. Each of its trillions of atoms serves as a single brick in the endless parallel walls and ceilings of its crystalline structure. It is a paragon of orderliness. In opposition to their organised structural arrangement, however, the magnetic arrangement of these atoms is a mess. </p>
<p class="TXI">Each iron atom forms a dipole – a miniature magnet with one positive and one negative end. Heat unsteadies these atoms, flipping the direction of their poles around at random. On the micro-level this means many tiny magnets each exerting a force in its own direction. But as these forces work against each other, their net effect becomes negligible. When you zoom out, this mass of mini-magnets has no magnetism at all.</p>
<p class="TXI">As the temperature dips below 770°C, however, something changes. The direction of an individual atom is less likely to switch. With its dipole set in place, the atom starts to exert a constant pressure on its neighbours. This indicates to them which direction they too should be facing. Atoms with different directions vie for influence over the local group until eventually everyone falls in line, one way or the other. With all the small dipoles aligned, the net force is strong. The previously inert block of iron becomes a powerful magnet.</p> <a id="page_84"></a>
<p class="TXI">Philip Warren Anderson, an American physicist who won a Nobel Prize working on such phenomena, wrote in a now-famous essay entitled ‘More is different’ that ‘the behaviour of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles’. That is, the collective action of many small particles – organised only through their local interactions – can produce a function not directly possible in any of them alone. Physicists have formalised these interactions as equations and successfully used them to explain the behaviour of metals, gases and ice. </p>
<p class="TXI">In the late 1970s, a colleague of Anderson’s, John J. Hopfield, saw in these mathematical models of magnetism a structure akin to that of the brain. Hopfield used this insight to bring under mathematical control a long-lasting mystery: the question of how neurons make and maintain memories.</p>
<p class="center">* * *</p>
<p class="TXT">Richard Semon was wrong.</p>
<p class="TXI">A German biologist working at the turn of the twentieth century, Semon wrote two lengthy books on the science of memory. They were filled with detailed descriptions of experimental results, theories and a vocabulary for describing memory’s impact on ‘organic tissue’. Semon’s work was insightful, honest and clear – but it contained a major flaw. Just as French naturalist Jean-Baptiste Lamarck believed (in contrast to our current understanding of evolution) that traits acquired by an animal in its lifetime could be passed to its offspring, <a id="page_85"></a>Semon proposed that <span class="italic">memories</span> acquired by an animal could be passed down. That is, he believed that an organism’s learned responses to its own environment would arise without instruction in its offspring. As a result of this mistaken intuition, much of Semon’s otherwise valuable work was slowly cast aside and forgotten.</p>
<p class="TXI">Being wrong about memory isn’t unusual. Philosopher René Descartes, for example, thought memories were activated by a small gland directing the flow of ‘animal spirits’. What’s unique about Semon is that, despite the flaw in his work that sentenced him to historical obscurity, one of his contributions remained influential long enough to spawn an entire body of research. This small artefact of his efforts is the ‘engram’ – a word coined by Semon in <span class="italic">The</span>
<span class="italic">Mneme</span> in 1904, and subsequently learned by millions of students of psychology and neuroscience.</p>
<p class="TXI">At the time Semon was writing, memory had only recently come under scientific scrutiny – and most of the results were purely about memorisation skills, not about biology. For example, people would be trained to memorise pairs of nonsense words (such as ‘wsp’ and ‘niq’) and then were tested on their ability to retrieve the second word when prompted with the first. This type of memory, known as <span class="italic">associative</span> memory, would become a target of research for decades to come. But Semon was interested in more than just behaviour; he wanted to know what changes in an animal’s physiology could support such associative memories.</p>
<p class="TXI">Led by scant experimental data, he broke up the process of creating and recovering memories into multiple components. Finding common words too <a id="page_86"></a>vague and overloaded, he created novel terms for these divisions of labour. The word that would become so influential, the engram, was defined as ‘the enduring though primarily latent modification in the irritable substance produced by a stimulus’. Or, to put it more plainly: the physical changes in the brain that happen when a memory is formed. Another term, ‘ecphory’, was assigned to the ‘influences which awake the mnemic trace or engram out of its latent state into one of manifested activity’. This distinction between engram and ecphory (or between the processes that lay a memory and those that retrieve it) was one of the many conceptual advances that Semon’s work provided. Despite the fact that his name and most of his language have disappeared from the literature, many of Semon’s conceptual insights were correct and they form the core of how memories are modelled today.</p>
<p class="TXI">In 1950, American psychologist Karl Lashley published ‘In search of the engram’, a paper that solidified the legacy of the word. It also set a rather dismal tone for the field. The paper was so titled because the search was all Lashley felt he had accomplished in 30 years of experiments. Lashley’s experiments involved training animals to make an association (for example, to react in a specific way when shown a circle versus an ‘X’) or learn a task, such as how to run through a particular maze. He would then surgically remove specific brain areas or connection pathways and observe how behaviour was impacted post-operatively. Lashley couldn’t find any area or pattern of lesions that reliably interfered with memory. He concluded that memories must thus somehow be distributed equally across the <a id="page_87"></a>brain, rather than in any single area. But based on some calculations about how many neurons could be used for a memory and the number of pathways between them, he was uncertain about how this was possible. His landmark article thus reads as something of a white flag, a surrendering of any attempt to draw conclusions about the location of memory in the face of a mass of inconsistent data. The physical nature of memory remained to Lashley as vexing as ever.</p>
<p class="TXI">At the same time, however, a former student of Lashley’s was developing his own theories on learning and memory. </p>
<p class="TXI">Donald Hebb, a Canadian psychologist whose early work as a school teacher grew his interest in the mind, was intent on making psychology a biological science. In his 1949 book, <span class="italic">The Organization of Behavior</span>, he describes the task of a psychologist as ‘reducing the vagaries of human thought to a mechanical process of cause and effect’. And in that book, he lays down the mechanical process he believed to be behind memory formation.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> Overcoming the limited, and sometimes misleading, physiological data available at the time, Hebb came to this principle about the physical underpinnings of learning largely through intuition. Yet it would go on to have huge empirical success. The principle, now known as Hebbian learning, is succinctly described by the phrase ‘neurons that fire together wire together’.</p> <a id="page_88"></a>
<p class="TXI">Hebbian learning describes what happens at the small junction between two neurons where one can send a signal to the other, a space called the synapse. Suppose there are two neurons, A and B. The axon from neuron A makes a synaptic connection on to the dendrite or cell body of neuron B (making it the ‘pre-synaptic’ neuron, and neuron B the ‘post-synaptic’ neuron, see Figure 7). In Hebbian learning, if neuron A repeatedly fires before neuron B, the connection from A to B will strengthen. A stronger connection means that the next time A fires it will be more effective in causing B to fire. In this way, activity determines connectivity and connectivity determines activity.</p>
<p class="image-fig" id="fig7.jpg">
<img alt="" src="Images/chapter-04-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 7</span>
</span></p>
<p class="TXI">Hebb’s approach, with its focus on the synapse, situates the engram as both local and global: local because a memory’s imprint occurs at the small gap where one neuron meets another, but global because these changes may be happening at synapses all across the brain. It also makes memory the natural consequence of experience: with pliable synapses, any activation of the brain has the potential to leave a trace. </p>
<p class="TXI">Lashley, a dutiful scientist intent on following the facts, accepted that the engram must be distributed based on his own experiments. But he found no satisfaction in <a id="page_89"></a>Hebb’s solution, which – though an enticing and elegant theory – was based more on speculation than on hard evidence. He turned down Hebb’s offer to be a co-author on the work.</p>
<p class="TXI">Lashley may not have supported Hebb’s ideas, but since the publication of his book countless experiments have. Sea slugs – foot-long slimy brown invertebrates with only about 20,000 neurons – became a creature of much study in this area, due to their ability to learn a very basic association. These shell-less slugs have a gill on their backs that, if threatened, can be quickly retracted for safe keeping. In the lab, a short electric shock will cause the gill to withdraw. If such a shock is repeatedly preceded by a harmless light touch, the slug will eventually start withdrawing in response to the touch alone, demonstrating an association between the touch and what’s expected to come next. It is the marine critter equivalent of learning to pair ‘wsp’ with ‘niq’. This association was shown, in line with Hebb’s theory of learning, to be mediated by a strengthening of the connections between the neurons that represent the touch and those that lead to the gill’s response. The change in behaviour was forged through a changing of connections. </p>
<p class="TXI">Hebbian learning has not just been observed; it’s been controlled as well. In 1999, Princeton researchers showed that genetically modifying proteins in the cell membrane that contribute to synaptic changes can control a mouse’s capacity for learning. Increasing the function of these receptors enhances the ability of mice to remember objects they’ve been shown before. Interfering with these proteins impairs it.</p> <a id="page_90"></a>
<p class="TXI">It is now established science that experience leads to the activation of neurons and that activating neurons can alter the connections between them. This story is accepted as at least a partial answer to the question of the engram. But, as Semon describes it, the engram itself is only part of the story of memory. Memory also requires <span class="italic">re</span>membering. How can this way of depositing memories allow for long-term storage and recall? </p>
<p class="center">* * *</p>
<p class="TXT">It was no real surprise that John J. Hopfield became a physicist. Born in 1933 to John Hopfield Sr, a man who made a name for himself in ultraviolet spectroscopy, and Helen Hopfield, who studied atmospheric electromagnetic radiation, Hopfield Jr grew up in a household where physics was as much a philosophy as it was a science. ‘Physics was a point of view that the world around us is, with effort, ingenuity and adequate resources, understandable in a predictive and reasonably quantitative fashion,’ Hopfield wrote in an autobiography. ‘Being a physicist is a dedication to a quest for this kind of understanding.’ And a physicist is what he would be.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> </p>
<p class="TXI">Hopfield, a tall and lanky man with an engaging smile, earned his PhD in 1958 from Cornell University. He further emulated his father by receiving a Guggenheim fellowship, using it to study at the Cavendish Laboratory <a id="page_91"></a>at Cambridge. But even by this stage, Hopfield’s enthusiasm for the subject of his PhD – condensed matter physics – was waning. ‘In 1968, I had run out of problems … to which my particular talents seemed useful,’ he later wrote.</p>
<p class="TXI">Hopfield’s gateway from physics to biology was hemoglobin, a molecule that both serves a crucial biological function as the carrier of oxygen in blood and could be studied with many of the techniques of experimental physics at the time. Hopfield worked on hemoglobin’s structure for several years at Bell Labs, but he found his real calling in biology after being invited to a seminar series on neuroscience in Boston in the late 1970s. There he encountered a variegated group of clinicians and neuroscientists, gathered together to address the deep question of how the mind emerges from the brain. Hopfield was captivated. </p>
<p class="TXI">Mathematically minded as he was, though, Hopfield was dismayed by the qualitative approach to the brain he saw on display. He was concerned that, despite their obvious talents in biology, these researchers, ‘would never possibly solve the problem because the solution can be expressed only in an appropriate mathematical language and structure’.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> This was a language that physicists had. Hopfield therefore made a point of using his physicist’s skillset even as he embarked on a study of memory. In his eyes, certain physicists of the time who made the leap <a id="page_92"></a>to biology had immigrated fully, taking on the questions, culture and vocabulary of their new land. He wanted to firmly retain his citizenship as a physicist.</p>
<p class="TXI">In 1982 Hopfield published ‘Neural networks and physical systems with emergent collective computational abilities’, which laid out the description and results of what is now known as the Hopfield network. This was Hopfield’s first paper on the topic; he was only dipping his toe into the field of neuroscience and yet it made quite the splash.</p>
<p class="image-fig" id="fig8.jpg">
<img alt="" src="Images/chapter-04-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 8</span>
</span></p>
<p class="TXI">The Hopfield network (see Figure 8) is a mathematical model of neurons that can implement what Hopfield described as ‘content-addressable memory’. This term, coming from computer science, refers to the notion that a full memory can be retrieved from just a small component of it. The network that Hopfield designed for this task is simply composed. It is made only of binary neurons (like the McCulloch-Pitts neurons introduced in the last chapter), which can be either ‘on’ or ‘off’. It is therefore the interactions between these neurons from which the intriguing behaviours of this network emerge. </p>
<p class="TXI">The Hopfield network is <span class="italic">recurrent</span>, meaning that each neuron’s activity is determined by that of any of the <a id="page_93"></a>others in the network. Therefore, each neuron’s activity serves as both input and output to its neighbours. Specifically, each input a neuron receives from another neuron is multiplied by a particular number – a synaptic weight. These weighted inputs are then added together and compared to a threshold: if the sum is greater than (or equal to) the threshold, the neuron’s activity level is 1 (‘on’), otherwise it’s 0 (‘off’). This output then feeds into the input calculations of the other neurons in the network, whose outputs feed back into more input calculations and so on and so on.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p>
<p class="TXI">Like bodies in a mosh pit, the components of a recurrent system push and pull on each other, with the state of a unit at any given moment determined by those that surround it. The neurons in a Hopfield network are thus just like the atoms of iron constantly influencing each other through their magnetic interactions. The effects of this incessant interaction can be myriad and complex. To predict the patterns these interlocking parts will generate is essentially impossible without the precision of a mathematical model. Hopfield was intimately familiar with these models and their ability to show how local interactions lead to the emergence of global behaviour. </p>
<p class="TXI">Hopfield found that if the weights between the neurons in his network are just right the network as a <a id="page_94"></a>whole can implement associative memory. To understand this, we must first define what counts as a memory in this abstract model. Imagine that each neuron in a Hopfield network represents a single object: neuron A is a rocking chair, neuron B is a bike, neuron C is an elephant, and so on. To represent a particular memory, say that of your childhood bedroom, the neurons that represent all the objects in that room – the bed, your toys, photographs on the wall – should be ‘on’; while those that represent objects not in that room – the moon, a city bus, kitchen knives – should be ‘off’. The network as a whole is then in the ‘your childhood bedroom’ activity state. A different activity state – with different sets of neurons ‘on’ or ‘off’ – would represent a different memory. </p>
<p class="TXI">In associative memory, a small input to the network reactivates an entire memory state. For example, seeing a picture of yourself on your childhood bed may activate some of the neurons that represent your bedroom: the bed neurons and pillow neurons, and so on. In the Hopfield network, the connections between these neurons and the ones that represent other parts of the bedroom – the curtains and your toys and your desk – cause these other neurons to become active, recreating the full bedroom experience. Negatively weighted connections between the bedroom neurons and those that represent, say, a local park, ensure that the bedroom memory is not infiltrated by other items. That way you don’t end up remembering a swing set next to your closet. </p>
<p class="TXI">As some neurons turn on and others off, it is their interactivity that brings the full memory into stark relief. <a id="page_95"></a>The heavy-lifting of memory is thus done by the synapses. It is the strength of these connections that carries out the formidable yet delicate task of memory retrieval. </p>
<p class="TXI">In the language of physics, a fully retrieved memory is an example of an <span class="italic">attractor</span>. An attractor is, in short, a popular pattern of activity. It is one that other patterns of activity will evolve towards, just as water is pulled down a drain. A memory is an attractor because the activation of a few of the neurons that form the memory will drive the network to fill in the rest. Once a network is in an attractor state, it remains there with the neurons fixed in their ‘on’ or ‘off’ positions. Always fond of describing things in terms of energy, physicists consider attractors ‘low energy’ states. They’re a comfortable position for a system to be in; that is what makes them attractive and stable. </p>
<p class="TXI">Imagine a trampoline with a person standing on it. A ball placed anywhere on the trampoline will roll towards the person and stay there. The ball being in the divot created by the person is thus an attractor state for this system. If two people of the same size were standing opposite each other on the trampoline, the system would have two attractors. The ball would roll towards whomever it was initially closest to, but all roads would still lead to an attractor. Memory systems wouldn’t be of much use if they could only store one memory, so it is important that the Hopfield network can sustain multiple attractors. The same way the ball is compelled towards the nearest low point on the trampoline, initial neural activity states evolve towards the nearest, most similar memory (see Figure 9). The initial states that lead to a specific memory attractor – for example, the picture <a id="page_96"></a>of your childhood bed that reignites a memory of the whole room or a trip to a beach that ignites the memory of a childhood holiday – are said to be in that memory’s ‘basin of attraction’.</p>
<p class="TXI">
<span class="italic">The Pleasures of Memory</span> is a 1792 poem by Samuel Rogers. Reflecting on the universal journey on which memory can take the mind, he wrote:</p>
<p class="EXTF">
<span class="italic">Lulled in the countless chambers of the brain,</span></p>
<p class="EXTM">
<span class="italic">Our thoughts are linked by many a hidden chain.</span></p>
<p class="EXTM">
<span class="italic">Awake but one, and lo, what myriads rise!</span></p>
<p class="EXTL">
<span class="italic">Each stamps its image as the other flies!</span></p>
<p class="TXT">Rogers’ ‘hidden chain’ can be found in the pattern of weights that reignite a memory in the Hopfield network. <a id="page_97"></a>Indeed, the attractor model aligns with much of our intuition about memory. It implicitly addresses the time it takes for memories to be restored, as the network needs time to activate the right neurons. Attractors can also be slightly displaced in the network, creating memories that are mostly correct, with a detail or two changed. And memories that are too similar may simply merge into one. While collapsing memory to a series of zeros and ones may seem an affront to the richness of our experience of it, it is the condensation of this seemingly ineffable process that puts an understanding of it within reach.</p>
<p class="image-fig" id="fig9.jpg">
<img alt="" src="Images/chapter-04-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 9</span>
</span></p>
<p class="TXI">In the Hopfield network, how robustly neurons are connected with each other defines which patterns of neural activity form a memory. The engram is therefore in the weights – but how does it get there? How can an experience create just the right weights to make a memory? Hebb tells us that memories should come out of a strengthening of the connections between neurons that have similar activity – and in the Hopfield network that is just how it’s done.</p>
<p class="TXI">The Hopfield network encodes a set of memories through a simple procedure. For every experience in which two neurons are either both active or inactive, the connection between them is strengthened. In this way, the neurons that fire together come to be wired together. On the other hand, for every pattern where one neuron is active and the other is inactive, the connection is weakened.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> After this learning <a id="page_98"></a>procedure, neurons that are commonly co-active in memories will have a strong positive connection, those that have opposite activity patterns will have strong negative connections and others will fall somewhere in between. This is just the connectivity needed to form attractors.</p>
<p class="TXI">Attractors are not trivial phenomena. After all, if all the neurons in a network are constantly sending and receiving inputs, why should we assume their activity would ever settle into a memory state, let alone the <span class="italic">right</span> memory state? So, to be certain that the right attractors would form in these networks, Hopfield had to make a pretty strange assumption: weights in the Hopfield network are <span class="italic">symmetric</span>. That means the strength of the connection from neuron A to neuron B is always the same as the strength from B to A. Enforcing this rule offered a mathematical guarantee of attractors. The problem is that the odds of finding a population of neurons like this in the brain are dismal to say the least. It would require that each axon going out of one cell and forming a synapse with another be matched exactly by that same cell sending its axon back, connecting to the first cell with the same strength. Biology simply isn’t that clean.</p>
<p class="TXI">This illuminates the ever-present tension in the mathematical approach to biology. The physicist’s perspective, which depends on an almost irrational degree of simplification, is at constant odds with the biology, full as it is of messy, inconvenient details. In this case, the details of the maths demanded symmetric weights in order to make any definitive statement about attractors and thus to make progress on modelling the <a id="page_99"></a>process of memory. A biologist would likely have dismissed the assumption outright.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup></p>
<p class="TXI">Hopfield, with one foot on either side of the mathematics–biology divide, knew to appreciate the perspective of the neuroscientists. To ease their concerns, he showed in his original paper that – even though it couldn’t be guaranteed mathematically – networks that allowed asymmetric weights still seemed able to learn and sustain attractors relatively well. </p>
<p class="TXI">The Hopfield network thus offered a proof of concept that Hebb’s ideas about learning could actually work. Beyond that, it offered a chance to study memory mathematically – to quantify it. For example, precisely how many memories can a network hold? This is a question that can only be asked with a precise model of memory in mind. In the simplest version of the Hopfield network, the number of memories depends on the number of neurons in the network. A network with 1,000 neurons, for example, can store about 140 memories; 2,000 neurons can store 280; 10,000 can store 1,400 and so on. If the number of memories remains less than about 14 per cent the number of neurons, each memory will be restored with minimal error. Adding more memories, however, will be like the final addition to a house of cards that causes it to cave in. When pushed past its capacity, the Hopfield network collapses: inputs <a id="page_100"></a>go towards meaningless attractors and no memories are successfully recovered. It’s a phenomenon given the appropriately dramatic name ‘blackout catastrophe’.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup></p>
<p class="TXI">Precision cannot be evaded; once this estimate of memory capacity is found, it’s reasonable to start asking if it aligns with the number of memories we know to be stored by the brain. A landmark study in 1973 showed that people who had been shown more than 10,000 images (each only once and for only a brief period of time) were quite capable of recognising them later on. The 10 million neurons in the perirhinal cortex – a brain region implicated in visual memory – could store this amount of images, but it wouldn’t leave much space for anything else. Therefore, there seemed to be a problem with Hebbian learning.</p>
<p class="TXI">This problem becomes less problematic, however, when we realise that recognition is not recall. That is, a feeling of familiarity when seeing an image can happen without the ability to regenerate that image from scratch. The Hopfield network is remarkable for being capable of the latter, more difficult task – it fully completes a memory from a partial bit of it. But the former task is still important. Thanks to researchers working at the University of Bristol, it’s now known that recognition can also be performed by a network that uses Hebbian learning. These networks, when assessed on their ability to label an input as novel or familiar, have a significantly <a id="page_101"></a>higher capacity: 1,000 neurons can now recognise as many as 23,000 images. Just as Semon so presciently identified, this is an example of an issue that arises from relying on common language to parcel up the functions of the brain. What feels like simply ‘memory’ to us crumbles when pierced by the scrutiny of science and mathematics into a smattering of different skills.</p>
<p class="center">* * *</p>
<p class="TXT">When, in 1953, American doctor William Scoville removed the hippocampus from each side of 27-year-old Henry Molaison’s brain, he thought he was helping prevent Molaison’s seizures. What Scoville didn’t know was the incredible impact this procedure would have on the science of memory. Molaison (more famously known as ‘H. M.’ in scientific papers to hide his identity until his death in 2008) did find some relief from his seizures after the procedure, but he never formed another conscious memory again. Molaison’s subsequent and permanent amnesia initiated a course of research that centred the hippocampus – a curved finger-length structure deep in the brain – as a hub in the memory-formation system. Evading Lashley’s troubled search, this is a location that does play a special role in storing memories.</p>
<p class="TXI">Current theories of hippocampal function go as follows: information about the world first reaches the hippocampus at the dentate gyrus, a region that runs along the bottom edge of the hippocampus. Here, the representation is primed and prepped to be in a form more amenable to memory storage. The dentate gyrus then sends connections on to where attractors are <a id="page_102"></a>believed to form, an area called CA3; CA3 has extensive recurrent connections that make it a prime substrate for Hopfield network-like effects. This area then sends output to another region called CA1, which acts as a relay station; it sends the remembered information back to the rest of the brain (see Figure 10).</p>
<p class="image-fig" id="fig10.jpg">
<img alt="" src="Images/chapter-04-image-04.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 10</span>
</span></p>
<p class="TXI">What’s interesting about this final step – and what may have muddied Lashley’s original findings – is that these projections out to different areas of the brain are believed to facilitate the <span class="italic">copying</span> of memories. In this way, CA3 acts as a buffer, or warehouse, holding on to memories until they can be transferred to other brain areas. It does so by reactivating the memory in those areas. The hippocampus thus helps the rest of the brain memorise things using the same strategy you’d use to study for a test: repetition. By repeatedly reactivating the same group of neurons elsewhere in the brain, the hippocampus gives those neurons the chance to undergo Hebbian learning themselves. Eventually, their own weights have changed enough for the memory to be safely stored there.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> With <a id="page_103"></a>his hippocampus gone, Molaison had no warehouse for his experiences, no way to replay his memories back to his brain. </p>
<p class="TXI">With knowledge of this memory storehouse in the brain, researchers can look into how it works. Particularly, they can look for attractors in it.</p>
<p class="TXI">In 2005, scientists at University College London recorded the activity of hippocampal cells in rats. The rats got used to being in two different enclosures – a circular one and a square one. Their hippocampal neurons showed one pattern of activity when they were in the circle and a different pattern when they were in the square. The test for attractors came when an animal was placed into a new ‘squircle’ environment, the shape of which was somewhere in between a circle and a square. The researchers found that if the environment was more square-like the neural activity went to the pattern associated with the square environment; more circle-like and it went to that of the circle. Crucially, there were no intermediate representations in response to intermediate environments, only all circle or all square. This makes the memories of the circle and square environments attractors. An initial input that isn’t exactly one or the other is unstable; it gets inescapably driven towards the nearest established memory. </p>
<p class="TXI">The Hopfield network made manifest the theories of Hebb and showed how attractors – normally studied in physics – could explain the mysteries of memory. Yet Hopfield knew the limitations of bringing mathematics to real brains in real laboratories. He described his own model as a ‘mere parody of the complexities of neurobiology’. Indeed, as the creation of a physicist, it <a id="page_104"></a>lacks all the gooey richness of biology. But as a parody capable of powerful computations, it has offered many insights as well – insights that didn’t end with simple storage and recall. </p>
<p class="center">* * *</p>
<p class="TXT">You’re eating dinner in your kitchen when your roommate comes home. When you see them, you remember that last night you finished a book they had lent you and you want to return it before they leave for a trip the next day. So, you put down your food, head out of the kitchen and go down the hallway. You walk up the stairs, turn, enter your room and think: ‘Wait, what am I doing here?’ </p>
<p class="TXI">The sensation is a common one. So much so it’s been given a name: ‘destinesia’ or amnesia about why you’ve gone to where you are. It’s a failure of what’s called ‘working memory’, the ability to hold an idea in mind, even just for the 10 seconds it takes to walk from room to room. Working memory is crucial for just about all aspects of cognition: it’s hard to make a decision or work through a plan if you keep forgetting what you’re thinking about.</p>
<p class="TXI">Psychologists have been studying working memory for decades. The term itself was first coined in the 1960 book <span class="italic">Plans and the Structure of Behavior</span> written by George A. Miller and fellow scientists working at the Center for Advanced Study in the Behavioral Sciences in California. But the concept was explored well before that. Indeed, Miller himself wrote one of the most influential papers on the topic four years previously, in 1956. Perhaps <a id="page_105"></a>anticipating its fame, Miller gave the paper the cheeky title ‘The magical number seven, plus or minus two’. What that magical number refers to is the number of items humans can hold in their working memory at any one time. </p>
<p class="TXI">An example of how to assess this is to 1) show a participant several coloured squares on a screen; 2) ask them to wait for some time between several seconds to minutes; 3) then show them a second set of coloured squares. The task of the subject is to indicate if the colours of the second set are the same as the colours of the first. People can do well on this task if the number of squares shown remains small, achieving nearly 100 per cent accuracy if only one square is shown. Adding in more squares makes the performance drop and drop, until past seven, it’s almost no different to random guessing. Whether seven really is a special value when it comes to this kind of working memory capacity is up for debate; some studies find lower limits, some higher. However, there’s no doubt that Miller’s paper made an impact and psychologists have worked to characterise nearly every aspect of working memory since, from what can be held in it to how long it can last. </p>
<p class="TXI">But the question remains of how the brain actually does this: where are working memories stored and in what way? A tried-and-tested method for answering such questions – lesion experiments – pointed to the prefrontal cortex, a large part of the brain just behind the forehead. Whether it was humans with unfortunate injuries or laboratory animals with the area removed, it was clear that damaging the prefrontal cortex reduced working memory substantially. Without it, animals can <a id="page_106"></a>hardly hold on to an idea for more than a second or two. Thoughts and experiences pass through their minds like water through cupped hands. </p>
<p class="TXI">With an ‘X’ marking the spot, neuroscientists then began to dig. Dropping an electrode into the prefrontal cortex of monkeys, in 1971 researchers at the University of California in Los Angeles eavesdropped on the neurons there. The scientists, Joaquin Fuster and Garrett Alexander, did this while the animals performed a task similar to the colour memory test. These tests are known as ‘delayed response’ tasks because they include a delay period wherein the important information is absent from the screen and must thus be held in memory. The question was: what are neurons in the prefrontal cortex doing during this delay?</p>
<p class="TXI">Most of the brain areas responsible for vision have a stereotyped response to this kind of task: the neurons respond strongly when the patterns on the screen initially show up and then again when they reappear after the delay, but during the delay period – when no visual inputs are actually entering the brain – these areas are mostly quiet. Out of sight really does mean out of mind for these neurons. What Fuster and Alexander found, however, was that cells in the prefrontal cortex were different. The neurons there that responded to the visual patterns kept firing even after the patterns disappeared; that is, they maintained their activity during the delay period. A physical signature of working memory at work!</p>
<p class="TXI">Countless experiments since have replicated these findings, showing maintained activity during delay periods under many different circumstances, both in <a id="page_107"></a>the prefrontal cortex and beyond. Experiments have also hinted that when these firing patterns are out of whack, working memory goes awry. In some experiments, for example, applying a brief electrical stimulation during the delay period can disrupt the ongoing activity and this leads to a dip in performance on delayed response tasks. </p>
<p class="TXI">What is so special about these neurons that they can do this? Why can they hold on to information and maintain their firing for seconds to minutes, when other neurons let theirs go? For this kind of sustained output, neurons usually need sustained input. But if delay activity occurs without any external input from an image then that sustained input must come from neighbouring neurons. Thus, delay activity can only be generated by a network of neurons working together, the connections between them conspiring to keep the activity alive. This is where the idea of attractors comes back into play. </p>
<p class="TXI">So far we’ve looked at attractors in Hopfield networks, which show how input cues reignite a memory. It may not be clear how this helps with working memory. After all, working memory is all about what happens after that ignition; after you stand up to get your roommate’s book, how do you keep that goal in mind? As it turns out, however, an attractor is exactly what’s needed in this situation, because an attractor stays put.</p>
<p class="TXI">Attractors are defined by derivatives. If we know the inputs a neuron gets and the weights those inputs are multiplied by, we can write down an equation – a derivative – describing how the activity of that neuron will change over time as a result of those inputs. If this derivative is zero that means there is no change in the <a id="page_108"></a>activity of the neuron over time; it just keeps firing at the same, constant rate. Recall that, because this neuron is part of a recurrent network, it not only gets input but also serves as input to other neurons. So, its activity goes into calculating the derivative of a neighbouring neuron. If none of the inputs to this neighbouring neuron are changing – that is, their derivatives are all zero as well – then it too will have a zero derivative and will keep firing at the same rate. When a network is in an attractor state, the derivative of each and every neuron in that network is zero.</p>
<p class="TXI">And that is how, if the connections between neurons are just right, memories started at one point in time can last for much longer. All the cells can maintain their firing rate because all the cells around them are doing the same. Nothing changes if nothing changes. </p>
<p class="TXI">The problem is that things do change. When you leave the kitchen and walk to your bedroom, you encounter all kinds of things – your shoes in the hallway, the bathroom you meant to clean, the sight of rain on the window – that could cause changes in the input to the neurons that are trying to hold on to the memory. And those changes could push the neurons out of the attractor state representing the book and off to somewhere else entirely. For working memory to function, the network needs to be good at resisting the influence of such distractors. A run-of-the-mill attractor can resist distracting input to an extent. Recall the trampoline example. If the person standing on the trampoline gave a little nudge to the ball, it would likely roll just out of its divot and then back in. With only a small perturbation the memory stays intact, but give the <a id="page_109"></a>ball a heartier kick and who knows where it will end up? Good memory should be robust to such distractions – so what could make a network good at holding on to memories? </p>
<p class="TXI">The dance between data and theory is a complex one, with no clear lead or follow. Sometimes mathematical models are developed just to fit a certain dataset. Other times the details from data are absent or ignored and theorists do as their name suggests: theorise about how a system <span class="italic">could</span> work before knowing how it does. When it comes to building a robust network for working memory, scientists in the 1990s went in the latter direction. They came up with what’s known as the ‘ring network’, a hand-designed model of a neural circuit that would be ideal for the robust maintenance of working memories. </p>
<p class="TXI">Unlike Hopfield networks, ring networks are well described by their name: they are composed of several neurons arranged in a ring, with each neuron connecting only to those near to it. Like Hopfield networks, these models have attractor states – activity patterns that are self-sustaining and can represent memories. But the attractor states in a ring model are different to those in a Hopfield network. Attractors in a Hopfield model are <span class="italic">discrete</span>. This means that each attractor state – the one for your childhood bedroom, the one for your childhood holiday, the one for your current bedroom – is entirely isolated from the rest. There is no smooth way to transition between these different memories, regardless of how similar they are; you have to completely leave one attractor state to get to another. Attractors in a ring network, on the other hand, are <span class="italic">continuous</span>. With continuous attractors, transitioning between similar <a id="page_110"></a>memories is easy. Rather than being thought of as a trampoline with people standing at different points, models with continuous attractor states are more like the gutter of a bowling lane: once the ball gets into the gutter it can’t easily get out, but it can move smoothly within it.</p>
<p class="TXI">Networks with continuous attractor states like the ring model are helpful for a variety of reasons and chief among them is the type of errors they make. It may seem silly to praise a memory system for its errors – wouldn’t we prefer no errors at all? – but if we assume that no network can have perfect memory, then the quality of the errors becomes very important. A ring network allows for small, sensible errors. </p>
<p class="TXI">Consider the example of the working memory test where subjects had to keep in mind the colour of shapes on a screen. Colours map well to ring networks because, as you’ll recall from art class, colours lie on a wheel. So, imagine a network of neurons arranged in a ring, with each neuron representing a slightly different colour. At one side of the ring are red-representing neurons, next to them are orange, then yellow and green; this brings us to the side opposite the red, where there are the blue-representing neurons, which lead to the violet ones and back to red. </p>
<p class="TXI">In this task, when a shape is seen, it creates activity in the neurons that represent its colour, while the other neurons remain silent. This creates a little ‘bump’ of activity on the ring, centred on the remembered colour. If any distracting input comes in while the subject tries to hold on to this colour memory – from other random sights in the room, for example – it may push or pull the activity bump away from the desired colour. But – and this is the crucial <a id="page_111"></a>point – it will only be able to push it to a very nearby place on the ring. So red may become red-orange or green may become teal. But the memory of red would be very unlikely to become green. Or, for that matter, to become no colour at all; that is, there will always be a bump <span class="italic">somewhere</span> on the ring. These properties are all a direct result of the gutter-like nature of a continuous attractor – it has low resistance for moving between nearby states, but high resistance to perturbations otherwise.</p>
<p class="TXI">Another benefit of the ring network is that it can be used to do things. The ‘working’ in working memory is meant to counter the notion that memory is just about passively maintaining information. Rather, holding ideas in working memory lets us combine them with other information and come to new conclusions. An excellent example of this is the head direction system in rats, which also served as the inspiration for early ring network models. </p>
<p class="TXI">Rats (along with many other animals) have an internal compass: a set of neurons that keep track of the direction the animal is facing at all times. If the animal turns to face a new direction, the activity of these cells changes to reflect that change. Even if the rat sits still in a silent darkened room, these neurons continue to fire, holding on to the information about its direction. In 1995, a team from Bruce McNaughton’s lab at the University of Arizona and, separately, Kechen Zhang of the University of California, San Diego, posited that this set of cells could be well described by a ring network. Direction being one of those concepts that maps well to a circle, a bump of activity on the ring would be used to store the direction the animal was facing (See Figure 11).</p> <a id="page_112"></a>
<p class="TXI">But not only could a ring network explain how knowledge of head direction was maintained over time, it also served as a model of how the stored direction could change when the animal did. Head direction cells receive input from other neurons, such as those from the visual system and the vestibular system (which keeps track of bodily motion). If these inputs are hooked up to the ring network just right, they can push the bump of activity along to a new place on the ring. If the vestibular system says the body is now moving leftwards, for example, the bump gets pushed to the left. In this way, movement along the ring doesn’t create errors in memory, but rather updates the memory based on new information. ‘Working’ memory earns its name.</p>
<p class="image-fig" id="fig11.jpg">
<img alt="" src="Images/chapter-04-image-05.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 11</span>
</span></p>
<p class="TXI">Ring networks are a lovely solution to the complex problem of how to create robust and functional working memory systems. They are also beautiful mathematical objects. They display the desirable properties of simplicity and symmetry. They’re precise and finely tuned, elegant even.</p> <a id="page_113"></a>
<p class="TXI">As such, they are completely unrealistic. Because to the biologist, of course, ‘finely tuned’ are dirty words. Anything that requires delicate planning and pristine conditions to operate well won’t survive the chaos that is brain development and activity. Many of the desirable properties of ring networks only occur under very particular assumptions about the connectivity between neurons, assumptions that just don’t seem very realistic. So, despite all their desirable theoretical properties and useful abilities, the chances of seeing a ring network in the brain seemed slim. </p>
<p class="TXI">The discovery made in a research centre just outside Washington DC in 2015 was therefore all the more exciting. </p>
<p class="TXI">Janelia Research Campus is a world-class research facility hidden away in the idyllic former farmland of Ashburn, Virginia. Vivek Jayaraman has been at Janelia since 2006. He and his team of about half a dozen people work to understand navigation in <span class="italic">Drosophila melanogaster</span>, a species of fruit fly commonly studied in neuroscience. On a par with a grain of rice, the size of these animals is both a blessing and a curse. While they can be difficult to get hold of, these tiny flies only have around 135,000 neurons, roughly 0.2 per cent the amount of another popular lab animal, the mouse. On top of that, a lot is known about these neurons. Many of them are easily categorised based on the genes they express, and their numbers and locations are very similar across individuals. </p>
<p class="TXI">Like rodents, flies also have a system for keeping track of head direction. For the fly, these head direction neurons are located in a region known as the ellipsoid <a id="page_114"></a>body. The ellipsoid body is centrally placed in the fly brain and it has a unique shape: it has a hole in the middle with cells arranged all around that hole, forming a doughnut made of neurons – or in other words, a ring.</p>
<p class="TXI">Neurons arranged in a ring, however, do not necessarily make a ring network. So, what the Jayaraman lab set out to do was investigate whether this group of neurons that <span class="italic">looked</span> like a ring network, actually <span class="italic">behaved</span> like one. To do this, they put a special dye in the ellipsoid body neurons, one that makes them light up green when they’re active. They then had the fly walk around, while they filmed the neurons. If you were to look at these neurons on a screen, as the fly heads forwards, you’d see a flickering of little green points at one location on an otherwise black screen. Should the fly choose to make a turn, that flickering patch would swing around to a new location. Over time, as the fly moves and the green patch on the screen moves with it, the points that have lit up form a clear ring structure, matching the underlying shape of the ellipsoid body. If you turn the lights off in the room so the fly has no ability to see which way it’s facing, the green flicker still remains at the same location on that ring – a clear sign that the memory of heading direction is being maintained. </p>
<p class="TXI">In addition to observing the activity on the ring, the experimenters also manipulated it in order to probe the extremes of its behaviour. A true ring network can only support one ‘bump’ of activity; that is, only neurons at one location on the ring can be active at a given time. So, the researchers artificially stimulated neurons on the side of the ring opposite to those already active. This strong stimulation of the opposing neurons caused the original bump to shut down, and the bump at the new location was maintained, even after the stimulation was <a id="page_115"></a>turned off. Through these experiments, it became clear that the ellipsoid body was no imposter, but a vivid example of a theory come to life. </p>
<p class="TXI">This finding – a ring network in the literal, visible shape of a ring – feels a bit like nature winking at us. William Skaggs and the other authors of one of the original papers proposing the ring network explicitly doubted the possibility of such a finding: ‘For expository purposes it is helpful to think of the network as a set of circular layers; this does not reflect the anatomical organisation of the corresponding cells in the brain.’ Most theorists working on ring network models assumed that they’d be embedded in some larger, messier network of neurons. And that is bound to be the case for most systems in most species. This anomalously pristine example likely arises from a very precisely controlled genetic programme. Others will be much harder to spot. </p>
<p class="TXI">Even if we usually can’t see them directly, we can make predictions about behaviours we’d expect to see if the brain is using continuous attractors. In 1991, pioneering working memory researcher Patricia Goldman-Rakic found that blocking the function of the neuromodulator dopamine made it harder for monkeys to remember the location of items. Dopamine is known to alter the flow of ions into and out of a cell. In 2000, researchers at the Salk Institute in California showed how mimicking the presence of dopamine in a model with a continuous attractor enhanced the model’s memory.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup> It stabilised the activity of the neurons encoding the memory, making <a id="page_116"></a>them more resistant to irrelevant inputs. Because dopamine is associated with reward,<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> this model also predicts that under conditions where a person is anticipating a large reward their working memory will be better – and that is exactly what has been found. When people are promised more reward in turn for remembering something, their working memory is better. Here, the concept of an attractor works as the thread that stitches chemical changes together with cognitive ones. It links ions to experiences. </p>
<p class="TXI">Attractors are omnipresent in the physical world. They arise from local interactions between the parts of a system. Whether those parts are atoms in a metal, planets in a solar system or even people in a community, they will be compelled towards an attractor state and, without major disruptions, will stay there. Applying these concepts to the neurons that form a memory connects dots across biology and psychology. On one side, Hopfield networks link the formation and retrieval of memories to the way in which connections between neurons change. On the other, structures like ring networks underlie how ideas are held in the mind. In one simple framework we capture how memories are recorded, retained and reactivated.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-1" id="fn-1">1</a> ﻿Jerzy Konorski, a Polish neurophysiologist, published a book with very similar ideas the year before Hebb did. In fact, Konorski anticipated several important findings in neuroscience and psychology. However, the global East﻿–﻿West divide at the time isolated his contributions. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-2" id="fn-2">2</a> ﻿When Hopfield wrote on an undergraduate admission form that he intended to study ﻿‘﻿physics or chemistry﻿’﻿, his university advisor ﻿–﻿ a colleague of his father﻿’﻿s ﻿–﻿ crossed out the latter option saying, 
﻿‘﻿I don﻿’﻿t believe we need to consider chemistry.﻿’﻿ ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-3" id="fn-3">3</a> ﻿Hopfield﻿’﻿s attitude was not unique. The 1980s found many physicists, bored with their own field, looking at the brain and thinking, ﻿‘﻿I could solve that.﻿’﻿ After Hopfield﻿’﻿s success, this population increased even more.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-4" id="fn-4">4</a> ﻿While the calculation of an individual neuron﻿’﻿s activity in terms of inputs and weights is the same as described for the perceptron in the last chapter, the perceptron is a ﻿<span class="italic">feedforward</span>﻿ (not recurrent) network. Recurrence means that the connections can form loops: neuron A connects to neuron B, which connects back to neuron A, for example. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-5" id="fn-5">5</a> ﻿This second part ﻿–﻿ the idea that connection strength should ﻿<span class="italic">decrease</span>﻿ if a pre-synaptic neuron is highly active while the post-synaptic neuron remains quiet ﻿–﻿ was not part of Hebb﻿’﻿s original sketch, but it has since been borne out by experiments.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-6" id="fn-6">6</a> ﻿In fact, when Hopfield presented an early version of this work to a group of neuroscientists, one attendee commented that ﻿‘﻿it was a beautiful talk but unfortunately had nothing to do with neurobiology﻿’﻿. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-7" id="fn-7">7</a> ﻿You may know some people with stories of their own ﻿‘﻿blackout catastrophe﻿’﻿ after a night of drinking. However, the exact type of memory failure seen in Hopfield networks is not actually believed to occur in humans.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-8" id="fn-8">8</a> ﻿This process is believed to occur while you sleep.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-9" id="fn-9">9</a> ﻿This model was composed of the Hodgkin-Huxley style of neurons described in ﻿﻿Chapter 2﻿﻿, which makes incorporating dopamine﻿’﻿s effects on ion flows easy.﻿</p>
<p class="FN2"><a href="chapter4.xhtml#fnt-10" id="fn-10">10</a> ﻿Lots more on this in ﻿﻿Chapter 11﻿﻿!﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 4</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter4"><a href="contents.xhtml#re_chapter4">CHAPTER FOUR</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter4">Making and Maintaining Memories</a><a id="page_83"></a></p>
<p class="H1" id="b-9781472966445-ch354-sec4">
<span class="bold">
<span>The Hopfield network and attractors</span>
</span></p>
<p class="TXT">A block of iron at 770°C (1,418°F) is a sturdy grey mesh. Each of its trillions of atoms serves as a single brick in the endless parallel walls and ceilings of its crystalline structure. It is a paragon of orderliness. In opposition to their organised structural arrangement, however, the magnetic arrangement of these atoms is a mess. </p>
<p class="TXI">Each iron atom forms a dipole – a miniature magnet with one positive and one negative end. Heat unsteadies these atoms, flipping the direction of their poles around at random. On the micro-level this means many tiny magnets each exerting a force in its own direction. But as these forces work against each other, their net effect becomes negligible. When you zoom out, this mass of mini-magnets has no magnetism at all.</p>
<p class="TXI">As the temperature dips below 770°C, however, something changes. The direction of an individual atom is less likely to switch. With its dipole set in place, the atom starts to exert a constant pressure on its neighbours. This indicates to them which direction they too should be facing. Atoms with different directions vie for influence over the local group until eventually everyone falls in line, one way or the other. With all the small dipoles aligned, the net force is strong. The previously inert block of iron becomes a powerful magnet.</p> <a id="page_84"></a>
<p class="TXI">Philip Warren Anderson, an American physicist who won a Nobel Prize working on such phenomena, wrote in a now-famous essay entitled ‘More is different’ that ‘the behaviour of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles’. That is, the collective action of many small particles – organised only through their local interactions – can produce a function not directly possible in any of them alone. Physicists have formalised these interactions as equations and successfully used them to explain the behaviour of metals, gases and ice. </p>
<p class="TXI">In the late 1970s, a colleague of Anderson’s, John J. Hopfield, saw in these mathematical models of magnetism a structure akin to that of the brain. Hopfield used this insight to bring under mathematical control a long-lasting mystery: the question of how neurons make and maintain memories.</p>
<p class="center">* * *</p>
<p class="TXT">Richard Semon was wrong.</p>
<p class="TXI">A German biologist working at the turn of the twentieth century, Semon wrote two lengthy books on the science of memory. They were filled with detailed descriptions of experimental results, theories and a vocabulary for describing memory’s impact on ‘organic tissue’. Semon’s work was insightful, honest and clear – but it contained a major flaw. Just as French naturalist Jean-Baptiste Lamarck believed (in contrast to our current understanding of evolution) that traits acquired by an animal in its lifetime could be passed to its offspring, <a id="page_85"></a>Semon proposed that <span class="italic">memories</span> acquired by an animal could be passed down. That is, he believed that an organism’s learned responses to its own environment would arise without instruction in its offspring. As a result of this mistaken intuition, much of Semon’s otherwise valuable work was slowly cast aside and forgotten.</p>
<p class="TXI">Being wrong about memory isn’t unusual. Philosopher René Descartes, for example, thought memories were activated by a small gland directing the flow of ‘animal spirits’. What’s unique about Semon is that, despite the flaw in his work that sentenced him to historical obscurity, one of his contributions remained influential long enough to spawn an entire body of research. This small artefact of his efforts is the ‘engram’ – a word coined by Semon in <span class="italic">The</span>
<span class="italic">Mneme</span> in 1904, and subsequently learned by millions of students of psychology and neuroscience.</p>
<p class="TXI">At the time Semon was writing, memory had only recently come under scientific scrutiny – and most of the results were purely about memorisation skills, not about biology. For example, people would be trained to memorise pairs of nonsense words (such as ‘wsp’ and ‘niq’) and then were tested on their ability to retrieve the second word when prompted with the first. This type of memory, known as <span class="italic">associative</span> memory, would become a target of research for decades to come. But Semon was interested in more than just behaviour; he wanted to know what changes in an animal’s physiology could support such associative memories.</p>
<p class="TXI">Led by scant experimental data, he broke up the process of creating and recovering memories into multiple components. Finding common words too <a id="page_86"></a>vague and overloaded, he created novel terms for these divisions of labour. The word that would become so influential, the engram, was defined as ‘the enduring though primarily latent modification in the irritable substance produced by a stimulus’. Or, to put it more plainly: the physical changes in the brain that happen when a memory is formed. Another term, ‘ecphory’, was assigned to the ‘influences which awake the mnemic trace or engram out of its latent state into one of manifested activity’. This distinction between engram and ecphory (or between the processes that lay a memory and those that retrieve it) was one of the many conceptual advances that Semon’s work provided. Despite the fact that his name and most of his language have disappeared from the literature, many of Semon’s conceptual insights were correct and they form the core of how memories are modelled today.</p>
<p class="TXI">In 1950, American psychologist Karl Lashley published ‘In search of the engram’, a paper that solidified the legacy of the word. It also set a rather dismal tone for the field. The paper was so titled because the search was all Lashley felt he had accomplished in 30 years of experiments. Lashley’s experiments involved training animals to make an association (for example, to react in a specific way when shown a circle versus an ‘X’) or learn a task, such as how to run through a particular maze. He would then surgically remove specific brain areas or connection pathways and observe how behaviour was impacted post-operatively. Lashley couldn’t find any area or pattern of lesions that reliably interfered with memory. He concluded that memories must thus somehow be distributed equally across the <a id="page_87"></a>brain, rather than in any single area. But based on some calculations about how many neurons could be used for a memory and the number of pathways between them, he was uncertain about how this was possible. His landmark article thus reads as something of a white flag, a surrendering of any attempt to draw conclusions about the location of memory in the face of a mass of inconsistent data. The physical nature of memory remained to Lashley as vexing as ever.</p>
<p class="TXI">At the same time, however, a former student of Lashley’s was developing his own theories on learning and memory. </p>
<p class="TXI">Donald Hebb, a Canadian psychologist whose early work as a school teacher grew his interest in the mind, was intent on making psychology a biological science. In his 1949 book, <span class="italic">The Organization of Behavior</span>, he describes the task of a psychologist as ‘reducing the vagaries of human thought to a mechanical process of cause and effect’. And in that book, he lays down the mechanical process he believed to be behind memory formation.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> Overcoming the limited, and sometimes misleading, physiological data available at the time, Hebb came to this principle about the physical underpinnings of learning largely through intuition. Yet it would go on to have huge empirical success. The principle, now known as Hebbian learning, is succinctly described by the phrase ‘neurons that fire together wire together’.</p> <a id="page_88"></a>
<p class="TXI">Hebbian learning describes what happens at the small junction between two neurons where one can send a signal to the other, a space called the synapse. Suppose there are two neurons, A and B. The axon from neuron A makes a synaptic connection on to the dendrite or cell body of neuron B (making it the ‘pre-synaptic’ neuron, and neuron B the ‘post-synaptic’ neuron, see Figure 7). In Hebbian learning, if neuron A repeatedly fires before neuron B, the connection from A to B will strengthen. A stronger connection means that the next time A fires it will be more effective in causing B to fire. In this way, activity determines connectivity and connectivity determines activity.</p>
<p class="image-fig" id="fig7.jpg">
<img alt="" src="Images/chapter-04-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 7</span>
</span></p>
<p class="TXI">Hebb’s approach, with its focus on the synapse, situates the engram as both local and global: local because a memory’s imprint occurs at the small gap where one neuron meets another, but global because these changes may be happening at synapses all across the brain. It also makes memory the natural consequence of experience: with pliable synapses, any activation of the brain has the potential to leave a trace. </p>
<p class="TXI">Lashley, a dutiful scientist intent on following the facts, accepted that the engram must be distributed based on his own experiments. But he found no satisfaction in <a id="page_89"></a>Hebb’s solution, which – though an enticing and elegant theory – was based more on speculation than on hard evidence. He turned down Hebb’s offer to be a co-author on the work.</p>
<p class="TXI">Lashley may not have supported Hebb’s ideas, but since the publication of his book countless experiments have. Sea slugs – foot-long slimy brown invertebrates with only about 20,000 neurons – became a creature of much study in this area, due to their ability to learn a very basic association. These shell-less slugs have a gill on their backs that, if threatened, can be quickly retracted for safe keeping. In the lab, a short electric shock will cause the gill to withdraw. If such a shock is repeatedly preceded by a harmless light touch, the slug will eventually start withdrawing in response to the touch alone, demonstrating an association between the touch and what’s expected to come next. It is the marine critter equivalent of learning to pair ‘wsp’ with ‘niq’. This association was shown, in line with Hebb’s theory of learning, to be mediated by a strengthening of the connections between the neurons that represent the touch and those that lead to the gill’s response. The change in behaviour was forged through a changing of connections. </p>
<p class="TXI">Hebbian learning has not just been observed; it’s been controlled as well. In 1999, Princeton researchers showed that genetically modifying proteins in the cell membrane that contribute to synaptic changes can control a mouse’s capacity for learning. Increasing the function of these receptors enhances the ability of mice to remember objects they’ve been shown before. Interfering with these proteins impairs it.</p> <a id="page_90"></a>
<p class="TXI">It is now established science that experience leads to the activation of neurons and that activating neurons can alter the connections between them. This story is accepted as at least a partial answer to the question of the engram. But, as Semon describes it, the engram itself is only part of the story of memory. Memory also requires <span class="italic">re</span>membering. How can this way of depositing memories allow for long-term storage and recall? </p>
<p class="center">* * *</p>
<p class="TXT">It was no real surprise that John J. Hopfield became a physicist. Born in 1933 to John Hopfield Sr, a man who made a name for himself in ultraviolet spectroscopy, and Helen Hopfield, who studied atmospheric electromagnetic radiation, Hopfield Jr grew up in a household where physics was as much a philosophy as it was a science. ‘Physics was a point of view that the world around us is, with effort, ingenuity and adequate resources, understandable in a predictive and reasonably quantitative fashion,’ Hopfield wrote in an autobiography. ‘Being a physicist is a dedication to a quest for this kind of understanding.’ And a physicist is what he would be.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> </p>
<p class="TXI">Hopfield, a tall and lanky man with an engaging smile, earned his PhD in 1958 from Cornell University. He further emulated his father by receiving a Guggenheim fellowship, using it to study at the Cavendish Laboratory <a id="page_91"></a>at Cambridge. But even by this stage, Hopfield’s enthusiasm for the subject of his PhD – condensed matter physics – was waning. ‘In 1968, I had run out of problems … to which my particular talents seemed useful,’ he later wrote.</p>
<p class="TXI">Hopfield’s gateway from physics to biology was hemoglobin, a molecule that both serves a crucial biological function as the carrier of oxygen in blood and could be studied with many of the techniques of experimental physics at the time. Hopfield worked on hemoglobin’s structure for several years at Bell Labs, but he found his real calling in biology after being invited to a seminar series on neuroscience in Boston in the late 1970s. There he encountered a variegated group of clinicians and neuroscientists, gathered together to address the deep question of how the mind emerges from the brain. Hopfield was captivated. </p>
<p class="TXI">Mathematically minded as he was, though, Hopfield was dismayed by the qualitative approach to the brain he saw on display. He was concerned that, despite their obvious talents in biology, these researchers, ‘would never possibly solve the problem because the solution can be expressed only in an appropriate mathematical language and structure’.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> This was a language that physicists had. Hopfield therefore made a point of using his physicist’s skillset even as he embarked on a study of memory. In his eyes, certain physicists of the time who made the leap <a id="page_92"></a>to biology had immigrated fully, taking on the questions, culture and vocabulary of their new land. He wanted to firmly retain his citizenship as a physicist.</p>
<p class="TXI">In 1982 Hopfield published ‘Neural networks and physical systems with emergent collective computational abilities’, which laid out the description and results of what is now known as the Hopfield network. This was Hopfield’s first paper on the topic; he was only dipping his toe into the field of neuroscience and yet it made quite the splash.</p>
<p class="image-fig" id="fig8.jpg">
<img alt="" src="Images/chapter-04-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 8</span>
</span></p>
<p class="TXI">The Hopfield network (see Figure 8) is a mathematical model of neurons that can implement what Hopfield described as ‘content-addressable memory’. This term, coming from computer science, refers to the notion that a full memory can be retrieved from just a small component of it. The network that Hopfield designed for this task is simply composed. It is made only of binary neurons (like the McCulloch-Pitts neurons introduced in the last chapter), which can be either ‘on’ or ‘off’. It is therefore the interactions between these neurons from which the intriguing behaviours of this network emerge. </p>
<p class="TXI">The Hopfield network is <span class="italic">recurrent</span>, meaning that each neuron’s activity is determined by that of any of the <a id="page_93"></a>others in the network. Therefore, each neuron’s activity serves as both input and output to its neighbours. Specifically, each input a neuron receives from another neuron is multiplied by a particular number – a synaptic weight. These weighted inputs are then added together and compared to a threshold: if the sum is greater than (or equal to) the threshold, the neuron’s activity level is 1 (‘on’), otherwise it’s 0 (‘off’). This output then feeds into the input calculations of the other neurons in the network, whose outputs feed back into more input calculations and so on and so on.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p>
<p class="TXI">Like bodies in a mosh pit, the components of a recurrent system push and pull on each other, with the state of a unit at any given moment determined by those that surround it. The neurons in a Hopfield network are thus just like the atoms of iron constantly influencing each other through their magnetic interactions. The effects of this incessant interaction can be myriad and complex. To predict the patterns these interlocking parts will generate is essentially impossible without the precision of a mathematical model. Hopfield was intimately familiar with these models and their ability to show how local interactions lead to the emergence of global behaviour. </p>
<p class="TXI">Hopfield found that if the weights between the neurons in his network are just right the network as a <a id="page_94"></a>whole can implement associative memory. To understand this, we must first define what counts as a memory in this abstract model. Imagine that each neuron in a Hopfield network represents a single object: neuron A is a rocking chair, neuron B is a bike, neuron C is an elephant, and so on. To represent a particular memory, say that of your childhood bedroom, the neurons that represent all the objects in that room – the bed, your toys, photographs on the wall – should be ‘on’; while those that represent objects not in that room – the moon, a city bus, kitchen knives – should be ‘off’. The network as a whole is then in the ‘your childhood bedroom’ activity state. A different activity state – with different sets of neurons ‘on’ or ‘off’ – would represent a different memory. </p>
<p class="TXI">In associative memory, a small input to the network reactivates an entire memory state. For example, seeing a picture of yourself on your childhood bed may activate some of the neurons that represent your bedroom: the bed neurons and pillow neurons, and so on. In the Hopfield network, the connections between these neurons and the ones that represent other parts of the bedroom – the curtains and your toys and your desk – cause these other neurons to become active, recreating the full bedroom experience. Negatively weighted connections between the bedroom neurons and those that represent, say, a local park, ensure that the bedroom memory is not infiltrated by other items. That way you don’t end up remembering a swing set next to your closet. </p>
<p class="TXI">As some neurons turn on and others off, it is their interactivity that brings the full memory into stark relief. <a id="page_95"></a>The heavy-lifting of memory is thus done by the synapses. It is the strength of these connections that carries out the formidable yet delicate task of memory retrieval. </p>
<p class="TXI">In the language of physics, a fully retrieved memory is an example of an <span class="italic">attractor</span>. An attractor is, in short, a popular pattern of activity. It is one that other patterns of activity will evolve towards, just as water is pulled down a drain. A memory is an attractor because the activation of a few of the neurons that form the memory will drive the network to fill in the rest. Once a network is in an attractor state, it remains there with the neurons fixed in their ‘on’ or ‘off’ positions. Always fond of describing things in terms of energy, physicists consider attractors ‘low energy’ states. They’re a comfortable position for a system to be in; that is what makes them attractive and stable. </p>
<p class="TXI">Imagine a trampoline with a person standing on it. A ball placed anywhere on the trampoline will roll towards the person and stay there. The ball being in the divot created by the person is thus an attractor state for this system. If two people of the same size were standing opposite each other on the trampoline, the system would have two attractors. The ball would roll towards whomever it was initially closest to, but all roads would still lead to an attractor. Memory systems wouldn’t be of much use if they could only store one memory, so it is important that the Hopfield network can sustain multiple attractors. The same way the ball is compelled towards the nearest low point on the trampoline, initial neural activity states evolve towards the nearest, most similar memory (see Figure 9). The initial states that lead to a specific memory attractor – for example, the picture <a id="page_96"></a>of your childhood bed that reignites a memory of the whole room or a trip to a beach that ignites the memory of a childhood holiday – are said to be in that memory’s ‘basin of attraction’.</p>
<p class="TXI">
<span class="italic">The Pleasures of Memory</span> is a 1792 poem by Samuel Rogers. Reflecting on the universal journey on which memory can take the mind, he wrote:</p>
<p class="EXTF">
<span class="italic">Lulled in the countless chambers of the brain,</span></p>
<p class="EXTM">
<span class="italic">Our thoughts are linked by many a hidden chain.</span></p>
<p class="EXTM">
<span class="italic">Awake but one, and lo, what myriads rise!</span></p>
<p class="EXTL">
<span class="italic">Each stamps its image as the other flies!</span></p>
<p class="TXT">Rogers’ ‘hidden chain’ can be found in the pattern of weights that reignite a memory in the Hopfield network. <a id="page_97"></a>Indeed, the attractor model aligns with much of our intuition about memory. It implicitly addresses the time it takes for memories to be restored, as the network needs time to activate the right neurons. Attractors can also be slightly displaced in the network, creating memories that are mostly correct, with a detail or two changed. And memories that are too similar may simply merge into one. While collapsing memory to a series of zeros and ones may seem an affront to the richness of our experience of it, it is the condensation of this seemingly ineffable process that puts an understanding of it within reach.</p>
<p class="image-fig" id="fig9.jpg">
<img alt="" src="Images/chapter-04-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 9</span>
</span></p>
<p class="TXI">In the Hopfield network, how robustly neurons are connected with each other defines which patterns of neural activity form a memory. The engram is therefore in the weights – but how does it get there? How can an experience create just the right weights to make a memory? Hebb tells us that memories should come out of a strengthening of the connections between neurons that have similar activity – and in the Hopfield network that is just how it’s done.</p>
<p class="TXI">The Hopfield network encodes a set of memories through a simple procedure. For every experience in which two neurons are either both active or inactive, the connection between them is strengthened. In this way, the neurons that fire together come to be wired together. On the other hand, for every pattern where one neuron is active and the other is inactive, the connection is weakened.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> After this learning <a id="page_98"></a>procedure, neurons that are commonly co-active in memories will have a strong positive connection, those that have opposite activity patterns will have strong negative connections and others will fall somewhere in between. This is just the connectivity needed to form attractors.</p>
<p class="TXI">Attractors are not trivial phenomena. After all, if all the neurons in a network are constantly sending and receiving inputs, why should we assume their activity would ever settle into a memory state, let alone the <span class="italic">right</span> memory state? So, to be certain that the right attractors would form in these networks, Hopfield had to make a pretty strange assumption: weights in the Hopfield network are <span class="italic">symmetric</span>. That means the strength of the connection from neuron A to neuron B is always the same as the strength from B to A. Enforcing this rule offered a mathematical guarantee of attractors. The problem is that the odds of finding a population of neurons like this in the brain are dismal to say the least. It would require that each axon going out of one cell and forming a synapse with another be matched exactly by that same cell sending its axon back, connecting to the first cell with the same strength. Biology simply isn’t that clean.</p>
<p class="TXI">This illuminates the ever-present tension in the mathematical approach to biology. The physicist’s perspective, which depends on an almost irrational degree of simplification, is at constant odds with the biology, full as it is of messy, inconvenient details. In this case, the details of the maths demanded symmetric weights in order to make any definitive statement about attractors and thus to make progress on modelling the <a id="page_99"></a>process of memory. A biologist would likely have dismissed the assumption outright.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup></p>
<p class="TXI">Hopfield, with one foot on either side of the mathematics–biology divide, knew to appreciate the perspective of the neuroscientists. To ease their concerns, he showed in his original paper that – even though it couldn’t be guaranteed mathematically – networks that allowed asymmetric weights still seemed able to learn and sustain attractors relatively well. </p>
<p class="TXI">The Hopfield network thus offered a proof of concept that Hebb’s ideas about learning could actually work. Beyond that, it offered a chance to study memory mathematically – to quantify it. For example, precisely how many memories can a network hold? This is a question that can only be asked with a precise model of memory in mind. In the simplest version of the Hopfield network, the number of memories depends on the number of neurons in the network. A network with 1,000 neurons, for example, can store about 140 memories; 2,000 neurons can store 280; 10,000 can store 1,400 and so on. If the number of memories remains less than about 14 per cent the number of neurons, each memory will be restored with minimal error. Adding more memories, however, will be like the final addition to a house of cards that causes it to cave in. When pushed past its capacity, the Hopfield network collapses: inputs <a id="page_100"></a>go towards meaningless attractors and no memories are successfully recovered. It’s a phenomenon given the appropriately dramatic name ‘blackout catastrophe’.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup></p>
<p class="TXI">Precision cannot be evaded; once this estimate of memory capacity is found, it’s reasonable to start asking if it aligns with the number of memories we know to be stored by the brain. A landmark study in 1973 showed that people who had been shown more than 10,000 images (each only once and for only a brief period of time) were quite capable of recognising them later on. The 10 million neurons in the perirhinal cortex – a brain region implicated in visual memory – could store this amount of images, but it wouldn’t leave much space for anything else. Therefore, there seemed to be a problem with Hebbian learning.</p>
<p class="TXI">This problem becomes less problematic, however, when we realise that recognition is not recall. That is, a feeling of familiarity when seeing an image can happen without the ability to regenerate that image from scratch. The Hopfield network is remarkable for being capable of the latter, more difficult task – it fully completes a memory from a partial bit of it. But the former task is still important. Thanks to researchers working at the University of Bristol, it’s now known that recognition can also be performed by a network that uses Hebbian learning. These networks, when assessed on their ability to label an input as novel or familiar, have a significantly <a id="page_101"></a>higher capacity: 1,000 neurons can now recognise as many as 23,000 images. Just as Semon so presciently identified, this is an example of an issue that arises from relying on common language to parcel up the functions of the brain. What feels like simply ‘memory’ to us crumbles when pierced by the scrutiny of science and mathematics into a smattering of different skills.</p>
<p class="center">* * *</p>
<p class="TXT">When, in 1953, American doctor William Scoville removed the hippocampus from each side of 27-year-old Henry Molaison’s brain, he thought he was helping prevent Molaison’s seizures. What Scoville didn’t know was the incredible impact this procedure would have on the science of memory. Molaison (more famously known as ‘H. M.’ in scientific papers to hide his identity until his death in 2008) did find some relief from his seizures after the procedure, but he never formed another conscious memory again. Molaison’s subsequent and permanent amnesia initiated a course of research that centred the hippocampus – a curved finger-length structure deep in the brain – as a hub in the memory-formation system. Evading Lashley’s troubled search, this is a location that does play a special role in storing memories.</p>
<p class="TXI">Current theories of hippocampal function go as follows: information about the world first reaches the hippocampus at the dentate gyrus, a region that runs along the bottom edge of the hippocampus. Here, the representation is primed and prepped to be in a form more amenable to memory storage. The dentate gyrus then sends connections on to where attractors are <a id="page_102"></a>believed to form, an area called CA3; CA3 has extensive recurrent connections that make it a prime substrate for Hopfield network-like effects. This area then sends output to another region called CA1, which acts as a relay station; it sends the remembered information back to the rest of the brain (see Figure 10).</p>
<p class="image-fig" id="fig10.jpg">
<img alt="" src="Images/chapter-04-image-04.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 10</span>
</span></p>
<p class="TXI">What’s interesting about this final step – and what may have muddied Lashley’s original findings – is that these projections out to different areas of the brain are believed to facilitate the <span class="italic">copying</span> of memories. In this way, CA3 acts as a buffer, or warehouse, holding on to memories until they can be transferred to other brain areas. It does so by reactivating the memory in those areas. The hippocampus thus helps the rest of the brain memorise things using the same strategy you’d use to study for a test: repetition. By repeatedly reactivating the same group of neurons elsewhere in the brain, the hippocampus gives those neurons the chance to undergo Hebbian learning themselves. Eventually, their own weights have changed enough for the memory to be safely stored there.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> With <a id="page_103"></a>his hippocampus gone, Molaison had no warehouse for his experiences, no way to replay his memories back to his brain. </p>
<p class="TXI">With knowledge of this memory storehouse in the brain, researchers can look into how it works. Particularly, they can look for attractors in it.</p>
<p class="TXI">In 2005, scientists at University College London recorded the activity of hippocampal cells in rats. The rats got used to being in two different enclosures – a circular one and a square one. Their hippocampal neurons showed one pattern of activity when they were in the circle and a different pattern when they were in the square. The test for attractors came when an animal was placed into a new ‘squircle’ environment, the shape of which was somewhere in between a circle and a square. The researchers found that if the environment was more square-like the neural activity went to the pattern associated with the square environment; more circle-like and it went to that of the circle. Crucially, there were no intermediate representations in response to intermediate environments, only all circle or all square. This makes the memories of the circle and square environments attractors. An initial input that isn’t exactly one or the other is unstable; it gets inescapably driven towards the nearest established memory. </p>
<p class="TXI">The Hopfield network made manifest the theories of Hebb and showed how attractors – normally studied in physics – could explain the mysteries of memory. Yet Hopfield knew the limitations of bringing mathematics to real brains in real laboratories. He described his own model as a ‘mere parody of the complexities of neurobiology’. Indeed, as the creation of a physicist, it <a id="page_104"></a>lacks all the gooey richness of biology. But as a parody capable of powerful computations, it has offered many insights as well – insights that didn’t end with simple storage and recall. </p>
<p class="center">* * *</p>
<p class="TXT">You’re eating dinner in your kitchen when your roommate comes home. When you see them, you remember that last night you finished a book they had lent you and you want to return it before they leave for a trip the next day. So, you put down your food, head out of the kitchen and go down the hallway. You walk up the stairs, turn, enter your room and think: ‘Wait, what am I doing here?’ </p>
<p class="TXI">The sensation is a common one. So much so it’s been given a name: ‘destinesia’ or amnesia about why you’ve gone to where you are. It’s a failure of what’s called ‘working memory’, the ability to hold an idea in mind, even just for the 10 seconds it takes to walk from room to room. Working memory is crucial for just about all aspects of cognition: it’s hard to make a decision or work through a plan if you keep forgetting what you’re thinking about.</p>
<p class="TXI">Psychologists have been studying working memory for decades. The term itself was first coined in the 1960 book <span class="italic">Plans and the Structure of Behavior</span> written by George A. Miller and fellow scientists working at the Center for Advanced Study in the Behavioral Sciences in California. But the concept was explored well before that. Indeed, Miller himself wrote one of the most influential papers on the topic four years previously, in 1956. Perhaps <a id="page_105"></a>anticipating its fame, Miller gave the paper the cheeky title ‘The magical number seven, plus or minus two’. What that magical number refers to is the number of items humans can hold in their working memory at any one time. </p>
<p class="TXI">An example of how to assess this is to 1) show a participant several coloured squares on a screen; 2) ask them to wait for some time between several seconds to minutes; 3) then show them a second set of coloured squares. The task of the subject is to indicate if the colours of the second set are the same as the colours of the first. People can do well on this task if the number of squares shown remains small, achieving nearly 100 per cent accuracy if only one square is shown. Adding in more squares makes the performance drop and drop, until past seven, it’s almost no different to random guessing. Whether seven really is a special value when it comes to this kind of working memory capacity is up for debate; some studies find lower limits, some higher. However, there’s no doubt that Miller’s paper made an impact and psychologists have worked to characterise nearly every aspect of working memory since, from what can be held in it to how long it can last. </p>
<p class="TXI">But the question remains of how the brain actually does this: where are working memories stored and in what way? A tried-and-tested method for answering such questions – lesion experiments – pointed to the prefrontal cortex, a large part of the brain just behind the forehead. Whether it was humans with unfortunate injuries or laboratory animals with the area removed, it was clear that damaging the prefrontal cortex reduced working memory substantially. Without it, animals can <a id="page_106"></a>hardly hold on to an idea for more than a second or two. Thoughts and experiences pass through their minds like water through cupped hands. </p>
<p class="TXI">With an ‘X’ marking the spot, neuroscientists then began to dig. Dropping an electrode into the prefrontal cortex of monkeys, in 1971 researchers at the University of California in Los Angeles eavesdropped on the neurons there. The scientists, Joaquin Fuster and Garrett Alexander, did this while the animals performed a task similar to the colour memory test. These tests are known as ‘delayed response’ tasks because they include a delay period wherein the important information is absent from the screen and must thus be held in memory. The question was: what are neurons in the prefrontal cortex doing during this delay?</p>
<p class="TXI">Most of the brain areas responsible for vision have a stereotyped response to this kind of task: the neurons respond strongly when the patterns on the screen initially show up and then again when they reappear after the delay, but during the delay period – when no visual inputs are actually entering the brain – these areas are mostly quiet. Out of sight really does mean out of mind for these neurons. What Fuster and Alexander found, however, was that cells in the prefrontal cortex were different. The neurons there that responded to the visual patterns kept firing even after the patterns disappeared; that is, they maintained their activity during the delay period. A physical signature of working memory at work!</p>
<p class="TXI">Countless experiments since have replicated these findings, showing maintained activity during delay periods under many different circumstances, both in <a id="page_107"></a>the prefrontal cortex and beyond. Experiments have also hinted that when these firing patterns are out of whack, working memory goes awry. In some experiments, for example, applying a brief electrical stimulation during the delay period can disrupt the ongoing activity and this leads to a dip in performance on delayed response tasks. </p>
<p class="TXI">What is so special about these neurons that they can do this? Why can they hold on to information and maintain their firing for seconds to minutes, when other neurons let theirs go? For this kind of sustained output, neurons usually need sustained input. But if delay activity occurs without any external input from an image then that sustained input must come from neighbouring neurons. Thus, delay activity can only be generated by a network of neurons working together, the connections between them conspiring to keep the activity alive. This is where the idea of attractors comes back into play. </p>
<p class="TXI">So far we’ve looked at attractors in Hopfield networks, which show how input cues reignite a memory. It may not be clear how this helps with working memory. After all, working memory is all about what happens after that ignition; after you stand up to get your roommate’s book, how do you keep that goal in mind? As it turns out, however, an attractor is exactly what’s needed in this situation, because an attractor stays put.</p>
<p class="TXI">Attractors are defined by derivatives. If we know the inputs a neuron gets and the weights those inputs are multiplied by, we can write down an equation – a derivative – describing how the activity of that neuron will change over time as a result of those inputs. If this derivative is zero that means there is no change in the <a id="page_108"></a>activity of the neuron over time; it just keeps firing at the same, constant rate. Recall that, because this neuron is part of a recurrent network, it not only gets input but also serves as input to other neurons. So, its activity goes into calculating the derivative of a neighbouring neuron. If none of the inputs to this neighbouring neuron are changing – that is, their derivatives are all zero as well – then it too will have a zero derivative and will keep firing at the same rate. When a network is in an attractor state, the derivative of each and every neuron in that network is zero.</p>
<p class="TXI">And that is how, if the connections between neurons are just right, memories started at one point in time can last for much longer. All the cells can maintain their firing rate because all the cells around them are doing the same. Nothing changes if nothing changes. </p>
<p class="TXI">The problem is that things do change. When you leave the kitchen and walk to your bedroom, you encounter all kinds of things – your shoes in the hallway, the bathroom you meant to clean, the sight of rain on the window – that could cause changes in the input to the neurons that are trying to hold on to the memory. And those changes could push the neurons out of the attractor state representing the book and off to somewhere else entirely. For working memory to function, the network needs to be good at resisting the influence of such distractors. A run-of-the-mill attractor can resist distracting input to an extent. Recall the trampoline example. If the person standing on the trampoline gave a little nudge to the ball, it would likely roll just out of its divot and then back in. With only a small perturbation the memory stays intact, but give the <a id="page_109"></a>ball a heartier kick and who knows where it will end up? Good memory should be robust to such distractions – so what could make a network good at holding on to memories? </p>
<p class="TXI">The dance between data and theory is a complex one, with no clear lead or follow. Sometimes mathematical models are developed just to fit a certain dataset. Other times the details from data are absent or ignored and theorists do as their name suggests: theorise about how a system <span class="italic">could</span> work before knowing how it does. When it comes to building a robust network for working memory, scientists in the 1990s went in the latter direction. They came up with what’s known as the ‘ring network’, a hand-designed model of a neural circuit that would be ideal for the robust maintenance of working memories. </p>
<p class="TXI">Unlike Hopfield networks, ring networks are well described by their name: they are composed of several neurons arranged in a ring, with each neuron connecting only to those near to it. Like Hopfield networks, these models have attractor states – activity patterns that are self-sustaining and can represent memories. But the attractor states in a ring model are different to those in a Hopfield network. Attractors in a Hopfield model are <span class="italic">discrete</span>. This means that each attractor state – the one for your childhood bedroom, the one for your childhood holiday, the one for your current bedroom – is entirely isolated from the rest. There is no smooth way to transition between these different memories, regardless of how similar they are; you have to completely leave one attractor state to get to another. Attractors in a ring network, on the other hand, are <span class="italic">continuous</span>. With continuous attractors, transitioning between similar <a id="page_110"></a>memories is easy. Rather than being thought of as a trampoline with people standing at different points, models with continuous attractor states are more like the gutter of a bowling lane: once the ball gets into the gutter it can’t easily get out, but it can move smoothly within it.</p>
<p class="TXI">Networks with continuous attractor states like the ring model are helpful for a variety of reasons and chief among them is the type of errors they make. It may seem silly to praise a memory system for its errors – wouldn’t we prefer no errors at all? – but if we assume that no network can have perfect memory, then the quality of the errors becomes very important. A ring network allows for small, sensible errors. </p>
<p class="TXI">Consider the example of the working memory test where subjects had to keep in mind the colour of shapes on a screen. Colours map well to ring networks because, as you’ll recall from art class, colours lie on a wheel. So, imagine a network of neurons arranged in a ring, with each neuron representing a slightly different colour. At one side of the ring are red-representing neurons, next to them are orange, then yellow and green; this brings us to the side opposite the red, where there are the blue-representing neurons, which lead to the violet ones and back to red. </p>
<p class="TXI">In this task, when a shape is seen, it creates activity in the neurons that represent its colour, while the other neurons remain silent. This creates a little ‘bump’ of activity on the ring, centred on the remembered colour. If any distracting input comes in while the subject tries to hold on to this colour memory – from other random sights in the room, for example – it may push or pull the activity bump away from the desired colour. But – and this is the crucial <a id="page_111"></a>point – it will only be able to push it to a very nearby place on the ring. So red may become red-orange or green may become teal. But the memory of red would be very unlikely to become green. Or, for that matter, to become no colour at all; that is, there will always be a bump <span class="italic">somewhere</span> on the ring. These properties are all a direct result of the gutter-like nature of a continuous attractor – it has low resistance for moving between nearby states, but high resistance to perturbations otherwise.</p>
<p class="TXI">Another benefit of the ring network is that it can be used to do things. The ‘working’ in working memory is meant to counter the notion that memory is just about passively maintaining information. Rather, holding ideas in working memory lets us combine them with other information and come to new conclusions. An excellent example of this is the head direction system in rats, which also served as the inspiration for early ring network models. </p>
<p class="TXI">Rats (along with many other animals) have an internal compass: a set of neurons that keep track of the direction the animal is facing at all times. If the animal turns to face a new direction, the activity of these cells changes to reflect that change. Even if the rat sits still in a silent darkened room, these neurons continue to fire, holding on to the information about its direction. In 1995, a team from Bruce McNaughton’s lab at the University of Arizona and, separately, Kechen Zhang of the University of California, San Diego, posited that this set of cells could be well described by a ring network. Direction being one of those concepts that maps well to a circle, a bump of activity on the ring would be used to store the direction the animal was facing (See Figure 11).</p> <a id="page_112"></a>
<p class="TXI">But not only could a ring network explain how knowledge of head direction was maintained over time, it also served as a model of how the stored direction could change when the animal did. Head direction cells receive input from other neurons, such as those from the visual system and the vestibular system (which keeps track of bodily motion). If these inputs are hooked up to the ring network just right, they can push the bump of activity along to a new place on the ring. If the vestibular system says the body is now moving leftwards, for example, the bump gets pushed to the left. In this way, movement along the ring doesn’t create errors in memory, but rather updates the memory based on new information. ‘Working’ memory earns its name.</p>
<p class="image-fig" id="fig11.jpg">
<img alt="" src="Images/chapter-04-image-05.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 11</span>
</span></p>
<p class="TXI">Ring networks are a lovely solution to the complex problem of how to create robust and functional working memory systems. They are also beautiful mathematical objects. They display the desirable properties of simplicity and symmetry. They’re precise and finely tuned, elegant even.</p> <a id="page_113"></a>
<p class="TXI">As such, they are completely unrealistic. Because to the biologist, of course, ‘finely tuned’ are dirty words. Anything that requires delicate planning and pristine conditions to operate well won’t survive the chaos that is brain development and activity. Many of the desirable properties of ring networks only occur under very particular assumptions about the connectivity between neurons, assumptions that just don’t seem very realistic. So, despite all their desirable theoretical properties and useful abilities, the chances of seeing a ring network in the brain seemed slim. </p>
<p class="TXI">The discovery made in a research centre just outside Washington DC in 2015 was therefore all the more exciting. </p>
<p class="TXI">Janelia Research Campus is a world-class research facility hidden away in the idyllic former farmland of Ashburn, Virginia. Vivek Jayaraman has been at Janelia since 2006. He and his team of about half a dozen people work to understand navigation in <span class="italic">Drosophila melanogaster</span>, a species of fruit fly commonly studied in neuroscience. On a par with a grain of rice, the size of these animals is both a blessing and a curse. While they can be difficult to get hold of, these tiny flies only have around 135,000 neurons, roughly 0.2 per cent the amount of another popular lab animal, the mouse. On top of that, a lot is known about these neurons. Many of them are easily categorised based on the genes they express, and their numbers and locations are very similar across individuals. </p>
<p class="TXI">Like rodents, flies also have a system for keeping track of head direction. For the fly, these head direction neurons are located in a region known as the ellipsoid <a id="page_114"></a>body. The ellipsoid body is centrally placed in the fly brain and it has a unique shape: it has a hole in the middle with cells arranged all around that hole, forming a doughnut made of neurons – or in other words, a ring.</p>
<p class="TXI">Neurons arranged in a ring, however, do not necessarily make a ring network. So, what the Jayaraman lab set out to do was investigate whether this group of neurons that <span class="italic">looked</span> like a ring network, actually <span class="italic">behaved</span> like one. To do this, they put a special dye in the ellipsoid body neurons, one that makes them light up green when they’re active. They then had the fly walk around, while they filmed the neurons. If you were to look at these neurons on a screen, as the fly heads forwards, you’d see a flickering of little green points at one location on an otherwise black screen. Should the fly choose to make a turn, that flickering patch would swing around to a new location. Over time, as the fly moves and the green patch on the screen moves with it, the points that have lit up form a clear ring structure, matching the underlying shape of the ellipsoid body. If you turn the lights off in the room so the fly has no ability to see which way it’s facing, the green flicker still remains at the same location on that ring – a clear sign that the memory of heading direction is being maintained. </p>
<p class="TXI">In addition to observing the activity on the ring, the experimenters also manipulated it in order to probe the extremes of its behaviour. A true ring network can only support one ‘bump’ of activity; that is, only neurons at one location on the ring can be active at a given time. So, the researchers artificially stimulated neurons on the side of the ring opposite to those already active. This strong stimulation of the opposing neurons caused the original bump to shut down, and the bump at the new location was maintained, even after the stimulation was <a id="page_115"></a>turned off. Through these experiments, it became clear that the ellipsoid body was no imposter, but a vivid example of a theory come to life. </p>
<p class="TXI">This finding – a ring network in the literal, visible shape of a ring – feels a bit like nature winking at us. William Skaggs and the other authors of one of the original papers proposing the ring network explicitly doubted the possibility of such a finding: ‘For expository purposes it is helpful to think of the network as a set of circular layers; this does not reflect the anatomical organisation of the corresponding cells in the brain.’ Most theorists working on ring network models assumed that they’d be embedded in some larger, messier network of neurons. And that is bound to be the case for most systems in most species. This anomalously pristine example likely arises from a very precisely controlled genetic programme. Others will be much harder to spot. </p>
<p class="TXI">Even if we usually can’t see them directly, we can make predictions about behaviours we’d expect to see if the brain is using continuous attractors. In 1991, pioneering working memory researcher Patricia Goldman-Rakic found that blocking the function of the neuromodulator dopamine made it harder for monkeys to remember the location of items. Dopamine is known to alter the flow of ions into and out of a cell. In 2000, researchers at the Salk Institute in California showed how mimicking the presence of dopamine in a model with a continuous attractor enhanced the model’s memory.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup> It stabilised the activity of the neurons encoding the memory, making <a id="page_116"></a>them more resistant to irrelevant inputs. Because dopamine is associated with reward,<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> this model also predicts that under conditions where a person is anticipating a large reward their working memory will be better – and that is exactly what has been found. When people are promised more reward in turn for remembering something, their working memory is better. Here, the concept of an attractor works as the thread that stitches chemical changes together with cognitive ones. It links ions to experiences. </p>
<p class="TXI">Attractors are omnipresent in the physical world. They arise from local interactions between the parts of a system. Whether those parts are atoms in a metal, planets in a solar system or even people in a community, they will be compelled towards an attractor state and, without major disruptions, will stay there. Applying these concepts to the neurons that form a memory connects dots across biology and psychology. On one side, Hopfield networks link the formation and retrieval of memories to the way in which connections between neurons change. On the other, structures like ring networks underlie how ideas are held in the mind. In one simple framework we capture how memories are recorded, retained and reactivated.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-1" id="fn-1">1</a> ﻿Jerzy Konorski, a Polish neurophysiologist, published a book with very similar ideas the year before Hebb did. In fact, Konorski anticipated several important findings in neuroscience and psychology. However, the global East﻿–﻿West divide at the time isolated his contributions. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-2" id="fn-2">2</a> ﻿When Hopfield wrote on an undergraduate admission form that he intended to study ﻿‘﻿physics or chemistry﻿’﻿, his university advisor ﻿–﻿ a colleague of his father﻿’﻿s ﻿–﻿ crossed out the latter option saying, 
﻿‘﻿I don﻿’﻿t believe we need to consider chemistry.﻿’﻿ ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-3" id="fn-3">3</a> ﻿Hopfield﻿’﻿s attitude was not unique. The 1980s found many physicists, bored with their own field, looking at the brain and thinking, ﻿‘﻿I could solve that.﻿’﻿ After Hopfield﻿’﻿s success, this population increased even more.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-4" id="fn-4">4</a> ﻿While the calculation of an individual neuron﻿’﻿s activity in terms of inputs and weights is the same as described for the perceptron in the last chapter, the perceptron is a ﻿<span class="italic">feedforward</span>﻿ (not recurrent) network. Recurrence means that the connections can form loops: neuron A connects to neuron B, which connects back to neuron A, for example. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-5" id="fn-5">5</a> ﻿This second part ﻿–﻿ the idea that connection strength should ﻿<span class="italic">decrease</span>﻿ if a pre-synaptic neuron is highly active while the post-synaptic neuron remains quiet ﻿–﻿ was not part of Hebb﻿’﻿s original sketch, but it has since been borne out by experiments.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-6" id="fn-6">6</a> ﻿In fact, when Hopfield presented an early version of this work to a group of neuroscientists, one attendee commented that ﻿‘﻿it was a beautiful talk but unfortunately had nothing to do with neurobiology﻿’﻿. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-7" id="fn-7">7</a> ﻿You may know some people with stories of their own ﻿‘﻿blackout catastrophe﻿’﻿ after a night of drinking. However, the exact type of memory failure seen in Hopfield networks is not actually believed to occur in humans.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-8" id="fn-8">8</a> ﻿This process is believed to occur while you sleep.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-9" id="fn-9">9</a> ﻿This model was composed of the Hodgkin-Huxley style of neurons described in ﻿﻿Chapter 2﻿﻿, which makes incorporating dopamine﻿’﻿s effects on ion flows easy.﻿</p>
<p class="FN2"><a href="chapter4.xhtml#fnt-10" id="fn-10">10</a> ﻿Lots more on this in ﻿﻿Chapter 11﻿﻿!﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<meta charset="utf-8"/>
<title>Contents</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="FMT" id="re_contents">Contents</p>
<p class="TOC-CH"><a href="chapter1.xhtml#chapter1" id="re_chapter1">Chapter 1:  Spherical Cows</a></p>
<p class="TOC-CH"><a href="chapter2.xhtml#chapter2" id="re_chapter2">Chapter 2:  How Neurons Get Their Spike</a></p>
<p class="TOC-CH"><a href="chapter3.xhtml#chapter3" id="re_chapter3">Chapter 3:  Learning to Compute</a></p>
<p class="TOC-CH"><a href="chapter4.xhtml#chapter4" id="re_chapter4">Chapter 4:  Making and Maintaining Memories</a></p>
<p class="TOC-CH"><a href="chapter5.xhtml#chapter5" id="re_chapter5">Chapter 5:  Excitation and Inhibition</a></p>
<p class="TOC-CH"><a href="chapter6.xhtml#chapter6" id="re_chapter6">Chapter 6:  Stages of Sight</a></p>
<p class="TOC-CH"><a href="chapter7.xhtml#chapter7" id="re_chapter7">Chapter 7:  Cracking the Neural Code</a></p>
<p class="TOC-CH"><a href="chapter8.xhtml#chapter8" id="re_chapter8">Chapter 8:  Movement in Low Dimensions</a></p>
<p class="TOC-CH"><a href="chapter9.xhtml#chapter9" id="re_chapter9">Chapter 9:  From Structure to Function</a></p>
<p class="TOC-CH"><a href="chapter10.xhtml#chapter10" id="re_chapter10">Chapter 10:  Making Rational Decisions</a></p>
<p class="TOC-CH"><a href="chapter11.xhtml#chapter11" id="re_chapter11">Chapter 11:  How Rewards Guide Actions</a></p>
<p class="TOC-CH"><a href="chapter12.xhtml#chapter12" id="re_chapter12">Chapter 12:  Grand Unified Theories of the Brain</a></p>
<p class="TOC-CH1"><a href="Mathematical.xhtml#Mathematical" id="re_Mathematical">Mathematical Appendix</a></p>
<p class="TOC-CH"><a href="ack.xhtml#ack" id="re_ack">Acknowledgements</a></p>
<p class="TOC-CH"><a href="bib.xhtml#bib" id="re_bib">Bibliography</a></p>
<p class="TOC-CH"><a href="index.xhtml#index" id="re_index">Index</a></p>
</body>
</html>