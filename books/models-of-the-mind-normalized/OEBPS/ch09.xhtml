<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 9</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter9"><a href="contents.xhtml#re_chapter9">CHAPTER NINE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter9">From Structure to Function</a><a id="page_247"></a></p>
<p class="H1" id="b-9781472966445-ch968-sec10">
<span class="bold">
<span>Graph theory and network neuroscience</span>
</span></p>
<p class="TXT">In 1931, three years before his death, Santiago Ramón y Cajal gave the Cajal Institute in Madrid a trove of his personal possessions. The collection contained all manner of scientific trinkets: balances, slides, cameras, letters, books, microscopes, solutions, reagents. But the items most notable – the ones that would become nearly synonymous with the name Cajal – were the 1,907 scientific drawings he had created throughout his career.</p>
<p class="TXI">Most of these drawings were of different parts of the nervous system and were produced via a laborious cell-staining process. It started with a live animal, which was sacrificed and its tissues preserved. A chunk of the brain was then removed and soaked in a solution for two days, dried and soaked in a different solution – this one containing silver that would penetrate the cell structures – for another two days. At the end of this, the brain tissue was rinsed, dried again and cut into slices thin enough to fit on a microscope slide. Cajal looked at these slides through the eyepiece of his microscope and sketched what he saw. Starting first with pencil, he outlined every nook and cranny of each neuron’s shape on a piece of cardboard, including the thick cell bodies and the thin appendages that emerged from them. He then darkened in the cells with India ink, occasionally using watercolours <a id="page_248"></a>to add texture and dimension. The result was a set of haunting silhouettes of stark, black spider-like figures against beige and yellow backgrounds.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> The exact contours and configurations depended on the animal and the nerve fibres in question; more than 50 different species and nearly 20 different parts of the nervous system are portrayed on Cajal’s cardboard canvases.</p>
<p class="TXI">These hundreds of portraits represent the infatuation Cajal had with the <span class="italic">structure</span> of the nervous system. He sought enlightenment in the basic unit of the brain – the neuron. He fixated on how they were shaped and how they were arranged. A focus on its physical foundations was Cajal’s inroad to understanding how the brain worked. Function, he believed, could be found in structure. </p>
<p class="TXI">And he was right. Cajal was able to deduce important facts about the workings of the brain by looking long and hard at how it was built. One of his significant findings was about how signals flow through neurons. Through his many observations of different neurons in different sensory organs, Cajal noticed that the cells were always arranged a certain way. The many branched dendrites of a cell would face the direction the signal was coming from. The long singular axon, on the other hand, went towards the brain. In the olfactory system, for example, neurons with the chemical receptors capable of picking up odour molecules exist in the mucousy skin <a id="page_249"></a>inside the nose. These neurons send their axons up into the brain and make contact with the dendrites of cells in the olfactory bulb. These neurons then have axons that go further on into other parts of the brain. </p>
<p class="TXI">This pattern – which Cajal saw over and over – strongly suggested that signals flow from dendrites through to axons. Dendrites, he concluded, act as the receiver of signals for a cell and axons the sender of signals on to the next cell. So clear was Cajal on this that he added little arrows to his drawings of circuits like the olfactory system, indicating the presumed direction of information flow. Cajal, as we now know, was exactly correct. </p>
<p class="TXI">Cajal was one of the founding fathers of modern neuroscience. As such, his belief in the relationship between structure and function was entered into the DNA of the field. Reflections of this idea are peppered throughout neuroscience’s history. In a 1989 article, Peter Getting wrote that researchers in the 1960s could see, even through their limited data, that ‘the abilities of a network arose from the interconnection of simple elements into complex networks, thus, from connectivity emerged function’. Studies in the 1970s, he goes on to say, ‘were approached with several expectations in mind: first, a knowledge of the connectivity would explain how neural networks operated’. This attitude persists. A review written in 2016 by professors Xiao-Jing Wang and Henry Kennedy ends with the statement: ‘Establishing a firm link from structure to function is essential to understand complex neural dynamics.’</p>
<p class="TXI">Structure exists at many scales in the brain. Neuroscientists can look at the shape of a neuron, as <a id="page_250"></a>Cajal did. Or they can look at how neurons are wired up: does neuron A connect to neuron B? They can zoom out even further and ask how small populations of neurons interact. Or they can investigate brain-wide connectivity patterns by looking at the thick bundles of axons that connect distant brain regions. Any of these higher-level structures may hold secrets about function as well. </p>
<p class="TXI">But to unearth these secrets, neuroscientists need a way to see and study these structures clearly. Something that could’ve been considered a limitation of the staining method Cajal used – that it stained only a small number of neurons at a time – was actually an advantage that made it revolutionary. A method that stained all neurons in the field of view would’ve produced a black mess with no discernible structures; it would’ve missed the trees for the forest. Since neuroscientists have moved their study of structure away from single neurons and on to the more complicated subject of connections, networks and circuits, they may be at even higher risk of being overwhelmed by data and distracted by the wrong details. </p>
<p class="TXI">A much-needed method, however, has been found in a particular subfield of mathematics: graph theory. The language of graph theory offers a way to talk about neural networks that cuts away much of the detail. At the same time, its tools find features of neural structure that are near impossible to see without it. These features of the structure, some scientists now believe, can inspire new thoughts on the function of the nervous system. Swept up by the promise of graph theory’s methods, neuroscientists are currently applying it to everything <a id="page_251"></a>from brain development to disease. Though the dust has not yet settled on this new approach to the brain, its fresh take on old problems is exciting many. </p>
<p class="center">* * *</p>
<p class="TXT">In the eighteenth-century East Prussian capital of Königsberg, a river branched in two as it cut through the town, creating a small island in the middle. Connecting this island with parts of the town north, south and east of it were seven bridges. At some point a question arose among the citizens of Königsberg: is there a way to navigate through the city that crosses each of the bridges once and only once? When this playful question met the famous mathematician Leonhard Euler, the field of graph theory was born.</p>
<p class="TXI">Euler, a polymath who was born in Switzerland but lived in Russia, wrote ‘<span class="italic">Solutio problematis ad geometriam situs pertinentis</span>’ or ‘The solution of a problem relating to the geometry of position’ in 1736. In the paper, he answered the question definitively: a Königsberger could <span class="italic">not</span> take a walk through their town crossing each bridge exactly once. To prove this he had to simplify the town map into a skeleton of its full structure and work logically with it. He had shown, without using the word, how to turn data into a <span class="italic">graph</span> and how to perform computations on it (see Figure 20).</p>
<p class="TXI">Within the context of graph theory, the word ‘graph’ does not refer to a chart or a plot, as it does in common language. Rather, a graph is a mathematical object, composed of <span class="italic">nodes</span> and <span class="italic">edges</span> (in the modern parlance). Nodes are the base units of the graph and edges represent <a id="page_252"></a>the connections between them. In the Königsberg example, the bridges serve as edges that connect the four different land masses, the nodes. The degree of a node is the number of edges it has; the ‘degree’ of a land mass is thus the number of bridges that reach it.</p>
<p class="image-fig" id="fig20.jpg">
<img alt="" src="Images/chapter-01-image-04.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 20</span>
</span></p>
<p class="TXI">Euler approached the bridge-crossing question by first noting that a path through town could be written down as a list of nodes. Giving each land mass a letter name, the list ‘ABDC’, for example, would represent a path that goes from the island at the centre to the land at the bottom (via any bridge that connects them), then from there to the land mass on the right and then on to the land at the top. In such a path through a graph, one edge is traversed between each pair of nodes. Therefore, the number of bridges crossed is equal to the number of letters in the list minus one. For example, if you’ve crossed two bridges, you’d have three land masses on your list. </p>
<p class="TXI">Euler then noticed something important about the number of bridges that each land mass has. This number is related to how many times the land mass should show <a id="page_253"></a>up in the path list. For example, land mass B has three bridges, which means ‘B’ must appear twice in any path that crosses each bridge once – that is, there is no way to cross these three bridges without visiting B twice. The same is true for land masses C and D, as they both have two bridges too. And land mass A, with five bridges, needs to appear three times in the path list. </p>
<p class="TXI">Taken together, any path that satisfies these require­ments would be nine (2+2+2+3) letters long. A list of nine letters, however, represents a path that crosses <span class="italic">eight</span> bridges. Therefore, it is impossible to build a path that crosses each of the seven bridges only once. </p>
<p class="TXI">Using this relationship between a node’s degree and the number of times the node should appear in a path, Euler derived a set of general rules about what paths were possible. He could now say for <span class="italic">any</span> set of bridges connecting <span class="italic">any</span> patches of land whether a path crossing each bridge only once existed. </p>
<p class="TXI">More than that, it doesn’t matter if we are talking about land and bridges at all. The same procedure could be used to find paths for a town snowplough that needs to clear each street only once or to see if it’s possible to traverse Wikipedia clicking each hyperlink between sites just one time. This pliability is part of what gives graph theory its potency. By stripping away the details of any specific situation, it finds the structure that is similar to them all. This abstract and alien way of looking at a problem can open it up to new and innovative solutions, just as treating a walk through town as a list of letters helped Euler. </p>
<p class="TXI">Given this feature, graph theory found purpose in many fields. Chemists in the nineteenth century wrestled <a id="page_254"></a>for some time with how to represent the structure of molecules. By the 1860s, a system was developed that is still in use today: atoms are drawn as letters and the bonds between them as lines. In 1877, English mathematician James Joseph Sylvester saw in this graphical representation of molecules a parallel to the work being done by descendants of Euler in mathematics. He published a paper drawing out the analogy and used, for the first time, the word ‘graph’ to refer to this form. Since then, graph theory has helped solve many problems in chemistry. One of its most common applications is finding isomers – sets of molecules that are each made up of the same type and number of atoms, but differ in how those atoms are arranged. Because graph theory provides a formal language for describing the structure of atoms in a molecule, it is also well suited for enumerating all the structures that are possible given a particular set of atoms. Algorithms that do this can aid in the design of drugs and other desirable compounds. </p>
<p class="TXI">Like a chemical compound, the structure of the brain lends itself well to a graph. In the most basic mapping, neurons are the nodes and the connections between them are the edges. Alternatively, the nodes can be brain areas and the nerve tracts that connect them the edges. Whether working on the microscale of neurons or the macroscale of brain regions, putting the brain into the terms of graph theory exposes it to all the tools of analysis this field has developed. It is a way of formalising the informal quest that has always guided neuroscience. To speak of how structure births function first requires the ability to speak clearly about structure. Graph theory provides the language.</p><a id="page_255"></a>
<p class="TXI">Of course, there are differences between the brain and a Prussian town or a chemical compound. The connections in the brain aren’t always a two-way street like they are on a bridge or in a bond. One neuron can connect to another without it receiving a connection back. This unidirectional nature of neural connections is important for the way information flows through neural circuits. The most basic graph structures don’t capture this, but by the late 1800s, the concept of <span class="italic">directed</span> graphs had been added to the arsenal of mathematical descriptors. In a directed graph, edges are arrows that only flow one way. The degree of a node in a directed graph is thus broken down into two categories: the in-degree (for example, how many connections a neuron receives) and the out-degree (how many connections it sends out to other neurons). A study done on neurons in the cortex of monkeys found these two types of degree to be roughly equal, meaning neurons give as much as they receive. </p>
<p class="TXI">In 2018, mathematicians Katherine Morrison and Carina Curto built a model of a neural circuit with directed edges in order to answer a question not so dissimilar to the Königsberg bridge problem. Rather than determining what walks through town a certain set of bridges can support, they explored what sequence of neural firing a given circuit could produce. By bringing in tools from graph theory, Morrison and Curto figured out how to look at a structure of up to five model neurons and predict the order in which they will fire. Ordered patterns of neuronal firing are important for many of the brain’s functions, including memory and navigation. This five-neuron model may only be a toy <a id="page_256"></a>example, but it perfectly encapsulates the power promised by bringing graph theory into the study of the brain. </p>
<p class="TXI">For real brain networks, however, a more ‘global’ perspective needs to be taken.</p>
<p class="center">* * *</p>
<p class="TXT">Over the course of a few months in the late 1960s, a stockbroker living in Sharon, Massachusetts, was given 16 brown folders from the proprietor of a local clothing store. Strange though this was, the folders weren’t a surprise to the stockbroker. They were simply part of an unorthodox social experiment being run by the famous social psychologist Stanley Milgram. With this experiment, Milgram wanted to test just how big – or small – the world truly was. </p>
<p class="TXI">The phrase ‘it’s a small world’ is usually uttered when two strangers meet and serendipitously discover that they have a friend or relative in common. Milgram wanted to know just how often something like this could happen: what are the odds that two people chosen at random have a friend in common? Or a friend of a friend? Framed another way, if we could see the entire network of human connections – a graph where each node is a person and each edge a relationship – what would the average distance between people be? How many edges would we need to traverse to find a path between any two nodes?</p>
<p class="TXI">In a bold attempt to answer this question, Milgram chose a target person (in this case, the Massachusetts stockbroker) and several starters: unrelated people in another part of the country (in this case, mostly Omaha, <a id="page_257"></a>Nebraska). The starters were given a package with a folder and information about the target person. The instructions were simple: if you know the target, give the folder to them; otherwise, send it on to a friend of yours who you think has a better shot of knowing them. The next person would be told to follow the same instructions and, hopefully, eventually the folder would end up with the target. The senders were also asked to write their name in a register that was sent along with the package, so that Milgram could trace the path the folder took. </p>
<p class="TXI">Looking at a total of 44 folders that made it back to the stockbroker, Milgram found that the shortest path consisted of just two intermediate people and the longest had 10. The median was just five. Getting the folder through five people between the starter and the target involved six handoffs and thus the notion of ‘six degrees of separation’ – already posited by observant scientists and sociologists – was solidified.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> </p>
<p class="TXI">This concept percolated through the popular imagination. One day, in the late 1990s, graduate student Duncan Watts was asked by his father if he realised that he was only six handshakes away from the president. Watts, working for mathematician Steven Strogatz at the time, brought up this idea as they were discussing how groups of crickets could communicate. After this chance conversation, ‘small world’ would go from a quaint <a id="page_258"></a>expression to a mathematically defined property of a network.</p>
<p class="TXI">In 1998, Watts and Strogatz published a paper laying out just what it takes for a graph to function like a small world. The notion of a short average path length – the idea that any two nodes are separated by only a few steps – was a key component of it. One way to get short path lengths is to make the graph heavily interconnected, <span class="italic">i.e.</span> a graph where each node connects directly to many others. This trick, however, is in clear violation of what we know about social networks: the average citizen of America – a country of around 200 million at the time – had only about 500 acquaintances, according to Milgram. </p>
<p class="TXI">So, Watts and Strogatz limited their network simulations to those with sparse connections, but varied exactly what those connections looked like. They noticed that it was possible to have short path lengths in a network that was highly <span class="italic">clustered</span>. A cluster refers to a subset of nodes that are heavily interconnected, like the members of a family. In these networks, most nodes form edges just with other nodes in their cluster, but occasionally a connection is sent to a node in a distant cluster. The same way a train between two cities makes interactions between their citizens easier, these connections between different clusters in a network keeps the average path length low. </p>
<p class="TXI">Once these characteristics were identified in their models, Watts and Strogatz went looking for them in real data – and found them. The power grid system of the United States – turned into a graph by considering any generator or substation as a node and transmission lines as edges – has the low path length and high clustering of <a id="page_259"></a>a small world network. A graph made of actors with edges between any pairs that have starred in a movie together is the same. And the final place that they looked for, and found, a small world network was in the brain. </p>
<p class="TXI">More specifically, the structure Watts and Strogatz analysed was the nervous system of the tiny roundworm, <span class="italic">Caenorhabditis elegans</span>. Ignoring the directionality of the neural connections, Watts and Strogatz treated any connection as an edge and each of the 282 neurons in the worm’s wiring diagram as a node. They found that any two neurons could be connected by a path with, on average, only 2.65 neurons in between them and that the network contained far more clustering than would be expected if those 282 neurons were wired up randomly. </p>
<p class="TXI">Why should the nervous system of a nematode have the same shape as the social network of humans? The biggest reason may be energy costs. Neurons are hungry. They require a lot of energy to stay in working order and adding more or longer axons and dendrites only ups the bill. A fully interconnected brain is, thus, a prohibitively expensive brain. Yet if connections become too sparse the very function of the brain – processing and routing information – breaks down. A balance must be struck between the cost of wiring and the benefit of information sharing. Small worlds do just this. In a small world, the more common connections are the relatively cheap ones between cells in a local cluster. The pricey connections between faraway neurons are rare, but there are enough to keep information flowing. Evolution, it seems, has found small worldness to be the smart solution. </p>
<p class="TXI">Watts and Strogatz’s finding in the roundworm was the first time the nervous system was described in the <a id="page_260"></a>language of graph theory. Putting it into these terms made visible some of the constraints that are shared by the brain and other naturally occurring networks. Connections can be expensive to maintain, be they acquaintanceships or axons, and if these similarities exist between the roundworm and social networks it’s reasonable to expect that the structure of other nervous systems is dictated by them as well. </p>
<p class="TXI">But to speak about the structure of the nervous system requires that we know something about the structure of the nervous system. As it turns out, harvesting this information is a nuisance at best and an unprecedented technical hurdle at worst.</p>
<p class="center">* * *</p>
<p class="TXT">A ‘connectome’ is a graph describing the connections in a brain. While Watts and Strogatz were working with an incomplete version, the full roundworm connectome is defined by the full set of 302 neurons in the worm and the 7,286 connections between them. The roundworm was the first animal to have its entire adult connectome documented and, at present, it is the only one. </p>
<p class="TXI">This lack of connectomic data is due largely to the gruelling process by which it is collected. Mapping a full connectome at the neuron level requires fixing a brain in a preservative, cutting it into sheets thinner than a strand of hair, photographing each of these sheets with a microscope and feeding those photographs into a computer that recreates them as a 3D stack. Scientists then spend tens of thousands of hours staring at these photographs, tracing individual neurons through image <a id="page_261"></a>after image and noting where they make contact with each other.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> The process of unearthing the delicate structure of neural connections this way is as painstaking as a palaeontology dig. The price of all this slicing, stitching and tracing makes it unlikely that full connectomes are within reach for any but the smallest of species. The connectome of the fruit fly, an animal with a brain one-millionth the size of a human’s, is currently being assembled by a team of scientists and producing millions of gigabytes of data in the process. And, while any two roundworms are more or less alike, more complex species tend to have more individual differences, making the connectome of only a single fly or mammal a mere draw from a hat of possible connectomes. </p>
<p class="TXI">Luckily more indirect methods are available that allow for a rough draft of connectomes in many individuals and species. One approach involves recording from a neuron while electrically stimulating others around it. If stimulation of one of these nearby neurons reliably causes a spike in the recorded one, there’s likely a connection between them. Another option is tracers: chemicals that act like dyes that colour in a neuron. To see where inputs are coming from or outputs are going to, one just needs to look at where the dye shows up. None of these methods can create a complete connectome, but they do work to give a snapshot of connectivity in a certain area.</p> <a id="page_262"></a>
<p class="TXI">While connectivity had been studied long before it, the word ‘connectome’ wasn’t coined until 2005. In a visionary paper, psychologist Olaf Sporns and colleagues called on their fellow scientists to help build the connectome of the human brain, promising it would ‘significantly increase our understanding of how functional brain states emerge from their underlying structural substrate’. Getting connection data from humans is a staggering challenge, as many of the invasive methods used in animals are, for obvious reasons, not permissible. A quirk of brain biology, however, allows for a clever alternative. </p>
<p class="TXI">When the brain builds connections, protecting the cargo is key. Like water leaking out of a seeping hose, the electrical signal carried by an axon is at risk of fading away. This isn’t much of a problem for short axons connecting nearby cells, but those carrying signals from one region of the brain to another need protection. Long-range axons therefore get wrapped in layers and layers of a waxy blanket. This waxy substance, called myelin, contains a lot of water molecules. Magnetic resonance imaging (the same technology used to take pictures of tumours, aneurysms and head injuries) can detect the movement of these water molecules – information that is used to reconstruct the tracts of axons in the brain. Through this, it’s possible to see which brain regions are connected to each other. After the publication of Sporns’s plea, the Human Connectome Project was launched to map out the brain using this technique. </p>
<p class="TXI">Identifying long-range axons this way doesn’t produce the same kind of connectome that single-cell tracing methods do. It requires that scientists chunk the brain into <a id="page_263"></a>rough and possibly arbitrary regions; it’s therefore a much coarser description of connectivity. In addition, measuring water molecules isn’t a perfect way to pick up the axons between these areas, leading to errors or ambiguities. Even David Van Essen, one of the key scientists of the Human Connectome Project, warned the neuroscience community in 2016 that there are major technical limitations to this approach that shouldn’t be underappreciated. On the other hand, it is one of the only means by which we can peer into a living human brain; the desire to press forward with it makes sense. As Van Essen wrote: ‘Be optimistic, yet critical of glasses half full and half empty.’ </p>
<p class="TXI">Despite these limitations, neuroscientists in the early 2000s were inspired by the work of Watts and Strogatz 
to look at their field through the lens of graph theory and eagerly set their gaze upon any and all available connectome data. What they saw when they analysed it were small worlds in all directions. The reticular formation, for example, is an ancient part of the brain responsible for many aspects of bodily control. When a cell-level map of this region in cats was pieced together and analysed in 2006, it was the first vertebrate neural circuit to be given the graph-theory treatment. And it was found to be a small world. In studies of the connections between brain areas in both rats and monkeys, short path lengths and plenty of clusters were always found as well. Humans were finally brought into the small-world club in 2007 when researchers in Switzerland used MRI scans to parcellate the brain into a thousand different areas – each about the height and width of a hazelnut – and measured the connections between them.</p> <a id="page_264"></a>
<p class="TXI">Universal findings are a rare sight in neuroscience; the principles that operate on one set of neurons aren’t necessarily assumed to show up in another. As a conclusion that repeats itself across species and scales, small worldness is thus remarkable. Like the refrain of a siren song, it also implores more exploration. To see small worlds in so many places raises questions about how they got there and what roles they can play. While the answers to these questions are still being explored, without the language of graph theory they couldn’t have even been asked in the first place.</p>
<p class="center">* * *</p>
<p class="TXT">On 10 February 2010, approximately 23 per cent of all flights originating in the United States were cancelled. This historically large disruption was the result of a snowstorm in the north-east that closed a handful of airports, including Ronald Reagan in Washington DC and JFK in New York. Such a sizable dent in travel wouldn’t ordinarily come from closing a handful of airports – but these were not just any airports, they were hubs in the aviation network. </p>
<p class="TXI">Hubs are nodes in a graph that have a high degree – that is, they’re highly connected. They reside in the tails of the degree distribution: a plot that shows, for each degree value, how many nodes in the network have that degree (see Figure 21). For graphs like the aviation network or the structure of servers that make up the internet, this graph starts high – meaning that there are many nodes that have only a small number of connections – and fades as the number of connections increases, leading to a long, <a id="page_265"></a>low tail that represents the small number of nodes with very high degree, such as JFK airport. The high degree of hubs makes them powerful but also a potential vulnerability. Like removing the keystone from a stone archway, a targeted attack on one of its hubs can cause a network to collapse.</p>
<p class="image-fig" id="fig21.jpg">
<img alt="" src="Images/chapter-01-image-05.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 21</span>
</span></p>
<p class="TXI">The brain has hubs. In humans, they’re found sprinkled throughout the lobes. The cingulate, for example, which curves around the centre of the brain, serves as a hub; as does the precuneus, which sits atop the back of the cingulate.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> In studies of sleep, anaesthesia and people in comas, activity in these areas correlates with consciousness. The size of another hub, the superior frontal cortex, appears correlated with impulsivity and attention. Lesions of a fourth hub, located in the parietal cortex on the side <a id="page_266"></a>of the brain, cause patients to lose a sense of direction. In total, the population of hubs appears diverse in both location and function. The connective thread, if there is any between them, is just how complicated each one is. Regions of the brain like the visual cortex, auditory cortex and olfactory bulb – regions with clear and identifiable roles present right there in their names – don’t make it on to the list of hubs. Hub regions are complex; they pull in information from multiple sources and spread it out just as far. Their role as integrators seems a clear result of their placement in the network architecture. </p>
<p class="TXI">Hubs, in addition to integrating information in distinctive ways, may also be responsible for setting the brain’s clock. In the CA3 region (the memory warehouse of the hippocampus mentioned in <a href="chapter4.xhtml#chapter4">Chapter 4</a>), waves of electrical activity sweep through the neural population in the early days of development after birth. These waves ensure that the activity of the neurons and the strength of their connections are established correctly. And hub neurons are the likely coordinators of this important synchronised activity; they tend to start firing before these waves and stimulating them can instigate a wave. Other studies have even posited a role for hub regions in synchronising activity across the whole brain. Because of their high degree, a message from a hub is heard far and wide in the network. Furthermore, hubs in the brain tend to be strongly interconnected with each other, a network feature referred to as a ‘rich club’. Such connections can ensure all the hubs are themselves on the same page as they send out their synchronising signals. </p>
<p class="TXI">Even the way hub neurons develop points to their special place in the brain. The neurons that go on to form <a id="page_267"></a>the rich club in the roundworm, for example, are some of the first to appear as the nervous system grows. In just eight hours after an egg is fertilised, all of these hub neurons will be born; the rest of the nervous system won’t be finished until over a day later. Similarly, in humans, much of the basic hub structure is present in infants. </p>
<p class="TXI">If hubs are so central to brain function, what role might they play in <span class="italic">dys</span>function? Danielle Bassett explored this question as part of her far-ranging career at the intersection of networks and neuroscience.</p>
<p class="TXI">In the early 2000s, when the methods of graph theory were first being unfurled across the field of neuroscience, Bassett was a college student studying physics. At that time, it might have been a surprise to her to hear she’d go on to be called the ‘doyenne of network science’ by a well-known neuroscientist.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> Though working towards a physics degree was itself already somewhat surprising given her upbringing: Bassett was one of 11 children home-schooled in a religious family where women were expected to play more traditional roles. Her transition to neuroscience came during her PhD, when she worked with Edward Bullmore, a neuropsychiatrist at Cambridge University who was part of an initial wave of neuroscientists eager to apply graph theory to the brain. One of Bassett’s first projects was to see how the structure of the brain is affected by the common and crippling mental disorder schizophrenia. </p>
<p class="TXI">Schizophrenia is a disease characterised by delusions and disordered thought. Comparing the brains of <a id="page_268"></a>people with the disease to those without, Bassett found several differences in their network properties, including in the hubs. Regions in the frontal cortex that form hubs in healthy people, for example, don’t do so in schizophrenics. A disruption to the frontal cortex and its ability to rein in and control other parts of the brain could relate to the hallucinations and paranoia schizophrenia can elicit. And while the brain of a schizophrenic is still a small world, both the average path length and the strength of clustering are higher than in healthy people, making it seemingly more difficult for two disparate areas to communicate and get on the same page. </p>
<p class="TXI">As the first study to approach this disease from the perspective of graph theory, this work helped bring an old idea about ‘disconnection syndromes’ into the quantitative age. As early as the late nineteenth century, neurologists hypothesised that a disruption in anatomical connections could lead to disorders of thought. German physician Carl Wernicke in particular believed that higher cognitive functions did not reside in any single brain region, but rather emerged from interactions between them. As he wrote in 1885: ‘Any higher psychic process … rested on the mutual interaction of … fundamental psychic elements mediated by means of their manifold connections via the association fibres.’ Lesioning these ‘association fibres’, he posited, would impair complex functions like language, awareness and planning. </p>
<p class="TXI">Now that the tools of graph theory have met the study of ‘disconnection syndromes’, more diseases of this kind are being explored with modern approaches. One <a id="page_269"></a>common example is Alzheimer’s disease. When the brain-wide connectivity of older patients with Alzheimer’s was compared to those without it, it was found that those with Alzheimer’s had longer path lengths between brain areas. The confusion and cognitive impairment of Alzheimer’s disease may result, in part, from the breakdown of efficient communication between distant brain regions. Similar changes in brain network structures are seen to a lesser extent with normal ageing. </p>
<p class="TXI">Since starting her own lab at the University of Pennsylvania in 2013, Bassett has moved on from just observing the structure of the brain in health and disease to figuring out how to exploit it. The activity of complex networks can be hard to predict. A rumour whispered to a friend could die out immediately or spread across your social network, depending on the structure of that network and your friend’s place within it. The effects of stimulating or silencing neurons can be equally hard to anticipate. The Bassett lab is combining tools from engineering with knowledge about the structure of brain networks to make control of neural activity more tractable. In particular, models based on the brain-wide connectomes of individual people were used to determine exactly where brain stimulation should be applied in order to have the desired effect. The aim is to use this individualised treatment to get disorders like Parkinson’s disease and epilepsy under control. </p>
<p class="TXI">The hope that the metrics of graph theory may serve as markers of disease – potentially even early markers that could lead to preventative care – has made them quite popular in medical research. Thus far, the brain <a id="page_270"></a>disorders scrutinised by network analysis include Alzheimer’s, schizophrenia, traumatic brain injury, multiple sclerosis, epilepsy, autism and bipolar disorder. Results, however, have been mixed. As was pointed out, the MRI technique for collecting the data in the first place has its problems and some studies find disease signatures that others don’t. Overall, with so many excited scientists on the hunt for differences between diseased and healthy brains, some false positives and faulty data are bound to get caught up in the findings. But whether the results are solidified yet or not, it’s safe to say this new slate of tools has made an arrival on the clinical scene. </p>
<p class="center">* * *</p>
<p class="TXT">A developing brain is an eruption. Neurons bubble out of a neuronal nursery called the ventricular zone at a breakneck pace and pour into all corners of the burgeoning brain. Once there, they start making connections. These indiscriminate neurons form synapse after synapse with each other, frenetically linking cells near and far. At the height of synapse building in the human brain – during the third trimester of pregnancy – 40,000 such connections are constructed every second. Development is an explosion of neuronal and synaptic genesis. </p>
<p class="TXI">But just as soon as they come, many of these cells and connections go. An adult has far fewer neurons than they had in the womb; as many as half the neurons produced during development die. The number of <a id="page_271"></a>connections a neuron in the cortex makes peaks around the first year of life and gets reduced by a third thereafter. The brain is thus built via a surge and a retreat, a swelling and a shrinking. During development, the pruning of neurons and synapses is ruthless; only the useful survive. Synapses, for example, are built to carry signals between neurons. If no signal flows, the synapse must go. Out of this turmoil and turnover emerge operational neural circuits. It’s like encouraging the overgrowth of shrubbery for the purpose of carving delicate topiary out of it.</p>
<p class="TXI">Such is the way biology found to build the brain. But if you asked a graph theorist how to make a network, they’d give the exact opposite answer. The designer of a public transit system, for example, wouldn’t start by building a bunch of train stations and bus stops and connecting them all up just to see what gets used. No government would approve such a waste of resources. Rather, most graphs are built from the bottom up. For example, one strategy graph theorists use is to first build a graph that – using the fewest edges possible – has a path between any two nodes. This means some paths may be quite long, but by observing what paths get used the most (by commuters on a train or information travelling between servers on the internet) the network designer can identify where it would be useful to add a shortcut. Thus, the network gets more efficient by adding well-placed edges. </p>
<p class="TXI">The brain, however, doesn’t have a designer. There is no central planner that can look down and say: ‘It looks like signals would flow better if that neuron over there <a id="page_272"></a>was connected to this one here.’<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> That is why the brain needs to overproduce and prune. The only way the brain can make decisions about which connections should exist is by calculating the activity that passes through those connections. Individual neurons and synapses have elaborate molecular machinery for measuring how much use they’re getting and growing or shrivelling as a result. If the connection doesn’t exist in the first place, though, the activity on it can’t be measured.</p>
<p class="TXI">Pruning of connections in the brain starts off very strong, with synapses getting slashed left and right, but it then slows down over time. In 2015, scientists at the Salk Institute and Carnegie Mellon University explored why this pattern of pruning may be beneficial to the brain. To do so, they simulated networks that started overgrown and were pruned via a ‘use it or lose it’ principle. Importantly, they varied just how quickly this pruning happened. They found that networks that mimicked the pruning process in the brain (with the rate of pruning high initially and lowering over time) ended up with short average path lengths and were capable of effectively routing information even if some nodes or edges were deleted. This efficiency and robustness wasn’t as strong in networks where the pruning rate was constant, or increased, over time. A decreasing pruning rate, it seems, has the benefit of quickly eliminating useless connections while still <a id="page_273"></a>allowing the network enough time to fine-tune the structure that remains; a sculptor working with marble, similarly, may be quick to cut out the basic shape of a man, but carving the fine details of the body is a slow and careful process. While most physical networks like roads or telephone lines will never be built based on pruning, digital networks that don’t have costs associated with constructing edges – such as those formed by the wireless communication between mobile devices – could benefit from brain-inspired algorithms. </p>
<p class="center">* * *</p>
<p class="TXT">Network neuroscience, the name given to this practice of using the equipment of graph theory and network science to interrogate the structures of the brain, is a young field. <span class="italic">Network Neuroscience</span>, the first academic journal dedicated solely to this endeavour, was first published in 2017. New tools for mapping out connectomes at multiple scales have coincided with the computational power to analyse larger and larger datasets. The result is an electrified environment, with ever more and diverse studies of structure conducted every day. </p>
<p class="TXI">A reason for caution, however, may be found in the stomach of a lobster.</p>
<p class="TXI">The stomatogastric ganglion is a circuit of 25–30 neurons located in the gut of lobsters and other crustaceans. These neurons conspire through their connections to perform a basic but crucial job: produce the rhythmic muscle contractions that guide digestion. Eve Marder, a professor at Brandeis University in <a id="page_274"></a>Massachusetts, has spent half a century studying this handful of neurons. </p>
<p class="TXI">Marder was born and raised in New York but her education took her to Massachusetts and then California.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> While her doctorate work at the University of California, San Diego, was solidly in neuroscience, Marder always had an aptitude for mathematics: in primary school, she worked her way through maths textbooks meant for students two years her senior. This polymath personality permeates her science. Throughout her career she has collaborated with researchers from many backgrounds, including Larry Abbott (mentioned in <a href="chapter1.xhtml#chapter1">Chapter 1</a>), as he was making his transition from a particle physicist to renowned theoretical neuroscientist. Blending experimental exactness with a mathematical mindset, Marder has thoroughly probed the functioning of this little lobster circuit both physically and in computer simulations.</p>
<p class="TXI">The connectome of the lobster stomatogastric ganglion has been known since the 1980s. The 30 neurons of this ganglion form 195 connections and send outputs to the muscles of the stomach. For her PhD, Marder worked out what chemicals these neurons use to communicate. In addition to standard neurotransmitters – the chemicals that traverse the small synaptic cleft between the neuron releasing them and the one receiving <a id="page_275"></a>them – Marder also found a panoply of neuromodulators at play. </p>
<p class="TXI">Neuromodulators are chemicals that fiddle with the settings of a neural circuit. They can turn the strength of connections between neurons up or down and make neurons fire more, less or in different patterns. Neuromodulators effect these changes by latching on to receptors embedded in a neuron’s cell membrane. Part of what’s noteworthy about neuromodulators is where they come from and how they get to the neuron. In the most extreme case, a neuromodulator could be released from a different part of the brain or body and travel through the blood to its destination. Other times, a neuromodulator is released locally from nearby neurons – but whether from near or afar, neuromodulators tend to bathe a circuit indiscriminately, touching many neurons and synapses in a diffuse way. Whereas regular neurotransmission is like a letter sent between two neurons, neuromodulation is a leaflet sent out to the whole community.</p>
<p class="TXI">In the 1990s, Marder, along with members of her lab and the lab of Michael Nusbaum, a professor at the University of Pennsylvania, experimented with neuromodulators in the stomatogastric ganglion circuit. Ordinarily, the circuit produces a steady rhythm, with certain neurons in the population firing about once per second. But, when the experimenters released neuromodulators on to the circuit, this behaviour changed. Some neuromodulators made the rhythm increase: the same neurons fired, but more frequently. Others made the rhythm decrease, and some had a more dramatic effect, both disrupting the rhythm and <a id="page_276"></a>activating neurons that were ordinarily silent. The neuromodulators causing these changes were all released from neurons that normally provide input to this circuit. This means these different patterns of output are plausibly produced naturally throughout the animal’s life. In more artificial settings, neuromodulators added by the experimenters can cause even larger and more diverse changes. </p>
<p class="TXI">Importantly, throughout these experiments the underlying network never changed. No neurons were added or deleted, nor did they cut or grow any connections. The marked changes in behaviour stemmed solely from a small sprinkling of neuromodulators atop a steady structure. </p>
<p class="TXI">The massive effort poured into getting a connectome presupposes a certain amount of payoff that will come from having it, but the payoff is less if the structure-function relationship is looser than it may have seemed. If neuromodulators can release the activity of neurons in a circuit from the strict constraints of their architecture, then structure is not destiny. Perhaps this wouldn’t be such a concern if neuromodulation were a phenomenon specific to the stomatogastric ganglion. This, however, is far from the truth. Brains are constantly bathing in modulating molecules. Across species, neuromodulators are responsible for everything from sleeping to learning, moulting to eating. Neuromodulation is the rule, not the exception. </p>
<p class="TXI">Through mathematical simulations of the circuits she studies, Marder has explored not just how different behaviours arise from the same structure, but also how <a id="page_277"></a>
<span class="italic">different</span> structures can produce the <span class="italic">same</span> behaviours. Specifically, each lobster has a slightly different configuration of its gut circuitry: connections may be built stronger or weaker in one animal versus another. By simulating as many as 20 million possible ganglion circuits, Marder’s lab found that the vast majority aren’t capable of producing the needed rhythms, but certain specific configurations are. Each lobster, through some combination of genes and development, finds its way to one of these functioning configurations. The work makes an important point about individual brains: diversity doesn’t always mean difference. What may look like a deviation from the structural norm could in fact be a perfectly valid way to achieve the same outcomes. That these diverse structures create the same rhythms adds another wrinkle to the structure-function relationship.</p>
<p class="TXI">As much as Marder’s work shows the limits of structure for understanding function, it also shows the need for it. Her lifetime of work – and all the insights it has provided – is built atop the connectome. Without that detailed structural information, there is no structure-function relationship to be explored. As Marder wrote in 2012: ‘Detailed anatomical data are invaluable. No circuit can be fully understood without a connectivity diagram.’ However, she goes on to remark that ‘a connectivity diagram is only a necessary beginning, but not in itself, an answer’. In other words, when it comes to understanding the brain, knowing the structure of the nervous system is both completely necessary and utterly insufficient.</p><a id="page_278"></a>
<p class="TXI">So, it may not be possible to satisfy Cajal’s vision of intuiting the function of the nervous system from mere meditations on its structure. But the work of finding and formalising that structure is still an important prerequisite for any further understanding of the brain. Innovative methods for gathering connectome data are blossoming and the formalisms of graph theory are in place, prepared to take in and digest that data.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-1" id="fn-1">1</a> ﻿The images were attractive enough to be the centrepiece of a travelling art exhibition called ﻿<span class="italic">The Beautiful Brain</span>﻿, a fate that surely would﻿’﻿ve pleased Cajal. Before succumbing to his father﻿’﻿s wishes that he be a physician, Cajal dreamt of being an artist.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-2" id="fn-2">2</a> ﻿Milgram﻿’﻿s experiment has been criticised by later researchers for lacking rigour. He didn﻿’﻿t, for example, take into account the folders that never made it to the target person. As a result, whether six really is the magic number when it comes to degrees of separation between people has remained an open question. Luckily, data from social media sites is offering new ways to answer it. ﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-3" id="fn-3">3</a> ﻿There are currently some successful attempts to automate this arduous process. In the meantime, desperate scientists have also tried to turn this work into a game and get people all over the world to play it. It can be found at ﻿﻿eyewire.org﻿﻿.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-4" id="fn-4">4</a> ﻿A note of caution: precisely what features a brain area needs to have to be considered a hub is debated. And even applying the same definition to different datasets can lead to different conclusions. As it is still early days for this style of analysis, such kinks will take time to work out.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-5" id="fn-5">5</a> ﻿British neuroscientist Karl Friston bestowed this title on Bassett in a 2019 interview in ﻿<span class="italic">Science</span>﻿. We﻿’﻿ll hear more about Friston in ﻿﻿Chapter 12﻿﻿.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-6" id="fn-6">6</a> ﻿There may be an exception to this in very simple animals, such as ﻿<span class="italic">C. elegans</span>﻿, where it﻿’﻿s believed a lot of the information about who should connect to whom is encoded in the genome and is thus ﻿‘﻿designed﻿’﻿ through eons of natural selection.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-7" id="fn-7">7</a> ﻿Marder entered graduate school in 1969, a time at which women were becoming a more common sight in these programmes, but barriers still existed. As she recounts in her autobiography, ﻿‘﻿I knew it unlikely that I would get into Stanford biology because they were widely said to have a quota on women (2 out of 12).﻿’﻿﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 1</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter1"><a href="contents.xhtml#re_chapter1">CHAPTER ONE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter1">Spherical Cows</a><a id="page_7"></a></p>
<p class="H1" id="b-9781472966445-ch58-sec1">
<span class="bold">
<span>What mathematics has to offer</span>
</span></p>
<p class="TXT">The web-weaving spider <span class="italic">Cyclosa octotuberculata</span> inhabits several locations in and around Japan. About the size of a fingernail and covered in camouflaging specks of black, white and brown, this arachnid is a crafty predator. Sitting at the hub of its expertly built web, it waits to feel vibrations in the web’s threads that are caused by struggling prey. As soon as the spider senses this movement, it storms off in the direction of the signal, ready to devour its catch.</p>
<p class="TXI">Sometimes prey is more commonly found in one location on the web than others. Smart predators know to keep track of these regularities and exploit them. Certain birds, for example, will recall where food has been abundant recently and return to those areas at a later time. <span class="italic">Cyclosa octotuberculata</span> does something similar – but not identical. Rather than remembering the locations that have fared well – that is, rather than storing these locations in its mind and letting them influence its future attention – the spider literally weaves this information into its web. In particular, it uses its legs to tug on the specific silk threads from which prey has recently been detected, making them tighter. The tightened threads are more sensitive to vibrations, making future prey easier to detect on them.</p><a id="page_8"></a>
<p class="TXI">Making these alterations to its web, <span class="italic">Cyclosa octotuberculata</span> offloads some of the burden of cognition to its environment. It expels its current knowledge and memory into a compact yet meaningful physical form, making a mark on the world that can guide its future actions. The interacting system of the spider and its web is smarter than the spider could hope to be on its own. This outsourcing of intellect to the environment is known as ‘extended cognition’. </p>
<p class="TXI">Mathematics is a form of extended cognition.</p>
<p class="TXI">When a scientist, mathematician or engineer writes down an equation, they are expanding their own mental capacity. They are offloading their knowledge of a complicated relationship on to symbols on a page. By writing these symbols down, they leave a trail of their thinking for others and for themselves in the future. Cognitive scientists hypothesise that spiders and other small animals rely on extended cognition because their brains are too limited to do all the complex mental tasks required to thrive in their environment. We are no different. Without tools like mathematics our ability to think and act effectively in the world is severely limited. </p>
<p class="TXI">Mathematics makes us better in some of the same ways written language does. But mathematics goes beyond everyday language because it is a language that can do real work. The mechanics of mathematics – the rules for rearranging, substituting and expanding its symbols – are not arbitrary. They are a systematic way to export the process of thinking to paper or machines. Alfred Whitehead, a revered twentieth-century mathematician whose work we will encounter in <a href="chapter3.xhtml#chapter3">Chapter 3</a>, has been paraphrased as saying: ‘The ultimate <a id="page_9"></a>goal of mathematics is to eliminate any need for intelligent thought.’ </p>
<p class="TXI">Given this useful feature of mathematics, some scientific subjects – physics chief among them – have developed an ethos centred on rigorous quantitative thinking. Scientists in these fields have capitalised on the power of mathematics for centuries. They know that mathematics is the only language precise and efficient enough to describe the natural world. They know that the specialised notation of equations expertly compresses information, making an equation like a picture: it can be worth a thousand words. They also know that mathematics keeps scientists honest. When communicating through the formalism of mathematics, assumptions are laid bare and ambiguities have nowhere to hide. In this way, equations force clear and coherent thinking. As Bertrand Russell (a colleague of Whitehead whom we will also meet in <a href="chapter3.xhtml#chapter3">Chapter 3</a>) wrote: ‘Everything is vague to a degree you do not realise till you have tried to make it precise.’ </p>
<p class="TXI">The final lesson that quantitative scientists have learnt is that the beauty of mathematics lies in its ability to be both specific and universal. An equation can capture exactly how the pendulum in the barometrical clock on the Ministers’ Landing at Buckingham Palace will swing; the very same equation describes the electrical circuits responsible for broadcasting radio stations around the world. When an analogy exists between underlying mechanisms, equations serve as the embodiment of that analogy. As an invisible thread tying together disparate topics, mathematics is a means by which advances in one field can have surprising and disproportionate impacts on other, far-flung areas.</p><a id="page_10"></a>
<p class="TXI">Biology – including the study of the brain – has been slower to embrace mathematics than some other fields. A certain portion of biologists, for reasons good and bad, have historically eyed mathematics with some scepticism. In their opinion, mathematics is both too complex and too simple to be of much use.</p>
<p class="TXI">Some biologists find mathematics too complex because – trained as they are in the practical work of performing lab experiments and not in the abstract details of mathematical notion – they see lengthy equations as meaningless scribble on the page. Without seeing the function in the symbols, they’d rather do without them. As biologist Yuri Lazebnik wrote in a 2002 plea for more mathematics in his field: ‘In biology, we use several arguments to convince ourselves that problems that require calculus can be solved with arithmetic if one tries hard enough and does another series of experiments.’</p>
<p class="TXI">Yet mathematics is also considered too simple to capture the overwhelming richness of biological phenomena. An old joke among physicists highlights the sometimes absurd level of simplification that mathematical approaches can require. The joke starts with a dairy farmer struggling with milk production. After trying everything he could think to get his beloved cows to produce more, he decides to ask the physicist at the local university for help. The physicist listens carefully to the problem and goes back to his office to think. After some consideration, he comes back to the farmer and says: ‘I found a solution. First, we must assume a spherical cow in a vacuum … ’ </p>
<p class="TXI">Simplifying a problem is what opens it up to mathematical analysis, so inevitably some biological details get lost in <a id="page_11"></a>translation from the real world to the equations. As a result, those who use mathematics are frequently disparaged as being too disinterested in those details. In his 1897 book <span class="italic">Advice for a Young Investigator</span>, Santiago Ramón y Cajal (the father of modern neuroscience whose work is discussed in <a href="chapter9.xhtml#chapter9">Chapter 9</a>) wrote about these reality-avoiding theorists in a chapter entitled ‘Diseases of the Will’. He identified their symptoms as ‘a facility for exposition, a creative and restless imagination, an aversion to the laboratory, and an indomitable dislike for concrete science and seemingly unimportant data’. Cajal also lamented the theorist’s preference for beauty over facts. Biologists study living things that are abundant with specific traits and nuanced exceptions to any rule. Mathematicians – driven by simplicity, elegance and the need to make things manageable – squash that abundance when they put it into equations. </p>
<p class="TXI">Oversimplification and an obsession with aesthetics are legitimate pitfalls to avoid when applying mathematics to the real world. Yet, at the same time, the richness and complexity of biology is exactly why it needs mathematics.</p>
<p class="TXI">Consider a simple biological question. There are two types of animals in a forest: rabbits and foxes. Foxes eat rabbits, rabbits eat grass. If the forest starts off with a certain number of foxes and a certain number of rabbits, what will happen to these two populations? </p>
<p class="TXI">Perhaps the foxes ferociously gobble up the rabbits, bringing them to extinction. But then the foxes, having exhausted their food source, will starve and die off themselves. This leaves us with a rather empty forest. On the other hand, maybe the fox population isn’t so <a id="page_12"></a>ravenous. Perhaps they reduce the rabbit population to almost zero but not quite. The fox population still plummets as each individual struggles to find the remaining rabbits. But then – with most of the foxes gone – the rabbit population can rebound. Of course, now the food for the foxes is abundant again and, if enough of their population remains, they too can resurge.</p>
<p class="TXI">When it comes to knowing the outcome for the forest, there is a clear limitation to relying on intuition. Trying to ‘think through’ this scenario, as simple as it is, using just words and stories is insufficient. To make progress, we must define our terms precisely and state their relationships exactly – and that means we’re doing mathematics. </p>
<p class="TXI">In fact, the mathematical model of predator–prey interactions that can help us here is known as the Lotka–Volterra model and it was developed in the 1920s. The Lotka-Volterra model consists of two equations: one that describes the growth of the prey population in terms of the numbers of prey and predators, and another that describes the growth of the predator population in terms of the numbers of predators and prey. Using dynamical systems theory – a set of mathematical tools initially forged to describe the interactions of celestial bodies – these equations can tell us whether the foxes will eventually die off, or the rabbits will, or if they’ll carry on in this dance together forever. In this way, the use of mathematics makes us better at understanding biology. Without it, we are sadly limited by our own innate cognitive talents. As Lazebnik wrote: ‘Understanding 
[a complex] system without formal analytical tools requires geniuses, who are so rare even outside biology.’</p><a id="page_13"></a>
<p class="TXI">To look at a bit of biology and see how it can be reduced to variables and equations requires creativity, expertise and discernment. The scientist must see through the messy details of the real world and find the bare-bones structure that underlies it. Each component of their model must be defined appropriately and exactly. Once a structure is found and an equation written, however, the fruits of this discipline are manifest. Mathematical models are a way to describe a theory about how a biological system works precisely enough to communicate it to others. If this theory is a good one, the model can also be used to predict the outcomes of future experiments and to synthesise results from the past. And by running these equations on a computer, models provide a ‘virtual laboratory’, a way to quickly and easily plug in different values to see how different scenarios may turn out and even perform ‘experiments’ not yet feasible in the physical world. By working through scenarios and hypotheses digitally this way, models help scientists determine what parts of a system are important to its function and, importantly, which are not. </p>
<p class="TXI">Such integral work could hardly be carried out using simple stories unaccompanied by mathematics. As Larry Abbott, a prominent theoretical neuroscientist and co-author<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> of one of the most widely used textbooks on the subject, explained in a 2008 article:</p>
<p class="EXT">
<span class="italic">Equations force a model to be precise, complete and self-consistent, and they allow its full implications to be worked out. It is not difficult to find word models in the conclusions sections of older <a id="page_14"></a>neuroscience papers that sound reasonable but, when expressed as mathematical models, turn out to be inconsistent and unworkable. Mathematical formulation of a model forces it to be self-consistent and, although self-consistency is not necessarily truth, self-inconsistency is certainly falsehood.</span></p>
<p class="TXT">The brain – composed of (in the case of humans) some 100 billion neurons, each their own bubbling factory of chemicals and electricity, all interacting in a jumble of ways with their neighbours both near and far – is a prime example of a biological object too complex to be understood without mathematics. The brain is the seat of cognition and consciousness. It is responsible for how we feel, how we think, how we move, who we are. It is where days are planned, memories are stored, passions are felt, choices are made, words are read. It is the inspiration for artificial intelligence and the source of mental illness. To understand how all this can be accomplished by a single complex of cells, interfacing with a body and the world, demands mathematical modelling at multiple levels. </p>
<p class="TXI">Despite the hesitancy felt by some biologists, mathematical models can be found hidden in all corners of the history of neuroscience. And while it was traditionally the domain of adventurous physicists or wandering mathematicians, today ‘theoretical’ or ‘computational’ neuroscience is a fully developed subdivision of the neuroscience enterprise with dedicated journals, conferences, textbooks and funding sources. The mathematical mindset is influencing the whole of the study of the brain. As Abbott wrote: ‘Biology used to be a refuge for students fleeing mathematics, but now many <a id="page_15"></a>life sciences students have a solid knowledge of basic mathematics and computer programming, and those that don’t at least feel guilty about it.’<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> </p>
<p class="TXI">Yet the biologist’s apprehension around mathematical models should not be entirely dismissed. ‘All models are wrong,’ starts the popular phrase by statistician George Box. Indeed, all models <span class="italic">are</span> wrong, because all models ignore some details. All models are also wrong because they represent only a biased view of the processes they claim to capture. And all models are wrong because they favour simplicity over absolute accuracy. All models are wrong the same way all poems are wrong; they capture an essence, if not a perfect literal truth. ‘All models are wrong but some are useful,’ says Box. If the farmer in the old joke reminded the physicist that cows are not, in fact, spherical, the physicist’s response would be, ‘Who cares?’, or more accurately, ‘Do we need to care?’. Detail for detail’s sake is not a virtue. A map the size of the city has no good use. The art of mathematical modelling is in deciding which details matter and steadfastly ignoring those that do not. </p>
<p class="TXI">This book charts the influence of mathematical thinking – borrowed from physics, engineering, statistics and computer science – on the study of the brain. Each chapter tells, for a different topic in neuroscience, the story of the biology, the mathematics and the interplay between the two. No special knowledge of mathematics is assumed <a id="page_16"></a>on the part of the reader; the ideas behind the equations will be explained.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> No single theory of the brain is being proposed; different models solve different problems and offer complementary approaches to understanding. </p>
<p class="TXI">The chapters are ordered from the low to the high level: from the physics of single cells up to the mathematics of behaviour. The stories in these chapters include the struggles encountered in unifying mathematics and biology, and the scientists who did the struggling. They show that sometimes experiments inform models and sometimes models inform experiments. They also show that a model can be anything from a few equations confined to a page to countless lines of code run on supercomputers. In this way, the book is a tapestry of the many forms mathematical models of the brain can take. Yet while the topics and models covered are diverse, common themes do reappear throughout the pages. </p>
<p class="TXI">Of course, everything in this book may be wrong. It may be wrong because it is science and our understanding of the world is ever-evolving. It may be wrong because it is history and there is always more than one way to tell a story. And, most importantly, it <span class="italic">is</span> wrong because it is mathematics. Mathematical models of the mind do not make for perfect replicas of the brain, nor should we strive for them to be. Yet in the study of the most complex object in the known universe, mathematical models are not just useful but absolutely essential. The brain will not be understood though words alone.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter1.xhtml#fnt-1" id="fn-1">1</a> ﻿Along with Peter Dayan, whom we will meet in ﻿﻿Chapter 11﻿﻿.﻿</p>
<p class="FN1"><a href="chapter1.xhtml#fnt-2" id="fn-2">2</a> ﻿This guilt may not be entirely new. Charles Darwin, certainly a successful biologist, wrote in an 1887 autobiography: ﻿‘﻿I have deeply regretted that I did not proceed far enough at least to understand something of the great leading principles of mathematics, for men thus endowed seem to have an extra sense.﻿’﻿ ﻿</p>
<p class="FN1"><a href="chapter1.xhtml#fnt-3" id="fn-3">3</a> ﻿However, for the mathematically inclined, an appendix elaborating on one of the main equations per chapter is provided at the end of the book. ﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 4</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter4"><a href="contents.xhtml#re_chapter4">CHAPTER FOUR</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter4">Making and Maintaining Memories</a><a id="page_83"></a></p>
<p class="H1" id="b-9781472966445-ch354-sec4">
<span class="bold">
<span>The Hopfield network and attractors</span>
</span></p>
<p class="TXT">A block of iron at 770°C (1,418°F) is a sturdy grey mesh. Each of its trillions of atoms serves as a single brick in the endless parallel walls and ceilings of its crystalline structure. It is a paragon of orderliness. In opposition to their organised structural arrangement, however, the magnetic arrangement of these atoms is a mess. </p>
<p class="TXI">Each iron atom forms a dipole – a miniature magnet with one positive and one negative end. Heat unsteadies these atoms, flipping the direction of their poles around at random. On the micro-level this means many tiny magnets each exerting a force in its own direction. But as these forces work against each other, their net effect becomes negligible. When you zoom out, this mass of mini-magnets has no magnetism at all.</p>
<p class="TXI">As the temperature dips below 770°C, however, something changes. The direction of an individual atom is less likely to switch. With its dipole set in place, the atom starts to exert a constant pressure on its neighbours. This indicates to them which direction they too should be facing. Atoms with different directions vie for influence over the local group until eventually everyone falls in line, one way or the other. With all the small dipoles aligned, the net force is strong. The previously inert block of iron becomes a powerful magnet.</p> <a id="page_84"></a>
<p class="TXI">Philip Warren Anderson, an American physicist who won a Nobel Prize working on such phenomena, wrote in a now-famous essay entitled ‘More is different’ that ‘the behaviour of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles’. That is, the collective action of many small particles – organised only through their local interactions – can produce a function not directly possible in any of them alone. Physicists have formalised these interactions as equations and successfully used them to explain the behaviour of metals, gases and ice. </p>
<p class="TXI">In the late 1970s, a colleague of Anderson’s, John J. Hopfield, saw in these mathematical models of magnetism a structure akin to that of the brain. Hopfield used this insight to bring under mathematical control a long-lasting mystery: the question of how neurons make and maintain memories.</p>
<p class="center">* * *</p>
<p class="TXT">Richard Semon was wrong.</p>
<p class="TXI">A German biologist working at the turn of the twentieth century, Semon wrote two lengthy books on the science of memory. They were filled with detailed descriptions of experimental results, theories and a vocabulary for describing memory’s impact on ‘organic tissue’. Semon’s work was insightful, honest and clear – but it contained a major flaw. Just as French naturalist Jean-Baptiste Lamarck believed (in contrast to our current understanding of evolution) that traits acquired by an animal in its lifetime could be passed to its offspring, <a id="page_85"></a>Semon proposed that <span class="italic">memories</span> acquired by an animal could be passed down. That is, he believed that an organism’s learned responses to its own environment would arise without instruction in its offspring. As a result of this mistaken intuition, much of Semon’s otherwise valuable work was slowly cast aside and forgotten.</p>
<p class="TXI">Being wrong about memory isn’t unusual. Philosopher René Descartes, for example, thought memories were activated by a small gland directing the flow of ‘animal spirits’. What’s unique about Semon is that, despite the flaw in his work that sentenced him to historical obscurity, one of his contributions remained influential long enough to spawn an entire body of research. This small artefact of his efforts is the ‘engram’ – a word coined by Semon in <span class="italic">The</span>
<span class="italic">Mneme</span> in 1904, and subsequently learned by millions of students of psychology and neuroscience.</p>
<p class="TXI">At the time Semon was writing, memory had only recently come under scientific scrutiny – and most of the results were purely about memorisation skills, not about biology. For example, people would be trained to memorise pairs of nonsense words (such as ‘wsp’ and ‘niq’) and then were tested on their ability to retrieve the second word when prompted with the first. This type of memory, known as <span class="italic">associative</span> memory, would become a target of research for decades to come. But Semon was interested in more than just behaviour; he wanted to know what changes in an animal’s physiology could support such associative memories.</p>
<p class="TXI">Led by scant experimental data, he broke up the process of creating and recovering memories into multiple components. Finding common words too <a id="page_86"></a>vague and overloaded, he created novel terms for these divisions of labour. The word that would become so influential, the engram, was defined as ‘the enduring though primarily latent modification in the irritable substance produced by a stimulus’. Or, to put it more plainly: the physical changes in the brain that happen when a memory is formed. Another term, ‘ecphory’, was assigned to the ‘influences which awake the mnemic trace or engram out of its latent state into one of manifested activity’. This distinction between engram and ecphory (or between the processes that lay a memory and those that retrieve it) was one of the many conceptual advances that Semon’s work provided. Despite the fact that his name and most of his language have disappeared from the literature, many of Semon’s conceptual insights were correct and they form the core of how memories are modelled today.</p>
<p class="TXI">In 1950, American psychologist Karl Lashley published ‘In search of the engram’, a paper that solidified the legacy of the word. It also set a rather dismal tone for the field. The paper was so titled because the search was all Lashley felt he had accomplished in 30 years of experiments. Lashley’s experiments involved training animals to make an association (for example, to react in a specific way when shown a circle versus an ‘X’) or learn a task, such as how to run through a particular maze. He would then surgically remove specific brain areas or connection pathways and observe how behaviour was impacted post-operatively. Lashley couldn’t find any area or pattern of lesions that reliably interfered with memory. He concluded that memories must thus somehow be distributed equally across the <a id="page_87"></a>brain, rather than in any single area. But based on some calculations about how many neurons could be used for a memory and the number of pathways between them, he was uncertain about how this was possible. His landmark article thus reads as something of a white flag, a surrendering of any attempt to draw conclusions about the location of memory in the face of a mass of inconsistent data. The physical nature of memory remained to Lashley as vexing as ever.</p>
<p class="TXI">At the same time, however, a former student of Lashley’s was developing his own theories on learning and memory. </p>
<p class="TXI">Donald Hebb, a Canadian psychologist whose early work as a school teacher grew his interest in the mind, was intent on making psychology a biological science. In his 1949 book, <span class="italic">The Organization of Behavior</span>, he describes the task of a psychologist as ‘reducing the vagaries of human thought to a mechanical process of cause and effect’. And in that book, he lays down the mechanical process he believed to be behind memory formation.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> Overcoming the limited, and sometimes misleading, physiological data available at the time, Hebb came to this principle about the physical underpinnings of learning largely through intuition. Yet it would go on to have huge empirical success. The principle, now known as Hebbian learning, is succinctly described by the phrase ‘neurons that fire together wire together’.</p> <a id="page_88"></a>
<p class="TXI">Hebbian learning describes what happens at the small junction between two neurons where one can send a signal to the other, a space called the synapse. Suppose there are two neurons, A and B. The axon from neuron A makes a synaptic connection on to the dendrite or cell body of neuron B (making it the ‘pre-synaptic’ neuron, and neuron B the ‘post-synaptic’ neuron, see Figure 7). In Hebbian learning, if neuron A repeatedly fires before neuron B, the connection from A to B will strengthen. A stronger connection means that the next time A fires it will be more effective in causing B to fire. In this way, activity determines connectivity and connectivity determines activity.</p>
<p class="image-fig" id="fig7.jpg">
<img alt="" src="Images/chapter-04-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 7</span>
</span></p>
<p class="TXI">Hebb’s approach, with its focus on the synapse, situates the engram as both local and global: local because a memory’s imprint occurs at the small gap where one neuron meets another, but global because these changes may be happening at synapses all across the brain. It also makes memory the natural consequence of experience: with pliable synapses, any activation of the brain has the potential to leave a trace. </p>
<p class="TXI">Lashley, a dutiful scientist intent on following the facts, accepted that the engram must be distributed based on his own experiments. But he found no satisfaction in <a id="page_89"></a>Hebb’s solution, which – though an enticing and elegant theory – was based more on speculation than on hard evidence. He turned down Hebb’s offer to be a co-author on the work.</p>
<p class="TXI">Lashley may not have supported Hebb’s ideas, but since the publication of his book countless experiments have. Sea slugs – foot-long slimy brown invertebrates with only about 20,000 neurons – became a creature of much study in this area, due to their ability to learn a very basic association. These shell-less slugs have a gill on their backs that, if threatened, can be quickly retracted for safe keeping. In the lab, a short electric shock will cause the gill to withdraw. If such a shock is repeatedly preceded by a harmless light touch, the slug will eventually start withdrawing in response to the touch alone, demonstrating an association between the touch and what’s expected to come next. It is the marine critter equivalent of learning to pair ‘wsp’ with ‘niq’. This association was shown, in line with Hebb’s theory of learning, to be mediated by a strengthening of the connections between the neurons that represent the touch and those that lead to the gill’s response. The change in behaviour was forged through a changing of connections. </p>
<p class="TXI">Hebbian learning has not just been observed; it’s been controlled as well. In 1999, Princeton researchers showed that genetically modifying proteins in the cell membrane that contribute to synaptic changes can control a mouse’s capacity for learning. Increasing the function of these receptors enhances the ability of mice to remember objects they’ve been shown before. Interfering with these proteins impairs it.</p> <a id="page_90"></a>
<p class="TXI">It is now established science that experience leads to the activation of neurons and that activating neurons can alter the connections between them. This story is accepted as at least a partial answer to the question of the engram. But, as Semon describes it, the engram itself is only part of the story of memory. Memory also requires <span class="italic">re</span>membering. How can this way of depositing memories allow for long-term storage and recall? </p>
<p class="center">* * *</p>
<p class="TXT">It was no real surprise that John J. Hopfield became a physicist. Born in 1933 to John Hopfield Sr, a man who made a name for himself in ultraviolet spectroscopy, and Helen Hopfield, who studied atmospheric electromagnetic radiation, Hopfield Jr grew up in a household where physics was as much a philosophy as it was a science. ‘Physics was a point of view that the world around us is, with effort, ingenuity and adequate resources, understandable in a predictive and reasonably quantitative fashion,’ Hopfield wrote in an autobiography. ‘Being a physicist is a dedication to a quest for this kind of understanding.’ And a physicist is what he would be.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> </p>
<p class="TXI">Hopfield, a tall and lanky man with an engaging smile, earned his PhD in 1958 from Cornell University. He further emulated his father by receiving a Guggenheim fellowship, using it to study at the Cavendish Laboratory <a id="page_91"></a>at Cambridge. But even by this stage, Hopfield’s enthusiasm for the subject of his PhD – condensed matter physics – was waning. ‘In 1968, I had run out of problems … to which my particular talents seemed useful,’ he later wrote.</p>
<p class="TXI">Hopfield’s gateway from physics to biology was hemoglobin, a molecule that both serves a crucial biological function as the carrier of oxygen in blood and could be studied with many of the techniques of experimental physics at the time. Hopfield worked on hemoglobin’s structure for several years at Bell Labs, but he found his real calling in biology after being invited to a seminar series on neuroscience in Boston in the late 1970s. There he encountered a variegated group of clinicians and neuroscientists, gathered together to address the deep question of how the mind emerges from the brain. Hopfield was captivated. </p>
<p class="TXI">Mathematically minded as he was, though, Hopfield was dismayed by the qualitative approach to the brain he saw on display. He was concerned that, despite their obvious talents in biology, these researchers, ‘would never possibly solve the problem because the solution can be expressed only in an appropriate mathematical language and structure’.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> This was a language that physicists had. Hopfield therefore made a point of using his physicist’s skillset even as he embarked on a study of memory. In his eyes, certain physicists of the time who made the leap <a id="page_92"></a>to biology had immigrated fully, taking on the questions, culture and vocabulary of their new land. He wanted to firmly retain his citizenship as a physicist.</p>
<p class="TXI">In 1982 Hopfield published ‘Neural networks and physical systems with emergent collective computational abilities’, which laid out the description and results of what is now known as the Hopfield network. This was Hopfield’s first paper on the topic; he was only dipping his toe into the field of neuroscience and yet it made quite the splash.</p>
<p class="image-fig" id="fig8.jpg">
<img alt="" src="Images/chapter-04-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 8</span>
</span></p>
<p class="TXI">The Hopfield network (see Figure 8) is a mathematical model of neurons that can implement what Hopfield described as ‘content-addressable memory’. This term, coming from computer science, refers to the notion that a full memory can be retrieved from just a small component of it. The network that Hopfield designed for this task is simply composed. It is made only of binary neurons (like the McCulloch-Pitts neurons introduced in the last chapter), which can be either ‘on’ or ‘off’. It is therefore the interactions between these neurons from which the intriguing behaviours of this network emerge. </p>
<p class="TXI">The Hopfield network is <span class="italic">recurrent</span>, meaning that each neuron’s activity is determined by that of any of the <a id="page_93"></a>others in the network. Therefore, each neuron’s activity serves as both input and output to its neighbours. Specifically, each input a neuron receives from another neuron is multiplied by a particular number – a synaptic weight. These weighted inputs are then added together and compared to a threshold: if the sum is greater than (or equal to) the threshold, the neuron’s activity level is 1 (‘on’), otherwise it’s 0 (‘off’). This output then feeds into the input calculations of the other neurons in the network, whose outputs feed back into more input calculations and so on and so on.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p>
<p class="TXI">Like bodies in a mosh pit, the components of a recurrent system push and pull on each other, with the state of a unit at any given moment determined by those that surround it. The neurons in a Hopfield network are thus just like the atoms of iron constantly influencing each other through their magnetic interactions. The effects of this incessant interaction can be myriad and complex. To predict the patterns these interlocking parts will generate is essentially impossible without the precision of a mathematical model. Hopfield was intimately familiar with these models and their ability to show how local interactions lead to the emergence of global behaviour. </p>
<p class="TXI">Hopfield found that if the weights between the neurons in his network are just right the network as a <a id="page_94"></a>whole can implement associative memory. To understand this, we must first define what counts as a memory in this abstract model. Imagine that each neuron in a Hopfield network represents a single object: neuron A is a rocking chair, neuron B is a bike, neuron C is an elephant, and so on. To represent a particular memory, say that of your childhood bedroom, the neurons that represent all the objects in that room – the bed, your toys, photographs on the wall – should be ‘on’; while those that represent objects not in that room – the moon, a city bus, kitchen knives – should be ‘off’. The network as a whole is then in the ‘your childhood bedroom’ activity state. A different activity state – with different sets of neurons ‘on’ or ‘off’ – would represent a different memory. </p>
<p class="TXI">In associative memory, a small input to the network reactivates an entire memory state. For example, seeing a picture of yourself on your childhood bed may activate some of the neurons that represent your bedroom: the bed neurons and pillow neurons, and so on. In the Hopfield network, the connections between these neurons and the ones that represent other parts of the bedroom – the curtains and your toys and your desk – cause these other neurons to become active, recreating the full bedroom experience. Negatively weighted connections between the bedroom neurons and those that represent, say, a local park, ensure that the bedroom memory is not infiltrated by other items. That way you don’t end up remembering a swing set next to your closet. </p>
<p class="TXI">As some neurons turn on and others off, it is their interactivity that brings the full memory into stark relief. <a id="page_95"></a>The heavy-lifting of memory is thus done by the synapses. It is the strength of these connections that carries out the formidable yet delicate task of memory retrieval. </p>
<p class="TXI">In the language of physics, a fully retrieved memory is an example of an <span class="italic">attractor</span>. An attractor is, in short, a popular pattern of activity. It is one that other patterns of activity will evolve towards, just as water is pulled down a drain. A memory is an attractor because the activation of a few of the neurons that form the memory will drive the network to fill in the rest. Once a network is in an attractor state, it remains there with the neurons fixed in their ‘on’ or ‘off’ positions. Always fond of describing things in terms of energy, physicists consider attractors ‘low energy’ states. They’re a comfortable position for a system to be in; that is what makes them attractive and stable. </p>
<p class="TXI">Imagine a trampoline with a person standing on it. A ball placed anywhere on the trampoline will roll towards the person and stay there. The ball being in the divot created by the person is thus an attractor state for this system. If two people of the same size were standing opposite each other on the trampoline, the system would have two attractors. The ball would roll towards whomever it was initially closest to, but all roads would still lead to an attractor. Memory systems wouldn’t be of much use if they could only store one memory, so it is important that the Hopfield network can sustain multiple attractors. The same way the ball is compelled towards the nearest low point on the trampoline, initial neural activity states evolve towards the nearest, most similar memory (see Figure 9). The initial states that lead to a specific memory attractor – for example, the picture <a id="page_96"></a>of your childhood bed that reignites a memory of the whole room or a trip to a beach that ignites the memory of a childhood holiday – are said to be in that memory’s ‘basin of attraction’.</p>
<p class="TXI">
<span class="italic">The Pleasures of Memory</span> is a 1792 poem by Samuel Rogers. Reflecting on the universal journey on which memory can take the mind, he wrote:</p>
<p class="EXTF">
<span class="italic">Lulled in the countless chambers of the brain,</span></p>
<p class="EXTM">
<span class="italic">Our thoughts are linked by many a hidden chain.</span></p>
<p class="EXTM">
<span class="italic">Awake but one, and lo, what myriads rise!</span></p>
<p class="EXTL">
<span class="italic">Each stamps its image as the other flies!</span></p>
<p class="TXT">Rogers’ ‘hidden chain’ can be found in the pattern of weights that reignite a memory in the Hopfield network. <a id="page_97"></a>Indeed, the attractor model aligns with much of our intuition about memory. It implicitly addresses the time it takes for memories to be restored, as the network needs time to activate the right neurons. Attractors can also be slightly displaced in the network, creating memories that are mostly correct, with a detail or two changed. And memories that are too similar may simply merge into one. While collapsing memory to a series of zeros and ones may seem an affront to the richness of our experience of it, it is the condensation of this seemingly ineffable process that puts an understanding of it within reach.</p>
<p class="image-fig" id="fig9.jpg">
<img alt="" src="Images/chapter-04-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 9</span>
</span></p>
<p class="TXI">In the Hopfield network, how robustly neurons are connected with each other defines which patterns of neural activity form a memory. The engram is therefore in the weights – but how does it get there? How can an experience create just the right weights to make a memory? Hebb tells us that memories should come out of a strengthening of the connections between neurons that have similar activity – and in the Hopfield network that is just how it’s done.</p>
<p class="TXI">The Hopfield network encodes a set of memories through a simple procedure. For every experience in which two neurons are either both active or inactive, the connection between them is strengthened. In this way, the neurons that fire together come to be wired together. On the other hand, for every pattern where one neuron is active and the other is inactive, the connection is weakened.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> After this learning <a id="page_98"></a>procedure, neurons that are commonly co-active in memories will have a strong positive connection, those that have opposite activity patterns will have strong negative connections and others will fall somewhere in between. This is just the connectivity needed to form attractors.</p>
<p class="TXI">Attractors are not trivial phenomena. After all, if all the neurons in a network are constantly sending and receiving inputs, why should we assume their activity would ever settle into a memory state, let alone the <span class="italic">right</span> memory state? So, to be certain that the right attractors would form in these networks, Hopfield had to make a pretty strange assumption: weights in the Hopfield network are <span class="italic">symmetric</span>. That means the strength of the connection from neuron A to neuron B is always the same as the strength from B to A. Enforcing this rule offered a mathematical guarantee of attractors. The problem is that the odds of finding a population of neurons like this in the brain are dismal to say the least. It would require that each axon going out of one cell and forming a synapse with another be matched exactly by that same cell sending its axon back, connecting to the first cell with the same strength. Biology simply isn’t that clean.</p>
<p class="TXI">This illuminates the ever-present tension in the mathematical approach to biology. The physicist’s perspective, which depends on an almost irrational degree of simplification, is at constant odds with the biology, full as it is of messy, inconvenient details. In this case, the details of the maths demanded symmetric weights in order to make any definitive statement about attractors and thus to make progress on modelling the <a id="page_99"></a>process of memory. A biologist would likely have dismissed the assumption outright.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup></p>
<p class="TXI">Hopfield, with one foot on either side of the mathematics–biology divide, knew to appreciate the perspective of the neuroscientists. To ease their concerns, he showed in his original paper that – even though it couldn’t be guaranteed mathematically – networks that allowed asymmetric weights still seemed able to learn and sustain attractors relatively well. </p>
<p class="TXI">The Hopfield network thus offered a proof of concept that Hebb’s ideas about learning could actually work. Beyond that, it offered a chance to study memory mathematically – to quantify it. For example, precisely how many memories can a network hold? This is a question that can only be asked with a precise model of memory in mind. In the simplest version of the Hopfield network, the number of memories depends on the number of neurons in the network. A network with 1,000 neurons, for example, can store about 140 memories; 2,000 neurons can store 280; 10,000 can store 1,400 and so on. If the number of memories remains less than about 14 per cent the number of neurons, each memory will be restored with minimal error. Adding more memories, however, will be like the final addition to a house of cards that causes it to cave in. When pushed past its capacity, the Hopfield network collapses: inputs <a id="page_100"></a>go towards meaningless attractors and no memories are successfully recovered. It’s a phenomenon given the appropriately dramatic name ‘blackout catastrophe’.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup></p>
<p class="TXI">Precision cannot be evaded; once this estimate of memory capacity is found, it’s reasonable to start asking if it aligns with the number of memories we know to be stored by the brain. A landmark study in 1973 showed that people who had been shown more than 10,000 images (each only once and for only a brief period of time) were quite capable of recognising them later on. The 10 million neurons in the perirhinal cortex – a brain region implicated in visual memory – could store this amount of images, but it wouldn’t leave much space for anything else. Therefore, there seemed to be a problem with Hebbian learning.</p>
<p class="TXI">This problem becomes less problematic, however, when we realise that recognition is not recall. That is, a feeling of familiarity when seeing an image can happen without the ability to regenerate that image from scratch. The Hopfield network is remarkable for being capable of the latter, more difficult task – it fully completes a memory from a partial bit of it. But the former task is still important. Thanks to researchers working at the University of Bristol, it’s now known that recognition can also be performed by a network that uses Hebbian learning. These networks, when assessed on their ability to label an input as novel or familiar, have a significantly <a id="page_101"></a>higher capacity: 1,000 neurons can now recognise as many as 23,000 images. Just as Semon so presciently identified, this is an example of an issue that arises from relying on common language to parcel up the functions of the brain. What feels like simply ‘memory’ to us crumbles when pierced by the scrutiny of science and mathematics into a smattering of different skills.</p>
<p class="center">* * *</p>
<p class="TXT">When, in 1953, American doctor William Scoville removed the hippocampus from each side of 27-year-old Henry Molaison’s brain, he thought he was helping prevent Molaison’s seizures. What Scoville didn’t know was the incredible impact this procedure would have on the science of memory. Molaison (more famously known as ‘H. M.’ in scientific papers to hide his identity until his death in 2008) did find some relief from his seizures after the procedure, but he never formed another conscious memory again. Molaison’s subsequent and permanent amnesia initiated a course of research that centred the hippocampus – a curved finger-length structure deep in the brain – as a hub in the memory-formation system. Evading Lashley’s troubled search, this is a location that does play a special role in storing memories.</p>
<p class="TXI">Current theories of hippocampal function go as follows: information about the world first reaches the hippocampus at the dentate gyrus, a region that runs along the bottom edge of the hippocampus. Here, the representation is primed and prepped to be in a form more amenable to memory storage. The dentate gyrus then sends connections on to where attractors are <a id="page_102"></a>believed to form, an area called CA3; CA3 has extensive recurrent connections that make it a prime substrate for Hopfield network-like effects. This area then sends output to another region called CA1, which acts as a relay station; it sends the remembered information back to the rest of the brain (see Figure 10).</p>
<p class="image-fig" id="fig10.jpg">
<img alt="" src="Images/chapter-04-image-04.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 10</span>
</span></p>
<p class="TXI">What’s interesting about this final step – and what may have muddied Lashley’s original findings – is that these projections out to different areas of the brain are believed to facilitate the <span class="italic">copying</span> of memories. In this way, CA3 acts as a buffer, or warehouse, holding on to memories until they can be transferred to other brain areas. It does so by reactivating the memory in those areas. The hippocampus thus helps the rest of the brain memorise things using the same strategy you’d use to study for a test: repetition. By repeatedly reactivating the same group of neurons elsewhere in the brain, the hippocampus gives those neurons the chance to undergo Hebbian learning themselves. Eventually, their own weights have changed enough for the memory to be safely stored there.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> With <a id="page_103"></a>his hippocampus gone, Molaison had no warehouse for his experiences, no way to replay his memories back to his brain. </p>
<p class="TXI">With knowledge of this memory storehouse in the brain, researchers can look into how it works. Particularly, they can look for attractors in it.</p>
<p class="TXI">In 2005, scientists at University College London recorded the activity of hippocampal cells in rats. The rats got used to being in two different enclosures – a circular one and a square one. Their hippocampal neurons showed one pattern of activity when they were in the circle and a different pattern when they were in the square. The test for attractors came when an animal was placed into a new ‘squircle’ environment, the shape of which was somewhere in between a circle and a square. The researchers found that if the environment was more square-like the neural activity went to the pattern associated with the square environment; more circle-like and it went to that of the circle. Crucially, there were no intermediate representations in response to intermediate environments, only all circle or all square. This makes the memories of the circle and square environments attractors. An initial input that isn’t exactly one or the other is unstable; it gets inescapably driven towards the nearest established memory. </p>
<p class="TXI">The Hopfield network made manifest the theories of Hebb and showed how attractors – normally studied in physics – could explain the mysteries of memory. Yet Hopfield knew the limitations of bringing mathematics to real brains in real laboratories. He described his own model as a ‘mere parody of the complexities of neurobiology’. Indeed, as the creation of a physicist, it <a id="page_104"></a>lacks all the gooey richness of biology. But as a parody capable of powerful computations, it has offered many insights as well – insights that didn’t end with simple storage and recall. </p>
<p class="center">* * *</p>
<p class="TXT">You’re eating dinner in your kitchen when your roommate comes home. When you see them, you remember that last night you finished a book they had lent you and you want to return it before they leave for a trip the next day. So, you put down your food, head out of the kitchen and go down the hallway. You walk up the stairs, turn, enter your room and think: ‘Wait, what am I doing here?’ </p>
<p class="TXI">The sensation is a common one. So much so it’s been given a name: ‘destinesia’ or amnesia about why you’ve gone to where you are. It’s a failure of what’s called ‘working memory’, the ability to hold an idea in mind, even just for the 10 seconds it takes to walk from room to room. Working memory is crucial for just about all aspects of cognition: it’s hard to make a decision or work through a plan if you keep forgetting what you’re thinking about.</p>
<p class="TXI">Psychologists have been studying working memory for decades. The term itself was first coined in the 1960 book <span class="italic">Plans and the Structure of Behavior</span> written by George A. Miller and fellow scientists working at the Center for Advanced Study in the Behavioral Sciences in California. But the concept was explored well before that. Indeed, Miller himself wrote one of the most influential papers on the topic four years previously, in 1956. Perhaps <a id="page_105"></a>anticipating its fame, Miller gave the paper the cheeky title ‘The magical number seven, plus or minus two’. What that magical number refers to is the number of items humans can hold in their working memory at any one time. </p>
<p class="TXI">An example of how to assess this is to 1) show a participant several coloured squares on a screen; 2) ask them to wait for some time between several seconds to minutes; 3) then show them a second set of coloured squares. The task of the subject is to indicate if the colours of the second set are the same as the colours of the first. People can do well on this task if the number of squares shown remains small, achieving nearly 100 per cent accuracy if only one square is shown. Adding in more squares makes the performance drop and drop, until past seven, it’s almost no different to random guessing. Whether seven really is a special value when it comes to this kind of working memory capacity is up for debate; some studies find lower limits, some higher. However, there’s no doubt that Miller’s paper made an impact and psychologists have worked to characterise nearly every aspect of working memory since, from what can be held in it to how long it can last. </p>
<p class="TXI">But the question remains of how the brain actually does this: where are working memories stored and in what way? A tried-and-tested method for answering such questions – lesion experiments – pointed to the prefrontal cortex, a large part of the brain just behind the forehead. Whether it was humans with unfortunate injuries or laboratory animals with the area removed, it was clear that damaging the prefrontal cortex reduced working memory substantially. Without it, animals can <a id="page_106"></a>hardly hold on to an idea for more than a second or two. Thoughts and experiences pass through their minds like water through cupped hands. </p>
<p class="TXI">With an ‘X’ marking the spot, neuroscientists then began to dig. Dropping an electrode into the prefrontal cortex of monkeys, in 1971 researchers at the University of California in Los Angeles eavesdropped on the neurons there. The scientists, Joaquin Fuster and Garrett Alexander, did this while the animals performed a task similar to the colour memory test. These tests are known as ‘delayed response’ tasks because they include a delay period wherein the important information is absent from the screen and must thus be held in memory. The question was: what are neurons in the prefrontal cortex doing during this delay?</p>
<p class="TXI">Most of the brain areas responsible for vision have a stereotyped response to this kind of task: the neurons respond strongly when the patterns on the screen initially show up and then again when they reappear after the delay, but during the delay period – when no visual inputs are actually entering the brain – these areas are mostly quiet. Out of sight really does mean out of mind for these neurons. What Fuster and Alexander found, however, was that cells in the prefrontal cortex were different. The neurons there that responded to the visual patterns kept firing even after the patterns disappeared; that is, they maintained their activity during the delay period. A physical signature of working memory at work!</p>
<p class="TXI">Countless experiments since have replicated these findings, showing maintained activity during delay periods under many different circumstances, both in <a id="page_107"></a>the prefrontal cortex and beyond. Experiments have also hinted that when these firing patterns are out of whack, working memory goes awry. In some experiments, for example, applying a brief electrical stimulation during the delay period can disrupt the ongoing activity and this leads to a dip in performance on delayed response tasks. </p>
<p class="TXI">What is so special about these neurons that they can do this? Why can they hold on to information and maintain their firing for seconds to minutes, when other neurons let theirs go? For this kind of sustained output, neurons usually need sustained input. But if delay activity occurs without any external input from an image then that sustained input must come from neighbouring neurons. Thus, delay activity can only be generated by a network of neurons working together, the connections between them conspiring to keep the activity alive. This is where the idea of attractors comes back into play. </p>
<p class="TXI">So far we’ve looked at attractors in Hopfield networks, which show how input cues reignite a memory. It may not be clear how this helps with working memory. After all, working memory is all about what happens after that ignition; after you stand up to get your roommate’s book, how do you keep that goal in mind? As it turns out, however, an attractor is exactly what’s needed in this situation, because an attractor stays put.</p>
<p class="TXI">Attractors are defined by derivatives. If we know the inputs a neuron gets and the weights those inputs are multiplied by, we can write down an equation – a derivative – describing how the activity of that neuron will change over time as a result of those inputs. If this derivative is zero that means there is no change in the <a id="page_108"></a>activity of the neuron over time; it just keeps firing at the same, constant rate. Recall that, because this neuron is part of a recurrent network, it not only gets input but also serves as input to other neurons. So, its activity goes into calculating the derivative of a neighbouring neuron. If none of the inputs to this neighbouring neuron are changing – that is, their derivatives are all zero as well – then it too will have a zero derivative and will keep firing at the same rate. When a network is in an attractor state, the derivative of each and every neuron in that network is zero.</p>
<p class="TXI">And that is how, if the connections between neurons are just right, memories started at one point in time can last for much longer. All the cells can maintain their firing rate because all the cells around them are doing the same. Nothing changes if nothing changes. </p>
<p class="TXI">The problem is that things do change. When you leave the kitchen and walk to your bedroom, you encounter all kinds of things – your shoes in the hallway, the bathroom you meant to clean, the sight of rain on the window – that could cause changes in the input to the neurons that are trying to hold on to the memory. And those changes could push the neurons out of the attractor state representing the book and off to somewhere else entirely. For working memory to function, the network needs to be good at resisting the influence of such distractors. A run-of-the-mill attractor can resist distracting input to an extent. Recall the trampoline example. If the person standing on the trampoline gave a little nudge to the ball, it would likely roll just out of its divot and then back in. With only a small perturbation the memory stays intact, but give the <a id="page_109"></a>ball a heartier kick and who knows where it will end up? Good memory should be robust to such distractions – so what could make a network good at holding on to memories? </p>
<p class="TXI">The dance between data and theory is a complex one, with no clear lead or follow. Sometimes mathematical models are developed just to fit a certain dataset. Other times the details from data are absent or ignored and theorists do as their name suggests: theorise about how a system <span class="italic">could</span> work before knowing how it does. When it comes to building a robust network for working memory, scientists in the 1990s went in the latter direction. They came up with what’s known as the ‘ring network’, a hand-designed model of a neural circuit that would be ideal for the robust maintenance of working memories. </p>
<p class="TXI">Unlike Hopfield networks, ring networks are well described by their name: they are composed of several neurons arranged in a ring, with each neuron connecting only to those near to it. Like Hopfield networks, these models have attractor states – activity patterns that are self-sustaining and can represent memories. But the attractor states in a ring model are different to those in a Hopfield network. Attractors in a Hopfield model are <span class="italic">discrete</span>. This means that each attractor state – the one for your childhood bedroom, the one for your childhood holiday, the one for your current bedroom – is entirely isolated from the rest. There is no smooth way to transition between these different memories, regardless of how similar they are; you have to completely leave one attractor state to get to another. Attractors in a ring network, on the other hand, are <span class="italic">continuous</span>. With continuous attractors, transitioning between similar <a id="page_110"></a>memories is easy. Rather than being thought of as a trampoline with people standing at different points, models with continuous attractor states are more like the gutter of a bowling lane: once the ball gets into the gutter it can’t easily get out, but it can move smoothly within it.</p>
<p class="TXI">Networks with continuous attractor states like the ring model are helpful for a variety of reasons and chief among them is the type of errors they make. It may seem silly to praise a memory system for its errors – wouldn’t we prefer no errors at all? – but if we assume that no network can have perfect memory, then the quality of the errors becomes very important. A ring network allows for small, sensible errors. </p>
<p class="TXI">Consider the example of the working memory test where subjects had to keep in mind the colour of shapes on a screen. Colours map well to ring networks because, as you’ll recall from art class, colours lie on a wheel. So, imagine a network of neurons arranged in a ring, with each neuron representing a slightly different colour. At one side of the ring are red-representing neurons, next to them are orange, then yellow and green; this brings us to the side opposite the red, where there are the blue-representing neurons, which lead to the violet ones and back to red. </p>
<p class="TXI">In this task, when a shape is seen, it creates activity in the neurons that represent its colour, while the other neurons remain silent. This creates a little ‘bump’ of activity on the ring, centred on the remembered colour. If any distracting input comes in while the subject tries to hold on to this colour memory – from other random sights in the room, for example – it may push or pull the activity bump away from the desired colour. But – and this is the crucial <a id="page_111"></a>point – it will only be able to push it to a very nearby place on the ring. So red may become red-orange or green may become teal. But the memory of red would be very unlikely to become green. Or, for that matter, to become no colour at all; that is, there will always be a bump <span class="italic">somewhere</span> on the ring. These properties are all a direct result of the gutter-like nature of a continuous attractor – it has low resistance for moving between nearby states, but high resistance to perturbations otherwise.</p>
<p class="TXI">Another benefit of the ring network is that it can be used to do things. The ‘working’ in working memory is meant to counter the notion that memory is just about passively maintaining information. Rather, holding ideas in working memory lets us combine them with other information and come to new conclusions. An excellent example of this is the head direction system in rats, which also served as the inspiration for early ring network models. </p>
<p class="TXI">Rats (along with many other animals) have an internal compass: a set of neurons that keep track of the direction the animal is facing at all times. If the animal turns to face a new direction, the activity of these cells changes to reflect that change. Even if the rat sits still in a silent darkened room, these neurons continue to fire, holding on to the information about its direction. In 1995, a team from Bruce McNaughton’s lab at the University of Arizona and, separately, Kechen Zhang of the University of California, San Diego, posited that this set of cells could be well described by a ring network. Direction being one of those concepts that maps well to a circle, a bump of activity on the ring would be used to store the direction the animal was facing (See Figure 11).</p> <a id="page_112"></a>
<p class="TXI">But not only could a ring network explain how knowledge of head direction was maintained over time, it also served as a model of how the stored direction could change when the animal did. Head direction cells receive input from other neurons, such as those from the visual system and the vestibular system (which keeps track of bodily motion). If these inputs are hooked up to the ring network just right, they can push the bump of activity along to a new place on the ring. If the vestibular system says the body is now moving leftwards, for example, the bump gets pushed to the left. In this way, movement along the ring doesn’t create errors in memory, but rather updates the memory based on new information. ‘Working’ memory earns its name.</p>
<p class="image-fig" id="fig11.jpg">
<img alt="" src="Images/chapter-04-image-05.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 11</span>
</span></p>
<p class="TXI">Ring networks are a lovely solution to the complex problem of how to create robust and functional working memory systems. They are also beautiful mathematical objects. They display the desirable properties of simplicity and symmetry. They’re precise and finely tuned, elegant even.</p> <a id="page_113"></a>
<p class="TXI">As such, they are completely unrealistic. Because to the biologist, of course, ‘finely tuned’ are dirty words. Anything that requires delicate planning and pristine conditions to operate well won’t survive the chaos that is brain development and activity. Many of the desirable properties of ring networks only occur under very particular assumptions about the connectivity between neurons, assumptions that just don’t seem very realistic. So, despite all their desirable theoretical properties and useful abilities, the chances of seeing a ring network in the brain seemed slim. </p>
<p class="TXI">The discovery made in a research centre just outside Washington DC in 2015 was therefore all the more exciting. </p>
<p class="TXI">Janelia Research Campus is a world-class research facility hidden away in the idyllic former farmland of Ashburn, Virginia. Vivek Jayaraman has been at Janelia since 2006. He and his team of about half a dozen people work to understand navigation in <span class="italic">Drosophila melanogaster</span>, a species of fruit fly commonly studied in neuroscience. On a par with a grain of rice, the size of these animals is both a blessing and a curse. While they can be difficult to get hold of, these tiny flies only have around 135,000 neurons, roughly 0.2 per cent the amount of another popular lab animal, the mouse. On top of that, a lot is known about these neurons. Many of them are easily categorised based on the genes they express, and their numbers and locations are very similar across individuals. </p>
<p class="TXI">Like rodents, flies also have a system for keeping track of head direction. For the fly, these head direction neurons are located in a region known as the ellipsoid <a id="page_114"></a>body. The ellipsoid body is centrally placed in the fly brain and it has a unique shape: it has a hole in the middle with cells arranged all around that hole, forming a doughnut made of neurons – or in other words, a ring.</p>
<p class="TXI">Neurons arranged in a ring, however, do not necessarily make a ring network. So, what the Jayaraman lab set out to do was investigate whether this group of neurons that <span class="italic">looked</span> like a ring network, actually <span class="italic">behaved</span> like one. To do this, they put a special dye in the ellipsoid body neurons, one that makes them light up green when they’re active. They then had the fly walk around, while they filmed the neurons. If you were to look at these neurons on a screen, as the fly heads forwards, you’d see a flickering of little green points at one location on an otherwise black screen. Should the fly choose to make a turn, that flickering patch would swing around to a new location. Over time, as the fly moves and the green patch on the screen moves with it, the points that have lit up form a clear ring structure, matching the underlying shape of the ellipsoid body. If you turn the lights off in the room so the fly has no ability to see which way it’s facing, the green flicker still remains at the same location on that ring – a clear sign that the memory of heading direction is being maintained. </p>
<p class="TXI">In addition to observing the activity on the ring, the experimenters also manipulated it in order to probe the extremes of its behaviour. A true ring network can only support one ‘bump’ of activity; that is, only neurons at one location on the ring can be active at a given time. So, the researchers artificially stimulated neurons on the side of the ring opposite to those already active. This strong stimulation of the opposing neurons caused the original bump to shut down, and the bump at the new location was maintained, even after the stimulation was <a id="page_115"></a>turned off. Through these experiments, it became clear that the ellipsoid body was no imposter, but a vivid example of a theory come to life. </p>
<p class="TXI">This finding – a ring network in the literal, visible shape of a ring – feels a bit like nature winking at us. William Skaggs and the other authors of one of the original papers proposing the ring network explicitly doubted the possibility of such a finding: ‘For expository purposes it is helpful to think of the network as a set of circular layers; this does not reflect the anatomical organisation of the corresponding cells in the brain.’ Most theorists working on ring network models assumed that they’d be embedded in some larger, messier network of neurons. And that is bound to be the case for most systems in most species. This anomalously pristine example likely arises from a very precisely controlled genetic programme. Others will be much harder to spot. </p>
<p class="TXI">Even if we usually can’t see them directly, we can make predictions about behaviours we’d expect to see if the brain is using continuous attractors. In 1991, pioneering working memory researcher Patricia Goldman-Rakic found that blocking the function of the neuromodulator dopamine made it harder for monkeys to remember the location of items. Dopamine is known to alter the flow of ions into and out of a cell. In 2000, researchers at the Salk Institute in California showed how mimicking the presence of dopamine in a model with a continuous attractor enhanced the model’s memory.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup> It stabilised the activity of the neurons encoding the memory, making <a id="page_116"></a>them more resistant to irrelevant inputs. Because dopamine is associated with reward,<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> this model also predicts that under conditions where a person is anticipating a large reward their working memory will be better – and that is exactly what has been found. When people are promised more reward in turn for remembering something, their working memory is better. Here, the concept of an attractor works as the thread that stitches chemical changes together with cognitive ones. It links ions to experiences. </p>
<p class="TXI">Attractors are omnipresent in the physical world. They arise from local interactions between the parts of a system. Whether those parts are atoms in a metal, planets in a solar system or even people in a community, they will be compelled towards an attractor state and, without major disruptions, will stay there. Applying these concepts to the neurons that form a memory connects dots across biology and psychology. On one side, Hopfield networks link the formation and retrieval of memories to the way in which connections between neurons change. On the other, structures like ring networks underlie how ideas are held in the mind. In one simple framework we capture how memories are recorded, retained and reactivated.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-1" id="fn-1">1</a> ﻿Jerzy Konorski, a Polish neurophysiologist, published a book with very similar ideas the year before Hebb did. In fact, Konorski anticipated several important findings in neuroscience and psychology. However, the global East﻿–﻿West divide at the time isolated his contributions. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-2" id="fn-2">2</a> ﻿When Hopfield wrote on an undergraduate admission form that he intended to study ﻿‘﻿physics or chemistry﻿’﻿, his university advisor ﻿–﻿ a colleague of his father﻿’﻿s ﻿–﻿ crossed out the latter option saying, 
﻿‘﻿I don﻿’﻿t believe we need to consider chemistry.﻿’﻿ ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-3" id="fn-3">3</a> ﻿Hopfield﻿’﻿s attitude was not unique. The 1980s found many physicists, bored with their own field, looking at the brain and thinking, ﻿‘﻿I could solve that.﻿’﻿ After Hopfield﻿’﻿s success, this population increased even more.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-4" id="fn-4">4</a> ﻿While the calculation of an individual neuron﻿’﻿s activity in terms of inputs and weights is the same as described for the perceptron in the last chapter, the perceptron is a ﻿<span class="italic">feedforward</span>﻿ (not recurrent) network. Recurrence means that the connections can form loops: neuron A connects to neuron B, which connects back to neuron A, for example. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-5" id="fn-5">5</a> ﻿This second part ﻿–﻿ the idea that connection strength should ﻿<span class="italic">decrease</span>﻿ if a pre-synaptic neuron is highly active while the post-synaptic neuron remains quiet ﻿–﻿ was not part of Hebb﻿’﻿s original sketch, but it has since been borne out by experiments.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-6" id="fn-6">6</a> ﻿In fact, when Hopfield presented an early version of this work to a group of neuroscientists, one attendee commented that ﻿‘﻿it was a beautiful talk but unfortunately had nothing to do with neurobiology﻿’﻿. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-7" id="fn-7">7</a> ﻿You may know some people with stories of their own ﻿‘﻿blackout catastrophe﻿’﻿ after a night of drinking. However, the exact type of memory failure seen in Hopfield networks is not actually believed to occur in humans.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-8" id="fn-8">8</a> ﻿This process is believed to occur while you sleep.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-9" id="fn-9">9</a> ﻿This model was composed of the Hodgkin-Huxley style of neurons described in ﻿﻿Chapter 2﻿﻿, which makes incorporating dopamine﻿’﻿s effects on ion flows easy.﻿</p>
<p class="FN2"><a href="chapter4.xhtml#fnt-10" id="fn-10">10</a> ﻿Lots more on this in ﻿﻿Chapter 11﻿﻿!﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 9</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter9"><a href="contents.xhtml#re_chapter9">CHAPTER NINE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter9">From Structure to Function</a><a id="page_247"></a></p>
<p class="H1" id="b-9781472966445-ch968-sec10">
<span class="bold">
<span>Graph theory and network neuroscience</span>
</span></p>
<p class="TXT">In 1931, three years before his death, Santiago Ramón y Cajal gave the Cajal Institute in Madrid a trove of his personal possessions. The collection contained all manner of scientific trinkets: balances, slides, cameras, letters, books, microscopes, solutions, reagents. But the items most notable – the ones that would become nearly synonymous with the name Cajal – were the 1,907 scientific drawings he had created throughout his career.</p>
<p class="TXI">Most of these drawings were of different parts of the nervous system and were produced via a laborious cell-staining process. It started with a live animal, which was sacrificed and its tissues preserved. A chunk of the brain was then removed and soaked in a solution for two days, dried and soaked in a different solution – this one containing silver that would penetrate the cell structures – for another two days. At the end of this, the brain tissue was rinsed, dried again and cut into slices thin enough to fit on a microscope slide. Cajal looked at these slides through the eyepiece of his microscope and sketched what he saw. Starting first with pencil, he outlined every nook and cranny of each neuron’s shape on a piece of cardboard, including the thick cell bodies and the thin appendages that emerged from them. He then darkened in the cells with India ink, occasionally using watercolours <a id="page_248"></a>to add texture and dimension. The result was a set of haunting silhouettes of stark, black spider-like figures against beige and yellow backgrounds.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> The exact contours and configurations depended on the animal and the nerve fibres in question; more than 50 different species and nearly 20 different parts of the nervous system are portrayed on Cajal’s cardboard canvases.</p>
<p class="TXI">These hundreds of portraits represent the infatuation Cajal had with the <span class="italic">structure</span> of the nervous system. He sought enlightenment in the basic unit of the brain – the neuron. He fixated on how they were shaped and how they were arranged. A focus on its physical foundations was Cajal’s inroad to understanding how the brain worked. Function, he believed, could be found in structure. </p>
<p class="TXI">And he was right. Cajal was able to deduce important facts about the workings of the brain by looking long and hard at how it was built. One of his significant findings was about how signals flow through neurons. Through his many observations of different neurons in different sensory organs, Cajal noticed that the cells were always arranged a certain way. The many branched dendrites of a cell would face the direction the signal was coming from. The long singular axon, on the other hand, went towards the brain. In the olfactory system, for example, neurons with the chemical receptors capable of picking up odour molecules exist in the mucousy skin <a id="page_249"></a>inside the nose. These neurons send their axons up into the brain and make contact with the dendrites of cells in the olfactory bulb. These neurons then have axons that go further on into other parts of the brain. </p>
<p class="TXI">This pattern – which Cajal saw over and over – strongly suggested that signals flow from dendrites through to axons. Dendrites, he concluded, act as the receiver of signals for a cell and axons the sender of signals on to the next cell. So clear was Cajal on this that he added little arrows to his drawings of circuits like the olfactory system, indicating the presumed direction of information flow. Cajal, as we now know, was exactly correct. </p>
<p class="TXI">Cajal was one of the founding fathers of modern neuroscience. As such, his belief in the relationship between structure and function was entered into the DNA of the field. Reflections of this idea are peppered throughout neuroscience’s history. In a 1989 article, Peter Getting wrote that researchers in the 1960s could see, even through their limited data, that ‘the abilities of a network arose from the interconnection of simple elements into complex networks, thus, from connectivity emerged function’. Studies in the 1970s, he goes on to say, ‘were approached with several expectations in mind: first, a knowledge of the connectivity would explain how neural networks operated’. This attitude persists. A review written in 2016 by professors Xiao-Jing Wang and Henry Kennedy ends with the statement: ‘Establishing a firm link from structure to function is essential to understand complex neural dynamics.’</p>
<p class="TXI">Structure exists at many scales in the brain. Neuroscientists can look at the shape of a neuron, as <a id="page_250"></a>Cajal did. Or they can look at how neurons are wired up: does neuron A connect to neuron B? They can zoom out even further and ask how small populations of neurons interact. Or they can investigate brain-wide connectivity patterns by looking at the thick bundles of axons that connect distant brain regions. Any of these higher-level structures may hold secrets about function as well. </p>
<p class="TXI">But to unearth these secrets, neuroscientists need a way to see and study these structures clearly. Something that could’ve been considered a limitation of the staining method Cajal used – that it stained only a small number of neurons at a time – was actually an advantage that made it revolutionary. A method that stained all neurons in the field of view would’ve produced a black mess with no discernible structures; it would’ve missed the trees for the forest. Since neuroscientists have moved their study of structure away from single neurons and on to the more complicated subject of connections, networks and circuits, they may be at even higher risk of being overwhelmed by data and distracted by the wrong details. </p>
<p class="TXI">A much-needed method, however, has been found in a particular subfield of mathematics: graph theory. The language of graph theory offers a way to talk about neural networks that cuts away much of the detail. At the same time, its tools find features of neural structure that are near impossible to see without it. These features of the structure, some scientists now believe, can inspire new thoughts on the function of the nervous system. Swept up by the promise of graph theory’s methods, neuroscientists are currently applying it to everything <a id="page_251"></a>from brain development to disease. Though the dust has not yet settled on this new approach to the brain, its fresh take on old problems is exciting many. </p>
<p class="center">* * *</p>
<p class="TXT">In the eighteenth-century East Prussian capital of Königsberg, a river branched in two as it cut through the town, creating a small island in the middle. Connecting this island with parts of the town north, south and east of it were seven bridges. At some point a question arose among the citizens of Königsberg: is there a way to navigate through the city that crosses each of the bridges once and only once? When this playful question met the famous mathematician Leonhard Euler, the field of graph theory was born.</p>
<p class="TXI">Euler, a polymath who was born in Switzerland but lived in Russia, wrote ‘<span class="italic">Solutio problematis ad geometriam situs pertinentis</span>’ or ‘The solution of a problem relating to the geometry of position’ in 1736. In the paper, he answered the question definitively: a Königsberger could <span class="italic">not</span> take a walk through their town crossing each bridge exactly once. To prove this he had to simplify the town map into a skeleton of its full structure and work logically with it. He had shown, without using the word, how to turn data into a <span class="italic">graph</span> and how to perform computations on it (see Figure 20).</p>
<p class="TXI">Within the context of graph theory, the word ‘graph’ does not refer to a chart or a plot, as it does in common language. Rather, a graph is a mathematical object, composed of <span class="italic">nodes</span> and <span class="italic">edges</span> (in the modern parlance). Nodes are the base units of the graph and edges represent <a id="page_252"></a>the connections between them. In the Königsberg example, the bridges serve as edges that connect the four different land masses, the nodes. The degree of a node is the number of edges it has; the ‘degree’ of a land mass is thus the number of bridges that reach it.</p>
<p class="image-fig" id="fig20.jpg">
<img alt="" src="Images/chapter-01-image-04.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 20</span>
</span></p>
<p class="TXI">Euler approached the bridge-crossing question by first noting that a path through town could be written down as a list of nodes. Giving each land mass a letter name, the list ‘ABDC’, for example, would represent a path that goes from the island at the centre to the land at the bottom (via any bridge that connects them), then from there to the land mass on the right and then on to the land at the top. In such a path through a graph, one edge is traversed between each pair of nodes. Therefore, the number of bridges crossed is equal to the number of letters in the list minus one. For example, if you’ve crossed two bridges, you’d have three land masses on your list. </p>
<p class="TXI">Euler then noticed something important about the number of bridges that each land mass has. This number is related to how many times the land mass should show <a id="page_253"></a>up in the path list. For example, land mass B has three bridges, which means ‘B’ must appear twice in any path that crosses each bridge once – that is, there is no way to cross these three bridges without visiting B twice. The same is true for land masses C and D, as they both have two bridges too. And land mass A, with five bridges, needs to appear three times in the path list. </p>
<p class="TXI">Taken together, any path that satisfies these require­ments would be nine (2+2+2+3) letters long. A list of nine letters, however, represents a path that crosses <span class="italic">eight</span> bridges. Therefore, it is impossible to build a path that crosses each of the seven bridges only once. </p>
<p class="TXI">Using this relationship between a node’s degree and the number of times the node should appear in a path, Euler derived a set of general rules about what paths were possible. He could now say for <span class="italic">any</span> set of bridges connecting <span class="italic">any</span> patches of land whether a path crossing each bridge only once existed. </p>
<p class="TXI">More than that, it doesn’t matter if we are talking about land and bridges at all. The same procedure could be used to find paths for a town snowplough that needs to clear each street only once or to see if it’s possible to traverse Wikipedia clicking each hyperlink between sites just one time. This pliability is part of what gives graph theory its potency. By stripping away the details of any specific situation, it finds the structure that is similar to them all. This abstract and alien way of looking at a problem can open it up to new and innovative solutions, just as treating a walk through town as a list of letters helped Euler. </p>
<p class="TXI">Given this feature, graph theory found purpose in many fields. Chemists in the nineteenth century wrestled <a id="page_254"></a>for some time with how to represent the structure of molecules. By the 1860s, a system was developed that is still in use today: atoms are drawn as letters and the bonds between them as lines. In 1877, English mathematician James Joseph Sylvester saw in this graphical representation of molecules a parallel to the work being done by descendants of Euler in mathematics. He published a paper drawing out the analogy and used, for the first time, the word ‘graph’ to refer to this form. Since then, graph theory has helped solve many problems in chemistry. One of its most common applications is finding isomers – sets of molecules that are each made up of the same type and number of atoms, but differ in how those atoms are arranged. Because graph theory provides a formal language for describing the structure of atoms in a molecule, it is also well suited for enumerating all the structures that are possible given a particular set of atoms. Algorithms that do this can aid in the design of drugs and other desirable compounds. </p>
<p class="TXI">Like a chemical compound, the structure of the brain lends itself well to a graph. In the most basic mapping, neurons are the nodes and the connections between them are the edges. Alternatively, the nodes can be brain areas and the nerve tracts that connect them the edges. Whether working on the microscale of neurons or the macroscale of brain regions, putting the brain into the terms of graph theory exposes it to all the tools of analysis this field has developed. It is a way of formalising the informal quest that has always guided neuroscience. To speak of how structure births function first requires the ability to speak clearly about structure. Graph theory provides the language.</p><a id="page_255"></a>
<p class="TXI">Of course, there are differences between the brain and a Prussian town or a chemical compound. The connections in the brain aren’t always a two-way street like they are on a bridge or in a bond. One neuron can connect to another without it receiving a connection back. This unidirectional nature of neural connections is important for the way information flows through neural circuits. The most basic graph structures don’t capture this, but by the late 1800s, the concept of <span class="italic">directed</span> graphs had been added to the arsenal of mathematical descriptors. In a directed graph, edges are arrows that only flow one way. The degree of a node in a directed graph is thus broken down into two categories: the in-degree (for example, how many connections a neuron receives) and the out-degree (how many connections it sends out to other neurons). A study done on neurons in the cortex of monkeys found these two types of degree to be roughly equal, meaning neurons give as much as they receive. </p>
<p class="TXI">In 2018, mathematicians Katherine Morrison and Carina Curto built a model of a neural circuit with directed edges in order to answer a question not so dissimilar to the Königsberg bridge problem. Rather than determining what walks through town a certain set of bridges can support, they explored what sequence of neural firing a given circuit could produce. By bringing in tools from graph theory, Morrison and Curto figured out how to look at a structure of up to five model neurons and predict the order in which they will fire. Ordered patterns of neuronal firing are important for many of the brain’s functions, including memory and navigation. This five-neuron model may only be a toy <a id="page_256"></a>example, but it perfectly encapsulates the power promised by bringing graph theory into the study of the brain. </p>
<p class="TXI">For real brain networks, however, a more ‘global’ perspective needs to be taken.</p>
<p class="center">* * *</p>
<p class="TXT">Over the course of a few months in the late 1960s, a stockbroker living in Sharon, Massachusetts, was given 16 brown folders from the proprietor of a local clothing store. Strange though this was, the folders weren’t a surprise to the stockbroker. They were simply part of an unorthodox social experiment being run by the famous social psychologist Stanley Milgram. With this experiment, Milgram wanted to test just how big – or small – the world truly was. </p>
<p class="TXI">The phrase ‘it’s a small world’ is usually uttered when two strangers meet and serendipitously discover that they have a friend or relative in common. Milgram wanted to know just how often something like this could happen: what are the odds that two people chosen at random have a friend in common? Or a friend of a friend? Framed another way, if we could see the entire network of human connections – a graph where each node is a person and each edge a relationship – what would the average distance between people be? How many edges would we need to traverse to find a path between any two nodes?</p>
<p class="TXI">In a bold attempt to answer this question, Milgram chose a target person (in this case, the Massachusetts stockbroker) and several starters: unrelated people in another part of the country (in this case, mostly Omaha, <a id="page_257"></a>Nebraska). The starters were given a package with a folder and information about the target person. The instructions were simple: if you know the target, give the folder to them; otherwise, send it on to a friend of yours who you think has a better shot of knowing them. The next person would be told to follow the same instructions and, hopefully, eventually the folder would end up with the target. The senders were also asked to write their name in a register that was sent along with the package, so that Milgram could trace the path the folder took. </p>
<p class="TXI">Looking at a total of 44 folders that made it back to the stockbroker, Milgram found that the shortest path consisted of just two intermediate people and the longest had 10. The median was just five. Getting the folder through five people between the starter and the target involved six handoffs and thus the notion of ‘six degrees of separation’ – already posited by observant scientists and sociologists – was solidified.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> </p>
<p class="TXI">This concept percolated through the popular imagination. One day, in the late 1990s, graduate student Duncan Watts was asked by his father if he realised that he was only six handshakes away from the president. Watts, working for mathematician Steven Strogatz at the time, brought up this idea as they were discussing how groups of crickets could communicate. After this chance conversation, ‘small world’ would go from a quaint <a id="page_258"></a>expression to a mathematically defined property of a network.</p>
<p class="TXI">In 1998, Watts and Strogatz published a paper laying out just what it takes for a graph to function like a small world. The notion of a short average path length – the idea that any two nodes are separated by only a few steps – was a key component of it. One way to get short path lengths is to make the graph heavily interconnected, <span class="italic">i.e.</span> a graph where each node connects directly to many others. This trick, however, is in clear violation of what we know about social networks: the average citizen of America – a country of around 200 million at the time – had only about 500 acquaintances, according to Milgram. </p>
<p class="TXI">So, Watts and Strogatz limited their network simulations to those with sparse connections, but varied exactly what those connections looked like. They noticed that it was possible to have short path lengths in a network that was highly <span class="italic">clustered</span>. A cluster refers to a subset of nodes that are heavily interconnected, like the members of a family. In these networks, most nodes form edges just with other nodes in their cluster, but occasionally a connection is sent to a node in a distant cluster. The same way a train between two cities makes interactions between their citizens easier, these connections between different clusters in a network keeps the average path length low. </p>
<p class="TXI">Once these characteristics were identified in their models, Watts and Strogatz went looking for them in real data – and found them. The power grid system of the United States – turned into a graph by considering any generator or substation as a node and transmission lines as edges – has the low path length and high clustering of <a id="page_259"></a>a small world network. A graph made of actors with edges between any pairs that have starred in a movie together is the same. And the final place that they looked for, and found, a small world network was in the brain. </p>
<p class="TXI">More specifically, the structure Watts and Strogatz analysed was the nervous system of the tiny roundworm, <span class="italic">Caenorhabditis elegans</span>. Ignoring the directionality of the neural connections, Watts and Strogatz treated any connection as an edge and each of the 282 neurons in the worm’s wiring diagram as a node. They found that any two neurons could be connected by a path with, on average, only 2.65 neurons in between them and that the network contained far more clustering than would be expected if those 282 neurons were wired up randomly. </p>
<p class="TXI">Why should the nervous system of a nematode have the same shape as the social network of humans? The biggest reason may be energy costs. Neurons are hungry. They require a lot of energy to stay in working order and adding more or longer axons and dendrites only ups the bill. A fully interconnected brain is, thus, a prohibitively expensive brain. Yet if connections become too sparse the very function of the brain – processing and routing information – breaks down. A balance must be struck between the cost of wiring and the benefit of information sharing. Small worlds do just this. In a small world, the more common connections are the relatively cheap ones between cells in a local cluster. The pricey connections between faraway neurons are rare, but there are enough to keep information flowing. Evolution, it seems, has found small worldness to be the smart solution. </p>
<p class="TXI">Watts and Strogatz’s finding in the roundworm was the first time the nervous system was described in the <a id="page_260"></a>language of graph theory. Putting it into these terms made visible some of the constraints that are shared by the brain and other naturally occurring networks. Connections can be expensive to maintain, be they acquaintanceships or axons, and if these similarities exist between the roundworm and social networks it’s reasonable to expect that the structure of other nervous systems is dictated by them as well. </p>
<p class="TXI">But to speak about the structure of the nervous system requires that we know something about the structure of the nervous system. As it turns out, harvesting this information is a nuisance at best and an unprecedented technical hurdle at worst.</p>
<p class="center">* * *</p>
<p class="TXT">A ‘connectome’ is a graph describing the connections in a brain. While Watts and Strogatz were working with an incomplete version, the full roundworm connectome is defined by the full set of 302 neurons in the worm and the 7,286 connections between them. The roundworm was the first animal to have its entire adult connectome documented and, at present, it is the only one. </p>
<p class="TXI">This lack of connectomic data is due largely to the gruelling process by which it is collected. Mapping a full connectome at the neuron level requires fixing a brain in a preservative, cutting it into sheets thinner than a strand of hair, photographing each of these sheets with a microscope and feeding those photographs into a computer that recreates them as a 3D stack. Scientists then spend tens of thousands of hours staring at these photographs, tracing individual neurons through image <a id="page_261"></a>after image and noting where they make contact with each other.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> The process of unearthing the delicate structure of neural connections this way is as painstaking as a palaeontology dig. The price of all this slicing, stitching and tracing makes it unlikely that full connectomes are within reach for any but the smallest of species. The connectome of the fruit fly, an animal with a brain one-millionth the size of a human’s, is currently being assembled by a team of scientists and producing millions of gigabytes of data in the process. And, while any two roundworms are more or less alike, more complex species tend to have more individual differences, making the connectome of only a single fly or mammal a mere draw from a hat of possible connectomes. </p>
<p class="TXI">Luckily more indirect methods are available that allow for a rough draft of connectomes in many individuals and species. One approach involves recording from a neuron while electrically stimulating others around it. If stimulation of one of these nearby neurons reliably causes a spike in the recorded one, there’s likely a connection between them. Another option is tracers: chemicals that act like dyes that colour in a neuron. To see where inputs are coming from or outputs are going to, one just needs to look at where the dye shows up. None of these methods can create a complete connectome, but they do work to give a snapshot of connectivity in a certain area.</p> <a id="page_262"></a>
<p class="TXI">While connectivity had been studied long before it, the word ‘connectome’ wasn’t coined until 2005. In a visionary paper, psychologist Olaf Sporns and colleagues called on their fellow scientists to help build the connectome of the human brain, promising it would ‘significantly increase our understanding of how functional brain states emerge from their underlying structural substrate’. Getting connection data from humans is a staggering challenge, as many of the invasive methods used in animals are, for obvious reasons, not permissible. A quirk of brain biology, however, allows for a clever alternative. </p>
<p class="TXI">When the brain builds connections, protecting the cargo is key. Like water leaking out of a seeping hose, the electrical signal carried by an axon is at risk of fading away. This isn’t much of a problem for short axons connecting nearby cells, but those carrying signals from one region of the brain to another need protection. Long-range axons therefore get wrapped in layers and layers of a waxy blanket. This waxy substance, called myelin, contains a lot of water molecules. Magnetic resonance imaging (the same technology used to take pictures of tumours, aneurysms and head injuries) can detect the movement of these water molecules – information that is used to reconstruct the tracts of axons in the brain. Through this, it’s possible to see which brain regions are connected to each other. After the publication of Sporns’s plea, the Human Connectome Project was launched to map out the brain using this technique. </p>
<p class="TXI">Identifying long-range axons this way doesn’t produce the same kind of connectome that single-cell tracing methods do. It requires that scientists chunk the brain into <a id="page_263"></a>rough and possibly arbitrary regions; it’s therefore a much coarser description of connectivity. In addition, measuring water molecules isn’t a perfect way to pick up the axons between these areas, leading to errors or ambiguities. Even David Van Essen, one of the key scientists of the Human Connectome Project, warned the neuroscience community in 2016 that there are major technical limitations to this approach that shouldn’t be underappreciated. On the other hand, it is one of the only means by which we can peer into a living human brain; the desire to press forward with it makes sense. As Van Essen wrote: ‘Be optimistic, yet critical of glasses half full and half empty.’ </p>
<p class="TXI">Despite these limitations, neuroscientists in the early 2000s were inspired by the work of Watts and Strogatz 
to look at their field through the lens of graph theory and eagerly set their gaze upon any and all available connectome data. What they saw when they analysed it were small worlds in all directions. The reticular formation, for example, is an ancient part of the brain responsible for many aspects of bodily control. When a cell-level map of this region in cats was pieced together and analysed in 2006, it was the first vertebrate neural circuit to be given the graph-theory treatment. And it was found to be a small world. In studies of the connections between brain areas in both rats and monkeys, short path lengths and plenty of clusters were always found as well. Humans were finally brought into the small-world club in 2007 when researchers in Switzerland used MRI scans to parcellate the brain into a thousand different areas – each about the height and width of a hazelnut – and measured the connections between them.</p> <a id="page_264"></a>
<p class="TXI">Universal findings are a rare sight in neuroscience; the principles that operate on one set of neurons aren’t necessarily assumed to show up in another. As a conclusion that repeats itself across species and scales, small worldness is thus remarkable. Like the refrain of a siren song, it also implores more exploration. To see small worlds in so many places raises questions about how they got there and what roles they can play. While the answers to these questions are still being explored, without the language of graph theory they couldn’t have even been asked in the first place.</p>
<p class="center">* * *</p>
<p class="TXT">On 10 February 2010, approximately 23 per cent of all flights originating in the United States were cancelled. This historically large disruption was the result of a snowstorm in the north-east that closed a handful of airports, including Ronald Reagan in Washington DC and JFK in New York. Such a sizable dent in travel wouldn’t ordinarily come from closing a handful of airports – but these were not just any airports, they were hubs in the aviation network. </p>
<p class="TXI">Hubs are nodes in a graph that have a high degree – that is, they’re highly connected. They reside in the tails of the degree distribution: a plot that shows, for each degree value, how many nodes in the network have that degree (see Figure 21). For graphs like the aviation network or the structure of servers that make up the internet, this graph starts high – meaning that there are many nodes that have only a small number of connections – and fades as the number of connections increases, leading to a long, <a id="page_265"></a>low tail that represents the small number of nodes with very high degree, such as JFK airport. The high degree of hubs makes them powerful but also a potential vulnerability. Like removing the keystone from a stone archway, a targeted attack on one of its hubs can cause a network to collapse.</p>
<p class="image-fig" id="fig21.jpg">
<img alt="" src="Images/chapter-01-image-05.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 21</span>
</span></p>
<p class="TXI">The brain has hubs. In humans, they’re found sprinkled throughout the lobes. The cingulate, for example, which curves around the centre of the brain, serves as a hub; as does the precuneus, which sits atop the back of the cingulate.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> In studies of sleep, anaesthesia and people in comas, activity in these areas correlates with consciousness. The size of another hub, the superior frontal cortex, appears correlated with impulsivity and attention. Lesions of a fourth hub, located in the parietal cortex on the side <a id="page_266"></a>of the brain, cause patients to lose a sense of direction. In total, the population of hubs appears diverse in both location and function. The connective thread, if there is any between them, is just how complicated each one is. Regions of the brain like the visual cortex, auditory cortex and olfactory bulb – regions with clear and identifiable roles present right there in their names – don’t make it on to the list of hubs. Hub regions are complex; they pull in information from multiple sources and spread it out just as far. Their role as integrators seems a clear result of their placement in the network architecture. </p>
<p class="TXI">Hubs, in addition to integrating information in distinctive ways, may also be responsible for setting the brain’s clock. In the CA3 region (the memory warehouse of the hippocampus mentioned in <a href="chapter4.xhtml#chapter4">Chapter 4</a>), waves of electrical activity sweep through the neural population in the early days of development after birth. These waves ensure that the activity of the neurons and the strength of their connections are established correctly. And hub neurons are the likely coordinators of this important synchronised activity; they tend to start firing before these waves and stimulating them can instigate a wave. Other studies have even posited a role for hub regions in synchronising activity across the whole brain. Because of their high degree, a message from a hub is heard far and wide in the network. Furthermore, hubs in the brain tend to be strongly interconnected with each other, a network feature referred to as a ‘rich club’. Such connections can ensure all the hubs are themselves on the same page as they send out their synchronising signals. </p>
<p class="TXI">Even the way hub neurons develop points to their special place in the brain. The neurons that go on to form <a id="page_267"></a>the rich club in the roundworm, for example, are some of the first to appear as the nervous system grows. In just eight hours after an egg is fertilised, all of these hub neurons will be born; the rest of the nervous system won’t be finished until over a day later. Similarly, in humans, much of the basic hub structure is present in infants. </p>
<p class="TXI">If hubs are so central to brain function, what role might they play in <span class="italic">dys</span>function? Danielle Bassett explored this question as part of her far-ranging career at the intersection of networks and neuroscience.</p>
<p class="TXI">In the early 2000s, when the methods of graph theory were first being unfurled across the field of neuroscience, Bassett was a college student studying physics. At that time, it might have been a surprise to her to hear she’d go on to be called the ‘doyenne of network science’ by a well-known neuroscientist.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> Though working towards a physics degree was itself already somewhat surprising given her upbringing: Bassett was one of 11 children home-schooled in a religious family where women were expected to play more traditional roles. Her transition to neuroscience came during her PhD, when she worked with Edward Bullmore, a neuropsychiatrist at Cambridge University who was part of an initial wave of neuroscientists eager to apply graph theory to the brain. One of Bassett’s first projects was to see how the structure of the brain is affected by the common and crippling mental disorder schizophrenia. </p>
<p class="TXI">Schizophrenia is a disease characterised by delusions and disordered thought. Comparing the brains of <a id="page_268"></a>people with the disease to those without, Bassett found several differences in their network properties, including in the hubs. Regions in the frontal cortex that form hubs in healthy people, for example, don’t do so in schizophrenics. A disruption to the frontal cortex and its ability to rein in and control other parts of the brain could relate to the hallucinations and paranoia schizophrenia can elicit. And while the brain of a schizophrenic is still a small world, both the average path length and the strength of clustering are higher than in healthy people, making it seemingly more difficult for two disparate areas to communicate and get on the same page. </p>
<p class="TXI">As the first study to approach this disease from the perspective of graph theory, this work helped bring an old idea about ‘disconnection syndromes’ into the quantitative age. As early as the late nineteenth century, neurologists hypothesised that a disruption in anatomical connections could lead to disorders of thought. German physician Carl Wernicke in particular believed that higher cognitive functions did not reside in any single brain region, but rather emerged from interactions between them. As he wrote in 1885: ‘Any higher psychic process … rested on the mutual interaction of … fundamental psychic elements mediated by means of their manifold connections via the association fibres.’ Lesioning these ‘association fibres’, he posited, would impair complex functions like language, awareness and planning. </p>
<p class="TXI">Now that the tools of graph theory have met the study of ‘disconnection syndromes’, more diseases of this kind are being explored with modern approaches. One <a id="page_269"></a>common example is Alzheimer’s disease. When the brain-wide connectivity of older patients with Alzheimer’s was compared to those without it, it was found that those with Alzheimer’s had longer path lengths between brain areas. The confusion and cognitive impairment of Alzheimer’s disease may result, in part, from the breakdown of efficient communication between distant brain regions. Similar changes in brain network structures are seen to a lesser extent with normal ageing. </p>
<p class="TXI">Since starting her own lab at the University of Pennsylvania in 2013, Bassett has moved on from just observing the structure of the brain in health and disease to figuring out how to exploit it. The activity of complex networks can be hard to predict. A rumour whispered to a friend could die out immediately or spread across your social network, depending on the structure of that network and your friend’s place within it. The effects of stimulating or silencing neurons can be equally hard to anticipate. The Bassett lab is combining tools from engineering with knowledge about the structure of brain networks to make control of neural activity more tractable. In particular, models based on the brain-wide connectomes of individual people were used to determine exactly where brain stimulation should be applied in order to have the desired effect. The aim is to use this individualised treatment to get disorders like Parkinson’s disease and epilepsy under control. </p>
<p class="TXI">The hope that the metrics of graph theory may serve as markers of disease – potentially even early markers that could lead to preventative care – has made them quite popular in medical research. Thus far, the brain <a id="page_270"></a>disorders scrutinised by network analysis include Alzheimer’s, schizophrenia, traumatic brain injury, multiple sclerosis, epilepsy, autism and bipolar disorder. Results, however, have been mixed. As was pointed out, the MRI technique for collecting the data in the first place has its problems and some studies find disease signatures that others don’t. Overall, with so many excited scientists on the hunt for differences between diseased and healthy brains, some false positives and faulty data are bound to get caught up in the findings. But whether the results are solidified yet or not, it’s safe to say this new slate of tools has made an arrival on the clinical scene. </p>
<p class="center">* * *</p>
<p class="TXT">A developing brain is an eruption. Neurons bubble out of a neuronal nursery called the ventricular zone at a breakneck pace and pour into all corners of the burgeoning brain. Once there, they start making connections. These indiscriminate neurons form synapse after synapse with each other, frenetically linking cells near and far. At the height of synapse building in the human brain – during the third trimester of pregnancy – 40,000 such connections are constructed every second. Development is an explosion of neuronal and synaptic genesis. </p>
<p class="TXI">But just as soon as they come, many of these cells and connections go. An adult has far fewer neurons than they had in the womb; as many as half the neurons produced during development die. The number of <a id="page_271"></a>connections a neuron in the cortex makes peaks around the first year of life and gets reduced by a third thereafter. The brain is thus built via a surge and a retreat, a swelling and a shrinking. During development, the pruning of neurons and synapses is ruthless; only the useful survive. Synapses, for example, are built to carry signals between neurons. If no signal flows, the synapse must go. Out of this turmoil and turnover emerge operational neural circuits. It’s like encouraging the overgrowth of shrubbery for the purpose of carving delicate topiary out of it.</p>
<p class="TXI">Such is the way biology found to build the brain. But if you asked a graph theorist how to make a network, they’d give the exact opposite answer. The designer of a public transit system, for example, wouldn’t start by building a bunch of train stations and bus stops and connecting them all up just to see what gets used. No government would approve such a waste of resources. Rather, most graphs are built from the bottom up. For example, one strategy graph theorists use is to first build a graph that – using the fewest edges possible – has a path between any two nodes. This means some paths may be quite long, but by observing what paths get used the most (by commuters on a train or information travelling between servers on the internet) the network designer can identify where it would be useful to add a shortcut. Thus, the network gets more efficient by adding well-placed edges. </p>
<p class="TXI">The brain, however, doesn’t have a designer. There is no central planner that can look down and say: ‘It looks like signals would flow better if that neuron over there <a id="page_272"></a>was connected to this one here.’<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> That is why the brain needs to overproduce and prune. The only way the brain can make decisions about which connections should exist is by calculating the activity that passes through those connections. Individual neurons and synapses have elaborate molecular machinery for measuring how much use they’re getting and growing or shrivelling as a result. If the connection doesn’t exist in the first place, though, the activity on it can’t be measured.</p>
<p class="TXI">Pruning of connections in the brain starts off very strong, with synapses getting slashed left and right, but it then slows down over time. In 2015, scientists at the Salk Institute and Carnegie Mellon University explored why this pattern of pruning may be beneficial to the brain. To do so, they simulated networks that started overgrown and were pruned via a ‘use it or lose it’ principle. Importantly, they varied just how quickly this pruning happened. They found that networks that mimicked the pruning process in the brain (with the rate of pruning high initially and lowering over time) ended up with short average path lengths and were capable of effectively routing information even if some nodes or edges were deleted. This efficiency and robustness wasn’t as strong in networks where the pruning rate was constant, or increased, over time. A decreasing pruning rate, it seems, has the benefit of quickly eliminating useless connections while still <a id="page_273"></a>allowing the network enough time to fine-tune the structure that remains; a sculptor working with marble, similarly, may be quick to cut out the basic shape of a man, but carving the fine details of the body is a slow and careful process. While most physical networks like roads or telephone lines will never be built based on pruning, digital networks that don’t have costs associated with constructing edges – such as those formed by the wireless communication between mobile devices – could benefit from brain-inspired algorithms. </p>
<p class="center">* * *</p>
<p class="TXT">Network neuroscience, the name given to this practice of using the equipment of graph theory and network science to interrogate the structures of the brain, is a young field. <span class="italic">Network Neuroscience</span>, the first academic journal dedicated solely to this endeavour, was first published in 2017. New tools for mapping out connectomes at multiple scales have coincided with the computational power to analyse larger and larger datasets. The result is an electrified environment, with ever more and diverse studies of structure conducted every day. </p>
<p class="TXI">A reason for caution, however, may be found in the stomach of a lobster.</p>
<p class="TXI">The stomatogastric ganglion is a circuit of 25–30 neurons located in the gut of lobsters and other crustaceans. These neurons conspire through their connections to perform a basic but crucial job: produce the rhythmic muscle contractions that guide digestion. Eve Marder, a professor at Brandeis University in <a id="page_274"></a>Massachusetts, has spent half a century studying this handful of neurons. </p>
<p class="TXI">Marder was born and raised in New York but her education took her to Massachusetts and then California.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> While her doctorate work at the University of California, San Diego, was solidly in neuroscience, Marder always had an aptitude for mathematics: in primary school, she worked her way through maths textbooks meant for students two years her senior. This polymath personality permeates her science. Throughout her career she has collaborated with researchers from many backgrounds, including Larry Abbott (mentioned in <a href="chapter1.xhtml#chapter1">Chapter 1</a>), as he was making his transition from a particle physicist to renowned theoretical neuroscientist. Blending experimental exactness with a mathematical mindset, Marder has thoroughly probed the functioning of this little lobster circuit both physically and in computer simulations.</p>
<p class="TXI">The connectome of the lobster stomatogastric ganglion has been known since the 1980s. The 30 neurons of this ganglion form 195 connections and send outputs to the muscles of the stomach. For her PhD, Marder worked out what chemicals these neurons use to communicate. In addition to standard neurotransmitters – the chemicals that traverse the small synaptic cleft between the neuron releasing them and the one receiving <a id="page_275"></a>them – Marder also found a panoply of neuromodulators at play. </p>
<p class="TXI">Neuromodulators are chemicals that fiddle with the settings of a neural circuit. They can turn the strength of connections between neurons up or down and make neurons fire more, less or in different patterns. Neuromodulators effect these changes by latching on to receptors embedded in a neuron’s cell membrane. Part of what’s noteworthy about neuromodulators is where they come from and how they get to the neuron. In the most extreme case, a neuromodulator could be released from a different part of the brain or body and travel through the blood to its destination. Other times, a neuromodulator is released locally from nearby neurons – but whether from near or afar, neuromodulators tend to bathe a circuit indiscriminately, touching many neurons and synapses in a diffuse way. Whereas regular neurotransmission is like a letter sent between two neurons, neuromodulation is a leaflet sent out to the whole community.</p>
<p class="TXI">In the 1990s, Marder, along with members of her lab and the lab of Michael Nusbaum, a professor at the University of Pennsylvania, experimented with neuromodulators in the stomatogastric ganglion circuit. Ordinarily, the circuit produces a steady rhythm, with certain neurons in the population firing about once per second. But, when the experimenters released neuromodulators on to the circuit, this behaviour changed. Some neuromodulators made the rhythm increase: the same neurons fired, but more frequently. Others made the rhythm decrease, and some had a more dramatic effect, both disrupting the rhythm and <a id="page_276"></a>activating neurons that were ordinarily silent. The neuromodulators causing these changes were all released from neurons that normally provide input to this circuit. This means these different patterns of output are plausibly produced naturally throughout the animal’s life. In more artificial settings, neuromodulators added by the experimenters can cause even larger and more diverse changes. </p>
<p class="TXI">Importantly, throughout these experiments the underlying network never changed. No neurons were added or deleted, nor did they cut or grow any connections. The marked changes in behaviour stemmed solely from a small sprinkling of neuromodulators atop a steady structure. </p>
<p class="TXI">The massive effort poured into getting a connectome presupposes a certain amount of payoff that will come from having it, but the payoff is less if the structure-function relationship is looser than it may have seemed. If neuromodulators can release the activity of neurons in a circuit from the strict constraints of their architecture, then structure is not destiny. Perhaps this wouldn’t be such a concern if neuromodulation were a phenomenon specific to the stomatogastric ganglion. This, however, is far from the truth. Brains are constantly bathing in modulating molecules. Across species, neuromodulators are responsible for everything from sleeping to learning, moulting to eating. Neuromodulation is the rule, not the exception. </p>
<p class="TXI">Through mathematical simulations of the circuits she studies, Marder has explored not just how different behaviours arise from the same structure, but also how <a id="page_277"></a>
<span class="italic">different</span> structures can produce the <span class="italic">same</span> behaviours. Specifically, each lobster has a slightly different configuration of its gut circuitry: connections may be built stronger or weaker in one animal versus another. By simulating as many as 20 million possible ganglion circuits, Marder’s lab found that the vast majority aren’t capable of producing the needed rhythms, but certain specific configurations are. Each lobster, through some combination of genes and development, finds its way to one of these functioning configurations. The work makes an important point about individual brains: diversity doesn’t always mean difference. What may look like a deviation from the structural norm could in fact be a perfectly valid way to achieve the same outcomes. That these diverse structures create the same rhythms adds another wrinkle to the structure-function relationship.</p>
<p class="TXI">As much as Marder’s work shows the limits of structure for understanding function, it also shows the need for it. Her lifetime of work – and all the insights it has provided – is built atop the connectome. Without that detailed structural information, there is no structure-function relationship to be explored. As Marder wrote in 2012: ‘Detailed anatomical data are invaluable. No circuit can be fully understood without a connectivity diagram.’ However, she goes on to remark that ‘a connectivity diagram is only a necessary beginning, but not in itself, an answer’. In other words, when it comes to understanding the brain, knowing the structure of the nervous system is both completely necessary and utterly insufficient.</p><a id="page_278"></a>
<p class="TXI">So, it may not be possible to satisfy Cajal’s vision of intuiting the function of the nervous system from mere meditations on its structure. But the work of finding and formalising that structure is still an important prerequisite for any further understanding of the brain. Innovative methods for gathering connectome data are blossoming and the formalisms of graph theory are in place, prepared to take in and digest that data.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-1" id="fn-1">1</a> ﻿The images were attractive enough to be the centrepiece of a travelling art exhibition called ﻿<span class="italic">The Beautiful Brain</span>﻿, a fate that surely would﻿’﻿ve pleased Cajal. Before succumbing to his father﻿’﻿s wishes that he be a physician, Cajal dreamt of being an artist.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-2" id="fn-2">2</a> ﻿Milgram﻿’﻿s experiment has been criticised by later researchers for lacking rigour. He didn﻿’﻿t, for example, take into account the folders that never made it to the target person. As a result, whether six really is the magic number when it comes to degrees of separation between people has remained an open question. Luckily, data from social media sites is offering new ways to answer it. ﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-3" id="fn-3">3</a> ﻿There are currently some successful attempts to automate this arduous process. In the meantime, desperate scientists have also tried to turn this work into a game and get people all over the world to play it. It can be found at ﻿﻿eyewire.org﻿﻿.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-4" id="fn-4">4</a> ﻿A note of caution: precisely what features a brain area needs to have to be considered a hub is debated. And even applying the same definition to different datasets can lead to different conclusions. As it is still early days for this style of analysis, such kinks will take time to work out.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-5" id="fn-5">5</a> ﻿British neuroscientist Karl Friston bestowed this title on Bassett in a 2019 interview in ﻿<span class="italic">Science</span>﻿. We﻿’﻿ll hear more about Friston in ﻿﻿Chapter 12﻿﻿.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-6" id="fn-6">6</a> ﻿There may be an exception to this in very simple animals, such as ﻿<span class="italic">C. elegans</span>﻿, where it﻿’﻿s believed a lot of the information about who should connect to whom is encoded in the genome and is thus ﻿‘﻿designed﻿’﻿ through eons of natural selection.﻿</p>
<p class="FN1"><a href="chapter9.xhtml#fnt-7" id="fn-7">7</a> ﻿Marder entered graduate school in 1969, a time at which women were becoming a more common sight in these programmes, but barriers still existed. As she recounts in her autobiography, ﻿‘﻿I knew it unlikely that I would get into Stanford biology because they were widely said to have a quota on women (2 out of 12).﻿’﻿﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<meta charset="utf-8"/>
<title>Contents</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="FMT" id="re_contents">Contents</p>
<p class="TOC-CH"><a href="chapter1.xhtml#chapter1" id="re_chapter1">Chapter 1:  Spherical Cows</a></p>
<p class="TOC-CH"><a href="chapter2.xhtml#chapter2" id="re_chapter2">Chapter 2:  How Neurons Get Their Spike</a></p>
<p class="TOC-CH"><a href="chapter3.xhtml#chapter3" id="re_chapter3">Chapter 3:  Learning to Compute</a></p>
<p class="TOC-CH"><a href="chapter4.xhtml#chapter4" id="re_chapter4">Chapter 4:  Making and Maintaining Memories</a></p>
<p class="TOC-CH"><a href="chapter5.xhtml#chapter5" id="re_chapter5">Chapter 5:  Excitation and Inhibition</a></p>
<p class="TOC-CH"><a href="chapter6.xhtml#chapter6" id="re_chapter6">Chapter 6:  Stages of Sight</a></p>
<p class="TOC-CH"><a href="chapter7.xhtml#chapter7" id="re_chapter7">Chapter 7:  Cracking the Neural Code</a></p>
<p class="TOC-CH"><a href="chapter8.xhtml#chapter8" id="re_chapter8">Chapter 8:  Movement in Low Dimensions</a></p>
<p class="TOC-CH"><a href="chapter9.xhtml#chapter9" id="re_chapter9">Chapter 9:  From Structure to Function</a></p>
<p class="TOC-CH"><a href="chapter10.xhtml#chapter10" id="re_chapter10">Chapter 10:  Making Rational Decisions</a></p>
<p class="TOC-CH"><a href="chapter11.xhtml#chapter11" id="re_chapter11">Chapter 11:  How Rewards Guide Actions</a></p>
<p class="TOC-CH"><a href="chapter12.xhtml#chapter12" id="re_chapter12">Chapter 12:  Grand Unified Theories of the Brain</a></p>
<p class="TOC-CH1"><a href="Mathematical.xhtml#Mathematical" id="re_Mathematical">Mathematical Appendix</a></p>
<p class="TOC-CH"><a href="ack.xhtml#ack" id="re_ack">Acknowledgements</a></p>
<p class="TOC-CH"><a href="bib.xhtml#bib" id="re_bib">Bibliography</a></p>
<p class="TOC-CH"><a href="index.xhtml#index" id="re_index">Index</a></p>
</body>
</html>