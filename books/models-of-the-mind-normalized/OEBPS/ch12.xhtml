<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 12</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter12"><a href="contents.xhtml#re_chapter12">CHAPTER TWELVE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter12">Grand Unified Theories of the Brain</a><a id="page_341"></a></p>
<p class="H1" id="b-9781472966445-ch1290-sec13">
<span class="bold">
<span>Free energy principle, Thousand Brains Theory and integrated information theory</span>
</span></p>
<p class="TXT">One of the biggest shockwaves in the history of science hit physics in the mid-nineteenth century. James Clerk Maxwell, a Scottish mathematician, published his seven-part paper ‘A dynamical theory of the electromagnetic field’ in 1865. Through this marathon of insightful analogies and equations, Maxwell demonstrated a deep and important relationship between two already important forms of physical interaction: electricity and magnetism. Specifically, by defining electromagnetic field theory, Maxwell created the mathematical infrastructure needed to see the equations of electricity and magnetism as two sides of the same coin. In the process, he concluded that a third important object – light – was a wave in this electromagnetic field. </p>
<p class="TXI">Scientists had, of course, studied electricity, magnetism and light for centuries before Maxwell. And they had learned a fair bit about them, how they interact and how they can be harnessed. But Maxwell’s unification provided something profoundly different – a whole new way to interpret the physical world. It was the first domino to fall in a series of landmark discoveries in foundational <a id="page_342"></a>physics and paved the way for many of today’s technologies. Einstein’s work, for example, is built on electromagnetic field theory and he reportedly credited his success to standing ‘on the shoulders of Maxwell’. </p>
<p class="TXI">More than its direct impact on research, however, Maxwell’s theory planted the thought in the minds of future physicists that more subterranean relationships may exist among physical forces. Digging up these connections became a major aim of theoretical physics. By the twentieth century an explicit search for what are known as grand unified theories (GUTs) was on. At the top of the to-do list was finding a GUT that could further unify electromagnetism with two other forces: the weak force (which governs radioactive decay) and the strong force (which holds atomic nuclei together). A big step in this direction came in the early 1970s with the discovery that the weak force and electromagnetism become one at very high temperatures. Even so, bringing in the strong and weak forces still leaves out another big one, gravity. Physicists, therefore, remain in pursuit of a full GUT. </p>
<p class="TXI">GUTs tap into the aesthetic preferences of many physicists: simplicity, elegance, totality. They demonstrate how the whole can become greater than the sum of its parts. Before identifying a GUT, scientists are like the blind men touching an elephant in the old parable. They’re each relying on what little information they can grab from the trunk, the leg or the tail. Through this they come up with separate and incomplete stories about what each bit is doing. Once the entire elephant is seen, however, the pieces lock into place and each is understood in context of the others. The deep wisdom obtained by finally finding a GUT can’t be approximated <a id="page_343"></a>by studying the parts separately. Therefore, as difficult as they can be to find, the search for GUTs is largely deemed a worthwhile endeavour by the physics community. As physicist Dimitri Nanopoulos wrote in 1979, shortly after helping to coin the phrase, ‘grand unified theories give a very nice and plausible explanation of a whole lot of different and at first sight unrelated phenomena, and they definitely have the merit and right to be taken seriously’.</p>
<p class="TXI">But should GUTs of the brain be taken seriously? The notion that a small number of simple principles or equations will be able to explain everything about the form and function of the brain is appealing for the same reasons GUTs are coveted in physics. However, most scientists who study the brain are doubtful they could exist. As psychologists Michael Anderson and Tony Chemero wrote: ‘There is every reason to think that there can be no grand unified theory of brain function because there is every reason to think that an organ as complex as the brain functions according to diverse principles.’ A GUT for the brain, as great as it would be, is considered a fantasy by many. </p>
<p class="TXI">On the other hand, so much of what has been imported from physics to neuroscience – the models, the equations, the mindset – has helped advance the field in one direction or another. GUTs, core as they are to modern physics, are hard to ignore. They can be tantalising for those who study the brain even if they seem unlikely and to some scientists they are simply too seductive to pass up. </p>
<p class="TXI">Searching for a GUT in the brain is a high risk-high reward endeavour. As such, it tends to require a big <a id="page_344"></a>personality to lead it. Most candidate GUTs of the brain have a frontman of sorts – a scientist, usually the one who first developed the theory, who functions as the public face of it. To get a GUT to succeed also requires dedication: supporters of a theory will work for years, even decades, on refining it. And they’re always on the prowl for new ways of applying their theory to every facet of the brain they can find. Advocacy is important, too; even the grandest of GUTs can’t help to explain much if no one has heard of it. Therefore, many papers, articles and books have been written to communicate GUTs not just to the scientific community but to the world at large. It’s best for GUT enthusiasts to have a thick skin, however. Pushing of such theories can be met with disdain from the mass of scientists doing the more reliable work of studying the brain one piece at a time. </p>
<p class="TXI">Sociologist Murray S. Davis offered a reflection on theories in his 1971 article entitled ‘That’s interesting!’ In it, he said: ‘It has long been thought that a theorist is considered great because his theories are true, but this is false. A theorist is considered great, not because his theories are true, but because they are <span class="italic">interesting</span> … In fact, the truth of a theory has very little to do with its impact, for a theory can continue to be found interesting even though its truth is disputed – even refuted!’ Grand unified theories of the brain, whatever their truth may be, are surely interesting.</p>
<p class="center">* * *</p>
<p class="TXT">Generally jovial and soft-spoken, British neuroscientist Karl Friston doesn’t quite fit the profile for the leader of <a id="page_345"></a>an ambitious and controversial scientific movement. Yet he certainly has the devoted following of one. Scientists – ranging from students to professors, including those well outside the traditional bounds of neuroscience – gather ritualistically on Mondays to receive a few moments of his insights each. They are there to seek his unique wisdom mainly on one topic. It is an all-encompassing framework on which Friston has been building an understanding of the brain, behaviour and beyond for over 15 years: the free energy principle.</p>
<p class="TXI">‘Free energy’ is a mathematical term defined by differences between probability distributions. Yet its meaning in Friston’s framework can be summarised somewhat simply: free energy is the difference between the brain’s predictions about the world and the actual information it receives. The free energy principle says that everything the brain does can be understood as an attempt to minimise free energy – that is, to make the brain’s predictions align as much as possible with reality. </p>
<p class="TXI">Inspired by this way of understanding, many researchers have gone on a search for where predictions may be made in the brain and how they may get checked against reality. A small industry of research built around the idea of ‘predictive coding’ explores how this could be happening in sensory processing in particular.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> In most predictive coding models, information gets sent as normal through the sensory processing system. For <a id="page_346"></a>example, auditory information comes in from the ears, gets relayed first through regions in the brainstem and midbrain, and then goes on to be passed sequentially through several areas in the cortex. This <span class="italic">forward</span> pathway is widely accepted as crucial for how sensory information gets turned into perception, even by researchers who don’t hold much stock in the theory of predictive coding.</p>
<p class="TXI">What makes predictive coding unique is what it claims about the <span class="italic">backward</span> pathway – connections going from latter areas to earlier ones (say from the second auditory area in the cortex back to the first). In general, scientists have hypothesised many different roles for these projections. According to the predictive coding hypothesis, these connections carry predictions. For example, when listening to your favourite song, your auditory system may have very precise knowledge about the upcoming notes and lyrics. Under a predictive coding model, these predictions would be sent backwards and get combined with forward-coming information about what’s actually happening in the world. By comparing these two streams, the brain can calculate the error between its prediction and reality. In fact, in most predictive coding models, specific ‘error’ neurons are tasked with just this calculation. Their activity thus indicates just how wrong the brain was: if they are firing a lot, the error in prediction was high, if they’re quiet, it was low. In this way, the activity of these neurons is a physical instantiation of free energy. And, according to the free energy principle, the brain should aim to make these neurons fire as little as possible.</p>
<p class="TXI">Do such error neurons exist in sensory pathways? And does the brain learn to quiet them by making <a id="page_347"></a>better predictions about the world? Scientists have been looking for answers to these questions for many years. A study conducted by researchers at Goethe University Frankfurt, for example, found that some neurons in the auditory system do decrease their firing when an expected sound is heard. Specifically, the researchers trained mice to press a noise-making lever. When the mice heard the expected sound after pressing the lever, their neurons responded less than if that same sound were played at random or if the lever made an unexpected sound. This suggests the mice had a prediction in mind and the neurons in their auditory system were firing more when that prediction was violated. Overall, however, the evidence for predictive coding is mixed. Not all studies that go looking for error neurons find them and, even when they do, these neurons don’t always behave exactly as the predictive coding hypothesis would predict. </p>
<p class="TXI">Making the brain a better predictive machine might seem like the most obvious way of minimising free energy, but it’s not the only one. Because free energy is the difference between the brain’s prediction and experience, it can also be minimised by controlling experience. Imagine a bird that has grown accustomed to flying around a certain forest; it can predict which trees will be good for nest building, where the best food is found and so on. One day it flies a little beyond its normal range and finds itself in a city. Experiencing tall buildings and traffic for the first time, its ability to predict almost anything about the world around it is low. This big discrepancy between prediction and experience means free energy is high. To bring its free energy back <a id="page_348"></a>down, the bird could stick around and hope its sensory systems adapt to be able to predict the features of city life. Or it could simply fly back to the forest it came from. The presence of this second option – choosing actions that result in predictable sensory experiences – is what makes the free energy principle a candidate GUT of the brain. Rather than just explaining features of sensory processing, this principle can encompass decisions about behaviour as well. </p>
<p class="TXI">The free energy principle has indeed been invoked to explain perception, action and everything in between.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> This includes processes like learning, sleep and attention, as well as disorders like schizophrenia and addiction. It is also argued that the principle can account for the anatomy of neurons and brain areas, along with the details of how they communicate. In fact, Friston hasn’t even constrained free energy to the brain. He’s argued for it as a guiding principle of all of biology and evolution and even as a way of understanding the fundamentals of physics. </p>
<p class="TXI">This tendency to try to wrap complex topics into simple packages has been with Friston throughout his life. In a 2018 <span class="italic">Wired</span> profile, he recalls a thought he had as a teenager: ‘There must be a way of understanding everything by starting from nothing … If I’m only allowed to start off with one point in the entire universe, can I derive everything else I need from that?’ In <a id="page_349"></a>Friston’s world, the free energy principle is now the nearly nothing that can explain almost everything.</p>
<p class="TXI">Outside Friston’s world, however, the capabilities of the free energy principle aren’t always as obvious. Given its grand promises, countless scientists have attempted to understand the ins and outs of Friston’s theory. Few (even those who count themselves fans of the principle) consider their attempts wholly successful. It’s not necessarily that the equations are too complicated – many of the scientists trying have dedicated their lives to understanding the mathematics of the mind. Rather, how to extrapolate and apply the free energy principle to all the nooks and crannies of brain function requires a type of intuition that seems to run strongest in Friston himself. Without a clear and objective means of interpreting free energy in any particular case, Friston is left to play the role of free energy whisperer, laying out his take on its implications in countless papers, talks, and Monday meetings. </p>
<p class="TXI">The confusion around the free energy principle likely results from a feature of it that Friston readily acknowledges: it’s not falsifiable. Most hypotheses about how the brain functions are falsifiable – that is, they make claims that can be proven wrong through experiments. The free energy principle, however, is more a way of looking at the brain than a strong or specific claim about how it works. As Friston said: ‘The free energy principle is what it is—a principle … there’s not much you can do with it, unless you ask whether measurable systems conform to the principle.’ In other words, rather than trying to make clean predictions about the brain based on the free energy principle, <a id="page_350"></a>scientists should instead ask if the principle helps them see things in a new light. Trying to figure out how a bit of the brain works? Ask if it somehow minimises free energy. If that leads to progress, great; if not, that’s fine, too. In this way, the free energy principle is meant to at best offer a scaffolding on which to hang facts about the brain. Insofar as it can connect a great many facts, it is grand and somewhat unifying; however – without falsifiability – its status as a theory is more questionable. </p>
<p class="center">* * *</p>
<p class="TXT">Numenta is a small tech company based in Redwood City, California. It was founded by Jeff Hawkins, an entrepreneur who previously founded two companies that produced predecessors to the modern smartphone. Numenta, on the other hand, makes software. The company designs data-processing algorithms aimed to help stockbrokers, energy distributors, IT companies and the like identify and track patterns in streams of incoming data. Numenta’s main goal, however, is to reverse-engineer the brain. </p>
<p class="TXI">Even as he weaved his way through an illustrious career in tech, Hawkins always harboured an interest in the brain. Despite never earning a degree in the field himself, he started the Redwood Neuroscience Institute in 2002. The institute would go on to become part of the University of California, Berkeley, and Hawkins would go on to Numenta in 2005. The work of Numenta is based mainly on ideas presented in a 2004 book Hawkins co-authored with Sandra Blakeslee, <span class="italic">On Intelligence</span>. The book summarises Hawkins’ theory about <a id="page_351"></a>how the neocortex – the thin layer of brain tissue covering the surface of mammalian brains – works to produce sensation, cognition, learning, movement and more. It’s a set of ideas that now goes under the name ‘The Thousand Brains Theory of Intelligence’. </p>
<p class="TXI">At the centre of the Thousand Brains Theory is a piece of neuro-architecture known as the cortical column. Cortical columns are small patches of cells, less than the tip of a pencil in diameter and about four times that in length. They’re so-named because they form cylinders running from the top of the neocortex through to the bottom, like so many parallel strands of spaghetti. Looking at a column length-wise, it resembles sheets of sediment: the neurons are segregated into six visibly identifiable layers. Neurons in different layers interact with each other by sending connections up or down. Typically, all the neurons in a column perform a similar function; they may all, for example, respond in a similar way to a sensory input. Yet the different layers do seem to serve some different purposes: some layers, for example, get input from other brain areas and other layers send output off. </p>
<p class="TXI">Vernon Mountcastle, the sensory neuroscientist who first identified these columns in the mid-twentieth century, believed they represented a fundamental anatomical unit of the brain. Though it went against the dogma of the time, Mountcastle saw potential in the idea of a single repeating unit that tiles the whole of the neocortex – a single unit that could process the full variety of information the cortex receives. Hawkins agrees. In his book, he describes Mountcastle’s work as ‘the Rosetta stone of neuroscience’ because it is ‘a single <a id="page_352"></a>idea that united all the diverse and wondrous capabilities of the human mind’. </p>
<p class="TXI">To understand what Hawkins thinks these mini-processing units do we have to consider both time and space. ‘If you accept the fact intelligent machines are going to work on the principles of the neocortex,’ Hawkins said in a 2014 interview, ‘[time] is the entire thing.’ Inputs to the brain are constantly changing; this makes a static model of brain function woefully incomplete. What’s more, the outputs of the brain – the behaviours produced by the body – are extended through both space and time. According to Hawkins, actively moving the body through space and getting dynamic streams of sensory data in return helps the brain build a deep understanding of the world. </p>
<p class="TXI">Neuroscientists know a bit about how animals move through the world. It has a lot to do with a type of neuron called a ‘grid cell’ (see Figure 26).<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> Grid cells are neurons that are active when an animal is in certain special locations. Imagine a mouse running around an open field. One of its grid cells will be active when the mouse is right in the middle of the field. That same cell will <span class="italic">also</span> be active when the mouse has moved a few body lengths north of the centre and then again a few lengths north of that. The same pattern of activity would also be seen if the mouse went 60 degrees west of north instead. In fact, if you made a map of all the places this cell is active it would form a polka-dot pattern across the <a id="page_353"></a>whole field. These polka dots would all be evenly spaced at the vertices of a triangular grid (hence the name). Different grid cells differ in the size and orientation of this grid, but they all share this common feature.</p>
<p class="image-fig" id="fig26.jpg">
<img alt="" src="Images/chapter-12-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 26</span>
</span></p>
<p class="TXI">Impressed by their ability to represent space, Hawkins made grid cells an integral part of his theory of how the neocortex learns about the world. There is a problem, however: grid cells aren’t found in the neocortex. Instead, they reside in an evolutionarily older part of the brain known as the entorhinal cortex. Despite little evidence for grid cells outside of this region, Hawkins hypothesises that they are actually hiding away in the sixth layer of every column in the neocortex. </p>
<p class="TXI">What exactly are they doing there? To explain this, Hawkins likes to use the example of running your finger around a coffee cup (Hawkins actually attributes the origins of his theory to a eureka moment he had while contemplating a coffee cup and will even bring the cup to talks for demonstration). Columns in the sensory processing part of the neocortex will get inputs from the fingertip. According to Hawkins’ theory, the grid cells at the bottom of these columns will also be tracking the location of the fingertip. Combining information about <a id="page_354"></a>where the finger is and what the cup feels like there, the column can learn the shape of the object as it navigates around it. The next time the same object is encountered the column can use this stored knowledge to recognise it. </p>
<p class="TXI">As these cortical columns exist across the neocortex, this process could be happening in parallel everywhere. The columns that represent other parts of the hand, for example, would be building their own models of the coffee cup as they come into contact with it. And areas in the visual system would combine visual information with the location of the eyes to build their own understanding of the cup, too. In total, a cohesive understanding of the world is built through these distributed contributions, like thousands of brains working in unison.</p>
<p class="TXI">Hawkins’ theory is ever-evolving and many of the details are still to be worked out, but his hopes for it are high. The way he sees it, just as columns could learn the shapes of literal objects, so too could they learn the shapes of abstract ones. Navigating the space of thought or language can be accomplished with the same mechanisms for navigating the real, physical world. If this is true, it explains how a repeating pattern in the neocortex can be used to do so many different things, from vision to audition, movement to mathematics. </p>
<p class="TXI">Exactly how identical these columns are, however, is a subject of debate. At a glance the neocortex may appear like a uniform tessellation, but upon closer inspection differences do emerge. Some studies have found that the size of columns, the number and type of neurons they contain, and how they interact with each other varies across regions of the neocortex. If these columns are not <a id="page_355"></a>actually anatomically identical, they may differ in function, too. This means that each brain region could be more specialised for the kind of tasks it has to perform than the Thousand Brains Theory assumes. If that’s the case, the hope for a universal algorithm of intelligence may be dashed. </p>
<p class="TXI">As seen throughout this book, mathematical models of the brain are usually built by first identifying – out of the mounds of available data – a selection of facts that seem relevant. Those facts are then simplified and pieced together in a way that demonstrates how, in theory, a bit of the brain could work. Additionally, in figuring out how exactly to cobble this toy version of biology together, some new and surprising predictions may be made. When compared to this schema, the Thousand Brains Theory is a model like any other in neuroscience. Indeed, many of the component concepts – columns, grid cells, object recognition – have already been studied extensively by means of both experimental and computational work. In this way, the theory is not unique; it’s a guess that may be right, may be wrong or may – like many theories – be a little of both. </p>
<p class="TXI">Perhaps what sets the work of Hawkins and Numenta apart then is simply the unrelenting optimism that this one <span class="italic">is</span> different – that for the first time a key that will unlock all the doors of the cortex is truly within reach. When asked in a 2019 interview about how far off a full understanding of the neocortex is, Hawkins said: ‘I feel I’ve already passed a step function. So, if I can do my job correctly over the next five years – meaning I can proselytise these ideas, I can convince other people they’re right, we can show that other machine learning <a id="page_356"></a>people should pay attention to these ideas – then we’re definitely in an under 20-year timeframe.’ That’s a confidence not commonly seen in scientists – and because Hawkins has the private funding to back it, it’s a confidence that is not constrained by normal scientific pressures. </p>
<p class="TXI">Hawkins is known to be self-assured in his claims about the brain. However, his past ability to deliver on promises for breakthrough brain-based algorithms has been questionable. Geoffrey Hinton, a leader in artificial intelligence research, has described Hawkins’ contributions to the field as ‘disappointing’. And in 2015, psychology professor Gary Marcus compared Numenta’s work to other AI techniques by saying: ‘I have not seen a knock-down argument that they yield better performance in any major challenge area.’ What chance does the Thousand Brains Theory have of ever providing the field with a set of truly universal mechanisms of intelligence? Only time – a concept so central to Hawkins’ thinking – will tell. </p>
<p class="center">* * *</p>
<p class="TXT">By some accounts, no theory of the brain could be complete without explaining its biggest and most enduring mystery: consciousness. The C-word can be a tough subject for scientists, laden as it is with centuries of philosophical baggage. Yet, in the eyes of certain researchers, a precise scientific definition that could be used to not just identify but also <span class="italic">quantify</span> consciousness anywhere it may exist is the holy grail of their work. It is also the promise of ‘integrated information theory’.</p> <a id="page_357"></a>
<p class="TXI">Integrated information theory (or IIT) is an attempt to define consciousness with an equation. It was originally put forth by Italian neuroscientist Giulio Tononi in 2004 and has been iterated on by him and others ever since. IIT is designed to measure consciousness in anything: in computers, rocks and aliens as easily as in brains. By making a universal claim of what consciousness is, it differs from the more biology-centred theories devised by some neuroscientists.</p>
<p class="TXI">IIT is able to free itself from the specific physical features of the brain because its inspiration comes from another source entirely: introspection. By reflecting on the first-person conscious experience, Tononi came up with five important traits fundamental to consciousness; these are the ‘axioms’ on which IIT is built. The first axiom is the basic fact that consciousness exists. Others include the observation that a conscious experience is composed of multiple distinct sensations, the experience is specific, it appears to us as an integrated whole and it is uniquely what it is – no more or less. </p>
<p class="TXI">Tononi considered what kinds of information-processing systems could give rise to these axioms of experience. Through this, he was able to map the axioms to mathematical terms. The end result is a unified measure of what is called ‘integrated information’, a value Tononi symbolises with the Greek letter <span class="italic">phi</span>. In total, phi indicates just how intermixed the information in a system is. The right kind of intermixing is supposed to give rise to the richness and wholeness of experience. According to IIT, the higher the phi that a system has, the more conscious it is.</p> <a id="page_358"></a>
<p class="TXI">As it turns out, calculating the phi for a system with any reasonable amount of complexity is nearly impossible. For the human brain, it would first require conducting a near endless amount of experiments in order to probe how the different sub-structures of the brain interact. Even if that could be done, a long and gruelling series of computations would then begin. To overcome this hurdle, multiple approximations to phi have been devised. Through this, it is possible to make educated guesses about the phi in a system. This has been used to explain why certain brain states lead to more conscious experience than others. For example, during sleep, the ability of neurons to communicate effectively is interrupted. This makes the brain less able to integrate information, resulting in lower phi. According to Tononi’s theory, similar reasoning can explain the unconsciousness that comes with seizures as well. </p>
<p class="TXI">The theory also makes some, perhaps surprising, predictions. For example, the phi of an average thermostat is small, but still not zero. This implies that the device regulating your room temperature has some amount of conscious experience. What’s more, some very simple devices – if built just right – can actually have a value of phi much higher than the estimated phi of the human brain. These counterintuitive conclusions make some scientists and philosophers sceptical of IIT. </p>
<p class="TXI">Another critique of the theory is targeted at its axiomatic basis. According to this argument, the axioms Tononi chose aren’t the only ones that a theory of consciousness could be built on. And his way of mapping these axioms to mathematics isn’t obviously the only, or best, way either. The problem is: if the foundations of IIT <a id="page_359"></a>are arbitrary, then how can we trust the conclusions that spring from them, especially when they surprise us? </p>
<p class="TXI">An informal survey of consciousness scientists conducted in 2018 revealed that IIT was not the favoured theory among experts (it came in fourth after two other theories and the catch-all category of ‘other’). But the same survey found that IIT fared better among non-experts: in fact, it was rated first among the subset of non-experts who felt they had enough knowledge to respond. Some of the survey authors suspect this may be a result of IIT’s PR. From the outside, the theory looks well founded if only because it has the authority of hard math behind it. And more than most scientific theories of consciousness, IIT has been featured in the popular press. This includes writings by Christof Koch, a prominent neuroscientist who has become a collaborator of Tononi’s and a public advocate of IIT. In his book, <span class="italic">Consciousness: Confessions of a Romantic Reductionist</span>, Koch describes his personal journey through the scientific study of consciousness, including work he did with Nobel Prize winner Francis Crick, and his views on IIT.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> Such popular accounts may be effective in getting the theory out to a broader audience, but don’t necessarily convince scientists in the know. </p>
<p class="TXI">Even scientists who lack faith in the power of IIT still tend to applaud the attempt. Consciousness is a notoriously <a id="page_360"></a>difficult concept to tame, which makes IIT’s effort to submit it to orderly scientific inquiry still a step in the right direction. As vocal critic of IIT physicist Scott Aaronson wrote on his blog: ‘The fact that integrated information theory is wrong – demonstrably wrong, for reasons that go to its core – puts it in something like the top 2 per cent of all mathematical theories of consciousness ever proposed. Almost all competing theories of consciousness, it seems to me, have been so vague, fluffy and malleable that they can only <span class="bold">aspire</span> to wrongness.’ </p>
<p class="center">* * *</p>
<p class="TXT">GUTs can be a slippery thing. To be grand and unifying, they must make simple claims about an incredibly complex object. Almost any statement about ‘the brain’ is guaranteed to have exceptions lurking somewhere. Therefore, making a GUT too grand means it won’t actually be able to explain much specific data. But, tie it too much to specific data and it’s no longer grand. Whether untestable, untested, or tested and failed, GUTs of the brain, in trying to explain too much, risk explaining nothing at all.</p>
<p class="TXI">While this presents an uphill battle for GUT-seeking neuroscientists, it’s less of a challenge in physics. The reason for this difference may be simple: evolution. Nervous systems evolved over eons to suit the needs of a series of specific animals in specific locations facing specific challenges. When studying such a product of natural selection, scientists aren’t entitled to simplicity. Biology took whatever route it needed to create functioning organisms, without regard to how understandable any part of them would be. It should be no surprise, then, to find <a id="page_361"></a>that the brain is a mere hodgepodge of different components and mechanisms. That’s all it needs to be to function. In total, there is no guarantee – and maybe not even any compelling reasons to expect – that the brain can be described by simple laws.</p>
<p class="TXI">Some scientists choose to embrace this mess. Instead of reducing the brain to its barest elements, they build a form of ‘grand unified model’ that sticks all of the parts together. While traditional GUTs have the simplicity of a steak cooked with just a sprinkle of salt, these models are more like a big pot of soup. And while they’re not as sleek and elegant as GUTs, they may be better equipped to get the job done. </p>
<p class="TXI">The hyper-detailed simulation built by the Blue Brain Project, as discussed in <a href="chapter2.xhtml#chapter2">Chapter 2</a>, is one example of this more inclusive approach. These researchers extracted countless details about neurons and synapses through a series of painstaking experiments. They then pieced all this data back together into an elaborate computational model of just a small speck of the brain. Such an approach assumes that each detail is precious and that the brain won’t be understood by stripping them away. It’s a wholehearted embrace of the nuance of biology, with the hope that by throwing everything together, a fuller understanding of what makes the brain work will emerge. The trouble here, however, is scale. A bottom-up approach to rebuilding the brain can only proceed one neuron at a time, which means a complete model is a long way off. </p>
<p class="TXI">The Semantic Pointer Architecture Unified Network, better known as SPAUN, comes at things from a very different direction. Rather than capturing <a id="page_362"></a>all the minutiae of the neurobiology, SPAUN – developed by a team working under Chris Eliasmith at University of Waterloo in Ontario, Canada – is about making a model of the brain that works. That means getting the same sensory inputs and having the same motor outputs. Specifically, SPAUN has access to images as input and controls a simulated arm to write its outputs. In between this input and output is a complex web of 2.5 million simple model neurons, arranged to broadly mimic the structure of the whole brain. Through these neural connections SPAUN can perform seven different cognitive and motor tasks, such as drawing digits, recalling lists of objects and completing simple patterns. In this way, SPAUN eschews elegance for function. Of course, the human brain contains tens of thousands of times as many neurons and can do a lot more than seven tasks. Whether the principles of utility and scale that got SPAUN to where it is can take it all the way up to a full model of the brain – or whether more of the nuances of neurons need to be added – is unknown.</p>
<p class="TXI">True GUTs aim to condense. They melt diverse information down into a compact and digestible form. This makes GUTs seem satisfying because they give the sense that the workings of the brain can be fully grasped in one hand. Models like SPAUN and the Blue Brain Project simulation, however, are expansive. They bring in many sources of data and use them to build an elaborate structure. In this way, they sacrifice interpretability for accuracy. They aim to explain everything by incorporating everything there is to explain.</p> <a id="page_363"></a>
<p class="TXI">Though as with all models, even these more expansive ones are still not perfect replicas. Makers of these models still need to choose what to include and what not to include, what they aim to explain and what they can ignore. When aiming for something akin to a GUT, the hope is always to find the simplest set of principles that can explain the largest set of facts. With an object as dense and messy as the brain, that simple set may still be pretty complicated. To know in advance what level of detail and what magnitude of scale will be needed to capture the relevant features of brain function is impossible. It is only through the building and testing of models that progress on that question can be made. </p>
<p class="TXI">On the whole, neuroscience has enjoyed a very fruitful relationship with the ‘harder’, more quantitative sciences. It has received many gifts from the likes of physics, mathematics and engineering. These analogies, methods and tools have shifted thinking about everything from neurons to behaviour. And the study of the brain has given back in return, providing inspiration for artificial intelligence and a testing ground for mathematical techniques. </p>
<p class="TXI">But neuroscience is not physics. It must avoid playing the role of the kid sibling, trying to follow exactly in the footsteps of this older discipline. The principles that guide physics and the strategies that have led it to success won’t always work when applied to biology. Inspiration, therefore, must be taken with care. When building models of the mind, the aesthetics of the mathematics is not the only guiding light. Rather, this influence needs <a id="page_364"></a>always to be weighed against the unique realities of the brain. When balanced just right, the intricacies of the biology can be reduced to mathematics in a way that produces true insights and isn’t overly influenced by other fields. In this way, the study of the brain is forging its own path for how to use mathematics to understand the natural world.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter12.xhtml#fnt-1" id="fn-1">1</a> ﻿The predictive coding scheme was actually developed in the absence of any influence from Friston or free energy; it debuted in a paper by Rajesh Rao and Dana Ballard in 1999. However, free energy fans have since eagerly explored it. ﻿</p>
<p class="FN1"><a href="chapter12.xhtml#fnt-2" id="fn-2">2</a> ﻿When fully splayed out, the tendrils of the free energy principle reach into many of the topics covered in this book. It builds off the notion of a Bayesian brain (﻿﻿Chapter 10﻿﻿), interacts with ideas from information theory (﻿﻿Chapter 7﻿﻿), uses equations from statistical mechanics (Chapters 4 and 5) and explains elements of visual processing (﻿﻿Chapter 6﻿﻿).﻿</p>
<p class="FN1"><a href="chapter12.xhtml#fnt-3" id="fn-3">3</a> ﻿Edvard Moser, May-Britt Moser and John O﻿’﻿Keefe were awarded the Nobel Prize in 2014 for their discovery of these cells, along with the closely related, but more obviously named, ﻿‘﻿place cells﻿’﻿. ﻿</p>
<p class="FN1"><a href="chapter12.xhtml#fnt-4" id="fn-4">4</a> ﻿Tononi himself has also written a book aiming to explain his theory to a broader audience. In ﻿<span class="italic">Phi: A Voyage from the Brain to the Soul</span>﻿, Tononi tells a fictional tale of seventeenth-century scientist Galileo Galilei exploring notions of consciousness through interactions with characters inspired by Charles Darwin, Alan Turing and Crick.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 12</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter12"><a href="contents.xhtml#re_chapter12">CHAPTER TWELVE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter12">Grand Unified Theories of the Brain</a><a id="page_341"></a></p>
<p class="H1" id="b-9781472966445-ch1290-sec13">
<span class="bold">
<span>Free energy principle, Thousand Brains Theory and integrated information theory</span>
</span></p>
<p class="TXT">One of the biggest shockwaves in the history of science hit physics in the mid-nineteenth century. James Clerk Maxwell, a Scottish mathematician, published his seven-part paper ‘A dynamical theory of the electromagnetic field’ in 1865. Through this marathon of insightful analogies and equations, Maxwell demonstrated a deep and important relationship between two already important forms of physical interaction: electricity and magnetism. Specifically, by defining electromagnetic field theory, Maxwell created the mathematical infrastructure needed to see the equations of electricity and magnetism as two sides of the same coin. In the process, he concluded that a third important object – light – was a wave in this electromagnetic field. </p>
<p class="TXI">Scientists had, of course, studied electricity, magnetism and light for centuries before Maxwell. And they had learned a fair bit about them, how they interact and how they can be harnessed. But Maxwell’s unification provided something profoundly different – a whole new way to interpret the physical world. It was the first domino to fall in a series of landmark discoveries in foundational <a id="page_342"></a>physics and paved the way for many of today’s technologies. Einstein’s work, for example, is built on electromagnetic field theory and he reportedly credited his success to standing ‘on the shoulders of Maxwell’. </p>
<p class="TXI">More than its direct impact on research, however, Maxwell’s theory planted the thought in the minds of future physicists that more subterranean relationships may exist among physical forces. Digging up these connections became a major aim of theoretical physics. By the twentieth century an explicit search for what are known as grand unified theories (GUTs) was on. At the top of the to-do list was finding a GUT that could further unify electromagnetism with two other forces: the weak force (which governs radioactive decay) and the strong force (which holds atomic nuclei together). A big step in this direction came in the early 1970s with the discovery that the weak force and electromagnetism become one at very high temperatures. Even so, bringing in the strong and weak forces still leaves out another big one, gravity. Physicists, therefore, remain in pursuit of a full GUT. </p>
<p class="TXI">GUTs tap into the aesthetic preferences of many physicists: simplicity, elegance, totality. They demonstrate how the whole can become greater than the sum of its parts. Before identifying a GUT, scientists are like the blind men touching an elephant in the old parable. They’re each relying on what little information they can grab from the trunk, the leg or the tail. Through this they come up with separate and incomplete stories about what each bit is doing. Once the entire elephant is seen, however, the pieces lock into place and each is understood in context of the others. The deep wisdom obtained by finally finding a GUT can’t be approximated <a id="page_343"></a>by studying the parts separately. Therefore, as difficult as they can be to find, the search for GUTs is largely deemed a worthwhile endeavour by the physics community. As physicist Dimitri Nanopoulos wrote in 1979, shortly after helping to coin the phrase, ‘grand unified theories give a very nice and plausible explanation of a whole lot of different and at first sight unrelated phenomena, and they definitely have the merit and right to be taken seriously’.</p>
<p class="TXI">But should GUTs of the brain be taken seriously? The notion that a small number of simple principles or equations will be able to explain everything about the form and function of the brain is appealing for the same reasons GUTs are coveted in physics. However, most scientists who study the brain are doubtful they could exist. As psychologists Michael Anderson and Tony Chemero wrote: ‘There is every reason to think that there can be no grand unified theory of brain function because there is every reason to think that an organ as complex as the brain functions according to diverse principles.’ A GUT for the brain, as great as it would be, is considered a fantasy by many. </p>
<p class="TXI">On the other hand, so much of what has been imported from physics to neuroscience – the models, the equations, the mindset – has helped advance the field in one direction or another. GUTs, core as they are to modern physics, are hard to ignore. They can be tantalising for those who study the brain even if they seem unlikely and to some scientists they are simply too seductive to pass up. </p>
<p class="TXI">Searching for a GUT in the brain is a high risk-high reward endeavour. As such, it tends to require a big <a id="page_344"></a>personality to lead it. Most candidate GUTs of the brain have a frontman of sorts – a scientist, usually the one who first developed the theory, who functions as the public face of it. To get a GUT to succeed also requires dedication: supporters of a theory will work for years, even decades, on refining it. And they’re always on the prowl for new ways of applying their theory to every facet of the brain they can find. Advocacy is important, too; even the grandest of GUTs can’t help to explain much if no one has heard of it. Therefore, many papers, articles and books have been written to communicate GUTs not just to the scientific community but to the world at large. It’s best for GUT enthusiasts to have a thick skin, however. Pushing of such theories can be met with disdain from the mass of scientists doing the more reliable work of studying the brain one piece at a time. </p>
<p class="TXI">Sociologist Murray S. Davis offered a reflection on theories in his 1971 article entitled ‘That’s interesting!’ In it, he said: ‘It has long been thought that a theorist is considered great because his theories are true, but this is false. A theorist is considered great, not because his theories are true, but because they are <span class="italic">interesting</span> … In fact, the truth of a theory has very little to do with its impact, for a theory can continue to be found interesting even though its truth is disputed – even refuted!’ Grand unified theories of the brain, whatever their truth may be, are surely interesting.</p>
<p class="center">* * *</p>
<p class="TXT">Generally jovial and soft-spoken, British neuroscientist Karl Friston doesn’t quite fit the profile for the leader of <a id="page_345"></a>an ambitious and controversial scientific movement. Yet he certainly has the devoted following of one. Scientists – ranging from students to professors, including those well outside the traditional bounds of neuroscience – gather ritualistically on Mondays to receive a few moments of his insights each. They are there to seek his unique wisdom mainly on one topic. It is an all-encompassing framework on which Friston has been building an understanding of the brain, behaviour and beyond for over 15 years: the free energy principle.</p>
<p class="TXI">‘Free energy’ is a mathematical term defined by differences between probability distributions. Yet its meaning in Friston’s framework can be summarised somewhat simply: free energy is the difference between the brain’s predictions about the world and the actual information it receives. The free energy principle says that everything the brain does can be understood as an attempt to minimise free energy – that is, to make the brain’s predictions align as much as possible with reality. </p>
<p class="TXI">Inspired by this way of understanding, many researchers have gone on a search for where predictions may be made in the brain and how they may get checked against reality. A small industry of research built around the idea of ‘predictive coding’ explores how this could be happening in sensory processing in particular.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> In most predictive coding models, information gets sent as normal through the sensory processing system. For <a id="page_346"></a>example, auditory information comes in from the ears, gets relayed first through regions in the brainstem and midbrain, and then goes on to be passed sequentially through several areas in the cortex. This <span class="italic">forward</span> pathway is widely accepted as crucial for how sensory information gets turned into perception, even by researchers who don’t hold much stock in the theory of predictive coding.</p>
<p class="TXI">What makes predictive coding unique is what it claims about the <span class="italic">backward</span> pathway – connections going from latter areas to earlier ones (say from the second auditory area in the cortex back to the first). In general, scientists have hypothesised many different roles for these projections. According to the predictive coding hypothesis, these connections carry predictions. For example, when listening to your favourite song, your auditory system may have very precise knowledge about the upcoming notes and lyrics. Under a predictive coding model, these predictions would be sent backwards and get combined with forward-coming information about what’s actually happening in the world. By comparing these two streams, the brain can calculate the error between its prediction and reality. In fact, in most predictive coding models, specific ‘error’ neurons are tasked with just this calculation. Their activity thus indicates just how wrong the brain was: if they are firing a lot, the error in prediction was high, if they’re quiet, it was low. In this way, the activity of these neurons is a physical instantiation of free energy. And, according to the free energy principle, the brain should aim to make these neurons fire as little as possible.</p>
<p class="TXI">Do such error neurons exist in sensory pathways? And does the brain learn to quiet them by making <a id="page_347"></a>better predictions about the world? Scientists have been looking for answers to these questions for many years. A study conducted by researchers at Goethe University Frankfurt, for example, found that some neurons in the auditory system do decrease their firing when an expected sound is heard. Specifically, the researchers trained mice to press a noise-making lever. When the mice heard the expected sound after pressing the lever, their neurons responded less than if that same sound were played at random or if the lever made an unexpected sound. This suggests the mice had a prediction in mind and the neurons in their auditory system were firing more when that prediction was violated. Overall, however, the evidence for predictive coding is mixed. Not all studies that go looking for error neurons find them and, even when they do, these neurons don’t always behave exactly as the predictive coding hypothesis would predict. </p>
<p class="TXI">Making the brain a better predictive machine might seem like the most obvious way of minimising free energy, but it’s not the only one. Because free energy is the difference between the brain’s prediction and experience, it can also be minimised by controlling experience. Imagine a bird that has grown accustomed to flying around a certain forest; it can predict which trees will be good for nest building, where the best food is found and so on. One day it flies a little beyond its normal range and finds itself in a city. Experiencing tall buildings and traffic for the first time, its ability to predict almost anything about the world around it is low. This big discrepancy between prediction and experience means free energy is high. To bring its free energy back <a id="page_348"></a>down, the bird could stick around and hope its sensory systems adapt to be able to predict the features of city life. Or it could simply fly back to the forest it came from. The presence of this second option – choosing actions that result in predictable sensory experiences – is what makes the free energy principle a candidate GUT of the brain. Rather than just explaining features of sensory processing, this principle can encompass decisions about behaviour as well. </p>
<p class="TXI">The free energy principle has indeed been invoked to explain perception, action and everything in between.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> This includes processes like learning, sleep and attention, as well as disorders like schizophrenia and addiction. It is also argued that the principle can account for the anatomy of neurons and brain areas, along with the details of how they communicate. In fact, Friston hasn’t even constrained free energy to the brain. He’s argued for it as a guiding principle of all of biology and evolution and even as a way of understanding the fundamentals of physics. </p>
<p class="TXI">This tendency to try to wrap complex topics into simple packages has been with Friston throughout his life. In a 2018 <span class="italic">Wired</span> profile, he recalls a thought he had as a teenager: ‘There must be a way of understanding everything by starting from nothing … If I’m only allowed to start off with one point in the entire universe, can I derive everything else I need from that?’ In <a id="page_349"></a>Friston’s world, the free energy principle is now the nearly nothing that can explain almost everything.</p>
<p class="TXI">Outside Friston’s world, however, the capabilities of the free energy principle aren’t always as obvious. Given its grand promises, countless scientists have attempted to understand the ins and outs of Friston’s theory. Few (even those who count themselves fans of the principle) consider their attempts wholly successful. It’s not necessarily that the equations are too complicated – many of the scientists trying have dedicated their lives to understanding the mathematics of the mind. Rather, how to extrapolate and apply the free energy principle to all the nooks and crannies of brain function requires a type of intuition that seems to run strongest in Friston himself. Without a clear and objective means of interpreting free energy in any particular case, Friston is left to play the role of free energy whisperer, laying out his take on its implications in countless papers, talks, and Monday meetings. </p>
<p class="TXI">The confusion around the free energy principle likely results from a feature of it that Friston readily acknowledges: it’s not falsifiable. Most hypotheses about how the brain functions are falsifiable – that is, they make claims that can be proven wrong through experiments. The free energy principle, however, is more a way of looking at the brain than a strong or specific claim about how it works. As Friston said: ‘The free energy principle is what it is—a principle … there’s not much you can do with it, unless you ask whether measurable systems conform to the principle.’ In other words, rather than trying to make clean predictions about the brain based on the free energy principle, <a id="page_350"></a>scientists should instead ask if the principle helps them see things in a new light. Trying to figure out how a bit of the brain works? Ask if it somehow minimises free energy. If that leads to progress, great; if not, that’s fine, too. In this way, the free energy principle is meant to at best offer a scaffolding on which to hang facts about the brain. Insofar as it can connect a great many facts, it is grand and somewhat unifying; however – without falsifiability – its status as a theory is more questionable. </p>
<p class="center">* * *</p>
<p class="TXT">Numenta is a small tech company based in Redwood City, California. It was founded by Jeff Hawkins, an entrepreneur who previously founded two companies that produced predecessors to the modern smartphone. Numenta, on the other hand, makes software. The company designs data-processing algorithms aimed to help stockbrokers, energy distributors, IT companies and the like identify and track patterns in streams of incoming data. Numenta’s main goal, however, is to reverse-engineer the brain. </p>
<p class="TXI">Even as he weaved his way through an illustrious career in tech, Hawkins always harboured an interest in the brain. Despite never earning a degree in the field himself, he started the Redwood Neuroscience Institute in 2002. The institute would go on to become part of the University of California, Berkeley, and Hawkins would go on to Numenta in 2005. The work of Numenta is based mainly on ideas presented in a 2004 book Hawkins co-authored with Sandra Blakeslee, <span class="italic">On Intelligence</span>. The book summarises Hawkins’ theory about <a id="page_351"></a>how the neocortex – the thin layer of brain tissue covering the surface of mammalian brains – works to produce sensation, cognition, learning, movement and more. It’s a set of ideas that now goes under the name ‘The Thousand Brains Theory of Intelligence’. </p>
<p class="TXI">At the centre of the Thousand Brains Theory is a piece of neuro-architecture known as the cortical column. Cortical columns are small patches of cells, less than the tip of a pencil in diameter and about four times that in length. They’re so-named because they form cylinders running from the top of the neocortex through to the bottom, like so many parallel strands of spaghetti. Looking at a column length-wise, it resembles sheets of sediment: the neurons are segregated into six visibly identifiable layers. Neurons in different layers interact with each other by sending connections up or down. Typically, all the neurons in a column perform a similar function; they may all, for example, respond in a similar way to a sensory input. Yet the different layers do seem to serve some different purposes: some layers, for example, get input from other brain areas and other layers send output off. </p>
<p class="TXI">Vernon Mountcastle, the sensory neuroscientist who first identified these columns in the mid-twentieth century, believed they represented a fundamental anatomical unit of the brain. Though it went against the dogma of the time, Mountcastle saw potential in the idea of a single repeating unit that tiles the whole of the neocortex – a single unit that could process the full variety of information the cortex receives. Hawkins agrees. In his book, he describes Mountcastle’s work as ‘the Rosetta stone of neuroscience’ because it is ‘a single <a id="page_352"></a>idea that united all the diverse and wondrous capabilities of the human mind’. </p>
<p class="TXI">To understand what Hawkins thinks these mini-processing units do we have to consider both time and space. ‘If you accept the fact intelligent machines are going to work on the principles of the neocortex,’ Hawkins said in a 2014 interview, ‘[time] is the entire thing.’ Inputs to the brain are constantly changing; this makes a static model of brain function woefully incomplete. What’s more, the outputs of the brain – the behaviours produced by the body – are extended through both space and time. According to Hawkins, actively moving the body through space and getting dynamic streams of sensory data in return helps the brain build a deep understanding of the world. </p>
<p class="TXI">Neuroscientists know a bit about how animals move through the world. It has a lot to do with a type of neuron called a ‘grid cell’ (see Figure 26).<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> Grid cells are neurons that are active when an animal is in certain special locations. Imagine a mouse running around an open field. One of its grid cells will be active when the mouse is right in the middle of the field. That same cell will <span class="italic">also</span> be active when the mouse has moved a few body lengths north of the centre and then again a few lengths north of that. The same pattern of activity would also be seen if the mouse went 60 degrees west of north instead. In fact, if you made a map of all the places this cell is active it would form a polka-dot pattern across the <a id="page_353"></a>whole field. These polka dots would all be evenly spaced at the vertices of a triangular grid (hence the name). Different grid cells differ in the size and orientation of this grid, but they all share this common feature.</p>
<p class="image-fig" id="fig26.jpg">
<img alt="" src="Images/chapter-12-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 26</span>
</span></p>
<p class="TXI">Impressed by their ability to represent space, Hawkins made grid cells an integral part of his theory of how the neocortex learns about the world. There is a problem, however: grid cells aren’t found in the neocortex. Instead, they reside in an evolutionarily older part of the brain known as the entorhinal cortex. Despite little evidence for grid cells outside of this region, Hawkins hypothesises that they are actually hiding away in the sixth layer of every column in the neocortex. </p>
<p class="TXI">What exactly are they doing there? To explain this, Hawkins likes to use the example of running your finger around a coffee cup (Hawkins actually attributes the origins of his theory to a eureka moment he had while contemplating a coffee cup and will even bring the cup to talks for demonstration). Columns in the sensory processing part of the neocortex will get inputs from the fingertip. According to Hawkins’ theory, the grid cells at the bottom of these columns will also be tracking the location of the fingertip. Combining information about <a id="page_354"></a>where the finger is and what the cup feels like there, the column can learn the shape of the object as it navigates around it. The next time the same object is encountered the column can use this stored knowledge to recognise it. </p>
<p class="TXI">As these cortical columns exist across the neocortex, this process could be happening in parallel everywhere. The columns that represent other parts of the hand, for example, would be building their own models of the coffee cup as they come into contact with it. And areas in the visual system would combine visual information with the location of the eyes to build their own understanding of the cup, too. In total, a cohesive understanding of the world is built through these distributed contributions, like thousands of brains working in unison.</p>
<p class="TXI">Hawkins’ theory is ever-evolving and many of the details are still to be worked out, but his hopes for it are high. The way he sees it, just as columns could learn the shapes of literal objects, so too could they learn the shapes of abstract ones. Navigating the space of thought or language can be accomplished with the same mechanisms for navigating the real, physical world. If this is true, it explains how a repeating pattern in the neocortex can be used to do so many different things, from vision to audition, movement to mathematics. </p>
<p class="TXI">Exactly how identical these columns are, however, is a subject of debate. At a glance the neocortex may appear like a uniform tessellation, but upon closer inspection differences do emerge. Some studies have found that the size of columns, the number and type of neurons they contain, and how they interact with each other varies across regions of the neocortex. If these columns are not <a id="page_355"></a>actually anatomically identical, they may differ in function, too. This means that each brain region could be more specialised for the kind of tasks it has to perform than the Thousand Brains Theory assumes. If that’s the case, the hope for a universal algorithm of intelligence may be dashed. </p>
<p class="TXI">As seen throughout this book, mathematical models of the brain are usually built by first identifying – out of the mounds of available data – a selection of facts that seem relevant. Those facts are then simplified and pieced together in a way that demonstrates how, in theory, a bit of the brain could work. Additionally, in figuring out how exactly to cobble this toy version of biology together, some new and surprising predictions may be made. When compared to this schema, the Thousand Brains Theory is a model like any other in neuroscience. Indeed, many of the component concepts – columns, grid cells, object recognition – have already been studied extensively by means of both experimental and computational work. In this way, the theory is not unique; it’s a guess that may be right, may be wrong or may – like many theories – be a little of both. </p>
<p class="TXI">Perhaps what sets the work of Hawkins and Numenta apart then is simply the unrelenting optimism that this one <span class="italic">is</span> different – that for the first time a key that will unlock all the doors of the cortex is truly within reach. When asked in a 2019 interview about how far off a full understanding of the neocortex is, Hawkins said: ‘I feel I’ve already passed a step function. So, if I can do my job correctly over the next five years – meaning I can proselytise these ideas, I can convince other people they’re right, we can show that other machine learning <a id="page_356"></a>people should pay attention to these ideas – then we’re definitely in an under 20-year timeframe.’ That’s a confidence not commonly seen in scientists – and because Hawkins has the private funding to back it, it’s a confidence that is not constrained by normal scientific pressures. </p>
<p class="TXI">Hawkins is known to be self-assured in his claims about the brain. However, his past ability to deliver on promises for breakthrough brain-based algorithms has been questionable. Geoffrey Hinton, a leader in artificial intelligence research, has described Hawkins’ contributions to the field as ‘disappointing’. And in 2015, psychology professor Gary Marcus compared Numenta’s work to other AI techniques by saying: ‘I have not seen a knock-down argument that they yield better performance in any major challenge area.’ What chance does the Thousand Brains Theory have of ever providing the field with a set of truly universal mechanisms of intelligence? Only time – a concept so central to Hawkins’ thinking – will tell. </p>
<p class="center">* * *</p>
<p class="TXT">By some accounts, no theory of the brain could be complete without explaining its biggest and most enduring mystery: consciousness. The C-word can be a tough subject for scientists, laden as it is with centuries of philosophical baggage. Yet, in the eyes of certain researchers, a precise scientific definition that could be used to not just identify but also <span class="italic">quantify</span> consciousness anywhere it may exist is the holy grail of their work. It is also the promise of ‘integrated information theory’.</p> <a id="page_357"></a>
<p class="TXI">Integrated information theory (or IIT) is an attempt to define consciousness with an equation. It was originally put forth by Italian neuroscientist Giulio Tononi in 2004 and has been iterated on by him and others ever since. IIT is designed to measure consciousness in anything: in computers, rocks and aliens as easily as in brains. By making a universal claim of what consciousness is, it differs from the more biology-centred theories devised by some neuroscientists.</p>
<p class="TXI">IIT is able to free itself from the specific physical features of the brain because its inspiration comes from another source entirely: introspection. By reflecting on the first-person conscious experience, Tononi came up with five important traits fundamental to consciousness; these are the ‘axioms’ on which IIT is built. The first axiom is the basic fact that consciousness exists. Others include the observation that a conscious experience is composed of multiple distinct sensations, the experience is specific, it appears to us as an integrated whole and it is uniquely what it is – no more or less. </p>
<p class="TXI">Tononi considered what kinds of information-processing systems could give rise to these axioms of experience. Through this, he was able to map the axioms to mathematical terms. The end result is a unified measure of what is called ‘integrated information’, a value Tononi symbolises with the Greek letter <span class="italic">phi</span>. In total, phi indicates just how intermixed the information in a system is. The right kind of intermixing is supposed to give rise to the richness and wholeness of experience. According to IIT, the higher the phi that a system has, the more conscious it is.</p> <a id="page_358"></a>
<p class="TXI">As it turns out, calculating the phi for a system with any reasonable amount of complexity is nearly impossible. For the human brain, it would first require conducting a near endless amount of experiments in order to probe how the different sub-structures of the brain interact. Even if that could be done, a long and gruelling series of computations would then begin. To overcome this hurdle, multiple approximations to phi have been devised. Through this, it is possible to make educated guesses about the phi in a system. This has been used to explain why certain brain states lead to more conscious experience than others. For example, during sleep, the ability of neurons to communicate effectively is interrupted. This makes the brain less able to integrate information, resulting in lower phi. According to Tononi’s theory, similar reasoning can explain the unconsciousness that comes with seizures as well. </p>
<p class="TXI">The theory also makes some, perhaps surprising, predictions. For example, the phi of an average thermostat is small, but still not zero. This implies that the device regulating your room temperature has some amount of conscious experience. What’s more, some very simple devices – if built just right – can actually have a value of phi much higher than the estimated phi of the human brain. These counterintuitive conclusions make some scientists and philosophers sceptical of IIT. </p>
<p class="TXI">Another critique of the theory is targeted at its axiomatic basis. According to this argument, the axioms Tononi chose aren’t the only ones that a theory of consciousness could be built on. And his way of mapping these axioms to mathematics isn’t obviously the only, or best, way either. The problem is: if the foundations of IIT <a id="page_359"></a>are arbitrary, then how can we trust the conclusions that spring from them, especially when they surprise us? </p>
<p class="TXI">An informal survey of consciousness scientists conducted in 2018 revealed that IIT was not the favoured theory among experts (it came in fourth after two other theories and the catch-all category of ‘other’). But the same survey found that IIT fared better among non-experts: in fact, it was rated first among the subset of non-experts who felt they had enough knowledge to respond. Some of the survey authors suspect this may be a result of IIT’s PR. From the outside, the theory looks well founded if only because it has the authority of hard math behind it. And more than most scientific theories of consciousness, IIT has been featured in the popular press. This includes writings by Christof Koch, a prominent neuroscientist who has become a collaborator of Tononi’s and a public advocate of IIT. In his book, <span class="italic">Consciousness: Confessions of a Romantic Reductionist</span>, Koch describes his personal journey through the scientific study of consciousness, including work he did with Nobel Prize winner Francis Crick, and his views on IIT.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> Such popular accounts may be effective in getting the theory out to a broader audience, but don’t necessarily convince scientists in the know. </p>
<p class="TXI">Even scientists who lack faith in the power of IIT still tend to applaud the attempt. Consciousness is a notoriously <a id="page_360"></a>difficult concept to tame, which makes IIT’s effort to submit it to orderly scientific inquiry still a step in the right direction. As vocal critic of IIT physicist Scott Aaronson wrote on his blog: ‘The fact that integrated information theory is wrong – demonstrably wrong, for reasons that go to its core – puts it in something like the top 2 per cent of all mathematical theories of consciousness ever proposed. Almost all competing theories of consciousness, it seems to me, have been so vague, fluffy and malleable that they can only <span class="bold">aspire</span> to wrongness.’ </p>
<p class="center">* * *</p>
<p class="TXT">GUTs can be a slippery thing. To be grand and unifying, they must make simple claims about an incredibly complex object. Almost any statement about ‘the brain’ is guaranteed to have exceptions lurking somewhere. Therefore, making a GUT too grand means it won’t actually be able to explain much specific data. But, tie it too much to specific data and it’s no longer grand. Whether untestable, untested, or tested and failed, GUTs of the brain, in trying to explain too much, risk explaining nothing at all.</p>
<p class="TXI">While this presents an uphill battle for GUT-seeking neuroscientists, it’s less of a challenge in physics. The reason for this difference may be simple: evolution. Nervous systems evolved over eons to suit the needs of a series of specific animals in specific locations facing specific challenges. When studying such a product of natural selection, scientists aren’t entitled to simplicity. Biology took whatever route it needed to create functioning organisms, without regard to how understandable any part of them would be. It should be no surprise, then, to find <a id="page_361"></a>that the brain is a mere hodgepodge of different components and mechanisms. That’s all it needs to be to function. In total, there is no guarantee – and maybe not even any compelling reasons to expect – that the brain can be described by simple laws.</p>
<p class="TXI">Some scientists choose to embrace this mess. Instead of reducing the brain to its barest elements, they build a form of ‘grand unified model’ that sticks all of the parts together. While traditional GUTs have the simplicity of a steak cooked with just a sprinkle of salt, these models are more like a big pot of soup. And while they’re not as sleek and elegant as GUTs, they may be better equipped to get the job done. </p>
<p class="TXI">The hyper-detailed simulation built by the Blue Brain Project, as discussed in <a href="chapter2.xhtml#chapter2">Chapter 2</a>, is one example of this more inclusive approach. These researchers extracted countless details about neurons and synapses through a series of painstaking experiments. They then pieced all this data back together into an elaborate computational model of just a small speck of the brain. Such an approach assumes that each detail is precious and that the brain won’t be understood by stripping them away. It’s a wholehearted embrace of the nuance of biology, with the hope that by throwing everything together, a fuller understanding of what makes the brain work will emerge. The trouble here, however, is scale. A bottom-up approach to rebuilding the brain can only proceed one neuron at a time, which means a complete model is a long way off. </p>
<p class="TXI">The Semantic Pointer Architecture Unified Network, better known as SPAUN, comes at things from a very different direction. Rather than capturing <a id="page_362"></a>all the minutiae of the neurobiology, SPAUN – developed by a team working under Chris Eliasmith at University of Waterloo in Ontario, Canada – is about making a model of the brain that works. That means getting the same sensory inputs and having the same motor outputs. Specifically, SPAUN has access to images as input and controls a simulated arm to write its outputs. In between this input and output is a complex web of 2.5 million simple model neurons, arranged to broadly mimic the structure of the whole brain. Through these neural connections SPAUN can perform seven different cognitive and motor tasks, such as drawing digits, recalling lists of objects and completing simple patterns. In this way, SPAUN eschews elegance for function. Of course, the human brain contains tens of thousands of times as many neurons and can do a lot more than seven tasks. Whether the principles of utility and scale that got SPAUN to where it is can take it all the way up to a full model of the brain – or whether more of the nuances of neurons need to be added – is unknown.</p>
<p class="TXI">True GUTs aim to condense. They melt diverse information down into a compact and digestible form. This makes GUTs seem satisfying because they give the sense that the workings of the brain can be fully grasped in one hand. Models like SPAUN and the Blue Brain Project simulation, however, are expansive. They bring in many sources of data and use them to build an elaborate structure. In this way, they sacrifice interpretability for accuracy. They aim to explain everything by incorporating everything there is to explain.</p> <a id="page_363"></a>
<p class="TXI">Though as with all models, even these more expansive ones are still not perfect replicas. Makers of these models still need to choose what to include and what not to include, what they aim to explain and what they can ignore. When aiming for something akin to a GUT, the hope is always to find the simplest set of principles that can explain the largest set of facts. With an object as dense and messy as the brain, that simple set may still be pretty complicated. To know in advance what level of detail and what magnitude of scale will be needed to capture the relevant features of brain function is impossible. It is only through the building and testing of models that progress on that question can be made. </p>
<p class="TXI">On the whole, neuroscience has enjoyed a very fruitful relationship with the ‘harder’, more quantitative sciences. It has received many gifts from the likes of physics, mathematics and engineering. These analogies, methods and tools have shifted thinking about everything from neurons to behaviour. And the study of the brain has given back in return, providing inspiration for artificial intelligence and a testing ground for mathematical techniques. </p>
<p class="TXI">But neuroscience is not physics. It must avoid playing the role of the kid sibling, trying to follow exactly in the footsteps of this older discipline. The principles that guide physics and the strategies that have led it to success won’t always work when applied to biology. Inspiration, therefore, must be taken with care. When building models of the mind, the aesthetics of the mathematics is not the only guiding light. Rather, this influence needs <a id="page_364"></a>always to be weighed against the unique realities of the brain. When balanced just right, the intricacies of the biology can be reduced to mathematics in a way that produces true insights and isn’t overly influenced by other fields. In this way, the study of the brain is forging its own path for how to use mathematics to understand the natural world.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter12.xhtml#fnt-1" id="fn-1">1</a> ﻿The predictive coding scheme was actually developed in the absence of any influence from Friston or free energy; it debuted in a paper by Rajesh Rao and Dana Ballard in 1999. However, free energy fans have since eagerly explored it. ﻿</p>
<p class="FN1"><a href="chapter12.xhtml#fnt-2" id="fn-2">2</a> ﻿When fully splayed out, the tendrils of the free energy principle reach into many of the topics covered in this book. It builds off the notion of a Bayesian brain (﻿﻿Chapter 10﻿﻿), interacts with ideas from information theory (﻿﻿Chapter 7﻿﻿), uses equations from statistical mechanics (Chapters 4 and 5) and explains elements of visual processing (﻿﻿Chapter 6﻿﻿).﻿</p>
<p class="FN1"><a href="chapter12.xhtml#fnt-3" id="fn-3">3</a> ﻿Edvard Moser, May-Britt Moser and John O﻿’﻿Keefe were awarded the Nobel Prize in 2014 for their discovery of these cells, along with the closely related, but more obviously named, ﻿‘﻿place cells﻿’﻿. ﻿</p>
<p class="FN1"><a href="chapter12.xhtml#fnt-4" id="fn-4">4</a> ﻿Tononi himself has also written a book aiming to explain his theory to a broader audience. In ﻿<span class="italic">Phi: A Voyage from the Brain to the Soul</span>﻿, Tononi tells a fictional tale of seventeenth-century scientist Galileo Galilei exploring notions of consciousness through interactions with characters inspired by Charles Darwin, Alan Turing and Crick.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 2</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter2"><a href="contents.xhtml#re_chapter2">CHAPTER TWO</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter2">How Neurons Get Their Spike</a><a id="page_17"></a></p>
<p class="H1" id="b-9781472966445-ch99-sec2">
<span class="bold">
<span>Leaky integrate-and-fire and Hodgkin-Huxley neurons</span>
</span></p>
<p class="TXT">‘The laws of action of the nervous principle are totally different from those of electricity,’ concluded Johannes Müller more than 600 pages into his 1840 textbook <span class="italic">Handbuch der Physiologie des Menschen</span>. ‘To speak, therefore, of an electric current in the nerves, is to use quite as symbolical an expression as if we compared the action of the nervous principle with light or magnetism.’</p>
<p class="TXI">Müller’s book – a wide-ranging tour through the new and uncertain terrain of the field of physiology – was widely read. Its publication (especially its near-immediate translation into English under the title <span class="italic">Elements of Physiology</span>) cemented Müller’s reputation as a trusted teacher and scientist. </p>
<p class="TXI">Müller was a professor at Humboldt University of Berlin from 1833 until his death 25 years later. He had a broad interest in biology and strong intellectual views. He was a believer in vitalism, the idea that life relied on a <span class="italic">Lebenskraft</span>, or vital organising force, that went beyond mere chemical and physical interactions. This philosophy could be found streaking through his physiology. In his book, he claims not only that the activity of nerves is not electric in nature, but that it may ultimately be ‘imponderable’, the question of its essence ‘not capable of solution by physiological facts’.</p> <a id="page_18"></a>
<p class="TXI">Müller, however, was wrong. Over the course of the following century, the spirit that animated the nerves would prove wholly reducible to the simple movement of charged particles. Electricity is indeed the ink in which the neural code is written. The nervous principle was perfectly ponderable after all.</p>
<p class="TXI">More than merely striking down Müller’s vitalism, the identification of this ‘bio-electricity’ in the nervous system provided an opportunity. By forging a path between the two rapidly developing studies of electricity and physiology, it allowed for the tools of the former to be applied to the problems of the latter. Specifically, equations – whittled down by countless experiments to capture the essential behaviours of wires, batteries and circuits – now offered a language in which to describe the nervous system. The two fields would come to share symbols, but their relationship was far more than the merely symbolic one Müller claimed. The proper study of the nervous system depended on collaboration with the study of electricity. The seeds of this collaboration, planted in the nineteenth century, would come to sprout in the twentieth and bloom in the twenty-first. </p>
<p class="center">* * *</p>
<p class="TXT">Walk into the home of an educated member of upper-class society in late eighteenth-century Europe and you may find, among shelves of other scientific tools and curiosities, a Leyden jar. Leyden jars, named after the Dutch town that was home to of one of their inventors, are glass jars like most others. However instead of storing jam or pickled vegetables, Leyden jars store charge. <a id="page_19"></a>Developed in the mid-eighteenth century, these devices marked a turning point in the study of electricity. As a literal form of lightning in a bottle, they let scientists and non-scientists alike control and transmit electricity for the first time – sometimes doling out shocks large enough to cause nosebleeds or unconsciousness. </p>
<p class="TXI">While its power may be large, the Leyden jar’s design is simple (see Figure 1). The bottom portion of the inside of the jar is covered in a metal foil, as is the same region on the outside. This creates a sandwich of glass in between the two layers of metal. Through a chain or rod inserted at the top of the jar, the internal foil gets pumped full of charged particles. Particles of opposite charge are attracted to each other, so if the particles going into the jar are positively charged, for example, then negatively charged ones will start to accrue on the outside. The particles can never reach each other, however, because the glass of the jar keeps them apart. Like two neighbourhood dogs separated by a fence, they can only line up on either side of the glass, desperately wishing to be closer.</p><a id="page_20"></a>
<p class="TXI">We would now call a device that stores charge like the Leyden jar a ‘capacitor’. The disparity in charge on either side of the glass creates a difference in potential energy known as voltage. Over time, as more and more charge is added to the jar, this voltage increases. If the glass barrier disappeared – or another path were provided for these particles to reach each other – that potential energy would turn into kinetic energy as the particles moved towards their counterparts. The higher the voltage was across the capacitor, the stronger this movement of charge – or current – would be. This is exactly how so many scientists and tinkerers ended up shocking themselves. By creating a link between the inside and outside of the jar with their hand, they opened a route for the flow of charged particles right through their body.</p>
<p class="image-fig" id="fig1.jpg">
<img alt="" src="Images/chapter-02-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 1</span>
</span></p>
<p class="TXI">Luigi Galvani was an Italian scientist born in 1737. Strongly religious throughout his life, he considered joining the church before eventually studying medicine at the University of Bologna. There he was educated not just in techniques of surgery and anatomy, but also in the fashionable topic of electricity. The laboratory he kept in his home – where he worked closely with his wife Lucia, the daughter of one of his professors – contained devices for exploring both the biological and the electric: scalpels and microscopes, along with electrostatic machines and, of course, Leyden jars. For his medical experiments, Galvani – like students of biology for centuries before and after him – focused on frogs. The muscles in a frog’s legs can keep working after death, a desirable feature when trying to simultaneously understand the workings of an animal and dissect it.</p> <a id="page_21"></a>
<p class="TXI">It was a result of his lab’s diversity – and potentially disorganisation – that landed Galvani in the pages of science textbooks. As the story goes, someone in the lab (possibly Lucia) touched a metal scalpel to the nerve of a dead frog’s leg at the exact moment that an errant spark from an electrical device caused the scalpel to carry charge. The leg muscles of the frog immediately contracted, an observation Galvani decided to enthusiastically pursue. In his 1791 book he describes many different preparations for his follow-up experiments on ‘animal electricity’, including comparing the efficacy of different types of metal in eliciting contractions and how he connected a wire to a frog’s nerve during a thunderstorm. He watched its legs contract with each lightning flash.</p>
<p class="TXI">There had always been some hints that life was making use of electricity. Ibn Rushd, a twelfth-century Muslim philosopher, anticipated several scientific findings when he noted that the ability of an electric fish to numb the fishermen in its waters may stem from the same force that pulls iron to a lodestone. And in the years before Galvani’s discovery, physicians were already exploring the application of electric currents to the body as a cure for everything from deafness to paralysis. But Galvani’s varied set of experiments took the study of bio-electricity beyond mere speculation and guesswork. He gathered the evidence to show that animal movement follows from the movement of electricity in the animal. He thus concluded that electricity was a force intrinsic to animals, a kind of fluid that flowed through their bodies as commonly as blood. </p>
<p class="TXI">In line with the spirit of amateur science at the time, upon hearing news of Galvani’s work many people set <a id="page_22"></a>out to replicate it. Putting their personal Leyden jars in contact with any frog they could capture, curious laymen saw the same contractions and convulsions as Galvani did. So broad was the impact of Galvani’s work – and along with it the idea of electrical animation – it made its way into the mind of English writer Mary Shelley, forming part of the inspiration for her novel <span class="italic">Frankenstein</span>.</p>
<p class="TXI">A healthy dose of scientific scepticism, however, meant that not all of Galvani’s academic peers were so enthusiastically accepting of his claims. Alessandro Volta – an Italian physicist after whom ‘voltage’ was named – acknowledged that electricity could indeed cause muscle contractions in animals. But he denied that this means animals <span class="italic">normally</span> use electricity to move. Volta didn’t see in Galvani’s experiments any evidence that animals were producing their own electricity. In fact, he found that contact between two different metals could create many, nearly imperceptible, electric forces and therefore any test of animal electricity using metals in contact could be contaminated by externally generated electricity. As Volta wrote in an 1800 publication: ‘I found myself obliged to combat the pretended animal electricity of Galvani and to declare it an external electricity moved by the mutual contact of metals of different kinds’.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">Unfortunately for Galvani, Volta was a younger man, more willing to engage in public debate and on his way up in the field. He was a formidable scientific opponent. The power of Volta’s personality meant Galvani’s ideas, though correct in many ways, would be eclipsed for decades.</p> <a id="page_23"></a>
<p class="TXI">Müller’s textbook came nearly 10 years after Volta’s death, but his objection to animal electricity followed similar lines. He simply didn’t believe electricity was the substance of nervous transmission and the weight of the evidence at the time couldn’t sway him. In addition to his vitalist tendencies, this stubbornness was perhaps due to Müller’s preference for observation over intervention. No matter how many examples of animals responding to externally applied electricity amassed over the years, it would never equal a direct observation of an animal generating its own electricity. ‘Observation is simple, indefatigable, industrious, upright, without any preconceived opinion,’ said Müller in his inaugural lecture at the University of Bonn. ‘Experiment is artificial, impatient, busy, digressive, passionate, unreliable.’ At the time, however, observation was impossible. No tool was powerful enough to pick up on the faint electrical signals carried by nerves in their natural state. </p>
<p class="TXI">That changed in 1847 when Emil du Bois-Reymond – one of Müller’s own students – fashioned a very sensitive galvanometer,<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> a device that measures current through its interaction with a magnetic field. His experiments were an attempt to replicate in nerves what Italian physicist Carlo Matteucci had recently observed in muscles. Using a galvanometer, Matteucci detected a small change in current coming from muscles after forcing them to contract. Searching for this signal in a nerve, however, demanded a stronger magnetic field to pick up the weaker current. In addition to designing proper insulation 
to prevent any interference from outside electricity, <a id="page_24"></a>du Bois-Reymond had to coil more than a mile of wire by hand (producing more than eight times the coils of Matteucci) to get a magnetic field strong enough for his purposes. His handiwork paid off. With his galvanometer measuring its response, du Bois-Reymond stimulated 
a nerve in various ways – including electrically or 
with chemicals like strychnine – and monitored the galvanometer’s reading of how the nerve responded. Each time, he saw the galvanometer’s needle shoot up. Electricity had been spotted at work in the nervous system. </p>
<p class="TXI">Du Bois-Reymond was a showman as much as he was a scientist and he lamented the dry presentation styles of his fellow scientists. To spread the fruits of his labour, he built several public-ready demonstrations of bio-electricity, including a set-up where he could make a needle move by squeezing his arm in a jar of salt water. All this helped ensure that his findings would be noticed and that du Bois-Reymond would be fondly regarded by the minds of his time. As he said: ‘Popularisers of science persist in the public mind as memorial stones of human progress long after the waves of oblivion have surged over the originators of the soundest research.’ </p>
<p class="TXI">Luckily his research was sound as well. Particularly, the follow-up work du Bois-Reymond carried out with his student Julius Bernstein would seal the fate of the theory of nervous electricity. Du Bois-Reymond’s original experiment had succeeded in showing a signature of current change in an activated nerve. But Bernstein, through clever and careful experimental design, was able to both amplify the strength of the signal and record it at a finer timescale – creating the first true observation of the elusive nervous signal.</p><a id="page_25"></a>
<p class="TXI">Bernstein’s experiments worked by first isolating a nerve and placing it on to his device. The nerve was then electrically stimulated at one end and Bernstein would look for the presence of any electrical activity some distance away. By recording with a precision of up to one-third of one-thousandth of a second, he saw how the nerve current changed characteristically over time after each stimulation. Depending on how far his recording site was from the stimulation site, there may be a brief pause as the electric event travelled down the nerve to reach the galvanometer. Once it got to where he was recording, however, he always saw the current rapidly decrease and then more slowly recover to its normal value.</p>
<p class="TXI">Bernstein’s result, published in the inaugural issue of the <span class="italic">European Journal of Physiology</span> in 1868, was the first known recording of what is now referred to as an ‘action potential’. An action potential is defined as a characteristic pattern of changes in the electrical properties of a cell. Neurons have action potentials. Certain other excitable cells, like those in the muscles or heart, do as well. </p>
<p class="TXI">This electrical disturbance travels across a cell’s membrane like a wave. In this way, action potentials help a cell carry a signal from one end of itself to the other. In the heart, for example, the ripple of an action potential helps coordinate a cell’s contraction. Action potentials are also a way for a cell to say something to other cells. In a neuron, when an action potential reaches the knobbly end of an outgrowth known as an axon, it pushes out neurotransmitters. These chemicals can reach other cells and trigger action potentials in them as well. In the case of the familiar frog nerve, the action potentials travelling down the leg lead to the <a id="page_26"></a>release of neurotransmitters on to the leg muscle. Action potentials in the muscle then cause it to twitch. </p>
<p class="TXI">Bernstein’s work was the first word in a long story about the action potential. Now recognised as the core unit of communication in the nervous system, the action potential forms the basis of modern neuroscience. This quick blip of electrical activity connects the brain to the body, the body to the brain and links all the neurons of the brain in between. </p>
<p class="TXI">With his glimpsing of current changes coming from the nerve, du Bois-Reymond wrote: ‘If I do not greatly deceive myself, I have succeeded in realising [...] the hundred years’ dream of physicists and physiologists, to wit, the identity of the nervous principle with electricity.’ The nervous principle was indeed identified in the action potential. Yet du Bois-Reymond had committed himself to a ‘mathematico-physical method’ of explaining biology, and while he had established the physical, he had not quite solved the mathematical. With a growing sense among scientists that proper science involved quantification, the job of describing the physical properties of the nervous principle was far from done. Indeed, it would take roughly another hundred years to capture the essence of the nervous principle in equations. </p>
<p class="center">* * *</p>
<p class="TXT">In contrast to the experience of Johannes Müller, when Georg Ohm published a book of his scientific findings he lost his job. </p>
<p class="TXI">Ohm was born in 1789, the son of a locksmith. He studied for only a short time at the university of his <a id="page_27"></a>hometown, Erlangen in Germany, and then spent years teaching mathematics and physics in various cities. Eventually, with an aim of becoming an academic, he started performing his own small experiments, particularly around the topic of electricity. For one test, he cut wires of various lengths out of different metals. He then applied voltage across the two ends of the wire and measured how much current flowed between them. Through this he was able to deduce a mathematical relationship between the length of a wire and its current: the longer the wire, the lower the current. </p>
<p class="TXI">By 1827, Ohm had collected this and other equations of electricity into his book, <span class="italic">The Galvanic Circuit Investigated Mathematically</span>. Quite contrary to its modern form, the study of electricity in the time of Ohm wasn’t a very mathematical discipline and Ohm’s peers didn’t like his attempts to make it one. One reviewer went so far as to say: ‘He who looks on the world with the eye of reverence must turn aside from this book as the result of an incurable delusion, whose sole effort is to detract from the dignity of nature.’ Having taken time away from his job to write the book in the hope it would land him a promotion, Ohm, with the book’s failure, ended up resigning instead. </p>
<p class="TXI">Ohm, however, was right. The key relationship he observed – that the current that runs through a wire is equal to the voltage across it divided by the wire’s resistance – is a cornerstone of modern electrical engineering taught to first-year physics students worldwide. This is now known as Ohm’s law and the standard unit of resistance is the ‘ohm’. Ohm wouldn’t know the full impact of his work in his lifetime, but he <a id="page_28"></a>did eventually get some recognition. At the age of 63 he was finally appointed a professor of experimental physics at the University of Munich, two years before he died. </p>
<p class="TXI">Resistance, as the name suggests, is a measure of opposition. It’s a description of just how much a material impedes the course of current. Most materials have some amount of resistance, but, as Ohm noted, the physical properties of a material determine just how resistant it is. Longer wires have higher resistance; thicker ones have lower. Just as the narrowing of an hourglass slows the flow of sand, wires with higher resistance hinder the flow of charged particles.</p>
<p class="TXI">Louis Lapicque knew of Ohm’s law. Born in France in 1866, shortly after the first recording of an action potential, Lapicque completed his doctorate at the Paris Medical School. He wrote his thesis on liver function and iron metabolism. Though his studies were scientific, his interests ranged more broadly from history to politics to sailing; he sometimes even took his boat to conferences across the English Channel.</p>
<p class="TXI">It was around the start of the twentieth century that Lapique started studying the nerve impulse. It would be the beginning of a decades-long project with his student-turned-wife-and-colleague Marcelle de Heredia, which centred on the concept of time in nerves. One of their earliest questions was: how long does it take to activate a nerve? It was well established by then that applying voltage across a nerve<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> caused a response – measured either as an action potential observed directly in the <a id="page_29"></a>nerve or as a muscle twitch that resulted from it. It was also clear that the amount of voltage applied mattered: higher voltage and the nerve would respond quicker, lower and it would respond slower. But what was the exact <span class="italic">mathematical</span> relationship between the stimulation applied and the time it took to get a response? </p>
<p class="TXI">This may sound like something of a small research question, a curiosity not of much consequence, but it was Lapicque’s approach to it that mattered. Because a proper physiologist also needed to be an engineer – designing and building all manner of electrical devices for stimulating and recording from nerve fibres – Lapicque knew the rules of electricity. He knew about capacitors, resistance, voltage and Ohm’s law. And it was with this knowledge that he composed a mathematical concept of the nerve that would answer his question – and many more to come. </p>
<p class="TXI">Understanding of the membranes that enclose cells had grown in the decades before Lapicque’s work. It was becoming clear that these bundles of biological molecules worked a little bit like a brick wall: they didn’t let much through. Some of the particles they were capable of keeping apart included ions – atoms of different elements like chloride, sodium or potassium that carry a positive or negative charge. So, just as charged particles could build up on either side of the glass in a Leyden jar, so too could they accrue on the inside and outside of a cell. As Lapicque wrote in his 1907 paper: ‘These ideas lead, when treated in the simplest possible way, to already established equations for the polarisation of metal electrodes.’ </p>
<p class="TXI">He thus came to describe the nerve in terms of an ‘equivalent circuit’. (see Figure 2) That is, he assumed <a id="page_30"></a>that different parts of the nerve acted like the different components of an electrical circuit. The first equivalence was made between the cell membrane and a capacitor, as the membrane could store charge in just the same way. But it was clear that these membranes weren’t acting as perfect capacitors; they couldn’t keep all of the charge apart. Instead, some amount of current seemed to flow between the inside and outside of the cell, allowing it to discharge slightly. A wire with some resistance could play this role. So Lapicque added a resistor to his circuit model of the nerve in parallel with the capacitor. This way, when current is injected into the circuit, some of that charge goes to the capacitor and some goes through the resistor. Trying to create a charge difference across the cell is therefore like pouring water into an imperfect bucket; much of it would stay in the bucket, but some would leak away.</p>
<p class="image-fig" id="fig2.jpg">
<img alt="" src="Images/chapter-02-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 2</span>
</span></p>
<p class="TXI">This analogy between a cell and a circuit made it possible for Lapicque to write down an equation. The equation described how the voltage across the cell <a id="page_31"></a>membrane should change over time, based on how much voltage was being applied to it and for how long. With this formalisation in mind, he could calculate when the nerve would respond.</p>
<p class="TXI">For the data to test his equation on, Lapicque turned to the standard frog-leg experiment: he applied different amounts of voltage to the frog’s nerve and recorded the time it took to see a response. Lapicque assumed that when the frog nerve responded it was because the voltage across its membrane had reached a certain threshold. He therefore calculated how long his model would take to reach that threshold for each different voltage applied. Comparing the predictions from his model with the results of his experiments, Lapicque found a good match. He could predict just how long a certain voltage would need to be applied in order to make the nerve respond.</p>
<p class="TXI">Lapicque wasn’t the first to write down such an equation. A previous scientist, Georges Weiss, offered a guess as to how to describe this relationship between voltage and time. And it was a relatively good guess too; it deviated from Lapicque’s predictions only somewhat, for example, in the case of voltages applied for a long time. But just as the smallest clue at a crime scene can change the narrative of the whole event, this slight difference between the predictions of Lapicque’s equation and what came before actually signalled a deep difference in understanding.</p>
<p class="TXI">Unlike Lapicque’s, Weiss’ equation wasn’t inspired by the mechanics of a cell nor was it meant to be interpreted as an equivalent circuit. It was more a description of the data than a model of it. Whereas a descriptive equation is <a id="page_32"></a>like a cartoon animation of an event – capturing its appearance but without any depth – a model is a re-enactment. A mathematical model of a nerve impulse thus needs to have the same moving parts as the nerve itself. Each variable should be mappable to a real physical entity and their interactions should mirror the real world as well. That is just what Lapicque’s equivalent circuit provided: an equation where the terms were interpretable.</p>
<p class="TXI">Others before Lapicque had seen the similarity between the electrical tools used to study the nerve and the nerve itself. Lapicque was building heavily on the work of Walther Nernst, who noticed that the membrane’s ability to separate ions could underlie the action potential. Another student of du Bois-Reymond, Ludimar Hermann, had spoken of the nerve in terms of capacitors and resistors. And even Galvani himself had a vision of a nerve that worked similarly to his Leyden jar. But with his explicit equivalent circuit and quantitative fit to data, Lapicque went a step further in making an argument for the nerve as a precise electrical device. As he wrote: ‘The physical interpretation that I reach today gives a precise meaning to several important previously known facts on excitability … It seems to me a reason to consider it a step in the direction of realism.’</p>
<p class="TXI">Due to their limited equipment, most neuroscientists of Lapicque’s time were recording from whole nerves. Nerves are bundles of many axons – the fibres through which individual neurons send their signals to other cells. Recording from many axons at once makes it easier to pick up the current changes they produce, but harder to see the detailed shape of those changes. Sticking an electrode into a single neuron, however, makes it possible <a id="page_33"></a>to record the voltage across its membrane directly. Once the technology to observe individual neurons became available in the early twentieth century, the action potential came into much clearer view. </p>
<p class="TXI">One defining feature of the action potential noticed by English physiologist Edgar Adrian in the 1920s is the ‘all-or-nothing’ principle.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> The all-or-nothing principle says that a neuron either emits an action potential or it doesn’t – nothing in between. In other words, any time a neuron gets enough input, the voltage across its membrane changes – and it changes in exactly the same way. So, just as a goal in hockey counts the same no matter how hard the puck is driven into the net, strongly stimulating a neuron doesn’t make its action potential any bigger or better. All stronger stimulation can do is make the neuron emit <span class="italic">more</span> of the exact same action potentials. In this way, the nervous system cares more about quantity than quality. </p>
<p class="TXI">The all-or-nothing nature of a neuron aligns with Lapicque’s intuition about a threshold. He knew that the voltage across the membrane needed to reach a certain value in order to see a response from the nerve. But once it got there, a response was a response. </p>
<p class="TXI">By the 1960s, the all-or-nothing principle was combined with Lapicque’s equation into a mathematical model known as a ‘leaky integrate-and-fire neuron’: ‘leaky’ because the presence of a resistor means some of the current leaks away; ‘integrate’ because the capacitor integrates the rest of it and stores it as charge; and ‘fire’ <a id="page_34"></a>because when the voltage across the capacitor reaches the threshold the neuron ‘fires’, or emits an action potential. After each ‘firing’ the voltage is reset to its baseline, only to reach threshold again if more input is given to the neuron. </p>
<p class="TXI">While the model is simple, it can replicate features of how real neurons fire: for example, with strong and constant input the model neuron will fire action potentials repeatedly, with only a small delay between each one; if the input is low enough, however, it can remain on indefinitely without ever causing a single action potential. </p>
<p class="TXI">These model neurons can also be made to form connections – strung together such that the firing of one generates input to another. This provides modellers with a broader power: to replicate, explore and understand the behaviour of not just individual neurons but whole networks of them. </p>
<p class="TXI">Since their inception, such models have been used to understand countless aspects of the brain, including disease. Parkinson’s disease is a disorder that impacts the firing of neurons in the basal ganglia. Located deep in the brain, the basal ganglia are composed of a variety of regions with elaborate Latin names. When the input to one region – the striatum – is perturbed by Parkinson’s disease, it knocks the rest of the basal ganglia off balance. As a result of changes to the striatum, the subthalamic nucleus (another region of the basal ganglia) starts to fire more, which causes neurons in the globus pallidus external (yet another basal ganglia region) to fire. But those neurons send connections <span class="italic">back</span> to the subthalamic nucleus that <a id="page_35"></a>
<span class="italic">prevent</span> those neurons from firing more – which also, in turn, shuts down the globus pallidus external itself. The result of this complicated web of connections is oscillations: neurons in this network fire more, then less, then more again. These rhythms appear to be related to the movement problems Parkinson’s patients have – tremors, slowed movements and rigidity. </p>
<p class="TXI">In 2011, researchers at the University of Freiburg built a computer model of these brain regions made up of 3,000 leaky integrate-and-fire model neurons. In the model, disturbing the cells that represented the striatum created the same problematic waves of activity seen in the subthalamic nuclei of Parkinson’s patients. With the model exhibiting signs of the disease, it could also be used to explore ways to treat it. For example, injecting pulses of input into the model’s subthalamic nucleus broke these waves down and restored normal activity. But the pulses had to be at just the right pace – too slow and the offending oscillations got worse, not better. Deep brain stimulation – a procedure where pulses of electrical activity are injected into the subthalamic nucleus of Parkinson’s patients – is known to help alleviate tremors. Doctors using this treatment also know that the rate of pulses has to be high, around 100 times per second. This model gives a hint as to why high rates of stimulation work better than lower ones. In this way, modelling the brain as a series of interconnected circuits illuminates how the application of electricity can fix its firing.</p>
<p class="TXI">Lapicque’s original interest was in the timing of neural firing. By piecing together the right components of an electrical circuit he captured the timing of action potentials correctly, but the creation of this circuit stand-in for a neuron did more than that. It formed <a id="page_36"></a>a solid foundation on which to build towering networks of thousands of interconnecting cells. Now computers across the world churn through the equations of these faux neurons, simulating how real neurons integrate and fire in both health and disease. </p>
<p class="center">* * *</p>
<p class="TXT">In the summer of 1939, Alan Hodgkin set out on a small fishing boat off the southern coast of England. His goal was to catch some squid, but mostly what he got was seasick. </p>
<p class="TXI">At the time, Hodgkin, a research fellow at Cambridge University, had only just arrived at the Marine Biological Association in Plymouth ready to embark on a new project studying the electrical properties of the squid giant axon. In particular, he wished to know how the action potential got its characteristic up-down shape (frequently referred to as a ‘spike’<sup><a href="#fn-5" id="fnt-5">5</a>
</sup>). A few weeks later he was joined by an equally green student collaborator, Andrew Huxley. Luckily, the men eventually figured out when and where their subject matter could be found in the sea. </p>
<p class="TXI">Though Huxley was a student of Hodgkin’s, the men were only four years apart in age. Hodgkin looked the role of a proper English gentleman: long face, sharp eyes, his hair neatly parted and swept to the side. Huxley was a bit more boyish with round cheeks and heavy eyebrows. Both men had skill in biology and physics, though each came to this pairing from the opposite side. </p>
<p class="TXI">Hodgkin primarily studied biology, but in his last term he was encouraged by a zoology professor to learn <a id="page_37"></a>as much mathematics and physics as he could. Hodgkin obliged, spending hours with textbooks on differential equations. Huxley was long interested in mechanics and engineering, but switched to a more biological track after a friend told him physiology classes would teach more lively and controversial topics. Huxley may have also been drawn to these subjects by the influence of his grandfather. Biologist Thomas Henry Huxley – known as ‘Darwin’s bulldog’ for his vociferous defence of Darwin’s theory of evolution – described physiology as ‘the mechanical engineering of living machines’. </p>
<p class="TXI">Lapicque’s model predicted when a cell would fire, but it still didn’t explain exactly what an action potential was. At the time of Hodgkin’s boat trip, the going theory of what happens when a neuron emits an action potential was still the one put forward by the original action potential observer himself, Julius Bernstein. It said that, at the time of this electrical event, the cell membrane temporarily breaks down. It therefore lets ions of all different kinds flow through, erasing the charge difference that is normally across it and creating the small current Bernstein saw with his galvanometer. </p>
<p class="TXI">But some of Hodgkin’s previous experiments on crabs told him this may not be quite right. He wanted to follow up on this work with the squid because the large size of the axon running along its mantle made precise measurements easier.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> Sticking an electrode into this axon, Hodgkin and Huxley recorded the voltage changes <a id="page_38"></a>that occurred during an action potential (See Figure 3). What they saw was a clear ‘overshoot’. That is, the voltage didn’t just go to zero, as would happen with a discharged capacitor, but rather it <span class="italic">reversed</span>. While a neuron normally has more positive charge on the outside of the cell than on the inside, during the peak of the action potential this pattern gets inverted and the inside becomes more positively charged than the outside. Simply letting more ions diffuse through the membrane wouldn’t lead to such a separation. Something more selective was at play.</p>
<p class="TXI">Only a short time after Hodgkin and Huxley made this discovery, their work was unfortunately interrupted. Hitler invaded Poland. The men needed to abandon the lab and join the war effort. Solving the mystery of the action potential would have to wait.</p>
<p class="image-fig" id="fig3.jpg">
<img alt="" src="Images/chapter-02-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 3</span>
</span></p>
<p class="TXI">When Hodgkin and Huxley returned to Plymouth eight years later the lab required a bit of reassembling: the building had been bombed in air raids and their equipment had been passed around to other scientists. But the men, each having picked up some extra quantitative skills as a result of their wartime assignments – Huxley performing data analysis for the <a id="page_39"></a>Gunnery Division of the Royal Navy and Hodgkin developing radar systems for the Air Force – were eager to get back to work on the physical mechanisms of the nerve impulse.</p>
<p class="TXI">For many of the following years, Hodgkin and Huxley (helped by fellow physiologist Bernard Katz) played with ions. By removing a certain type of ion from the neuron’s environment, they could determine which parts of the action potential relied on which kinds of charged particles. A neuron kept in a bath with less sodium had less of an overshoot. Add extra potassium to the bath and the undershoot – an effect at the very end of the action potential when the inside of the cell becomes more negative than normal – disappeared. The pair also experimented with a technique that let them directly control the voltage across the cell membrane. Changing this balance of charge created large changes in the flow of ions into and out of the cell. Remove the difference in charge across the membrane and stores of sodium outside the cell suddenly swim inwards; keep the cell in this state a bit longer and potassium ions from inside the cell rush out. </p>
<p class="TXI">The result of all these manipulations was a model. Specifically, Hodgkin and Huxley condensed their hard-won knowledge of the nuances of neural membranes into the form of an equivalent circuit and with it a corresponding set of equations. This equivalent circuit was more complex than Lapicque’s, however. It had more moving parts as it aimed to explain not just <span class="italic">when</span> an action potential happens but the full shape of the event itself. The main difference, though, came down to resistance.</p> <a id="page_40"></a>
<p class="TXI">In addition to the resistor Lapicque put in parallel with the membrane capacitor, Hodgkin and Huxley added two more – one specifically controlling the flow of sodium ions and the other controlling the flow of potassium ions. Such a separation of resistors assumed that different channels in the cell membrane were selectively allowing different ion types to pass. What’s more, the strength of these resistors – that is, the extent to which they block the flow of their respective ions – is not a fixed parameter in the model. Instead, they are dependent on the state of the voltage across the capacitor. The cell accomplishes this by opening or closing its ion channels as the voltage across its membrane changes. In this way, the membrane of the cell acts like the bouncer of a club: it assesses the population of particles on either side of itself and uses that to determine which ions get to enter and exit the cell. </p>
<p class="TXI">Having defined the equations of this circuit, Hodgkin and Huxley wanted to churn through the numbers to see if the voltage across the model’s capacitor really would mimic the characteristic whish and whoosh of an action potential. There was a problem, however. Cambridge was home to one of the earliest digital computers, a device that would’ve greatly sped up Hodgkin and Huxley’s calculations, but it was out of service. So, Huxley turned to a Brunsviga – a large, metal calculator powered by a hand-crank. As he sat for days putting in the value of the voltage at one point in time just to calculate what it would be at the next one-ten-thousandth of a second, Huxley actually found the work somewhat suspenseful. As he said in his Nobel lecture: ‘It was quite often exciting … Would the membrane <a id="page_41"></a>potential get away into a spike, or die in a subthreshold oscillation? Very often my expectations turned out to be wrong, and an important lesson I learnt from these manual computations was the complete inadequacy of one’s intuition in trying to deal with a system of this degree of complexity.’ </p>
<p class="TXI">With the calculations complete, Hodgkin and Huxley had a set of artificial action potentials, the behaviour of which formed a near-perfect mirror image of a real neuron’s spike. </p>
<p class="TXI">When injected with current, the Hodgkin-Huxley model cell displays a complex dance of changing voltage and resistances. First, the input fights against the cell’s natural state: it adds some positive charge to the largely negative inside of the cell. If this initial disturbance in the membrane’s voltage is large enough – that is, if the threshold is met – sodium channels start opening and a glut of positively charged sodium ions flood into the cell. This creates a positive feedback loop: the influx of sodium ions pushes the inside of the cell more positive and the resulting change in voltage lowers the sodium resistance even more. Soon, the difference in charge across the membrane disappears. The inside of the cell is briefly as positive as the outside, and then more so – the ‘overshoot’. As this is happening potassium channels are opening, letting positively charged potassium ions fall out of the cell. The sodium and potassium channels work like saloon doors, one letting ions in and the other out, but now the potassium ions are moving quicker. The work of the potassium ions reverses the trend in voltage. As this exodus of potassium again makes the inside of the cell more negative, sodium channels close. The <a id="page_42"></a>separation of charge across the membrane is being rebuilt. As the voltage nears its original value, positive charge continues to leak out of the still-open potassium channels – the ‘undershoot’. Eventually these too close, the voltage recovers and the cell has returned to normal, ready to fire again. The whole event takes less than one-half of one-hundredth of a second. </p>
<p class="TXI">According to Hodgkin, the pair built this mathematical model because ‘at first it might be thought that the response of a nerve to different electrical stimuli is too complicated and varied to be explained by these relatively simple conclusions’. But explain it they did. Like a juggler, the neuron combines simple parts in simple ways to create a splendidly intricate display. The Hodgkin-Huxley model makes clear that the action potential is a delicately controlled explosion occurring a billion times a second in your brain. </p>
<p class="TXI">The pair published their work – both experimental and computational – in a slew of <span class="italic">Journal of Physiology</span> papers in 1952. Eleven years later, they were awarded two-thirds of the Nobel Prize for ‘their discoveries concerning the ionic mechanisms involved in excitation and inhibition in the peripheral and central portions of the nerve cell membrane’. If doubts remained in the minds of any biologists about whether the nerve impulse was explainable in terms of ions and electricity, the work of Hodgkin and Huxley put those to rest. </p>
<p class="center">* * *</p>
<p class="TXT">‘The body <span class="italic">and dendrites</span> of a nerve cell are specialised for the reception and integration of information, <a id="page_43"></a>which is conveyed as impulses that are fired from other nerve cells along their axons’ (emphasis added). With this modest sentence John Eccles, an Australian neurophysiologist and the third awardee alongside Hodgkin and Huxley, began his Nobel lecture. The lecture goes on to describe the intricacies of ion flows that occur when one cell sends information to another. </p>
<p class="TXI">What the lecture doesn’t discuss is dendrites. Dendrites are the wispy tendrils that grow out of a neuron’s cell body. These offshoots, like tree roots, branch and stretch and branch again to cover a wide area around the cell. A cell casts its dendritic net out among nearby cells to collect input from them. </p>
<p class="TXI">Eccles had a complex relationship with dendrites. The type of neuron he studied – found in the spinal cord of cats – had elaborate dendritic branches. They spanned roughly 20 times the size of the cell’s body in all directions. Yet Eccles didn’t believe this cellular root system was terribly relevant. He conceded that the parts of the dendrites nearest the cell body may have some use: axons from other neurons land on these regions and their inputs will get transported immediately into the cell body where they can contribute to causing an action potential. But, he claimed, those farther out were simply too distant to do much: their signal wouldn’t survive the journey to the cell body. Instead, he assumed the cell used these arms to absorb and expel charged particles in order to keep its overall chemical balance intact. In Eccles’ eyes, therefore, dendrites were – at most – a wick that carried a flame a short way to the cell body and, at least, a straw slurping up some ions. </p>
<p class="TXI">Eccles’ position on dendrites put him at odds with his student, Wilfrid Rall. Rall earned a degree in physics <a id="page_44"></a>from Yale University in 1943 but, after his time working on the Manhattan Project, became interested in biology. He moved to New Zealand to work with Eccles on the effects of nerve stimulation in 1949. </p>
<p class="TXI">Given his background, Rall was quick to turn to mathematical analyses and simulations to understand a system as complex as a biological cell. And he was inspired and galvanised<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> by the work of Hodgkin and Huxley, which he heard about when Hodgkin visited the University of Chicago where Rall was getting his master’s degree. With this mathematical model in mind, Rall suspected that dendrites were capable of more than Eccles gave them credit for. After his time in New Zealand, Rall devoted a good portion of his career to proving the power of dendrites – and, in turn, proving the power of mathematical models to anticipate discoveries in biology. </p>
<p class="TXI">Building on the analogy of the cell as an electrical circuit, Rall modelled the thin cords of dendrites as just what they looked like: cables. The ‘cable theory’ approach to dendrites treats each section of a dendrite as a very narrow wire, the width of which – just as Ohm discovered – determines its resistance. Stringing these sections together, Rall explored how electrical activity at the far end of a dendrite can make its way to the cell body or vice versa. </p>
<p class="TXI">Adding more parts to this mathematical model, however, meant more numbers to crunch. The National Institutes of Health (NIH) in Bethesda, Maryland, where Rall worked, didn’t have a digital computer suitable for some of his larger simulations. When Rall wanted to run <a id="page_45"></a>the equations of a model with extensive dendrites, it was the job of Marjory Weiss, a programmer at the NIH, to drive a box of punch cards with the computer’s instructions to Washington DC to run them on a computer there. Rall couldn’t see the results of his model until she returned the next day. </p>
<p class="TXI">Through his elaborate mathematical explorations, Rall clearly showed – against the beliefs of Eccles – that a cell body with dendrites can have very different electrical properties than one without. A short description of Rall’s calculations, published in 1957, set off a years-long debate between the two men in the form of a volley of publications and presentations.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> Each pointed to experimental evidence and their own calculations to support their side. But slowly, over time, Eccles’ position shifted. By 1966, he had publicly accepted dendrites as a relevant cog in the neural machinery. Rall was right.</p>
<p class="TXI">Cable theory did more than just expose Eccles’ mistake. It also offered a way for Rall to explore in equations the many magical things that dendrites can do before the experimental techniques to do so were available. One important ability Rall identified was detecting order. Rall saw in his simulations that the order in which a dendrite gets input has important consequences for the cell’s response. If an input comes to a dendrite’s far end first, followed by more inputs closer and closer to the cell body, the cell may fire. If the <a id="page_46"></a>pattern is reversed, however, it won’t. This is because inputs coming in far from the cell body take longer to reach it. So, starting the inputs at the far end means they all reach the cell body at the same time. This creates a big change in the membrane’s voltage and possibly a spike. Going the other way, however, inputs come in at different times; this creates only a middling disturbance in voltage. In a race where runners start at different times and locations, the only way to get them to cross the finish line together is to give the farther ones a head start. </p>
<p class="TXI">Rall made this prediction in 1964. In 2010, it was shown to be true in real neurons. To test Rall’s hypothesis, researchers at University College London took a sample of neurons from rat brains. Placing these neurons in a dish, they were able to carefully control the release of neurotransmitters on to specific portions of a dendrite – portions as little as five microns (or the width of a red blood cell) apart. When this input went from the end of a dendrite to its root, the cell spiked 80 per cent of the time. In the other direction, it responded only half as often.</p>
<p class="TXI">This work shows how even the smallest bit of biology has a purpose. That the sections of a dendrite can work like the keys of a piano – where the same notes can be played in different ways to different effect – gives neurons new tricks. Specifically, it imbues neurons with the ability to identify sequences. There are many occasions where inputs sweeping across the dendrite from one direction should be treated differently from inputs sweeping the other way. For example, neurons in the retina have this kind of ‘direction selectivity’. This lets them signal which way objects in the visual field are moving.</p><a id="page_47"></a>
<p class="TXI">In many science classes, students are given small electrical circuit kits to play with. They can use wires of different resistances to connect up capacitors and batteries, maybe making a lightbulb light up or a fan spin. In much the same pick-and-play way, neuroscientists now build models of neurons. With the basic parts list of an electrical circuit, almost any observed property of a neuron’s activity can be mimicked. Rall helped add more parts to the kit.</p>
<p class="center">* * *</p>
<p class="TXT">If a standard neuron model is a small house built out of the bricks of electrical engineering, then the model constructed by the Blue Brain Project in 2015 is an entire metropolis. Eighty-two scientists across 12 institutions worked together as part of this unprecedented collaboration. Their goal was to replicate a portion of the rat brain about the size of a large grain of sand. They combed through previous studies and spent years performing their own experiments to collect every bit of data they could about the neurons in this region. They identified the ion channels they use, the length of their axons, the shapes of their dendrites, how closely they pack together and how frequently they connect. Through this, they identified 55 standard shapes that neurons can take, 11 different electrical response profiles they can have and a host of different ways they can interact.</p>
<p class="TXI">They used this data to build a simulation – a simulation that included more than 30,000 highly detailed model neurons forming 36 million connections. The full model required a specially built supercomputer to run through the billions of equations that defined it. Yet all of this <a id="page_48"></a>complexity still stemmed from the same basic principles of Lapicque, Hodgkin, Huxley and Rall. A lead researcher on the project, Idan Segev, summarised the approach: ‘Use Hodgkin-Huxley in an extended way and build a simulation of the way these cells are active, to get the music – the electrical activity – of this network of neurons that should imitate the real biological network that you’re trying to understand.’ </p>
<p class="TXI">As the team showed in their publication documenting the work, the model was able to reproduce several features of the real biological network. The simulation showed similar sequences of firing patterns over time, a diversity of responses across cell types and oscillations. More than just replicating results of past experiments, this real-to-life model also makes it possible to explore new experiments quickly and easily. Recreating the biology in a computer makes virtual investigations of this brain region as simple as writing a few lines of code – an approach known as ‘in silico’ neuroscience.</p>
<p class="TXI">Running such simulations can only give good predictions if the model underlying them is a reasonable facsimile of biology. Thanks to Lapicque, we know that using the equations of an electrical circuit as a stand-in for a neuron is a solid foundation on which to build models of the brain. It was his analogy that set off the study of the nerve as an electrical device. And the extension of his analogy by countless other scientists – many trained in both physics and physiology – expanded its explanatory power even further. The nervous system – against Müller’s intuitions – is brought to life by the flow of electricity and the study of it has been undeniably animated by the study of electricity.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-1" id="fn-1">1</a> ﻿In the course of proving that contact between dissimilar metals generates electricity, Volta ended up inventing the battery. ﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-2" id="fn-2">2</a> ﻿Named, of course, after our man Galvani.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-3" id="fn-3">3</a> ﻿Applying voltage was an easier way to control the flow of charge than injecting current directly.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-4" id="fn-4">4</a> ﻿More on Adrian, and what his discovery meant about how neurons represent information, in ﻿﻿Chapter 7﻿﻿.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-5" id="fn-5">5</a> ﻿Spike, firing, activity, action potential ﻿–﻿ the sacred emission of a neuron goes by many names.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-6" id="fn-6">6</a> ﻿The ﻿‘﻿squid giant axon﻿’﻿ that Hodgkin and Huxley were studying is a particularly large (about the width of a marker tip) axon of a rather average-sized squid. It is not, as many students of neuroscience initially believe, the axon of a giant squid. ﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-7" id="fn-7">7</a> ﻿Also named after our man Galvani.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-8" id="fn-8">8</a> ﻿According to Rall, Eccles even prevented his work from being published. With respect to a 1958 manuscript: ﻿‘﻿A negative referee persuaded the editors to reject this manuscript. The fact that this referee was Eccles, was clear from many marginal notes on the returned manuscript.﻿’﻿﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<meta charset="utf-8"/>
<title>Contents</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="FMT" id="re_contents">Contents</p>
<p class="TOC-CH"><a href="chapter1.xhtml#chapter1" id="re_chapter1">Chapter 1:  Spherical Cows</a></p>
<p class="TOC-CH"><a href="chapter2.xhtml#chapter2" id="re_chapter2">Chapter 2:  How Neurons Get Their Spike</a></p>
<p class="TOC-CH"><a href="chapter3.xhtml#chapter3" id="re_chapter3">Chapter 3:  Learning to Compute</a></p>
<p class="TOC-CH"><a href="chapter4.xhtml#chapter4" id="re_chapter4">Chapter 4:  Making and Maintaining Memories</a></p>
<p class="TOC-CH"><a href="chapter5.xhtml#chapter5" id="re_chapter5">Chapter 5:  Excitation and Inhibition</a></p>
<p class="TOC-CH"><a href="chapter6.xhtml#chapter6" id="re_chapter6">Chapter 6:  Stages of Sight</a></p>
<p class="TOC-CH"><a href="chapter7.xhtml#chapter7" id="re_chapter7">Chapter 7:  Cracking the Neural Code</a></p>
<p class="TOC-CH"><a href="chapter8.xhtml#chapter8" id="re_chapter8">Chapter 8:  Movement in Low Dimensions</a></p>
<p class="TOC-CH"><a href="chapter9.xhtml#chapter9" id="re_chapter9">Chapter 9:  From Structure to Function</a></p>
<p class="TOC-CH"><a href="chapter10.xhtml#chapter10" id="re_chapter10">Chapter 10:  Making Rational Decisions</a></p>
<p class="TOC-CH"><a href="chapter11.xhtml#chapter11" id="re_chapter11">Chapter 11:  How Rewards Guide Actions</a></p>
<p class="TOC-CH"><a href="chapter12.xhtml#chapter12" id="re_chapter12">Chapter 12:  Grand Unified Theories of the Brain</a></p>
<p class="TOC-CH1"><a href="Mathematical.xhtml#Mathematical" id="re_Mathematical">Mathematical Appendix</a></p>
<p class="TOC-CH"><a href="ack.xhtml#ack" id="re_ack">Acknowledgements</a></p>
<p class="TOC-CH"><a href="bib.xhtml#bib" id="re_bib">Bibliography</a></p>
<p class="TOC-CH"><a href="index.xhtml#index" id="re_index">Index</a></p>
</body>
</html>