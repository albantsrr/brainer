<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 6</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter6"><a href="contents.xhtml#re_chapter6">CHAPTER SIX</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter6">Stages of Sight</a><a id="page_151"></a></p>
<p class="H1" id="b-9781472966445-ch612-sec6">
<span class="bold">
<span>The Neocognitron and convolutional 
neural networks</span>
</span></p>
<p class="EXTF">
<span class="italic">The summer vision project is an attempt to use our summer workers effectively in the construction of a significant part of a visual system. The particular task was chosen partly because it can be segmented into sub-problems which will allow individuals to work independently and yet participate in the construction of a system complex enough to be a real landmark in the development of ‘pattern recognition’.</span></p>
<p class="right">Vision Memo No. 100 from the 
Massachusetts Institute of Technology 
Artificial Intelligence Group, 1966</p>
<p class="TXT-con">The summer of 1966 was meant to be the summer that a group of MIT professors solved the problem of artificial vision. The ‘summer workers’ they planned to use so effectively for this project were a group of a dozen or so undergraduate students at the university. In their memo laying out the project’s plan, the professors provided several specific skills they wanted the computer system the students were developing to perform. It should be able to define different textures and lighting in an image, label parts as foreground and parts as background, and identify whatever objects <a id="page_152"></a>were present. One professor<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> purportedly described the aims more casually as ‘linking a camera to a computer and getting the computer to describe what it saw’. </p>
<p class="TXI">The goals of this project were not completed that summer. Nor the next. Nor many after that. Indeed, some of the core issues raised in the description of the summer project remain open problems to this day. The hubris on display in that memo is not surprising for its time. As discussed in <a href="chapter3.xhtml#chapter3">Chapter 3</a>, the 1960s saw an explosion in computing abilities and, in turn, naive hopes about automating even the most complex tasks. If computers could now do anything asked of them, it was just a matter of knowing what to ask for. With something as simple and immediate as vision, how hard could that be?</p>
<p class="TXI">The answer is very hard. The act of visual processing – of taking in light through our eyes and making sense of the external world that reflected it – is an immensely complex one. Common sayings like ‘right in front of your eyes’ or ‘in plain sight’, which are used to indicate the effortlessness of vision, are deceitful. They obscure the significant challenges even the most basic visual inputs pose for the brain. Any sense of ease we have regarding vision is an illusion, one that was hard won through millions of years of evolution. </p>
<p class="TXI">The problem of vision is specifically one of reverse engineering. In the back of the eye, in the retina, there is <a id="page_153"></a>a wide flat sheet of cells called photoreceptors. These cells are sensitive to light. Each one indicates the presence or absence (and possibly wavelength) of the light hitting it at each moment by sending off a signal in the form of electrical activity. This two-dimensional flickering map of cellular activity is the only information from which the brain is allowed to reconstruct the three-dimensional world in front of it. </p>
<p class="TXI">Even something as simple as finding a chair in a room is a technically daunting endeavour. Chairs can be many different shapes and colours. They can also be nearby or far away, which makes their reflection on the retina larger or smaller. Is it bright in the room or dark? What direction is light coming from? Is the chair turned towards you or away? All of these factors impact the exact way in which photons of light hit the retina. But trillions of different patterns of light could end up meaning the same thing: a chair is there. The visual system somehow finds a way to solve this many-to-one mapping in less than a tenth of a second. </p>
<p class="TXI">At the time those MIT students were working to give the gift of sight to computers, physiologists were using their own tools to solve the mysteries of vision. This started with recording neural activity from the retina and moved on to neurons throughout the brain. With an estimated 30 per cent of the primate cortex playing some role in visual processing, this was no small undertaking.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> In the mid-twentieth century many of the scientists performing these experiments were based in <a id="page_154"></a>the Boston area (many at MIT itself or just north of it, at Harvard) and they were quickly amassing a lot of data that they needed to somehow make sense of.</p>
<p class="TXI">Perhaps it was the physical proximity. Perhaps it was a tacit acknowledgement of the immense challenge they had each set for themselves. Perhaps in the early days the communities were just too small to keep to themselves. Whatever the reason, neuroscientists and computer scientists forged a long history of collaborating in their attempts to understand the fundamental questions of vision. The study of vision – of how patterns can be found in points of light – is full of direct influence from the biological to the artificial and vice versa. The harmony may not have been constant: when computer science embarked on methods that were useful but didn’t resemble the brain, the fields diverged. And when neuroscientists dig into the nitty-gritty detail of the cells, chemicals and proteins that carry out biological vision, computer scientists largely turn away. But the impacts of the mutual influence are still undeniable, and plainly visible in the most modern models and technologies. </p>
<p class="center">* * *</p>
<p class="TXT">The earliest efforts to automate vision came before modern computers. Though implemented in the form of mechanical gadgetry, some of the ideas that powered these machines prepared the field for the later emergence of computer vision. One such idea was <span class="italic">template matching</span>. </p>
<p class="TXI">In the 1920s, Emanuel Goldberg, a Russian chemist and engineer, set out to solve a problem banks and other offices had while searching their file systems for <a id="page_155"></a>documents. At the time, documents were stored on microfilm – strips of 35mm film that contained tiny images of documents that could be projected to a larger screen for reading. The ordering of the documents on the film had little relation to their contents, so finding a desired document – such as a cancelled cheque from a particular bank customer – involved much unstructured searching. Goldberg turned to a crude form of ‘image processing’ to automate this process. </p>
<p class="TXI">Under Goldberg’s plan, cashiers entering a new cheque into the filing system would need to mark it with a special symbol that indicated its contents. For example, three black dots in a row meant the customer’s name started with ‘A’, three black dots in a triangle meant it started with ‘B’ and so on. Now, if a cashier wanted to find the last cheque submitted by a 
Mr Berkshire, for example, they just needed to find the cheques marked with a triangle. The triangle pattern was thus a template and the goal of Goldberg’s machine was to match it. </p>
<p class="TXI">Physically, these templates took the form of cards with holes punched in them. So, when looking for Mr Berkshire’s documents, the cashier would take a card with three holes punched out in the shape of a triangle and place it in between the microfilm strip and a lightbulb. Each document on the strip would then be automatically pulled up to be aligned with the card, causing the light to shine through the holes on the card and then through the film itself. A photocell placed behind the film detected any light that came through and signalled this to the rest of the machine. For most of the documents, some light would get through as the <a id="page_156"></a>symbols on the film didn’t align with the holes on the card. But when the desired document appeared, the light shining through the card would be exactly blocked out by the pattern of black dots on the film. These mini eclipses meant no light would land on the photocell and this signalled to the rest of the machine, and to the cashier, that a match had been found. </p>
<p class="TXI">Goldberg’s approach required that the cashiers knew in advance exactly what symbol they were looking for and had a card to match it. Crude though it was, this style of template matching became the dominant approach for much of the history of artificial vision. When computers appeared on the scene, the form of the templates migrated from the physical to the digital. </p>
<p class="TXI">In a computer, images are represented as a grid of pixel values (see Figure 14). Each pixel value is a number indicating the intensity of the colour in the tiny square region of the picture it represents.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> In the digital world, a template is also just a grid of numbers, one that defines the desired pattern. So, the template for three dots in the shape of a triangle may be a grid of mostly zeros except for three precisely placed pixels with value one. The role of the light shining through the template card in Goldberg’s machine was replaced in the computer by a mathematical operation: multiplication. If each pixel value in the image is multiplied by the value at the same location in the template, the result can actually tell us if the image is a match.</p> <a id="page_157"></a>
<p class="image-fig" id="fig14.jpg">
<img alt="" src="Images/chapter-06-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 14</span>
</span></p>
<p class="TXI">Let’s say we are looking for a smiling face in a black and white image (where black pixels have a value of one and white have a value of zero). Given a template for the face, we can compare it with an image through multiplication. If the image is indeed of the face we are searching for, the values that comprise the template will be very similar to those in the image. Therefore, zeros in the template will be multiplied by zeros in the image and ones in the template will be multiplied by ones in the image. Adding up all the resulting values from this multiplication gives us the number of black pixels that are the same in the template and the image, which in the case of a match would be many. If the image we’re given is of a frowning face instead, some of the pixels around the mouth in the image won’t match the template. There, zeros in the template will be multiplied by ones in the image and vice versa. Because the product at those pixel locations will be zero, the sum across the whole image won’t be so high. In this way, a simple sum of products gives a measure of how much an image matches a template.</p><a id="page_158"></a>
<p class="TXI">This method found wide use in many different industries. Templates have been used to count crowd sizes by finding faces in a picture. Known geographical features have also been located in satellite images via templates. The number and model of cars that pass through an intersection can be tracked as well. With template matching, all we need to do is define what we want and multiplication will tell us if it’s a match. </p>
<p class="center">* * *</p>
<p class="TXT">Imagine a stadium – one just like where you’d watch a football game – but in this stadium, instead of screaming fans, the stands are full of screaming demons. And what they are screaming about isn’t players on the field, but rather an image. Specifically, each of these demons has its own preferred letter of the alphabet and when it sees something that looks like that letter on the field it shrieks. The louder the shriek the more the image on the field looks like the demon’s favourite letter. Up in the skybox is another demon. This one doesn’t look at the field or do any screaming itself, but merely observes all the other demons in the stadium. It finds the demon shrieking the loudest and determines that the image on the field must be that demon’s favourite letter.</p>
<p class="TXI">This is how Oliver Selfridge described the process of template matching at a 1958 conference. Selfridge was a mathematician, computer scientist and associate director of Lincoln Labs at MIT, a research centre focused on national security applications of technology. Selfridge didn’t publish many papers himself. He never finished his own PhD dissertation either (he did however end up <a id="page_159"></a>writing several children’s books; presumably these contained fewer demons). Despite this lack of academic output, his ideas infiltrated the research community nonetheless, largely due to the circles in which he moved. After earning a bachelor’s degree in mathematics from MIT at only 19 years old, Selfridge was advised in his PhD work by the notable mathematician Norbert Wiener and remained in contact with him. Selfridge also went on to supervise Marvin Minsky, the prominent AI researcher from <a href="chapter3.xhtml#chapter3">Chapter 3</a>. And as a graduate student, Selfridge was friends with Warren McCulloch and for a time lived with Walter Pitts (you’ll recall this pair of neuroscientists from <a href="chapter3.xhtml#chapter3">Chapter 3</a> as well). Selfridge benefited from letting his ideas marinate in this social stew of prominent scientists. </p>
<p class="TXI">To map Selfridge’s unique analogy to the concept of template matching, we just have to think of each demon as holding its own grid of numbers that represents the shape of its letter. They multiply their grid with the image and sum up those products (just as described above) and scream at a volume determined by that sum. Selfridge doesn’t give much of an indication as to why he chose to give such a demonic description of visual processing. His only reflection on it is to say: ‘We are not going to apologise for a frequent use of anthropomorphic or biomorphic terminology. They seem to be useful words to describe our notions.’<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p><a id="page_160"></a>
<p class="TXI">Most of the notions in Selfridge’s presentation were actually about how the template matching approach is flawed. The demons – each individually checking if their favourite letter was in sight – weren’t being very efficient. They each performed their own completely separate computations, but it didn’t have to be that way. Many of the shapes that a demon may look for in its search for its letter are also used by other demons. For example, both the ‘A’-preferring demon and the ‘H’-preferring demon would be on the lookout for a horizontal bar. So why not introduce a separate group of demons, ones whose templates and screams correspond to more basic features of the image such as horizontal bars, vertical lines, slanted lines, dots, etc. The letter demons would then just listen to those demons rather than look at the images themselves and decide how much to scream based on whether the basic shapes of their letter are being yelled about. </p>
<p class="TXI">From bottom to top Selfridge defined a new style of stadium that contained three types of demon: the ‘computational’ (those that look at the image and yell about basic shapes), ‘cognitive’ (those that listen to the computational demons and yell about letters) and ‘decision’ (the one that listens to the cognitive demons and decides which letter is present). The name Selfridge gave to the model as a whole – to this stack of shrieking demons – was Pandemonium.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup></p>
<p class="TXI">Nefarious nomenclature aside, Selfridge’s intuitions on visual processing proved insightful. While conceptually <a id="page_161"></a>simple, template matching is practically challenging. The number of templates needed grows with the number of objects you want to be able to spot. If each image needs to be compared to every filter, that’s a lot of calculations. Templates also need to more or less match the image exactly. But because of the myriad different patterns of light that the same object can produce on a retina or camera lens, it’s nearly impossible to know how every pixel in an image should look when a given object is present. This makes templates very hard to design for any but the simplest of patterns. </p>
<p class="TXI">These issues make template matching a challenge both for artificial visual systems and for the brain. The ideas on display in Pandemonium, however, represent a more distributed approach, as the features detected by the computational demons are shared across cognitive demons. The approach is also <span class="italic">hierarchical</span>. That is, Pandemonium breaks the problem of vision into two stages: first look for the simple things, then for the more complex. </p>
<p class="TXI">Together, these properties make the system more flexible overall. If Pandemonium was set up to recognise the letters of the first half of the alphabet, for example, it would be in a pretty good position to recognise the rest. This is because the low-level computational demons would already know what kinds of basic shapes letters are made of. The cognitive demon for a new letter would just need to figure out the right way to listen to the demons below it. In this way, the elementary features work like a vocabulary – or a set of building blocks – that can be combined and recombined to detect additional complex patterns. Without this hierarchical structure and sharing of low-level features, a basic <a id="page_162"></a>template-matching approach would need to produce a new template for each letter from scratch.</p>
<p class="TXI">The design of Pandemonium does pose some questions. For example, how does each computational demon know what basic shape to scream about? And how do the cognitive demons know to whom they should listen? Selfridge proposes that the system learn the answers to these questions through trial and error. If, for example, adjusting how the ‘A’-preferring demon listens to those below it makes it better at detecting ‘A’s, then keep those changes; otherwise don’t and try something new. Or, if adding a computational demon to scream about a new low-level pattern makes the whole system better at letter detection, then that new demon stays; otherwise it’s out. This is an arduous process of course and it’s not guaranteed to work, but when it does it has the desirable effect of creating a system that is customised – automatically – for the type of objects it needs to detect. The strokes that comprise the symbols of the Japanese alphabet, for example, differ from those in the English alphabet. A system that learns would discover the different basic patterns for each. No prior or special knowledge needed, just let the model take a stab at the task.</p>
<p class="TXI">Computer scientist Leonard Uhr was impressed enough with the ideas of Selfridge and colleagues that he wanted to share their work more broadly. In 1963, he wrote in the <span class="italic">Psychological Bulletin</span> to an audience of psychologists about the strides computer scientists were making on the vision front. In his article, ‘“Pattern recognition” computers as models for form perception’, he indicates that the models of the time were ‘actually already in the position of suggesting physiological and <a id="page_163"></a>psychological experiments’ and even warns that ‘it would be unfortunate if psychologists did not play any part in this theoretical development of their own science’. The article is concrete evidence of the intertwined relationship the two fields have always had. But such explicit public pleas for collaboration weren’t always needed. Sometimes personal relationships were enough.</p>
<p class="TXI">Jerome Lettvin was a neurologist and psychiatrist from Chicago, Illinois. He was also a friend of Selfridge, having shared a house with him and Pitts as a young man. A self-described ‘overweight slob’, Lettvin wanted to be a poet but appeased his mother’s wishes and became a doctor instead. The most rebellion he managed was to occasionally abandon his medical practice to engage in some scientific research.</p>
<p class="TXI">Inspired by the work of his friend and former cohabitant, in the late 1950s Lettvin set out to search for neurons that responded to low-level features – that is, to the types of things computational demons would scream about. The animal he chose to look at was the frog. Frogs use sight mostly to make quick reflexive responses to prey or to predators and as a result their visual system is relatively simple. </p>
<p class="TXI">Inside the retina, individual light-detecting photore­ceptors send their information to another population of cells called ganglion cells. Each photoreceptor connects to many ganglion cells and each ganglion cell gets inputs from many photoreceptors. But, crucially, all these inputs come from a certain limited region of space. This makes a ganglion cell responsive only to light that hits the retina in that specific location – and each cell has its own preferred location.</p> <a id="page_164"></a>
<p class="TXI">At this point in time, ganglion cells were assumed not to do much computation themselves. They were thought of mostly as a relay – just sending information about photoreceptor activity along to the brain like a mail carrier. Such a picture would fit within a template-matching view of visual processing. If the role of the brain was to compare visual information from the eye to a set of stored templates, it wouldn’t want that information distorted in any way by the ganglion cells. But if the ganglion cells were part of a hierarchy – where each level played a small role in the eventual detection of complex objects – they should be specialised to detect useful elementary visual patterns. Rather than relaying information verbatim, then, they should be actively processing and repackaging it.</p>
<p class="TXI">Lettvin found – by recording the activity of these ganglion cells and showing all kinds of moving objects and patterns to the frog – that the hierarchy hypothesis was true. In fact, in a 1959 paper ‘What the frog’s eye tells the frog’s brain’ he and his co-authors describe four different types of ganglion cells that each responded to a different simple pattern. Some responded to swift large movements, others to when light turned to dark and still others to curved objects that jittered about. These different categories of responses proved that the ganglion cells were specifically built to detect different elementary patterns. Not only did these findings align with Selfridge’s notions of low-level feature detectors, but they also supported the idea that these features are specific to the type of objects the system needs to detect. For example, the last class of cells responded best when a small dark object moved quickly in fits and starts around a fixed <a id="page_165"></a>background. After describing these in the paper, Lettvin remarked: ‘Could one better describe a system for detecting an accessible bug?’ </p>
<p class="TXI">Selfridge’s intuitions were proving to be correct. With Lettvin’s finding in frogs, the community started to conceive of the visual system more as a stack of screaming demons and less as a store of template cards.</p>
<p class="center">* * *</p>
<p class="TXT">Around the same time as Lettvin’s work, two doctors at the John Hopkins University School of Medicine in Baltimore were exploring vision in cats. A cat’s visual system is more like ours than a frog’s. It is tasked with challenging problems related to tracking prey and navigating the environment and is, as a result, more elaborate. The work of the cat visual system is thus stretched over many brain areas and the one that doctors David Hubel<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> and Torsten Wiesel focused on was the primary visual cortex. This region at the back of the brain represents one of the earlier stages of visual processing in mammals; it gets its input from another brain area – the thalamus – that gets input from the retina itself. </p>
<p class="TXI">Previous work had investigated how the neurons in the thalamus and retina of cats behave. These cells tend to respond best to simple dots: either a small area of light surrounded by dark or a small area of dark surrounded <a id="page_166"></a>by light. And, as in the frog, each neuron has a specific location the dot needs to be in for it to respond. </p>
<p class="TXI">Hubel and Wiesel had access to equipment for producing dots at different locations in order to explore such retinal responses. So, this is the equipment they used, even as they investigated brain areas well beyond the retina. The method for displaying dots included sliding a small glass or metal plate with different cut-out patterns over a screen in front of the eye. Hubel and Wiesel used this to show slide after slide of dots to their feline subject as they measured the activity of a neuron in its primary visual cortex. But the dots simply didn’t do it for this neuron – the cell wouldn’t make a peep in response to the slides. Then the experimenters noticed something strange: occasionally the neuron would respond – not to the slides – but to the changing of them. As one plate was slid out and another in, the shadow from the edge of the glass swept across the cat’s retina. This created a moving line that reliably excited the neuron in the primary visual cortex. One of the most iconic discoveries in neuroscience had just occurred, almost by accident. </p>
<p class="TXI">Decades later, reflecting on the serendipity of this discovery, Hubel remarked: ‘In a certain early phase of science a degree of sloppiness can be a huge advantage.’ But that phase quickly passed. By 1960 he and Wiesel had moved their operation to Boston, to help establish the department of neurobiology at Harvard University and embarked on years of careful investigation into the responses of neurons in the visual system. </p>
<p class="TXI">Expanding on their happy accident, Hubel and Wiesel dug deep into how this responsiveness to moving <a id="page_167"></a>lines worked. One of their first findings was that the neurons in the primary visual cortex each have a preferred <span class="italic">orientation</span> in addition to a preferred location. A neuron won’t respond to just any line that shows up in its favourite location. Horizontal-preferring neurons require a horizontal line, vertical-preferring neurons require vertical lines, 30-degree-slant-preferring neurons require 30-degree slanted lines, and so on and so on. To get a sense of what this means, you can hold a pen out horizontally in front of your face and move it up and down. You’ve just excited a group of neurons in your primary visual cortex. Tilt the pen another way and you’ll excite a different group (you’ve now got at-home, targeted brain stimulation for free!).</p>
<p class="TXI">With their realisation about orientation, Hubel and Wiesel had discovered the alphabet used by the cat brain to represent images. Flies have bug detectors and cats (and other mammals) have line detectors. However, they didn’t stop at just observing these responses, they went further to ask how the neurons in the primary visual cortex could come to have such responses. After all, the cells they get their inputs from – those in the thalamus – respond to dots, not lines. So where did the preference for lines come from? </p>
<p class="TXI">The solution was to assume that neurons in the cortex get a perfectly selected set of inputs from the thalamus. A line, of course, is nothing more than a set of appropriately arranged dots. Inputs to a neuron in the primary visual cortex therefore must come from a set of thalamus neurons wherein each one represents a dot in a row of dots. That way, the primary visual neuron would fire the most when a line was covering all those dots (see Figure 15). Just like <a id="page_168"></a>the cognitive demons listening for the shrieks of the demons that look for parts of their letter, neurons in the primary visual cortex listen for the activity of neurons in the thalamus that make up their preferred line. </p>
<p class="TXI">Hubel and Wiesel noticed another kind of neuron, too: ones that also had preferred orientations, but weren’t quite as strict about location. These neurons would respond if a line appeared anywhere in a region that was about four times larger than that of the other neurons they recorded. How could these neurons come to have this response? The answer, again, was to assume they got just the right inputs. In particular, a ‘complex’ neuron – as Hubel and Wiesel labelled these cells – just needed input from a group of regular (or ‘simple’) neurons. All these simple cells should have the same preferred orientations but slightly different preferred locations. That way, a complex cell would inherit the orientation preference of its inputs, but have a spatial preference that is larger than any single one of them. This spatial flexibility is important. If we want to know if the letter ‘A’ is in front of us, a little bit of jitter in the exact location of its lines shouldn’t really matter. Complex cells are built to discard jitter.</p>
<p class="image-fig" id="fig15.jpg">
<img alt="" src="Images/chapter-06-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 15</span>
</span></p>
<p class="TXI">The discovery of complex cells provided an additional piece of the puzzle as to how points of light become perception. In addition to the feature detection done by simple cells, pooling of inputs across space was added to the list of computations performed by the visual system. For all the work they did dissecting this system, Hubel and Wiesel were awarded the Nobel Prize in 1981. In his Nobel lecture, Hubel put their goals plainly: ‘Our idea originally was to emphasise the tendency toward <a id="page_169"></a>increased complexity as one moves centrally along the visual path, and the possibility of accounting for a cell’s behaviour in terms of its inputs.’<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> This approach, while simple, sufficed to capture many of the basic properties of the visual-processing pathway. </p>
<p class="center">* * *</p>
<p class="TXT">On the other side of the world – at Japan’s national public broadcasting organisation, NHK, located in Tokyo – Kunihiko Fukushima heard about the simple properties of the visual system. Fukushima was an engineer and part of the research arm of NHK. Because NHK was a broadcasting company (and was broadcasting visual and audio signals into the eyes and ears of humans) <a id="page_170"></a>it also had groups of neurophysiologists and psychologists on staff to study how sensory signals are received by the brain. These three groups – the psychologists, physiologists and engineers – would meet regularly to share the work of their respective fields. One day, a colleague of Fukushima’s decided to present the work of Hubel and Wiesel. </p>
<p class="TXI">When Fukushima saw this clear description of the roles of neurons in the visual system, he set out to implement the very same functions in a computer model. His model used images of simple white patterns on a black background as input. To approximate the work of the thalamus, a sheet of artificial neurons was created that responded to white dots in the image. This served as a way to get the image information into the network. From here the input to the simple cells needed to be calculated. </p>
<p class="TXI">To do so, Fukushima used the standard approach of making a grid of numbers that represent the to-be-detected pattern – which in the case of a simple cell is a line with a specific orientation. In engineering terms, this grid of numbers is known as a <span class="italic">filter</span>. To mimic the spatial preferences of simple cells, Fukushima applied this filter separately at each location in the image. Specifically, the activity of one simple cell was calculated as the sum of the thalamus activity at one location multiplied by the filter. Sliding the filter across the whole image created a set of simple cells all with the same preferred orientation but different preferred locations. This is a process known in mathematics as a <span class="italic">convolution</span>. </p>
<p class="TXI">By producing multiple filters – each representing a line with a different orientation – and convolving each with the image, Fukushima produced a full population <a id="page_171"></a>of simple cells each with its own preferred orientation and location, just like the brain. For the complex cells, he simply gave them strong inputs from a handful of simple cells that all represented the same orientation in nearby locations. That way they would be active if the orientation appeared in any of those locations. </p>
<p class="TXI">This first version of Fukushima’s model was pretty much a direct translation of the physiological findings of Hubel and Wiesel into mathematics and computer code – and, in a way, it worked. It could do some simple visual tasks like finding curved lines in a black and white image, but it was far from a complete visual system and Fukushima knew that. As he later recounted in an interview, after publishing this work in the late 1960s, Fukushima waited patiently to see what Hubel and Wiesel would discover next; he wanted to know what the later stages of visual processing did so he could add them to his model. </p>
<p class="TXI">But the famous pair of physiologists never provided that information. After their initial work cataloguing cell types, Hubel and Wiesel explored the responses of cells in other visual areas, but were never able to give as clean a description as they had for the primary visual cortex. They eventually moved on to studying how the visual system develops in young animals. </p>
<p class="TXI">Without the script provided by biology, Fukushima needed to improvise. The solution he devised was to take the structure he had – that of simple cells projecting to complex cells – and repeat it. Stacking more simple and complex cells on top of each over and over creates an extended hierarchy that visual information can be passed through. This means, specifically, a second round of ‘simple’ cells comes after the initial layer of complex <a id="page_172"></a>cells. This second layer of simple cells would be on the lookout not for simple features in the image, but rather for simple ‘features’ in the activity of the complex cells from which they get their input. They’d still use filters and convolutions, but just applied to the activity of the neurons below them. Then these simple cells would send inputs to their own complex cells that respond to the same features in a slightly larger region of space – and then the whole process starts again. </p>
<p class="TXI">Simple cells look for patterns; complex cells forgive a slight misplacement of those patterns. Simple, complex; simple, complex. Over and over. Repeating this riff leads to cells that are responsive to all kinds of patterns. For a second-layer simple cell to respond to the letter ‘L’, for example, it just needs to get input from a horizontal-preferring complex cell at one location and a vertical-preferring one at the location just above and to the left of it. A third-layer simple cell could then easily respond to a rectangle by getting input from two appropriately placed ‘L’-cells. Go further and further up the chain and cells start responding to larger and more complex patterns, including full shapes, objects and even scenes. </p>
<p class="TXI">The only problem with extending Hubel and Wiesel’s findings this way was that Fukushima didn’t actually know how the cells in the different layers should connect to each other. The filters – those grids of numbers that would determine how the simple cells at any given layer respond – had to be filled. But how? For this, Fukushima took a page out of Selfridge’s book of Pandemonium and turned to learning. </p>
<p class="TXI">Rather than using the kind of trial and error Selfridge proposed, Fukushima used a version of learning that <a id="page_173"></a>doesn’t require knowing the right answers. In this form of learning, the model is simply shown a series of images without being told what’s in them. The activity of all the artificial neurons is calculated in response to each image and the connections between neurons change depending on how active they are (this may remind you of the Hebbian style of learning discussed in <a href="chapter4.xhtml#chapter4">Chapter 4</a>). If a neuron was very active in response to a particular image, for example, the connections from its very active inputs would be strengthened. As a result, that neuron would respond strongly to that and similar images in the future. This makes neurons responsive to specific shapes and different neurons diverge to have different responses. The network is therefore able to pick out a diversity of patterns in the input images. </p>
<p class="TXI">In the end, Fukushima’s model contained three layers of simple and complex cells, and was trained using computer-generated images of the digits zero to four. He dubbed the network the ‘Neocognitron’ and published the results of it in the journal <span class="italic">Biological Cybernetics</span> in 1980. </p>
<p class="TXI">In their original papers, Hubel and Wiesel made a point of stressing that their classification system and nomenclature was not meant to be taken as gospel. The brain is complicated and dividing neurons into only two categories could in no way capture the full diversity of responses and functions. It was just for convenience and expediency of communication that they proceeded in such a way. Yet Fukushima found success in doing the exact thing Hubel and Wiesel warned against: he <span class="italic">did</span> collapse the brimming complexity of the brain’s visual system into two very simple computations. He did take these descriptions as true, or true enough, and even <a id="page_174"></a>stretched them beyond what they were meant to describe. </p>
<p class="TXI">This practice – of collapsing and then expanding, of shaking the leaves off a tree and using it to build a house – is what all theorists and engineers know to be necessary if progress is to be made. Fukushima wanted to build a functioning visual system in a computer. Hubel and Wiesel provided a description of the brain’s visual system to a first approximation. Sometimes the first approximation is enough. </p>
<p class="center">* * *</p>
<p class="TXT">In 1987, like in any other year, the people of Buffalo, New York, mailed countless bills, birthday cards and letters through their local post office. What the citizens of the town didn’t know, as they inked the 5-digit zip code of the recipient on to their envelope, was that this bit of their handwriting would be immortalised – digitised and stored on computers across the country for years to come. It would become part of a database for researchers trying to teach computers how to read human handwriting and, in turn, revolutionise artificial vision. </p>
<p class="TXI">Some of the researchers working on this project were at Bell Labs, a research company owned by the telecommunications company AT&amp;T, located in suburban New Jersey. Among the group of mostly physicists was a 28-year-old French computer scientist named Yann LeCun. LeCun had read about Fukushima and his Neocognitron, and he recognised how the simple repeating architecture of that model could solve many of the hard problems of vision.</p> <a id="page_175"></a>
<p class="TXI">LeCun also recognised, however, that the way the model learned its connections needed to change. In particular, he wanted to move back towards the approach of Selfridge and give the model access to images paired with the correct labels of which digit is in them. So, he tweaked some of the mathematical details of the model to make it amenable to a different kind of learning. In this type of learning, if the model misclassifies an image (for example, labels a two as a six), all the connections in the model – those grids of numbers that define what patterns are searched for – are updated in a way that makes them less likely to misclassify that image in the future. In this way, the model learns what patterns are important for identifying digits. This may sound familiar because what LeCun used was the backpropagation algorithm described in <a href="chapter3.xhtml#chapter3">Chapter 3</a>. Do this with enough images and the model as a whole becomes quite good at classifying images of handwritten digits, even ones it’s never seen before. </p>
<p class="TXI">LeCun and his fellow researchers unveiled the impressive results of their model, trained on the thousands of Buffalo digits, in 1989. The ‘convolutional neural network’ – the name given to this style of model – was born. </p>
<p class="TXI">Just like the template-matching approaches that came before it, convolutional neural networks found applications in the real world. In 1997, these networks formed a core part of a software system AT&amp;T developed to automate the processing of cheques at banks across America. By 2000, it was estimated that between 10 and 20 per cent of cheques in America were being processed by this software. In a charming example of science fulfilling its destiny, Goldberg’s dream of equipping <a id="page_176"></a>banks with synthetic visual systems came true some 70 years after the invention of his microfilm machine. </p>
<p class="TXI">The method for training convolutional neural networks is a data-hungry one and the model will only learn to be as good as what’s fed into it. Just as important as getting the right model, then, is getting the right data. That is why it was so crucial to collect real samples of real digits written by real people. The Bell Lab researchers could’ve done as Fukushima did and made computer-generated images of numbers. But those would hardly capture the diversity, the nuance or the sloppiness in how digits are written in the wild. The letters that passed through the Buffalo post office contained nearly 10,000 examples of true, human handwriting, giving the model what it needed to truly learn. Once computer scientists saw the importance of real data, they were spurred to collect even more. A dataset of six times as many digits – named MNIST – was collected shortly after the Buffalo set. Surprisingly, this dataset remains one of the most commonly used for quickly testing out new models and algorithms for artificial vision. The digits for MNIST were written by Maryland high school students and US census takers.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> And while the writers <span class="italic">were</span> told what their digits were being used for in this case, they almost certainly wouldn’t have expected their handwriting to still be used by computer scientists some 30 years later. </p>
<p class="TXI">Tests of convolutional neural networks didn’t stop at digits, but when making the jump to more involved images they hit a snag. In the early 2000s, networks much like LeCun’s were trained on another dataset of 60,000 images, <a id="page_177"></a>this time made of objects. The images were small and grainy – only 32x32 pixels – and could be of either airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships or trucks. While still a simple task for us, this marked a serious increase in difficulty for the networks. The full ambiguity inherent in resolving a three-dimensional world from two-dimensional input comes into play when real images of real objects are used. The same models that could learn to recognise digits struggled to make sense of these more realistic pictures. This brain-like approach to artificial vision was failing at the basic visual processing brains do every day. </p>
<p class="TXI">A tide turned, however, in 2012 when Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton from the University of Toronto used a convolutional neural network to win a major image-recognition contest known as the ImageNet Large Scale Visual Recognition Challenge. The contest consisted of labelling images – large (224x224 pixels), real-life pictures taken by people around the world and pulled from image-hosting websites like Flickr – as belonging to one of a thousand different possible object categories. On this very convincing test of visual skill, the convolutional neural network got 62 per cent correct, beating the second-place algorithm by 10 percentage points.</p>
<p class="TXI">How did the Toronto team do so well? Did they discover some new computation needed for vision? Did they find a magical technique to help the model learn its connections better? The truth, in this case, is actually much more banal. The difference between this convolutional neural network and the ones that came before it was mainly one of size. The Toronto team’s network had a total of over 650,000 artificial neurons in it – about 80 times the size of LeCun’s digit-recognising <a id="page_178"></a>one. This network was so large, in fact, that it required some clever engineering to fit the model in the memory of the computer chips that were used to run it. The model went big in another way, too. All those neurons meant a lot more data was needed to train the connections between them. The model learned from 1.2 million labelled images collected by computer science professor Fei-Fei Li as part of the ImageNet database. </p>
<p class="TXI">A watershed year came in 2012 for convolutional neural networks. While the Toronto team’s advances were technically just a quantitative leap – upping the number of neurons and images – the stunning performance enhancement made a qualitative difference in the field. After seeing what they were capable of, researchers flocked to study convolutional neural networks and to make them even better. This usually went in the same direction: making them bigger, but important tweaks to their structure and to how they learned were found as well. </p>
<p class="TXI">By 2015, a convolutional neural network reached the level of performance expected of a human in the image-classification competition (which isn’t actually 100 per cent; some of the images can be confusing). And convolutional neural networks now form the base of almost any image-processing software: facial recognition on social media, pedestrian detection in self-driving cars and even automatic diagnosis of diseases in X-ray images. In an amusing bending of science in on itself, convolutional neural networks have even been used <span class="italic">by neuroscientists</span> to help automatically detect where neurons are in pictures of brain tissue. Artificial neural networks are now looking at real ones. </p>
<p class="TXI">It would seem that engineers made a smart move turning to the brain for inspiration on how to build a <a id="page_179"></a>visual system. Fukushima’s attention to the functions of neurons – and his condensing of those functions into simple operations – has paid dividends. But when he was taking the first steps in the development of these models, the computing resources and the data to make them shine simply wasn’t available. Decades later, the next generation of engineers picked up the project and brought it across the finish line. As a result, current convolutional neural networks can finally do many of the tasks originally asked for by the MIT summer project in 1966. </p>
<p class="TXI">But just as Selfridge’s Pandemonium helped inspire visual neuroscientists, the relationship between convolutional neural networks and the brain does not go only one way. Neuroscientists have come to reap rewards from the effort computer scientists put into making models that can solve real visual problems. That’s because not only are these large, heavily trained convolutional neural networks good at spotting objects in images, they’re also good at predicting how the brain will respond to those same images. </p>
<p class="center">* * *</p>
<p class="TXT">Visual processing gets started in the primary visual cortex – where Hubel and Wiesel did their recordings – but many areas are involved after that. The primary visual cortex sends connections to (you guessed it) the secondary visual cortex. And after a few more relays, the information ends up in the temporal cortex, located just behind the temples. </p>
<p class="TXI">The temporal cortex has been associated with object recognition for a long time. As early as the 1930s, researchers noticed that damage to this brain area leads to some strange behaviour. Patients with temporal cortex damage are bad <a id="page_180"></a>at deciding what things are important to look at and so get easily distracted. They also don’t show normal emotional responses to images; they can see pictures most people would find terrifying and hardly blink. And when they want to explore objects, they may do so not by looking at them but by putting them in their mouths. </p>
<p class="TXI">Understanding of this brain area was refined through decades of careful observation of patients or animals with brain lesions and eventually by recording the activity of its neurons. This led to the conclusion that a subpart of the temporal cortex – the ‘inferior’ part at the bottom, also called ‘IT’ – is the main location for object understanding. People with damage to IT have mostly normal behaviour and vision, but with the more specific problem of being unable to appropriately name or recognise objects; they may, for example, fail to recognise the faces of friends or confuse the identity of objects that appear similar.</p>
<p class="TXI">Accordingly, the neurons in this area respond to objects. Some neurons have clear preferences; one may fire when a clock is present, another for a house, another for a banana, <span class="italic">etc.</span> But other cells are less scrutable. They may prefer portions of objects or respond similarly to two different objects that have some features in common. Some cells also care about the angle the object is seen from, perhaps firing most if the object is seen straight on, but others are more forgiving and respond to an object at almost any angle. Some care about the size and location of the object, others don’t. In total, IT is a grab-bag of neurons interested in objects. While they’re not always easy to interpret, such object-driven responses make IT seem like the top of the visual-processing hierarchy, the last stop on the visual system express.</p> <a id="page_181"></a>
<p class="TXI">Neuroscientists have tried for decades to understand exactly how IT comes to have these responses. Frequently they followed in the footsteps of Fukushima and built models with stacks of simple and complex cells, hoping that these computations would mimic those that lead to IT activity and make the activity perfectly predictable. This approach worked to an extent, but just like with the Neocognitron, the models were small and they learned their connections from a small set of small images. To make real progress, neuroscientists needed to scale up their models just the same way computer scientists did.</p>
<p class="TXI">In 2014, two separate groups of scientists – one led by Nikolaus Kriegeskorte</p>
<p class="TXI">at Cambridge University and one by James DiCarlo at MIT – did exactly that. They showed real and diverse images of objects to subjects (humans and monkeys) and recorded the activity of different areas of their visual systems as they viewed them. They also showed the same images to a large convolutional neural network trained to classify real images. What both groups found was that these computer models provided a great approximation to biological vision. Particularly, they showed that if you want to guess how a neuron in IT will respond to a specific image, the best bet – better than any previous method neuroscientists had tried – was to look at how the artificial neurons in the network responded to it. Specifically, the neurons in the last layer of the network best predicted the activity of IT neurons. What’s more, the neurons in the second-to-last layer best predicted the activity of neurons in V4 – the area that gives input to IT. The convolutional neural network, it seemed, was mimicking the brain’s visual hierarchy.</p> <a id="page_182"></a>
<p class="TXI">By showing such a striking alignment between the model and the brain, this research ushered in a revolution in the study of biological vision. It demonstrated that neuroscientists were broadly on the right track, a track that started with Lettvin and Hubel and Wiesel, but that they needed to be bigger and bolder. If they wanted a model that could explain how animals see objects, the model itself needed to be able to see objects. </p>
<p class="TXI">Going this way, though, symbolised an abandonment of principles that some theorists hold dear: a striving for elegance, simplicity and efficiency in models. There’s nothing elegant or efficient about 650,000 artificial neurons wired up in whatever way they found to work. Compared to some of the most beloved and beautiful equations in science, these networks are hulking, unsightly beasts. But, in the end, they work – and there is no guarantee that anything more elegant will. </p>
<p class="TXI">Selfridge’s work pushed biologists to see the visual system as a hierarchy and the experiments that resulted from this planted the seeds for the design of convolutional neural networks. These seeds were incubated in computer science and, in the end, the collaboration yielded fruit for both sides. In general, the desire for artificial systems that can do real visual tasks in the real world has pushed the study of biological vision in directions it may not have gone on its own. Engineers and computer scientists have always enjoyed having the brain’s visual system to look to – not only for inspiration, but for proof that this challenging problem is solvable. This mutual appreciation and influence makes the story of the study of vision a uniquely interwoven one.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-1" id="fn-1">1</a> ﻿That professor would be Marvin Minsky and the professor that wrote the memo was Seymour Papert, both key participants in ﻿﻿Chapter 3﻿﻿. Indeed, as you﻿’﻿ll see, there are many overlapping players and themes in the histories of artificial neural networks and artificial vision.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-2" id="fn-2">2</a> ﻿Primates are admittedly fairly unusual in this sense. Rodent brains, for example, lean more towards processing smell.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-3" id="fn-3">3</a> ﻿Actually, pixels in colour images are defined by three numbers corresponding to the intensities of the red, green and blue components. For simplicity, we﻿’﻿ll speak of pixels as being a single number, despite the fact that this is only true for grayscale images.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-4" id="fn-4">4</a> ﻿Though in response to a colleague﻿’﻿s remark on it, Selfridge commented: ﻿‘﻿All of us have sinned in Adam, we have eaten of the tree of the knowledge of good and evil, and the demonological allegory is a very old one, indeed.﻿’﻿﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-5" id="fn-5">5</a> ﻿From the Greek for ﻿‘﻿all the demons﻿’﻿, introduced in John Milton﻿’﻿s ﻿<span class="italic">Paradise Lost</span>﻿.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-6" id="fn-6">6</a> ﻿Hubel was actually quite interested in mathematics and physics, and was accepted into a PhD programme in physics at the same time he was accepted to medical school. Truly torn, he waited until the last possible day to make the choice.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-7" id="fn-7">7</a> ﻿Hubel and Wiesel did not, however, mention Lettvin or his pioneering work in the frog during this speech. This was an omission Selfridge referred to as ﻿‘﻿rotten manners, putting it very mildly﻿’﻿.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-8" id="fn-8">8</a> ﻿You can guess who had the neater handwriting.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 3</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter3"><a href="contents.xhtml#re_chapter3">CHAPTER THREE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter3">Learning to Compute</a><a id="page_49"></a></p>
<p class="H1" id="b-9781472966445-ch222-sec3">
<span class="bold">
<span>McCulloch-Pitts, the Perceptron and 
artificial neural networks</span>
</span></p>
<p class="TXT">Cambridge University mathematician Bertrand Russell spent 10 years at the beginning of the twentieth century toiling towards a monumental goal: to identify the philosophical roots from which all of mathematics stems. Undertaken in collaboration with his former teacher Alfred Whitehead, this ambitious project produced a book, the <span class="italic">Principia Mathematica</span>, which was delivered to the publishers overdue and over budget. The authors themselves had to chip in towards the publishing costs just to get it done and they didn’t see any royalties for 40 years. </p>
<p class="TXI">But the financial hurdle was perhaps the smallest one to overcome in getting this opus finished. Russell had to fight against his own agitation with the scholarly material. According to his autobiography, he spent days staring at a blank sheet of paper and evenings contemplating jumping in front of a train. Work on the book also coincided with the dissolution of Russell’s marriage and a strain on his relationship with Whitehead – who, according to Russell, was fighting his own mental and marital battles at the time. The book was even physically demanding: Russell spent 12 hours a day at a desk writing out the intricate symbolism needed <a id="page_50"></a>to convey his complex mathematical ideas and when the time came to bring the manuscript to the publisher it was too large for him to carry. Despite it all, Russell and Whitehead eventually completed and published the text they hoped would tame the seemingly wild state of mathematics. </p>
<p class="TXI">The conceit of the <span class="italic">Principia</span> was that all of mathematics could be reduced to logic. In other words, Russell and Whitehead believed that a handful of basic statements, known as ‘expressions’, could be combined in just the right way to generate all the formalisms, claims and findings of mathematicians. These expressions didn’t stem from any observations of the real world. Rather, they were meant to be universal. For example, the expression: if X is true, then the statement ‘X is true or Y is true’ is also true. Such expressions are made up of propositions: fundamental units of logic that can be either true or false, written as letters like X or Y. These propositions are strung together with ‘Boolean’ operators<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> such as ‘and’, ‘or’ and ‘not’. </p>
<p class="TXI">In the first volume of the <span class="italic">Principia</span>, Russell and Whitehead provided fewer than two dozen of these abstract expressions. From these humble seeds, they built mathematics. They were even able to triumphantly conclude – after scores of symbol-filled pages – that 1+1=2. </p>
<p class="TXI">Russell and Whitehead’s demonstration that the full grandeur of mathematics could be captured with the simple <a id="page_51"></a>rules of logic<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> had immense philosophical implications as it provided proof of the power of logic. What’s more, it meant that a subsequent finding, made by a different pair of men some 30 years later, would have immense implications of its own. This finding said that neurons, simply via the nature of their anatomy and physiology, were implementing the rules of logic. It revolutionised the study of the brain and of intelligence itself. </p>
<p class="center">* * *</p>
<p class="TXT">When Detroit native Walter Pitts was just 12 years old, he was invited by Russell to join him as a graduate student at Cambridge University. The young boy had, the story goes, encountered a copy of the <span class="italic">Principia</span> after running into a library to avoid bullies. As he read, Pitts found what he believed to be errors in the work. So, he sent his notes on the subject off to Russell, who, presumably not knowing the boy’s age, then offered him the position. Pitts didn’t accept it. But a few years later, when Russell was visiting the University of Chicago, Pitts went to sit in on his lectures. Having fled an abusive family home to come to Chicago, Pitts decided not to return. He remained in the city, homeless. </p>
<p class="TXI">Luckily, the University of Chicago had another world-famous logician for Pitts to criticise – Rudolf Carnap. Again, Pitts wrote up notes – this time identifying issues in Carnap’s recent book <span class="italic">The Logical Syntax of Language</span> – and delivered them to Carnap’s office at the University of Chicago. Pitts didn’t stick around long enough to hear his <a id="page_52"></a>reaction, but Carnap, impressed, eventually chased down Pitts, whom he referred to as ‘that newsboy who understood logic’. On this occasion, the philosopher he critiqued actually did get Pitts to work with him. Though he never officially enrolled, Pitts functioned effectively as a graduate student for Carnap and fraternised with a group of scholars who were interested in the mathematics of biology.</p>
<p class="TXI">Warren McCulloch’s interest in philosophy took a more traditional form. Born in New Jersey, he studied the subject (along with psychology) at Yale and read many of the greats. He was most enamoured with Immanuel Kant and Gottfried Leibniz (whose ideas were very influential for Russell), and he read the <span class="italic">Principia</span> at the age of 25. But, despite the beard upon his long face, McCulloch was not a philosopher – he was a physiologist. He attended medical school in Manhattan and then went on to observe the panoply of ways in which the brain can break as a neurology intern at Bellevue Hospital and at the Rockland State Hospital psychiatric facility. In 1941, he joined the University of Illinois at Chicago as the director of the laboratory for basic research in the department of psychiatry. </p>
<p class="TXI">As with all great origin stories, there are conflicting accounts of how McCulloch and Pitts met. One claims that it happened when McCulloch spoke in front of a research group Pitts was a part of. Another story is that Carnap introduced them. Finally, a contemporary of the two men, Jerome Lettvin, claimed he introduced them and that all three bonded over a mutual love of Leibniz. In any case, by 1942, the 43-year-old McCulloch and his wife had taken the 18-year-old Pitts into their home, <a id="page_53"></a>and the two men were spending evenings drinking whisky and discussing logic. </p>
<p class="TXI">The wall between ‘mind’ and ‘body’ was strong among scientists in the early twentieth century. The mind was considered internal and intangible; the body, including the brain, was physical. Researchers on either side of this wall toiled diligently, but separately, at their own problems. Biologists, as we saw in the last chapter, were working hard to uncover the physical machinery of neurons: using pipettes and electrodes and chemicals to sort out what causes a spike and how. Psychiatrists, on the other hand, were attempting to uncover the machinery of the mind through lengthy sessions of Freudian psychoanalysis. Few on either side would attempt a glance over the wall at the other. They spoke separate languages and worked towards different goals. For most practitioners, the question of how neural building blocks could create the structure of the mind was not just unanswered, it was unasked. </p>
<p class="TXI">But McCulloch, from as early on as his time at medical school, had immersed himself in a crowd of scientists who did care about this question and allowed him the space to think about it. Eventually, through his physiological observations, he came up with a hunch. He saw in the emerging concepts of neuroscience a possible mapping to the notions of logic and computation he so adored in philosophy. To think of the brain as a computing device following the rules of logic – rather than just a bag of proteins and chemicals – would open the door to understanding thought in terms of neural activity.</p> <a id="page_54"></a>
<p class="TXI">Analytical skill, however, was not where McCulloch excelled. Some who knew him say he was too much of a romantic to be held down by such details. So, despite years of toying with these ideas in his mind and in conversation (even as a Bellevue intern he was accused of ‘trying to write an equation for the working of the brain’), McCulloch struggled with several technical issues of how to enact them. Pitts, however, was comparably unfazed by the analytical. As soon as he spoke with him about it, Pitts saw what approaches were needed to formally realise McCulloch’s intuitions. Not long after they met, one of the most influential papers on computation was written.</p>
<p class="TXI">‘A logical calculus of the ideas immanent in nervous activity’ was published in 1943. The paper is 17 pages long with many equations, only three references (one of which is to the <span class="italic">Principia</span>) and a single figure consisting of little neural circuits drawn by McCulloch’s daughter.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> </p>
<p class="TXI">The paper begins by reviewing the biology of neurons that was known at the time: neurons have cell bodies and axons; two neurons connect when the axon of the first meets the body of the second; through this connection one neuron provides input to the other; a certain amount of input is needed for a neuron to fire; a cell either fires a spike or it doesn’t – no half spikes or in-between spikes; and the input from some neurons – inhibitory neurons – has the power to prevent a cell from spiking.</p><a id="page_55"></a>
<p class="TXI">McCulloch and Pitts go on to explain how these biological details are congruent with Boolean logic. The core of their claim is that the activity state of each neuron – either firing or not – is like the truth value of a proposition – either true or false. In their own words, they ‘conceive of the response of any neuron as factually equivalent to a proposition which proposed its adequate stimulus’. </p>
<p class="TXI">By ‘its adequate stimulus’ they are referring to something about the world. Imagine a neuron in the visual cortex whose activity represents the statement ‘the current visual stimulus looks like a duck’. If that neuron is firing, that statement is true; if the neuron is not firing, it is false. Now imagine another neuron, in the auditory cortex, that represents the statement ‘the current auditory stimulus is quacking like a duck’. Again, if this neuron is firing, that statement is true, otherwise it is false. </p>
<p class="TXI">Now we can use the connections between neurons to enact Boolean operations. For example, by giving a third neuron inputs from both of these neurons, we could implement the rule ‘if it looks like a duck <span class="italic">and</span> it quacks like a duck, it’s a duck’. All we have to do is build the third neuron such that it will only fire if both of its input neurons are firing. That way, both ‘looks like a duck’ and ‘quacks like a duck’ have to be true in order for the conclusion represented by the third neuron (‘it’s a duck’) to be true. </p>
<p class="TXI">This describes the simple circuit needed to implement the Boolean operation ‘and’. McCulloch and Pitts in their paper show how to implement many others. To implement ‘or’ is very similar, however the strength of the connections from each neuron must be so strong <a id="page_56"></a>that one input alone is enough to make the output neuron fire. In this case, the ‘is a duck’ neuron would fire if the ‘looks like a duck’ neuron <span class="italic">or</span> the ‘quacks like a duck’ neuron (or both) were firing. The authors even show how to string together multiple Boolean operations. For example, to implement a statement like ‘X and not Y’, the neuron representing X connects to an output neuron with a strength enough to make it fire. But the neuron representing Y <span class="italic">inhibits</span> the output neuron, meaning it prevents it from firing. This way, the output neuron will only fire if the X-representing neuron <span class="italic">is</span> firing and the Y-representing neuron is <span class="italic">not</span> (see Figure 4). </p>
<p class="TXI">These circuits, which are meant to represent what networks of real neurons can do, became known as <span class="italic">artificial</span> neural networks. </p>
<p class="image-fig" id="fig4.jpg">
<img alt="" src="Images/chapter-01-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 4</span>
</span></p>
<p class="TXI">The ability to spot logic at play in the interactions of neurons came from McCulloch’s discerning eye. As a physiologist, he knew that neurons were more complex than his simple drawings and equations suggested. They had membranes, ion channels and forking paths of <a id="page_57"></a>dendrites. But the theory didn’t need their full complexity. So, like an impressionist painter using only the necessary strokes, he intentionally highlighted only the elements of neural activity required for the story he wanted to tell. In doing so, he demonstrated the artistry inherent to model-building; it is a subjective and creative process to decide which facts belong in the foreground. </p>
<p class="TXI">The radical story that McCulloch and Pitts told with their model – that neurons were performing a logical calculus – was the first attempt to use the principles of computation to turn the mind–body problem into a mind–body connection. Networks of neurons were now imbued with all the power of a formal logical system. Like a chain of falling dominoes, once certain truth values entered into a neural population (say, via sensory organs), a cascade of interactions could deduce the truth value of new and different statements. This meant a population of neurons could carry out endless computations: interpreting sensory inputs, developing conclusions, forming plans, reasoning through arguments, performing calculations and so on. </p>
<p class="TXI">With this step in their research, McCulloch and Pitts advanced the study of human thought and, at the same time, kicked it off its throne. The ‘mind’ lost its status as mysterious and ethereal once it was brought down to solid ground – that is, once its grand abilities were reduced to the firing of neurons. To adapt a quote from Lettvin, the brain could now be thought of as ‘a machine, meaty and miraculous, but still a machine’. More boldly still, McCulloch’s student Michael Arbib later remarked that the work ‘killed dualism’.</p><a id="page_58"></a>
<p class="TXI">Russell was known to lament that, despite the 20 years put into it and the impact it had on logicians and philosophers, the <span class="italic">Principia</span> had little effect on practising mathematicians. Its new take on the foundations of mathematics simply didn’t seem to mean much to those doing mathematics; it didn’t change their day-to-day work. The same could be said of McCulloch and Pitts’ discovery for neuroscientists of the time. Biologists, physiologists, anatomists – the scientists doing the labour of physically mining neurons for the details of their workings – didn’t take much from the theory. This was in part because it wasn’t obvious what experiments should follow from it. But it may also have stemmed from the very technical notation in the paper and its less-than-inviting writing style. In a review on nerve conduction written three years later, the author refers to the McCulloch-Pitts paper as ‘not for the lay reader’ and remarks that if this style of work is to be useful, it’s necessary for ‘physiologists to familiarise themselves with mathematical technology, or for mathematicians to elaborate at least their conclusions in a less formidable language’. The wall between mind and body may have come down, but the one between biologist and mathematician stood strong. </p>
<p class="TXI">There was a separate group of people – a group with the requisite technical know-how – who did take an interest in the logical calculus of neurons. In the post-war era, a series of meetings hosted by the philanthropic Macy Foundation brought together biologists and technologists, many of whom wished to use biological findings to build brain-like machines. McCulloch was an organiser of these meetings, and fellow attendees <a id="page_59"></a>included the ‘father of cybernetics’ Norbert Wiener and John von Neumann, the inventor of the modern computer architecture, who was directly inspired in its design by the McCulloch-Pitts neurons. As Lettvin described it 40 years later: ‘The whole field of neurology and neurobiology ignored the structure, the message and the form of McCulloch and Pitts’ theory. Instead, those who were inspired by it were those who were destined to become the aficionados of a new venture, now called Artificial Intelligence.’</p>
<p class="center">* * *</p>
<p class="EXTF">The <span class="italic">Navy last week demonstrated the embryo of an electronic computer named the Perceptron which, when completed in about a year, is expected to be the first non-living mechanism able to 
‘perceive, recognise, and identify its surroundings without human training or control.’ [...]</span> </p>
<p class="EXT-L">
<span class="italic">“‘Dr. Frank Rosenblatt, research psychologist at the Cornell Aeronautical Laboratory, Inc., Buffalo, NY, designer of the ­Perceptron, conducted the demonstration. The machine, he said, would be the first electronic device to think as the human brain. Like humans, Perceptron will make mistakes at first, ‘but it will grow wiser as it gains experience’, he said.</span></p>
<p class="TXT">This summary, from an article entitled ‘Electronic “brain” teaches itself’, appeared in the 13 July 1958 edition of the <span class="italic">New York Times</span>, opposite a letter to the editor about the ongoing debate on whether smoking causes cancer. Frank Rosenblatt, the 30-year-old architect of the project, was reaching beyond his training in experimental psychology to build a computer meant to rival the most advanced technology at the time.</p><a id="page_60"></a>
<p class="TXI">The computer in question was taller than the engineers who operated it and about twice as long. It was covered on either end in various control panels and readout mechanisms. Rosenblatt requested the services of three ‘professional people’ and an associated technical staff for 18 months to build it, and the estimated cost was $100,000 (around $870,000 today). The word ‘perceptron’, defined by Rosenblatt, is a generic term for a certain class of devices that can ‘recognise similarities or identities between patterns of optical, electrical or tonal information’. The Perceptron – the computer that was built in 1958 – was thus technically a subclass known as a ‘photoperceptron’ because it took as its input the output of a camera mounted on a tripod at one end of the machine. </p>
<p class="TXI">The Perceptron was, just like the models introduced in the McCulloch-Pitts paper, an artificial neural network. It was a simplified replica of what real neurons do and how they connect to each other. But rather than remaining a mathematical construct that exists only as the ink of equations on a page, the Perceptron was physically realised. The camera provided 400 inputs to this network in the form of a 20x20 grid of light sensors. Wires then randomly connected the output of these sensors to 1,000 ‘association units’ – small electrical circuits that summed up their inputs and switched to ‘on’ or ‘off’ as a result, just like a neuron. The output of these association units became the input to the ‘response units’, which themselves could be ‘on’ or ‘off’. The number of response units was equal to the number of mutually exclusive categories to which an image could belong. So, if the Navy wanted to use the Perceptron, say, to <a id="page_61"></a>determine if a jet was present in an image or not, there would be two response units: one for jet and one for no jet. At the end of the machine opposite the camera was a set of light bulbs that let the engineer know which of the response units was active – that is, which category the input belonged to. </p>
<p class="TXI">Implementing an artificial neural network this way was large and cumbersome, full of switches, plugboards and gas tubes. The same network made up of real neurons would be smaller than a grain of sea salt. But achieving this physical implementation was important. It meant that theories of how neurons compute could actually be tested in the real world on real data. Whereas the McCulloch-Pitts work was about proving a point in theory, the Perceptron put it into practice. </p>
<p class="TXI">Another important difference between the Perceptron and the McCulloch-Pitts network was that, as Rosenblatt told the <span class="italic">New York Times</span>, the Perceptron learns. In the McCulloch and Pitts paper, the authors make no reference to how the connectivity between the neurons comes to be. It is simply defined according to what logical function the network needs to carry out and it stays that way. For the Perceptron to learn, however, it must modify its connections.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> In fact, the Perceptron derives all its functionality from changing its connection strengths until they are just right. </p>
<p class="TXI">The type of learning the Perceptron engages in is known as ‘supervised’ learning. By providing pairs of inputs and outputs – say, a series of pictures and whether <a id="page_62"></a>they each contain a jet or not – the Perceptron learns to make this decision on its own. It does so by changing the strength of the connections – also known as the ‘weights’ – between the association units and the readouts. </p>
<p class="TXI">Specifically, when an image is provided to the network, it activates units first in the input layer, then in the association layer, and finally in the readout layer, indicating the network’s decision. If the network gets the classification wrong, the weights change according to these rules:</p>
<p class="NLF">1. If a readout unit is ‘off’ when it should be ‘on’, the connections from the ‘on’ association units to that readout unit are <span class="italic">strengthened</span>.</p>
<p class="NLL">2. If a readout unit is ‘on’ when it should be ‘off’, the connections from the ‘on’ association units to that readout unit are <span class="italic">weakened</span>.</p>
<p class="TXT">By following these rules, the network will start to correctly associate images with the category they belong to. If the network can learn to do this well, it will stop making errors and the weights will stop changing. </p>
<p class="TXI">This procedure for learning was, in many ways, the most remarkable part of the Perceptron. It was the conceptual key that could open all doors. Rather than needing to tell a computer exactly how to solve a problem, you need only show it some examples of that problem solved. This had the potential to revolutionise computing and Rosenblatt was not shy in saying so. He told the <span class="italic">New York Times</span> that Perceptrons would ‘be able to recognise people and call out their names’ and ‘to hear speech in one language and instantly translate it to <a id="page_63"></a>speech or writing in another language’. He also added that ‘it would be possible to build Perceptrons that could reproduce themselves on an assembly line and which would be “conscious” of their existence’. This was a bold statement, to say the least, and not everyone was happy with Rosenblatt’s public bravado. But the spirit of the claim – that a computer that could learn would expedite the solving of almost any problem – rang true.</p>
<p class="TXI">The power of learning, however, came with a price. Letting the system decide its own connectivity effectively divorced these connections from the concept of Boolean operators. The network <span class="italic">could</span> learn the connectivity that McCulloch and Pitts had identified as required for ‘and’, ‘or’, <span class="italic">etc.</span> But there was no requirement that it does, nor any need to understand the system in this light. Furthermore, while the association units in the Perceptron machine were designed to be only ‘on’ or ‘off’, the learning rule doesn’t actually require that they be this way. In fact, the activity level of these artificial neurons could be any positive number and the rule would still work.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> This makes the system more flexible, but without a binary ‘on’-‘off’ response it makes it harder to map the activity of these units to the binary truth values of propositions. Compared with the crisp and clear logic of the McCulloch-Pitts networks, the Perceptron was an uninterpretable mess. But it worked. Interpretability was sacrificed for ability.</p> <a id="page_64"></a>
<p class="TXI">The Perceptron machine and its associated learning procedure became a popular object of study in the burgeoning field of artificial intelligence. When it made the transition from a specific physical object (the Perceptron) to an abstract mathematical concept (the perceptron algorithm) the separate input and association layers were done away with. Instead, input units representing incoming data connected directly to the readout units and, through learning, these connections changed to make the network better at its task. How and what the perceptron in this simplified form could learn was studied from every angle. Researchers explored its workings mathematically using pen and paper, or physically by building their own perceptron machines, or – when digital computers finally became available – electronically by simulating it.</p>
<p class="TXI">The perceptron generated hope that humans could build machines that learn like we do; in this way it put the prospect of artificial intelligence within 
reach. Simultaneously, it provided a new way of understanding our own intelligence. It showed that artificial neural networks could compute without abiding by the strict rules of logic. If the perceptron could perceive without the use of propositions or operators, it follows that each neuron and connection in the brain needn’t have a clear role in terms of Boolean logic either. Instead, the brain could be working in a sloppier way, wherein, like the perceptron, the function of a network is distributed across its neurons and emerges out of the connections between them. This new approach to the study of the brain became known as ‘connectionism’.</p> <a id="page_65"></a>
<p class="TXI">The work of McCulloch and Pitts was an important stepping stone. As the first demonstration of how networks of neurons could think, it was responsible for getting neuroscience away from the shores of pure biology and into the sea of computation. This fact, rather than the veracity of its claims, is what earns it its place in history. The intellectual ancestor of McCulloch and Pitts’ work, the <span class="italic">Principia Mathematica</span>, could be said to have suffered a similar fate. In 1931, German mathematician Kurt Gödel published ‘On formally undecidable propositions of <span class="italic">Principia Mathematica</span> and related systems’. This paper took the <span class="italic">Principia Mathematica</span> as a starting point to show why its very goal – to explain all of mathematics from simple premises – was impossible to achieve. Russell and Whitehead had not, in fact, done what they believed they did.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> Gödel’s findings became known as the ‘incompleteness theorem’ and had a revolutionary effect on mathematics and philosophy. An effect that stemmed, in part, from Russell and Whitehead’s failed attempt.</p>
<p class="TXI">Russell and McCulloch were able to take the failings of their respective works in their stride. Pitts, on the other hand, was made of finer cloth. The realisation that the brain was not enacting the beautiful rules of logic tore him apart.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> This, along with pre-existing mental struggles and the end of a relationship with an important mentor, drove him to drink and experiment with other <a id="page_66"></a>drugs. He became erratic and delirious; he burned his work and withdrew from his friends. He died from the impacts of liver disease in 1969 – the same year McCulloch died. McCulloch was 70; Pitts was 46. </p>
<p class="TXI">* * *</p>
<p class="image-fig" id="fig5.jpg">
<img alt="" src="Images/chapter-01-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 5</span>
</span></p>
<p class="TXI">The cerebellum is a forest. Folded up neatly near where the spinal cord enters the skull, this bit of the brain is thick with different types of neurons, like different species of trees, all living in chaotic harmony (see Figure 5). The Purkinje cells are large, easily identified and heavily branched: from the body of these cells, dendrites stretch up and away, like a thousand alien hands raised in prayer. The granule cells are numerous and small – with cell bodies less than half the size of the Purkinje’s – but their reach is far. Their axons initially grow upwards, in parallel with the Purkinje cells’ dendrites. They then make a sharp right turn to run directly through the branches of the Purkinje cells, like power lines through treetops. This is where the granule cells make contact with the Purkinje cells: each Purkinje cell gets input from hundreds of <a id="page_67"></a>thousands of granule cells. Climbing fibres are axons that follow a longer path on their way to the Purkinje cells. These axons come from cells in a different brain region – the inferior olive – from which they navigate all the way to the bottom of the Purkinje cell bodies and creep up around them. Winding their way around the base of the Purkinje cell dendrites like ivy, the climbing fibres form connections. Unlike the granule cells, only a single climbing fibre targets each Purkinje cell. In the cerebellar landscape, Purkinje cells are thus central. They have scores of granule cells imposing on them from the top and a small yet precise set of climbing fibres closing in on them from the bottom. </p>
<p class="TXI">In its twisty, organic way, the circuitry of the cerebellum possesses an organisation and precision unbefitting of biology. It was in this biological wiring that James Albus, a PhD student in electrical engineering working at NASA, saw the principles of the perceptron at play. </p>
<p class="TXI">The cerebellum plays a crucial role in motor control; it helps with balance, coordination, reflexes and more. One of the most widely studied of its abilities is eye-blink conditioning. This is a trained reflex that can be found in everyday life. For example, if a determined parent or roommate tries to get you out of bed in the morning by pulling open the curtains, you’ll instinctively close your eyes in response to the sunlight. After a few days of this, simply the sound of the curtains being opened may be enough to make you blink in anticipation. </p>
<p class="TXI">In the lab, this process is studied in rabbits and the intruding sunlight is replaced by a small puff of air on the eye (just annoying enough to ensure they’ll want to avoid it). After several trials of playing a sound (such as a <a id="page_68"></a>short blip of a pure tone) and following it by this little air puff, the rabbit eventually learns to close its eyes immediately upon hearing the tone. Play the animal a new sound (for example, a clapping noise) that hasn’t been paired with air puffs and it won’t blink. This makes eye-blink conditioning a simple classification task: the rabbit has to decide if a sound it hears is indicative of an upcoming air puff (in which case the eyes should close) or if it is a neutral noise (in which case they can remain open). Disrupt the cerebellum and rabbits can’t learn this task. </p>
<p class="TXI">The Purkinje cells have the power to close the eyes. Specifically, a dip in their normally high firing rate will, via connections the Purkinje cells send out of this area, cause the eyes to shut. Based on this anatomy, Albus saw their place as the readout – that is, they indicate the outcome of the classification. </p>
<p class="TXI">The perceptron learns via supervision: it needs inputs and labels for those inputs to know when it has erred. Albus saw these two functions in the two different types of connections on to Purkinje cells. The granule cells pass along sensory signals; specifically, different granule cells fire depending on which sound is being played. The climbing fibres tell the cerebellum about the air puff; they fire when this annoyance is felt. Importantly, this means the climbing fibres signal an error. They indicate that the animal made a mistake in not closing its eyes when it should have. </p>
<p class="TXI">To prevent this error, the connections from the granule cells to the Purkinje cells need to change. In particular, Albus anticipated that any granule cells that were active before the climbing fibre was active (<span class="italic">i.e.</span>, before an error), <a id="page_69"></a>should weaken their connection to the Purkinje cell. That way, the next time those granule cells fire – <span class="italic">i.e.</span>, the next time the same sound is played – they <span class="italic">won’t</span> cause firing in the Purkinje cells. And that dip in Purkinje cell firing <span class="italic">will</span> cause the eyes to close. Through this changing of connection strengths, the animal learns from its past mistakes and avoids future air puffs to the eye.</p>
<p class="TXI">In this way, the Purkinje cell acts like a president advised by a cabinet of granule cell advisors. At first the Purkinje cell listens to all of them. But if it’s clear that some are providing bad advice – that is, their input is followed by negative news delivered by the climbing fibre – their influence over the Purkinje cell will fade. And the Purkinje cell will act better in the future. It is a process that directly mirrors the perceptron learning rule.</p>
<p class="TXI">When Albus proposed this mapping between the perceptron and the cerebellum in 1971,<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> his prediction about how the connections between granule cells and Purkinje cells should change was just that – a prediction. No one had directly observed this kind of learning in the cerebellum. But by the mid-1980s, evidence had piled up in Albus’ favour. It became clear that the strength of the connection between a granule cell and a Purkinje cell does decrease after an error. The particular molecular mechanisms of this process have even been revealed. We now know that granule cell inputs cause a receptor in <a id="page_70"></a>the membrane of the Purkinje cell to respond, effectively tagging which granule cell inputs were active at a given time. If a climbing fibre input comes later (during an air puff), it causes calcium to flood into the Purkinje cell. The presence of this calcium signals to all the tagged connections to decrease their strength. Patients with fragile X syndrome – a genetic disorder that leads to intellectual disabilities – appear to be missing a protein that regulates this connection from the granule cells on to the Purkinje cell. As a result, they have trouble learning tasks like eye-blink conditioning. </p>
<p class="TXI">The perceptron, with its explicit rules of how learning should proceed in a neural network, offered clear testable ideas for neuroscientists to hunt for – and find – in the brain. In doing so, it was able to connect science across scales. The smallest physical detail – calcium ions moving through the inside of a neuron, for example – inherits a much larger meaning in light of its role in computation. </p>
<p class="center">* * *</p>
<p class="TXT">The reign of the perceptron was cut short in 1969. And with a twist of Shakespearean irony, it was its namesake that killed it. </p>
<p class="TXI">
<span class="italic">Perceptrons</span> was written by Marvin Minsky and Seymour Papert, both mathematicians at the Massachusetts Institute of Technology. The book was subtitled <span class="italic">An Introduction to Computational Geometry</span> and had a simple abstract design on the cover. Minsky and Papert were drawn to write about the topic of perceptrons out of appreciation for Rosenblatt’s invention and a desire to explore it further. <a id="page_71"></a>In fact, Minsky and Papert met at a conference where they were presenting similar results from their explorations into how the perceptron learns. </p>
<p class="TXI">Papert was a native of South Africa with full cheeks, a healthy beard and not one, but <span class="italic">two</span>, PhDs in mathematics. He had a lifelong interest in education and how it could be transformed by computing. Minsky was less than a year older than Papert, with sharper features and large glasses. A New York native, he attended the Bronx High School of Science with Frank Rosenblatt; he was also mentored by McCulloch and Pitts. </p>
<p class="TXI">Minsky and Papert shared with McCulloch and Pitts the compulsive desire to formalise thinking. True advances in understanding computation, they believed, came from mathematical derivations. All the empirical success of the perceptron – whatever computing it was able to carry out or categories it was able to learn – meant next to nothing without a mathematical understanding of why and how it worked. </p>
<p class="TXI">At this time, the perceptron was attracting a lot of attention – and money – from the artificial intelligence community. But it wasn’t attracting the kind of mathematical scrutiny Minsky and Papert yearned for. The two were thus explicitly motivated to write their book by a desire to increase the rigour around the study of perceptrons – but also, as Papert later acknowledged, by some desire to decrease the reverence for them.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup></p>
<p class="TXI">The pages of <span class="italic">Perceptrons</span> are comprised mainly of proofs, theorems and derivations. Each contributes to a <a id="page_72"></a>story about the perceptron: defining what it is, what it can do and how it learns. Yet from the publication of these some 200 pages – a thorough exploration of the ins and outs of the perceptron’s workings – the message the community received was largely about its limitations. This is because Minsky and Papert had shown, conclusively, that certain simple computations were impossible for the perceptron to do. </p>
<p class="TXI">Consider a perceptron that has just two inputs, and each input can be ‘on’ or ‘off’. We want the perceptron to report if the two inputs are the same: to respond yes (<span class="italic">i.e.</span>, have its readout unit be on) if both inputs are on <span class="italic">or</span> if both inputs are off. But if one is on and the other is off, the readout unit should be off. Like sorting socks out of the laundry, the perceptron should only respond when it sees a matching pair.</p>
<p class="TXI">To make sure the readout doesn’t fire when only one input is on, the weights from each input need to be sufficiently low. They could, for example, each be half the amount needed to make the readout turn on. This way, when both are on, the readout <span class="italic">will</span> fire and it won’t fire when only one input is on. In this setup the readout is responding correctly for three of the four possible input conditions. But in the condition where both inputs are off, the readout will be off – an incorrect classification. </p>
<p class="TXI">As it turns out, no matter how much we fiddle with connection strengths, there is no way to satisfy all the needs of the classification at once. The perceptron simply cannot do it. And the problem with that is that no good model of the brain – or promising artificial intelligence – should fail at a task as simple as deciding if two things are the same or not.</p> <a id="page_73"></a>
<p class="TXI">Albus, whose paper on the cerebellum was published in 1971, knew of the limitations of the perceptron and knew that, despite these limitations, it was still powerful enough to be a model of the eye-blink conditioning task. But a model of the whole human brain, as Rosenblatt promised? Not possible.</p>
<p class="TXI">The portrait that Minsky and Papert painted forced researchers to see the perceptron’s powers clearly. Prior to the book, researchers were able to explore what the perceptron could do blindly, with the hope that the limits of its abilities were still far off, if they existed at all. Once the contours were put in stark relief, however, there was no denying that these boundaries existed, and that they existed much closer than expected. In truth, all this amounted to was an understanding of the perceptron – exactly what Minsky and Papert set out to do. But the end of ignorance around the perceptron meant the end of excitement around it as well. As Papert put it: ‘Being understood can be a fate as bad as death.’</p>
<p class="TXI">The period that followed the publication of <span class="italic">Perceptrons</span> is known as the ‘dark ages’ of connectionism. It was marked by significant decreases in funding to the research programmes that had grown out of Rosenblatt’s initial work. The neural network approach to building artificial intelligence was snuffed out. All the excessive promises, hopes and hype had to be retracted. Rosenblatt himself died tragically in a boating accident two years after the book was published and the field he helped build remained dormant for more than 10 years.</p>
<p class="TXI">But if the hype around the perceptron was excessive and ill informed, so too was the backlash against it. The limitations in Minsky and Papert’s book were true: the <a id="page_74"></a>perceptron in the form they were studying it was incapable of many things. But it didn’t need to keep that form. The same-or-not problem, for example, could be easily solved by adding an additional layer of neurons between the input and the readout. This layer could be composed of two neurons, one with weights that make it fire when both inputs are on and the other with weights that make it fire when both inputs are off. Now the readout neuron, which gets its input from these middle neurons, just needs to be active if one of the middle neurons is active. </p>
<p class="TXI">‘Multi-layer perceptrons’, as these new neural architectures were called, had the potential to bring connectionism back from the dead.<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> But before a full resurrection was possible, one problem had to be solved: learning. The original perceptron algorithm provided the recipe for setting the connections between the input neurons and the readout neurons – that is, the learning rule was designed for a two-layered network. If the new breed of neural networks was going to have three, four, five or more layers, how should the connections between all those layers be set? (see Figure 6) Despite all the good features of the perceptron learning rule – its simplicity, the proof that it could work, the fact that it had been spotted in the wild of the cerebellum – it was unable to answer this question. Knowing that a multi-layer perceptron <span class="italic">could</span> solve more complex problems was not enough to deliver <a id="page_75"></a>on the grand promises of connectionism. What was needed was for it to <span class="italic">learn</span> to solve those problems.</p>
<p class="center">* * *</p>
<p class="TXT">The Easter Sunday of the connectionist revival story came in 1986. The paper ‘Learning representations by back-propagating errors’, written by two cognitive scientists from the University of California San Diego, David Rumelhart and Ronald Williams, and a computer scientist from Carnegie Mellon, Geoffrey Hinton, was published on 9 October in the journal <span class="italic">Nature</span>. It presented a solution to the exact problem the field had: how to train multi-layer artificial neural networks. The learning algorithm in the paper, called ‘backpropagation’, became widely used by the community at the time. And it remains to this day the dominant way in which artificial neural networks are trained to do interesting tasks.</p>
<p class="image-fig" id="fig6.jpg">
<img alt="" src="Images/chapter-01-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 6</span>
</span></p>
<p class="TXI">The original perceptron’s learning rule works because, with only two layers, it’s easy to see how to fix what’s gone wrong. If a readout neuron is off when it should be on, connections going from the input layer to that neuron <a id="page_76"></a>need to get stronger and vice versa. The relationship between these connections and the readout is thus clear. The backpropagation algorithm has a more difficult problem to solve. In a network with many layers between the input and readout, the relationships between all these connections and the readout aren’t as clear. Now instead of a president and his or her advisors, we have a president, their advisors, and the employees of those advisors. The amount of trust an advisor has in any given employee – <span class="italic">i.e.</span>, the strength of the connection from that employee to the advisor – will certainly have an impact, ultimately, on what the president does. But this impact is harder to directly see and harder to fix if the president feels something is going wrong. </p>
<p class="TXI">What was needed was an explicit way to calculate how any connection in the network would impact the readout layer. As it turns out, mathematics offers a neat way to do this. Consider an artificial neural network with three layers: input, middle and readout. How do the connections from the input to the middle layer impact the readout? We know the activity of the middle layer is a result of the activities of the input neurons and the weights of their connections to the middle layer. With this knowledge, writing an equation for how these weights affect the activity at the middle layer is straightforward. We also know that the readout neurons follow the same rule: their activity is determined by the activities of the middle neurons and the weights connecting the middle neurons to the readout. Therefore, an equation describing how these weights impact the readout is also easy to get. The only thing left to do is find a way to string these two equations together. That <a id="page_77"></a>way we’ll have an equation that tells us directly how the connections from the input to the middle layer impact the readout. </p>
<p class="TXI">When forming a train in the game of dominoes, the number on the end of one tile needs to match the number on the start of another in order for them to connect. The same is true for stitching together these equations. Here, the common term that connects the two equations is the activity of the middle layer: this activity both determines the activity of the readout and is determined by the input-to-middle connections. After joining these equations via the middle layer activity, the impact of the input-to-middle layer connections on the readout can be calculated directly. And this makes it easy to figure out how those connections should change when the readout is wrong. In calculus, this linking together of relationships is known as the ‘chain rule’ and it is the core of the backpropagation algorithm. </p>
<p class="TXI">The chain rule was discovered over 200 years ago by none other than the idol of McCulloch and Pitts, philosopher and polymath Gottfried Leibniz. Given how useful the rule is, its application to the training of multi-layer neural networks was no surprise. In fact, the backpropagation algorithm appears to have been invented at least three separate times before 1986. But the 1986 paper was part of a perfect storm that ensured its findings would spread far and wide. The first reason for this was the content of the paper itself. Not only did it show that neural networks could be trained this way, it also analysed the workings of networks trained on several cognitive tasks, such as understanding relations on a family tree. Another component of the success was the <a id="page_78"></a>increase in computational power that came in the 1980s; this was important for making the training of multi-layer neural networks practically possible for researchers. Finally, the same year the paper was published, one of its authors, Rumelhart, also published a book on connectionism that included the backpropagation algorithm. That book – written with a different Carnegie Mellon professor, James McClelland – went on to sell an estimated 40,000 copies by the mid-1990s. Its title, <span class="italic">Parallel Distributed Processing</span>, lent its name to the entire research agenda of building artificial neural networks in the late 1980s and early 1990s. </p>
<p class="TXI">For somewhat similar reasons, the story of artificial neural networks took an even more dramatic turn roughly a decade into the new millennium. The heaps of data accumulated in the internet age united with the computational power of the twenty-first century to supercharge progress in this field. Networks with more and more layers could suddenly be trained on more and more complex tasks. Such scaled-up models – referred to as ‘deep neural networks’ – are currently transforming artificial intelligence and neuroscience alike. </p>
<p class="TXI">The deep neural networks of today are based on the same basic understanding of neurons as those of McCulloch and Pitts. Beyond that base inspiration, though, they don’t aim to directly replicate the human brain. They aren’t trying to mimic its structure or anatomy, for example.<sup><a href="#fn-11" id="fnt-11">11</a>
</sup> But they do aim to mimic human behaviour and they’re getting quite good at it. When <a id="page_79"></a>Google’s popular language translation service started using a deep neural network approach in 2016, it reduced translation errors by 50 per cent. YouTube also uses deep neural networks to help its recommendation algorithm better understand what videos people want to see. And when Apple’s voice assistant Siri responds to a command, it is a deep neural network that is doing the listening and the speaking. </p>
<p class="TXI">In total, deep neural networks can now be trained to find objects in images, play games, understand preferences, translate between different languages, turn speech into written words and turn written words into speech. Not unlike the original Perceptron machine, the computers these networks run on fill up rooms. They’re located in server centres across the globe, where they hum away processing the world’s image, text and audio data. Rosenblatt may have been pleased to see that some of his grand promises to the <span class="italic">New York Times</span> were indeed fulfilled. They just required a scale nearly a thousand times what he had available at the time. </p>
<p class="TXI">The backpropagation algorithm was necessary to boost artificial neural networks to the point where they could reach near-human levels of performance on some tasks. As a learning rule for neural networks, it really works. Unfortunately, that doesn’t mean it works like the brain. While the perceptron learning rule was something that could be seen at play between real neurons, the backpropagation algorithm is not. It was designed as a mathematical tool to make artificial neural networks work, not a model of how the brain learns (and its inventors were very clear on that from the start). The reason for this is that real neurons can typically only <a id="page_80"></a>know about the activity of the neurons they’re connected to – not about the activity of the neurons those neurons connect to and so on and so on. For this reason, there is no obvious way for real neurons to implement the chain rule. They must be doing something different. </p>
<p class="TXI">For some researchers – particularly researchers in the field of artificial intelligence – the artificial nature of backpropagation is no problem. Their goal is to build computers that can think, by whatever means necessary. But for other scientists – neuroscientists in particular – finding the learning algorithm of the brain is paramount. We know the brain is good at getting better; we see it when we learn a musical instrument, how to drive or how to read a new language. The question is how.</p>
<p class="TXI">Because backpropagation is what we know works, some neuroscientists are starting there. They’re checking for signs that the brain is doing something <span class="italic">like</span> backpropagation, even if it can’t do it exactly. Inspiration comes from the success story of finding a perceptron at work in the cerebellum. There, clues were present in the anatomy; the different placement of the climbing fibres and granule cells pointed to a different role for each. Other brain areas display patterns of connectivity which may hint at how they are learning. For example, in the neocortex, some neurons have dendrites that stretch out way above them. Faraway regions of the brain send inputs to these dendrites. Do they carry with them information about how these neurons have impacted those that come after them in the brain’s neural network? Could this information be used to change the strength of the network’s connections? Both neuroscientists and artificial intelligence researchers hold out hope that the <a id="page_81"></a>brain’s version of backpropagation will be found and that, when it is, it can be copied to create algorithms that learn even better and faster than today’s artificial neural networks. </p>
<p class="TXI">In their hunt to understand how the mind learns from supervision, modern researchers are doing just what McCulloch did. They’re looking at the piles of facts we have about the biology of the brain and trying to see in it a computational structure. Today, they are guided in their search by the workings of artificial systems. Tomorrow, the findings from biology will again guide the building of artificial intelligence. This back-and-forth defines the symbiotic relationship between these two fields. Researchers looking to build artificial neural networks can take inspiration from the patterns found in biological ones, while neuroscientists can look to the study of artificial intelligence to identify the computational role of biological details. In this way, artificial neural networks keep the study of the mind and the brain connected.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-1" id="fn-1">1</a> ﻿Named after the English mathematician George Boole. While they used his ideas, Russell and Whitehead didn﻿’﻿t actually use the term ﻿‘﻿Boolean﻿’﻿, as it wasn﻿’﻿t coined until 1913.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-2" id="fn-2">2</a> ﻿At least that﻿’﻿s what it looked like at the time ﻿…﻿ More on this later.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-3" id="fn-3">3</a> ﻿The use of the word ﻿‘﻿circuit﻿’﻿ here differs from that in the last chapter. In addition to its meaning as an electrical circuit, neuroscientists also use the word to refer to a group of neurons connected in a specific way. ﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-4" id="fn-4">4</a> ﻿More on how learning ﻿–﻿ and memory ﻿–﻿ relies on a change in connections in the next chapter.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-5" id="fn-5">5</a> ﻿This can be thought of as representing the ﻿<span class="italic">rate</span>﻿ of spiking of a neuron, rather than if the neuron is emitting a spike or not. Using this type of artificial neuron only requires a small modification to the learning procedure.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-6" id="fn-6">6</a> ﻿The cracks in the ﻿<span class="italic">Principia</span>﻿﻿’﻿s foundation were noticeable even when it was published. Some of the ﻿‘﻿basic﻿’﻿ premises it had to assume were not really very basic and were hard to justify.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-7" id="fn-7">7</a> ﻿This realisation came even more directly from a study on the frog brain that Pitts was involved with. More on that study in ﻿﻿Chapter 6﻿﻿.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-8" id="fn-8">8</a> ﻿The mapping is sometimes referred to as the ﻿‘﻿Marr-Albus-Ito﻿’﻿ theory of motor learning, named also after David Marr and Masao Ito, who both put forth similar models of how the cerebellum learns. ﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-9" id="fn-9">9</a> ﻿The particular words Papert used to describe his feelings about Perceptron-mania at the time were ﻿‘﻿hostility﻿’﻿ and ﻿‘﻿annoyance﻿’﻿.﻿</p>
<p class="FN2"><a href="chapter3.xhtml#fnt-10" id="fn-10">10</a> ﻿Technically they weren﻿’﻿t ﻿‘﻿new﻿’﻿. Minsky and Papert do reference multi-layer perceptrons in their book. However, they were dismissive about the potential powers of these devices and, unfortunately for science, did not encourage their further study. ﻿</p>
<p class="FN2"><a href="chapter3.xhtml#fnt-11" id="fn-11">11</a> ﻿With the exception of deep neural networks that are built to understand images, which we will hear all about in ﻿﻿Chapter 6﻿﻿.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 4</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter4"><a href="contents.xhtml#re_chapter4">CHAPTER FOUR</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter4">Making and Maintaining Memories</a><a id="page_83"></a></p>
<p class="H1" id="b-9781472966445-ch354-sec4">
<span class="bold">
<span>The Hopfield network and attractors</span>
</span></p>
<p class="TXT">A block of iron at 770°C (1,418°F) is a sturdy grey mesh. Each of its trillions of atoms serves as a single brick in the endless parallel walls and ceilings of its crystalline structure. It is a paragon of orderliness. In opposition to their organised structural arrangement, however, the magnetic arrangement of these atoms is a mess. </p>
<p class="TXI">Each iron atom forms a dipole – a miniature magnet with one positive and one negative end. Heat unsteadies these atoms, flipping the direction of their poles around at random. On the micro-level this means many tiny magnets each exerting a force in its own direction. But as these forces work against each other, their net effect becomes negligible. When you zoom out, this mass of mini-magnets has no magnetism at all.</p>
<p class="TXI">As the temperature dips below 770°C, however, something changes. The direction of an individual atom is less likely to switch. With its dipole set in place, the atom starts to exert a constant pressure on its neighbours. This indicates to them which direction they too should be facing. Atoms with different directions vie for influence over the local group until eventually everyone falls in line, one way or the other. With all the small dipoles aligned, the net force is strong. The previously inert block of iron becomes a powerful magnet.</p> <a id="page_84"></a>
<p class="TXI">Philip Warren Anderson, an American physicist who won a Nobel Prize working on such phenomena, wrote in a now-famous essay entitled ‘More is different’ that ‘the behaviour of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles’. That is, the collective action of many small particles – organised only through their local interactions – can produce a function not directly possible in any of them alone. Physicists have formalised these interactions as equations and successfully used them to explain the behaviour of metals, gases and ice. </p>
<p class="TXI">In the late 1970s, a colleague of Anderson’s, John J. Hopfield, saw in these mathematical models of magnetism a structure akin to that of the brain. Hopfield used this insight to bring under mathematical control a long-lasting mystery: the question of how neurons make and maintain memories.</p>
<p class="center">* * *</p>
<p class="TXT">Richard Semon was wrong.</p>
<p class="TXI">A German biologist working at the turn of the twentieth century, Semon wrote two lengthy books on the science of memory. They were filled with detailed descriptions of experimental results, theories and a vocabulary for describing memory’s impact on ‘organic tissue’. Semon’s work was insightful, honest and clear – but it contained a major flaw. Just as French naturalist Jean-Baptiste Lamarck believed (in contrast to our current understanding of evolution) that traits acquired by an animal in its lifetime could be passed to its offspring, <a id="page_85"></a>Semon proposed that <span class="italic">memories</span> acquired by an animal could be passed down. That is, he believed that an organism’s learned responses to its own environment would arise without instruction in its offspring. As a result of this mistaken intuition, much of Semon’s otherwise valuable work was slowly cast aside and forgotten.</p>
<p class="TXI">Being wrong about memory isn’t unusual. Philosopher René Descartes, for example, thought memories were activated by a small gland directing the flow of ‘animal spirits’. What’s unique about Semon is that, despite the flaw in his work that sentenced him to historical obscurity, one of his contributions remained influential long enough to spawn an entire body of research. This small artefact of his efforts is the ‘engram’ – a word coined by Semon in <span class="italic">The</span>
<span class="italic">Mneme</span> in 1904, and subsequently learned by millions of students of psychology and neuroscience.</p>
<p class="TXI">At the time Semon was writing, memory had only recently come under scientific scrutiny – and most of the results were purely about memorisation skills, not about biology. For example, people would be trained to memorise pairs of nonsense words (such as ‘wsp’ and ‘niq’) and then were tested on their ability to retrieve the second word when prompted with the first. This type of memory, known as <span class="italic">associative</span> memory, would become a target of research for decades to come. But Semon was interested in more than just behaviour; he wanted to know what changes in an animal’s physiology could support such associative memories.</p>
<p class="TXI">Led by scant experimental data, he broke up the process of creating and recovering memories into multiple components. Finding common words too <a id="page_86"></a>vague and overloaded, he created novel terms for these divisions of labour. The word that would become so influential, the engram, was defined as ‘the enduring though primarily latent modification in the irritable substance produced by a stimulus’. Or, to put it more plainly: the physical changes in the brain that happen when a memory is formed. Another term, ‘ecphory’, was assigned to the ‘influences which awake the mnemic trace or engram out of its latent state into one of manifested activity’. This distinction between engram and ecphory (or between the processes that lay a memory and those that retrieve it) was one of the many conceptual advances that Semon’s work provided. Despite the fact that his name and most of his language have disappeared from the literature, many of Semon’s conceptual insights were correct and they form the core of how memories are modelled today.</p>
<p class="TXI">In 1950, American psychologist Karl Lashley published ‘In search of the engram’, a paper that solidified the legacy of the word. It also set a rather dismal tone for the field. The paper was so titled because the search was all Lashley felt he had accomplished in 30 years of experiments. Lashley’s experiments involved training animals to make an association (for example, to react in a specific way when shown a circle versus an ‘X’) or learn a task, such as how to run through a particular maze. He would then surgically remove specific brain areas or connection pathways and observe how behaviour was impacted post-operatively. Lashley couldn’t find any area or pattern of lesions that reliably interfered with memory. He concluded that memories must thus somehow be distributed equally across the <a id="page_87"></a>brain, rather than in any single area. But based on some calculations about how many neurons could be used for a memory and the number of pathways between them, he was uncertain about how this was possible. His landmark article thus reads as something of a white flag, a surrendering of any attempt to draw conclusions about the location of memory in the face of a mass of inconsistent data. The physical nature of memory remained to Lashley as vexing as ever.</p>
<p class="TXI">At the same time, however, a former student of Lashley’s was developing his own theories on learning and memory. </p>
<p class="TXI">Donald Hebb, a Canadian psychologist whose early work as a school teacher grew his interest in the mind, was intent on making psychology a biological science. In his 1949 book, <span class="italic">The Organization of Behavior</span>, he describes the task of a psychologist as ‘reducing the vagaries of human thought to a mechanical process of cause and effect’. And in that book, he lays down the mechanical process he believed to be behind memory formation.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> Overcoming the limited, and sometimes misleading, physiological data available at the time, Hebb came to this principle about the physical underpinnings of learning largely through intuition. Yet it would go on to have huge empirical success. The principle, now known as Hebbian learning, is succinctly described by the phrase ‘neurons that fire together wire together’.</p> <a id="page_88"></a>
<p class="TXI">Hebbian learning describes what happens at the small junction between two neurons where one can send a signal to the other, a space called the synapse. Suppose there are two neurons, A and B. The axon from neuron A makes a synaptic connection on to the dendrite or cell body of neuron B (making it the ‘pre-synaptic’ neuron, and neuron B the ‘post-synaptic’ neuron, see Figure 7). In Hebbian learning, if neuron A repeatedly fires before neuron B, the connection from A to B will strengthen. A stronger connection means that the next time A fires it will be more effective in causing B to fire. In this way, activity determines connectivity and connectivity determines activity.</p>
<p class="image-fig" id="fig7.jpg">
<img alt="" src="Images/chapter-04-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 7</span>
</span></p>
<p class="TXI">Hebb’s approach, with its focus on the synapse, situates the engram as both local and global: local because a memory’s imprint occurs at the small gap where one neuron meets another, but global because these changes may be happening at synapses all across the brain. It also makes memory the natural consequence of experience: with pliable synapses, any activation of the brain has the potential to leave a trace. </p>
<p class="TXI">Lashley, a dutiful scientist intent on following the facts, accepted that the engram must be distributed based on his own experiments. But he found no satisfaction in <a id="page_89"></a>Hebb’s solution, which – though an enticing and elegant theory – was based more on speculation than on hard evidence. He turned down Hebb’s offer to be a co-author on the work.</p>
<p class="TXI">Lashley may not have supported Hebb’s ideas, but since the publication of his book countless experiments have. Sea slugs – foot-long slimy brown invertebrates with only about 20,000 neurons – became a creature of much study in this area, due to their ability to learn a very basic association. These shell-less slugs have a gill on their backs that, if threatened, can be quickly retracted for safe keeping. In the lab, a short electric shock will cause the gill to withdraw. If such a shock is repeatedly preceded by a harmless light touch, the slug will eventually start withdrawing in response to the touch alone, demonstrating an association between the touch and what’s expected to come next. It is the marine critter equivalent of learning to pair ‘wsp’ with ‘niq’. This association was shown, in line with Hebb’s theory of learning, to be mediated by a strengthening of the connections between the neurons that represent the touch and those that lead to the gill’s response. The change in behaviour was forged through a changing of connections. </p>
<p class="TXI">Hebbian learning has not just been observed; it’s been controlled as well. In 1999, Princeton researchers showed that genetically modifying proteins in the cell membrane that contribute to synaptic changes can control a mouse’s capacity for learning. Increasing the function of these receptors enhances the ability of mice to remember objects they’ve been shown before. Interfering with these proteins impairs it.</p> <a id="page_90"></a>
<p class="TXI">It is now established science that experience leads to the activation of neurons and that activating neurons can alter the connections between them. This story is accepted as at least a partial answer to the question of the engram. But, as Semon describes it, the engram itself is only part of the story of memory. Memory also requires <span class="italic">re</span>membering. How can this way of depositing memories allow for long-term storage and recall? </p>
<p class="center">* * *</p>
<p class="TXT">It was no real surprise that John J. Hopfield became a physicist. Born in 1933 to John Hopfield Sr, a man who made a name for himself in ultraviolet spectroscopy, and Helen Hopfield, who studied atmospheric electromagnetic radiation, Hopfield Jr grew up in a household where physics was as much a philosophy as it was a science. ‘Physics was a point of view that the world around us is, with effort, ingenuity and adequate resources, understandable in a predictive and reasonably quantitative fashion,’ Hopfield wrote in an autobiography. ‘Being a physicist is a dedication to a quest for this kind of understanding.’ And a physicist is what he would be.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> </p>
<p class="TXI">Hopfield, a tall and lanky man with an engaging smile, earned his PhD in 1958 from Cornell University. He further emulated his father by receiving a Guggenheim fellowship, using it to study at the Cavendish Laboratory <a id="page_91"></a>at Cambridge. But even by this stage, Hopfield’s enthusiasm for the subject of his PhD – condensed matter physics – was waning. ‘In 1968, I had run out of problems … to which my particular talents seemed useful,’ he later wrote.</p>
<p class="TXI">Hopfield’s gateway from physics to biology was hemoglobin, a molecule that both serves a crucial biological function as the carrier of oxygen in blood and could be studied with many of the techniques of experimental physics at the time. Hopfield worked on hemoglobin’s structure for several years at Bell Labs, but he found his real calling in biology after being invited to a seminar series on neuroscience in Boston in the late 1970s. There he encountered a variegated group of clinicians and neuroscientists, gathered together to address the deep question of how the mind emerges from the brain. Hopfield was captivated. </p>
<p class="TXI">Mathematically minded as he was, though, Hopfield was dismayed by the qualitative approach to the brain he saw on display. He was concerned that, despite their obvious talents in biology, these researchers, ‘would never possibly solve the problem because the solution can be expressed only in an appropriate mathematical language and structure’.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> This was a language that physicists had. Hopfield therefore made a point of using his physicist’s skillset even as he embarked on a study of memory. In his eyes, certain physicists of the time who made the leap <a id="page_92"></a>to biology had immigrated fully, taking on the questions, culture and vocabulary of their new land. He wanted to firmly retain his citizenship as a physicist.</p>
<p class="TXI">In 1982 Hopfield published ‘Neural networks and physical systems with emergent collective computational abilities’, which laid out the description and results of what is now known as the Hopfield network. This was Hopfield’s first paper on the topic; he was only dipping his toe into the field of neuroscience and yet it made quite the splash.</p>
<p class="image-fig" id="fig8.jpg">
<img alt="" src="Images/chapter-04-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 8</span>
</span></p>
<p class="TXI">The Hopfield network (see Figure 8) is a mathematical model of neurons that can implement what Hopfield described as ‘content-addressable memory’. This term, coming from computer science, refers to the notion that a full memory can be retrieved from just a small component of it. The network that Hopfield designed for this task is simply composed. It is made only of binary neurons (like the McCulloch-Pitts neurons introduced in the last chapter), which can be either ‘on’ or ‘off’. It is therefore the interactions between these neurons from which the intriguing behaviours of this network emerge. </p>
<p class="TXI">The Hopfield network is <span class="italic">recurrent</span>, meaning that each neuron’s activity is determined by that of any of the <a id="page_93"></a>others in the network. Therefore, each neuron’s activity serves as both input and output to its neighbours. Specifically, each input a neuron receives from another neuron is multiplied by a particular number – a synaptic weight. These weighted inputs are then added together and compared to a threshold: if the sum is greater than (or equal to) the threshold, the neuron’s activity level is 1 (‘on’), otherwise it’s 0 (‘off’). This output then feeds into the input calculations of the other neurons in the network, whose outputs feed back into more input calculations and so on and so on.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p>
<p class="TXI">Like bodies in a mosh pit, the components of a recurrent system push and pull on each other, with the state of a unit at any given moment determined by those that surround it. The neurons in a Hopfield network are thus just like the atoms of iron constantly influencing each other through their magnetic interactions. The effects of this incessant interaction can be myriad and complex. To predict the patterns these interlocking parts will generate is essentially impossible without the precision of a mathematical model. Hopfield was intimately familiar with these models and their ability to show how local interactions lead to the emergence of global behaviour. </p>
<p class="TXI">Hopfield found that if the weights between the neurons in his network are just right the network as a <a id="page_94"></a>whole can implement associative memory. To understand this, we must first define what counts as a memory in this abstract model. Imagine that each neuron in a Hopfield network represents a single object: neuron A is a rocking chair, neuron B is a bike, neuron C is an elephant, and so on. To represent a particular memory, say that of your childhood bedroom, the neurons that represent all the objects in that room – the bed, your toys, photographs on the wall – should be ‘on’; while those that represent objects not in that room – the moon, a city bus, kitchen knives – should be ‘off’. The network as a whole is then in the ‘your childhood bedroom’ activity state. A different activity state – with different sets of neurons ‘on’ or ‘off’ – would represent a different memory. </p>
<p class="TXI">In associative memory, a small input to the network reactivates an entire memory state. For example, seeing a picture of yourself on your childhood bed may activate some of the neurons that represent your bedroom: the bed neurons and pillow neurons, and so on. In the Hopfield network, the connections between these neurons and the ones that represent other parts of the bedroom – the curtains and your toys and your desk – cause these other neurons to become active, recreating the full bedroom experience. Negatively weighted connections between the bedroom neurons and those that represent, say, a local park, ensure that the bedroom memory is not infiltrated by other items. That way you don’t end up remembering a swing set next to your closet. </p>
<p class="TXI">As some neurons turn on and others off, it is their interactivity that brings the full memory into stark relief. <a id="page_95"></a>The heavy-lifting of memory is thus done by the synapses. It is the strength of these connections that carries out the formidable yet delicate task of memory retrieval. </p>
<p class="TXI">In the language of physics, a fully retrieved memory is an example of an <span class="italic">attractor</span>. An attractor is, in short, a popular pattern of activity. It is one that other patterns of activity will evolve towards, just as water is pulled down a drain. A memory is an attractor because the activation of a few of the neurons that form the memory will drive the network to fill in the rest. Once a network is in an attractor state, it remains there with the neurons fixed in their ‘on’ or ‘off’ positions. Always fond of describing things in terms of energy, physicists consider attractors ‘low energy’ states. They’re a comfortable position for a system to be in; that is what makes them attractive and stable. </p>
<p class="TXI">Imagine a trampoline with a person standing on it. A ball placed anywhere on the trampoline will roll towards the person and stay there. The ball being in the divot created by the person is thus an attractor state for this system. If two people of the same size were standing opposite each other on the trampoline, the system would have two attractors. The ball would roll towards whomever it was initially closest to, but all roads would still lead to an attractor. Memory systems wouldn’t be of much use if they could only store one memory, so it is important that the Hopfield network can sustain multiple attractors. The same way the ball is compelled towards the nearest low point on the trampoline, initial neural activity states evolve towards the nearest, most similar memory (see Figure 9). The initial states that lead to a specific memory attractor – for example, the picture <a id="page_96"></a>of your childhood bed that reignites a memory of the whole room or a trip to a beach that ignites the memory of a childhood holiday – are said to be in that memory’s ‘basin of attraction’.</p>
<p class="TXI">
<span class="italic">The Pleasures of Memory</span> is a 1792 poem by Samuel Rogers. Reflecting on the universal journey on which memory can take the mind, he wrote:</p>
<p class="EXTF">
<span class="italic">Lulled in the countless chambers of the brain,</span></p>
<p class="EXTM">
<span class="italic">Our thoughts are linked by many a hidden chain.</span></p>
<p class="EXTM">
<span class="italic">Awake but one, and lo, what myriads rise!</span></p>
<p class="EXTL">
<span class="italic">Each stamps its image as the other flies!</span></p>
<p class="TXT">Rogers’ ‘hidden chain’ can be found in the pattern of weights that reignite a memory in the Hopfield network. <a id="page_97"></a>Indeed, the attractor model aligns with much of our intuition about memory. It implicitly addresses the time it takes for memories to be restored, as the network needs time to activate the right neurons. Attractors can also be slightly displaced in the network, creating memories that are mostly correct, with a detail or two changed. And memories that are too similar may simply merge into one. While collapsing memory to a series of zeros and ones may seem an affront to the richness of our experience of it, it is the condensation of this seemingly ineffable process that puts an understanding of it within reach.</p>
<p class="image-fig" id="fig9.jpg">
<img alt="" src="Images/chapter-04-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 9</span>
</span></p>
<p class="TXI">In the Hopfield network, how robustly neurons are connected with each other defines which patterns of neural activity form a memory. The engram is therefore in the weights – but how does it get there? How can an experience create just the right weights to make a memory? Hebb tells us that memories should come out of a strengthening of the connections between neurons that have similar activity – and in the Hopfield network that is just how it’s done.</p>
<p class="TXI">The Hopfield network encodes a set of memories through a simple procedure. For every experience in which two neurons are either both active or inactive, the connection between them is strengthened. In this way, the neurons that fire together come to be wired together. On the other hand, for every pattern where one neuron is active and the other is inactive, the connection is weakened.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> After this learning <a id="page_98"></a>procedure, neurons that are commonly co-active in memories will have a strong positive connection, those that have opposite activity patterns will have strong negative connections and others will fall somewhere in between. This is just the connectivity needed to form attractors.</p>
<p class="TXI">Attractors are not trivial phenomena. After all, if all the neurons in a network are constantly sending and receiving inputs, why should we assume their activity would ever settle into a memory state, let alone the <span class="italic">right</span> memory state? So, to be certain that the right attractors would form in these networks, Hopfield had to make a pretty strange assumption: weights in the Hopfield network are <span class="italic">symmetric</span>. That means the strength of the connection from neuron A to neuron B is always the same as the strength from B to A. Enforcing this rule offered a mathematical guarantee of attractors. The problem is that the odds of finding a population of neurons like this in the brain are dismal to say the least. It would require that each axon going out of one cell and forming a synapse with another be matched exactly by that same cell sending its axon back, connecting to the first cell with the same strength. Biology simply isn’t that clean.</p>
<p class="TXI">This illuminates the ever-present tension in the mathematical approach to biology. The physicist’s perspective, which depends on an almost irrational degree of simplification, is at constant odds with the biology, full as it is of messy, inconvenient details. In this case, the details of the maths demanded symmetric weights in order to make any definitive statement about attractors and thus to make progress on modelling the <a id="page_99"></a>process of memory. A biologist would likely have dismissed the assumption outright.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup></p>
<p class="TXI">Hopfield, with one foot on either side of the mathematics–biology divide, knew to appreciate the perspective of the neuroscientists. To ease their concerns, he showed in his original paper that – even though it couldn’t be guaranteed mathematically – networks that allowed asymmetric weights still seemed able to learn and sustain attractors relatively well. </p>
<p class="TXI">The Hopfield network thus offered a proof of concept that Hebb’s ideas about learning could actually work. Beyond that, it offered a chance to study memory mathematically – to quantify it. For example, precisely how many memories can a network hold? This is a question that can only be asked with a precise model of memory in mind. In the simplest version of the Hopfield network, the number of memories depends on the number of neurons in the network. A network with 1,000 neurons, for example, can store about 140 memories; 2,000 neurons can store 280; 10,000 can store 1,400 and so on. If the number of memories remains less than about 14 per cent the number of neurons, each memory will be restored with minimal error. Adding more memories, however, will be like the final addition to a house of cards that causes it to cave in. When pushed past its capacity, the Hopfield network collapses: inputs <a id="page_100"></a>go towards meaningless attractors and no memories are successfully recovered. It’s a phenomenon given the appropriately dramatic name ‘blackout catastrophe’.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup></p>
<p class="TXI">Precision cannot be evaded; once this estimate of memory capacity is found, it’s reasonable to start asking if it aligns with the number of memories we know to be stored by the brain. A landmark study in 1973 showed that people who had been shown more than 10,000 images (each only once and for only a brief period of time) were quite capable of recognising them later on. The 10 million neurons in the perirhinal cortex – a brain region implicated in visual memory – could store this amount of images, but it wouldn’t leave much space for anything else. Therefore, there seemed to be a problem with Hebbian learning.</p>
<p class="TXI">This problem becomes less problematic, however, when we realise that recognition is not recall. That is, a feeling of familiarity when seeing an image can happen without the ability to regenerate that image from scratch. The Hopfield network is remarkable for being capable of the latter, more difficult task – it fully completes a memory from a partial bit of it. But the former task is still important. Thanks to researchers working at the University of Bristol, it’s now known that recognition can also be performed by a network that uses Hebbian learning. These networks, when assessed on their ability to label an input as novel or familiar, have a significantly <a id="page_101"></a>higher capacity: 1,000 neurons can now recognise as many as 23,000 images. Just as Semon so presciently identified, this is an example of an issue that arises from relying on common language to parcel up the functions of the brain. What feels like simply ‘memory’ to us crumbles when pierced by the scrutiny of science and mathematics into a smattering of different skills.</p>
<p class="center">* * *</p>
<p class="TXT">When, in 1953, American doctor William Scoville removed the hippocampus from each side of 27-year-old Henry Molaison’s brain, he thought he was helping prevent Molaison’s seizures. What Scoville didn’t know was the incredible impact this procedure would have on the science of memory. Molaison (more famously known as ‘H. M.’ in scientific papers to hide his identity until his death in 2008) did find some relief from his seizures after the procedure, but he never formed another conscious memory again. Molaison’s subsequent and permanent amnesia initiated a course of research that centred the hippocampus – a curved finger-length structure deep in the brain – as a hub in the memory-formation system. Evading Lashley’s troubled search, this is a location that does play a special role in storing memories.</p>
<p class="TXI">Current theories of hippocampal function go as follows: information about the world first reaches the hippocampus at the dentate gyrus, a region that runs along the bottom edge of the hippocampus. Here, the representation is primed and prepped to be in a form more amenable to memory storage. The dentate gyrus then sends connections on to where attractors are <a id="page_102"></a>believed to form, an area called CA3; CA3 has extensive recurrent connections that make it a prime substrate for Hopfield network-like effects. This area then sends output to another region called CA1, which acts as a relay station; it sends the remembered information back to the rest of the brain (see Figure 10).</p>
<p class="image-fig" id="fig10.jpg">
<img alt="" src="Images/chapter-04-image-04.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 10</span>
</span></p>
<p class="TXI">What’s interesting about this final step – and what may have muddied Lashley’s original findings – is that these projections out to different areas of the brain are believed to facilitate the <span class="italic">copying</span> of memories. In this way, CA3 acts as a buffer, or warehouse, holding on to memories until they can be transferred to other brain areas. It does so by reactivating the memory in those areas. The hippocampus thus helps the rest of the brain memorise things using the same strategy you’d use to study for a test: repetition. By repeatedly reactivating the same group of neurons elsewhere in the brain, the hippocampus gives those neurons the chance to undergo Hebbian learning themselves. Eventually, their own weights have changed enough for the memory to be safely stored there.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> With <a id="page_103"></a>his hippocampus gone, Molaison had no warehouse for his experiences, no way to replay his memories back to his brain. </p>
<p class="TXI">With knowledge of this memory storehouse in the brain, researchers can look into how it works. Particularly, they can look for attractors in it.</p>
<p class="TXI">In 2005, scientists at University College London recorded the activity of hippocampal cells in rats. The rats got used to being in two different enclosures – a circular one and a square one. Their hippocampal neurons showed one pattern of activity when they were in the circle and a different pattern when they were in the square. The test for attractors came when an animal was placed into a new ‘squircle’ environment, the shape of which was somewhere in between a circle and a square. The researchers found that if the environment was more square-like the neural activity went to the pattern associated with the square environment; more circle-like and it went to that of the circle. Crucially, there were no intermediate representations in response to intermediate environments, only all circle or all square. This makes the memories of the circle and square environments attractors. An initial input that isn’t exactly one or the other is unstable; it gets inescapably driven towards the nearest established memory. </p>
<p class="TXI">The Hopfield network made manifest the theories of Hebb and showed how attractors – normally studied in physics – could explain the mysteries of memory. Yet Hopfield knew the limitations of bringing mathematics to real brains in real laboratories. He described his own model as a ‘mere parody of the complexities of neurobiology’. Indeed, as the creation of a physicist, it <a id="page_104"></a>lacks all the gooey richness of biology. But as a parody capable of powerful computations, it has offered many insights as well – insights that didn’t end with simple storage and recall. </p>
<p class="center">* * *</p>
<p class="TXT">You’re eating dinner in your kitchen when your roommate comes home. When you see them, you remember that last night you finished a book they had lent you and you want to return it before they leave for a trip the next day. So, you put down your food, head out of the kitchen and go down the hallway. You walk up the stairs, turn, enter your room and think: ‘Wait, what am I doing here?’ </p>
<p class="TXI">The sensation is a common one. So much so it’s been given a name: ‘destinesia’ or amnesia about why you’ve gone to where you are. It’s a failure of what’s called ‘working memory’, the ability to hold an idea in mind, even just for the 10 seconds it takes to walk from room to room. Working memory is crucial for just about all aspects of cognition: it’s hard to make a decision or work through a plan if you keep forgetting what you’re thinking about.</p>
<p class="TXI">Psychologists have been studying working memory for decades. The term itself was first coined in the 1960 book <span class="italic">Plans and the Structure of Behavior</span> written by George A. Miller and fellow scientists working at the Center for Advanced Study in the Behavioral Sciences in California. But the concept was explored well before that. Indeed, Miller himself wrote one of the most influential papers on the topic four years previously, in 1956. Perhaps <a id="page_105"></a>anticipating its fame, Miller gave the paper the cheeky title ‘The magical number seven, plus or minus two’. What that magical number refers to is the number of items humans can hold in their working memory at any one time. </p>
<p class="TXI">An example of how to assess this is to 1) show a participant several coloured squares on a screen; 2) ask them to wait for some time between several seconds to minutes; 3) then show them a second set of coloured squares. The task of the subject is to indicate if the colours of the second set are the same as the colours of the first. People can do well on this task if the number of squares shown remains small, achieving nearly 100 per cent accuracy if only one square is shown. Adding in more squares makes the performance drop and drop, until past seven, it’s almost no different to random guessing. Whether seven really is a special value when it comes to this kind of working memory capacity is up for debate; some studies find lower limits, some higher. However, there’s no doubt that Miller’s paper made an impact and psychologists have worked to characterise nearly every aspect of working memory since, from what can be held in it to how long it can last. </p>
<p class="TXI">But the question remains of how the brain actually does this: where are working memories stored and in what way? A tried-and-tested method for answering such questions – lesion experiments – pointed to the prefrontal cortex, a large part of the brain just behind the forehead. Whether it was humans with unfortunate injuries or laboratory animals with the area removed, it was clear that damaging the prefrontal cortex reduced working memory substantially. Without it, animals can <a id="page_106"></a>hardly hold on to an idea for more than a second or two. Thoughts and experiences pass through their minds like water through cupped hands. </p>
<p class="TXI">With an ‘X’ marking the spot, neuroscientists then began to dig. Dropping an electrode into the prefrontal cortex of monkeys, in 1971 researchers at the University of California in Los Angeles eavesdropped on the neurons there. The scientists, Joaquin Fuster and Garrett Alexander, did this while the animals performed a task similar to the colour memory test. These tests are known as ‘delayed response’ tasks because they include a delay period wherein the important information is absent from the screen and must thus be held in memory. The question was: what are neurons in the prefrontal cortex doing during this delay?</p>
<p class="TXI">Most of the brain areas responsible for vision have a stereotyped response to this kind of task: the neurons respond strongly when the patterns on the screen initially show up and then again when they reappear after the delay, but during the delay period – when no visual inputs are actually entering the brain – these areas are mostly quiet. Out of sight really does mean out of mind for these neurons. What Fuster and Alexander found, however, was that cells in the prefrontal cortex were different. The neurons there that responded to the visual patterns kept firing even after the patterns disappeared; that is, they maintained their activity during the delay period. A physical signature of working memory at work!</p>
<p class="TXI">Countless experiments since have replicated these findings, showing maintained activity during delay periods under many different circumstances, both in <a id="page_107"></a>the prefrontal cortex and beyond. Experiments have also hinted that when these firing patterns are out of whack, working memory goes awry. In some experiments, for example, applying a brief electrical stimulation during the delay period can disrupt the ongoing activity and this leads to a dip in performance on delayed response tasks. </p>
<p class="TXI">What is so special about these neurons that they can do this? Why can they hold on to information and maintain their firing for seconds to minutes, when other neurons let theirs go? For this kind of sustained output, neurons usually need sustained input. But if delay activity occurs without any external input from an image then that sustained input must come from neighbouring neurons. Thus, delay activity can only be generated by a network of neurons working together, the connections between them conspiring to keep the activity alive. This is where the idea of attractors comes back into play. </p>
<p class="TXI">So far we’ve looked at attractors in Hopfield networks, which show how input cues reignite a memory. It may not be clear how this helps with working memory. After all, working memory is all about what happens after that ignition; after you stand up to get your roommate’s book, how do you keep that goal in mind? As it turns out, however, an attractor is exactly what’s needed in this situation, because an attractor stays put.</p>
<p class="TXI">Attractors are defined by derivatives. If we know the inputs a neuron gets and the weights those inputs are multiplied by, we can write down an equation – a derivative – describing how the activity of that neuron will change over time as a result of those inputs. If this derivative is zero that means there is no change in the <a id="page_108"></a>activity of the neuron over time; it just keeps firing at the same, constant rate. Recall that, because this neuron is part of a recurrent network, it not only gets input but also serves as input to other neurons. So, its activity goes into calculating the derivative of a neighbouring neuron. If none of the inputs to this neighbouring neuron are changing – that is, their derivatives are all zero as well – then it too will have a zero derivative and will keep firing at the same rate. When a network is in an attractor state, the derivative of each and every neuron in that network is zero.</p>
<p class="TXI">And that is how, if the connections between neurons are just right, memories started at one point in time can last for much longer. All the cells can maintain their firing rate because all the cells around them are doing the same. Nothing changes if nothing changes. </p>
<p class="TXI">The problem is that things do change. When you leave the kitchen and walk to your bedroom, you encounter all kinds of things – your shoes in the hallway, the bathroom you meant to clean, the sight of rain on the window – that could cause changes in the input to the neurons that are trying to hold on to the memory. And those changes could push the neurons out of the attractor state representing the book and off to somewhere else entirely. For working memory to function, the network needs to be good at resisting the influence of such distractors. A run-of-the-mill attractor can resist distracting input to an extent. Recall the trampoline example. If the person standing on the trampoline gave a little nudge to the ball, it would likely roll just out of its divot and then back in. With only a small perturbation the memory stays intact, but give the <a id="page_109"></a>ball a heartier kick and who knows where it will end up? Good memory should be robust to such distractions – so what could make a network good at holding on to memories? </p>
<p class="TXI">The dance between data and theory is a complex one, with no clear lead or follow. Sometimes mathematical models are developed just to fit a certain dataset. Other times the details from data are absent or ignored and theorists do as their name suggests: theorise about how a system <span class="italic">could</span> work before knowing how it does. When it comes to building a robust network for working memory, scientists in the 1990s went in the latter direction. They came up with what’s known as the ‘ring network’, a hand-designed model of a neural circuit that would be ideal for the robust maintenance of working memories. </p>
<p class="TXI">Unlike Hopfield networks, ring networks are well described by their name: they are composed of several neurons arranged in a ring, with each neuron connecting only to those near to it. Like Hopfield networks, these models have attractor states – activity patterns that are self-sustaining and can represent memories. But the attractor states in a ring model are different to those in a Hopfield network. Attractors in a Hopfield model are <span class="italic">discrete</span>. This means that each attractor state – the one for your childhood bedroom, the one for your childhood holiday, the one for your current bedroom – is entirely isolated from the rest. There is no smooth way to transition between these different memories, regardless of how similar they are; you have to completely leave one attractor state to get to another. Attractors in a ring network, on the other hand, are <span class="italic">continuous</span>. With continuous attractors, transitioning between similar <a id="page_110"></a>memories is easy. Rather than being thought of as a trampoline with people standing at different points, models with continuous attractor states are more like the gutter of a bowling lane: once the ball gets into the gutter it can’t easily get out, but it can move smoothly within it.</p>
<p class="TXI">Networks with continuous attractor states like the ring model are helpful for a variety of reasons and chief among them is the type of errors they make. It may seem silly to praise a memory system for its errors – wouldn’t we prefer no errors at all? – but if we assume that no network can have perfect memory, then the quality of the errors becomes very important. A ring network allows for small, sensible errors. </p>
<p class="TXI">Consider the example of the working memory test where subjects had to keep in mind the colour of shapes on a screen. Colours map well to ring networks because, as you’ll recall from art class, colours lie on a wheel. So, imagine a network of neurons arranged in a ring, with each neuron representing a slightly different colour. At one side of the ring are red-representing neurons, next to them are orange, then yellow and green; this brings us to the side opposite the red, where there are the blue-representing neurons, which lead to the violet ones and back to red. </p>
<p class="TXI">In this task, when a shape is seen, it creates activity in the neurons that represent its colour, while the other neurons remain silent. This creates a little ‘bump’ of activity on the ring, centred on the remembered colour. If any distracting input comes in while the subject tries to hold on to this colour memory – from other random sights in the room, for example – it may push or pull the activity bump away from the desired colour. But – and this is the crucial <a id="page_111"></a>point – it will only be able to push it to a very nearby place on the ring. So red may become red-orange or green may become teal. But the memory of red would be very unlikely to become green. Or, for that matter, to become no colour at all; that is, there will always be a bump <span class="italic">somewhere</span> on the ring. These properties are all a direct result of the gutter-like nature of a continuous attractor – it has low resistance for moving between nearby states, but high resistance to perturbations otherwise.</p>
<p class="TXI">Another benefit of the ring network is that it can be used to do things. The ‘working’ in working memory is meant to counter the notion that memory is just about passively maintaining information. Rather, holding ideas in working memory lets us combine them with other information and come to new conclusions. An excellent example of this is the head direction system in rats, which also served as the inspiration for early ring network models. </p>
<p class="TXI">Rats (along with many other animals) have an internal compass: a set of neurons that keep track of the direction the animal is facing at all times. If the animal turns to face a new direction, the activity of these cells changes to reflect that change. Even if the rat sits still in a silent darkened room, these neurons continue to fire, holding on to the information about its direction. In 1995, a team from Bruce McNaughton’s lab at the University of Arizona and, separately, Kechen Zhang of the University of California, San Diego, posited that this set of cells could be well described by a ring network. Direction being one of those concepts that maps well to a circle, a bump of activity on the ring would be used to store the direction the animal was facing (See Figure 11).</p> <a id="page_112"></a>
<p class="TXI">But not only could a ring network explain how knowledge of head direction was maintained over time, it also served as a model of how the stored direction could change when the animal did. Head direction cells receive input from other neurons, such as those from the visual system and the vestibular system (which keeps track of bodily motion). If these inputs are hooked up to the ring network just right, they can push the bump of activity along to a new place on the ring. If the vestibular system says the body is now moving leftwards, for example, the bump gets pushed to the left. In this way, movement along the ring doesn’t create errors in memory, but rather updates the memory based on new information. ‘Working’ memory earns its name.</p>
<p class="image-fig" id="fig11.jpg">
<img alt="" src="Images/chapter-04-image-05.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 11</span>
</span></p>
<p class="TXI">Ring networks are a lovely solution to the complex problem of how to create robust and functional working memory systems. They are also beautiful mathematical objects. They display the desirable properties of simplicity and symmetry. They’re precise and finely tuned, elegant even.</p> <a id="page_113"></a>
<p class="TXI">As such, they are completely unrealistic. Because to the biologist, of course, ‘finely tuned’ are dirty words. Anything that requires delicate planning and pristine conditions to operate well won’t survive the chaos that is brain development and activity. Many of the desirable properties of ring networks only occur under very particular assumptions about the connectivity between neurons, assumptions that just don’t seem very realistic. So, despite all their desirable theoretical properties and useful abilities, the chances of seeing a ring network in the brain seemed slim. </p>
<p class="TXI">The discovery made in a research centre just outside Washington DC in 2015 was therefore all the more exciting. </p>
<p class="TXI">Janelia Research Campus is a world-class research facility hidden away in the idyllic former farmland of Ashburn, Virginia. Vivek Jayaraman has been at Janelia since 2006. He and his team of about half a dozen people work to understand navigation in <span class="italic">Drosophila melanogaster</span>, a species of fruit fly commonly studied in neuroscience. On a par with a grain of rice, the size of these animals is both a blessing and a curse. While they can be difficult to get hold of, these tiny flies only have around 135,000 neurons, roughly 0.2 per cent the amount of another popular lab animal, the mouse. On top of that, a lot is known about these neurons. Many of them are easily categorised based on the genes they express, and their numbers and locations are very similar across individuals. </p>
<p class="TXI">Like rodents, flies also have a system for keeping track of head direction. For the fly, these head direction neurons are located in a region known as the ellipsoid <a id="page_114"></a>body. The ellipsoid body is centrally placed in the fly brain and it has a unique shape: it has a hole in the middle with cells arranged all around that hole, forming a doughnut made of neurons – or in other words, a ring.</p>
<p class="TXI">Neurons arranged in a ring, however, do not necessarily make a ring network. So, what the Jayaraman lab set out to do was investigate whether this group of neurons that <span class="italic">looked</span> like a ring network, actually <span class="italic">behaved</span> like one. To do this, they put a special dye in the ellipsoid body neurons, one that makes them light up green when they’re active. They then had the fly walk around, while they filmed the neurons. If you were to look at these neurons on a screen, as the fly heads forwards, you’d see a flickering of little green points at one location on an otherwise black screen. Should the fly choose to make a turn, that flickering patch would swing around to a new location. Over time, as the fly moves and the green patch on the screen moves with it, the points that have lit up form a clear ring structure, matching the underlying shape of the ellipsoid body. If you turn the lights off in the room so the fly has no ability to see which way it’s facing, the green flicker still remains at the same location on that ring – a clear sign that the memory of heading direction is being maintained. </p>
<p class="TXI">In addition to observing the activity on the ring, the experimenters also manipulated it in order to probe the extremes of its behaviour. A true ring network can only support one ‘bump’ of activity; that is, only neurons at one location on the ring can be active at a given time. So, the researchers artificially stimulated neurons on the side of the ring opposite to those already active. This strong stimulation of the opposing neurons caused the original bump to shut down, and the bump at the new location was maintained, even after the stimulation was <a id="page_115"></a>turned off. Through these experiments, it became clear that the ellipsoid body was no imposter, but a vivid example of a theory come to life. </p>
<p class="TXI">This finding – a ring network in the literal, visible shape of a ring – feels a bit like nature winking at us. William Skaggs and the other authors of one of the original papers proposing the ring network explicitly doubted the possibility of such a finding: ‘For expository purposes it is helpful to think of the network as a set of circular layers; this does not reflect the anatomical organisation of the corresponding cells in the brain.’ Most theorists working on ring network models assumed that they’d be embedded in some larger, messier network of neurons. And that is bound to be the case for most systems in most species. This anomalously pristine example likely arises from a very precisely controlled genetic programme. Others will be much harder to spot. </p>
<p class="TXI">Even if we usually can’t see them directly, we can make predictions about behaviours we’d expect to see if the brain is using continuous attractors. In 1991, pioneering working memory researcher Patricia Goldman-Rakic found that blocking the function of the neuromodulator dopamine made it harder for monkeys to remember the location of items. Dopamine is known to alter the flow of ions into and out of a cell. In 2000, researchers at the Salk Institute in California showed how mimicking the presence of dopamine in a model with a continuous attractor enhanced the model’s memory.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup> It stabilised the activity of the neurons encoding the memory, making <a id="page_116"></a>them more resistant to irrelevant inputs. Because dopamine is associated with reward,<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> this model also predicts that under conditions where a person is anticipating a large reward their working memory will be better – and that is exactly what has been found. When people are promised more reward in turn for remembering something, their working memory is better. Here, the concept of an attractor works as the thread that stitches chemical changes together with cognitive ones. It links ions to experiences. </p>
<p class="TXI">Attractors are omnipresent in the physical world. They arise from local interactions between the parts of a system. Whether those parts are atoms in a metal, planets in a solar system or even people in a community, they will be compelled towards an attractor state and, without major disruptions, will stay there. Applying these concepts to the neurons that form a memory connects dots across biology and psychology. On one side, Hopfield networks link the formation and retrieval of memories to the way in which connections between neurons change. On the other, structures like ring networks underlie how ideas are held in the mind. In one simple framework we capture how memories are recorded, retained and reactivated.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-1" id="fn-1">1</a> ﻿Jerzy Konorski, a Polish neurophysiologist, published a book with very similar ideas the year before Hebb did. In fact, Konorski anticipated several important findings in neuroscience and psychology. However, the global East﻿–﻿West divide at the time isolated his contributions. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-2" id="fn-2">2</a> ﻿When Hopfield wrote on an undergraduate admission form that he intended to study ﻿‘﻿physics or chemistry﻿’﻿, his university advisor ﻿–﻿ a colleague of his father﻿’﻿s ﻿–﻿ crossed out the latter option saying, 
﻿‘﻿I don﻿’﻿t believe we need to consider chemistry.﻿’﻿ ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-3" id="fn-3">3</a> ﻿Hopfield﻿’﻿s attitude was not unique. The 1980s found many physicists, bored with their own field, looking at the brain and thinking, ﻿‘﻿I could solve that.﻿’﻿ After Hopfield﻿’﻿s success, this population increased even more.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-4" id="fn-4">4</a> ﻿While the calculation of an individual neuron﻿’﻿s activity in terms of inputs and weights is the same as described for the perceptron in the last chapter, the perceptron is a ﻿<span class="italic">feedforward</span>﻿ (not recurrent) network. Recurrence means that the connections can form loops: neuron A connects to neuron B, which connects back to neuron A, for example. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-5" id="fn-5">5</a> ﻿This second part ﻿–﻿ the idea that connection strength should ﻿<span class="italic">decrease</span>﻿ if a pre-synaptic neuron is highly active while the post-synaptic neuron remains quiet ﻿–﻿ was not part of Hebb﻿’﻿s original sketch, but it has since been borne out by experiments.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-6" id="fn-6">6</a> ﻿In fact, when Hopfield presented an early version of this work to a group of neuroscientists, one attendee commented that ﻿‘﻿it was a beautiful talk but unfortunately had nothing to do with neurobiology﻿’﻿. ﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-7" id="fn-7">7</a> ﻿You may know some people with stories of their own ﻿‘﻿blackout catastrophe﻿’﻿ after a night of drinking. However, the exact type of memory failure seen in Hopfield networks is not actually believed to occur in humans.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-8" id="fn-8">8</a> ﻿This process is believed to occur while you sleep.﻿</p>
<p class="FN1"><a href="chapter4.xhtml#fnt-9" id="fn-9">9</a> ﻿This model was composed of the Hodgkin-Huxley style of neurons described in ﻿﻿Chapter 2﻿﻿, which makes incorporating dopamine﻿’﻿s effects on ion flows easy.﻿</p>
<p class="FN2"><a href="chapter4.xhtml#fnt-10" id="fn-10">10</a> ﻿Lots more on this in ﻿﻿Chapter 11﻿﻿!﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 6</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter6"><a href="contents.xhtml#re_chapter6">CHAPTER SIX</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter6">Stages of Sight</a><a id="page_151"></a></p>
<p class="H1" id="b-9781472966445-ch612-sec6">
<span class="bold">
<span>The Neocognitron and convolutional 
neural networks</span>
</span></p>
<p class="EXTF">
<span class="italic">The summer vision project is an attempt to use our summer workers effectively in the construction of a significant part of a visual system. The particular task was chosen partly because it can be segmented into sub-problems which will allow individuals to work independently and yet participate in the construction of a system complex enough to be a real landmark in the development of ‘pattern recognition’.</span></p>
<p class="right">Vision Memo No. 100 from the 
Massachusetts Institute of Technology 
Artificial Intelligence Group, 1966</p>
<p class="TXT-con">The summer of 1966 was meant to be the summer that a group of MIT professors solved the problem of artificial vision. The ‘summer workers’ they planned to use so effectively for this project were a group of a dozen or so undergraduate students at the university. In their memo laying out the project’s plan, the professors provided several specific skills they wanted the computer system the students were developing to perform. It should be able to define different textures and lighting in an image, label parts as foreground and parts as background, and identify whatever objects <a id="page_152"></a>were present. One professor<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> purportedly described the aims more casually as ‘linking a camera to a computer and getting the computer to describe what it saw’. </p>
<p class="TXI">The goals of this project were not completed that summer. Nor the next. Nor many after that. Indeed, some of the core issues raised in the description of the summer project remain open problems to this day. The hubris on display in that memo is not surprising for its time. As discussed in <a href="chapter3.xhtml#chapter3">Chapter 3</a>, the 1960s saw an explosion in computing abilities and, in turn, naive hopes about automating even the most complex tasks. If computers could now do anything asked of them, it was just a matter of knowing what to ask for. With something as simple and immediate as vision, how hard could that be?</p>
<p class="TXI">The answer is very hard. The act of visual processing – of taking in light through our eyes and making sense of the external world that reflected it – is an immensely complex one. Common sayings like ‘right in front of your eyes’ or ‘in plain sight’, which are used to indicate the effortlessness of vision, are deceitful. They obscure the significant challenges even the most basic visual inputs pose for the brain. Any sense of ease we have regarding vision is an illusion, one that was hard won through millions of years of evolution. </p>
<p class="TXI">The problem of vision is specifically one of reverse engineering. In the back of the eye, in the retina, there is <a id="page_153"></a>a wide flat sheet of cells called photoreceptors. These cells are sensitive to light. Each one indicates the presence or absence (and possibly wavelength) of the light hitting it at each moment by sending off a signal in the form of electrical activity. This two-dimensional flickering map of cellular activity is the only information from which the brain is allowed to reconstruct the three-dimensional world in front of it. </p>
<p class="TXI">Even something as simple as finding a chair in a room is a technically daunting endeavour. Chairs can be many different shapes and colours. They can also be nearby or far away, which makes their reflection on the retina larger or smaller. Is it bright in the room or dark? What direction is light coming from? Is the chair turned towards you or away? All of these factors impact the exact way in which photons of light hit the retina. But trillions of different patterns of light could end up meaning the same thing: a chair is there. The visual system somehow finds a way to solve this many-to-one mapping in less than a tenth of a second. </p>
<p class="TXI">At the time those MIT students were working to give the gift of sight to computers, physiologists were using their own tools to solve the mysteries of vision. This started with recording neural activity from the retina and moved on to neurons throughout the brain. With an estimated 30 per cent of the primate cortex playing some role in visual processing, this was no small undertaking.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> In the mid-twentieth century many of the scientists performing these experiments were based in <a id="page_154"></a>the Boston area (many at MIT itself or just north of it, at Harvard) and they were quickly amassing a lot of data that they needed to somehow make sense of.</p>
<p class="TXI">Perhaps it was the physical proximity. Perhaps it was a tacit acknowledgement of the immense challenge they had each set for themselves. Perhaps in the early days the communities were just too small to keep to themselves. Whatever the reason, neuroscientists and computer scientists forged a long history of collaborating in their attempts to understand the fundamental questions of vision. The study of vision – of how patterns can be found in points of light – is full of direct influence from the biological to the artificial and vice versa. The harmony may not have been constant: when computer science embarked on methods that were useful but didn’t resemble the brain, the fields diverged. And when neuroscientists dig into the nitty-gritty detail of the cells, chemicals and proteins that carry out biological vision, computer scientists largely turn away. But the impacts of the mutual influence are still undeniable, and plainly visible in the most modern models and technologies. </p>
<p class="center">* * *</p>
<p class="TXT">The earliest efforts to automate vision came before modern computers. Though implemented in the form of mechanical gadgetry, some of the ideas that powered these machines prepared the field for the later emergence of computer vision. One such idea was <span class="italic">template matching</span>. </p>
<p class="TXI">In the 1920s, Emanuel Goldberg, a Russian chemist and engineer, set out to solve a problem banks and other offices had while searching their file systems for <a id="page_155"></a>documents. At the time, documents were stored on microfilm – strips of 35mm film that contained tiny images of documents that could be projected to a larger screen for reading. The ordering of the documents on the film had little relation to their contents, so finding a desired document – such as a cancelled cheque from a particular bank customer – involved much unstructured searching. Goldberg turned to a crude form of ‘image processing’ to automate this process. </p>
<p class="TXI">Under Goldberg’s plan, cashiers entering a new cheque into the filing system would need to mark it with a special symbol that indicated its contents. For example, three black dots in a row meant the customer’s name started with ‘A’, three black dots in a triangle meant it started with ‘B’ and so on. Now, if a cashier wanted to find the last cheque submitted by a 
Mr Berkshire, for example, they just needed to find the cheques marked with a triangle. The triangle pattern was thus a template and the goal of Goldberg’s machine was to match it. </p>
<p class="TXI">Physically, these templates took the form of cards with holes punched in them. So, when looking for Mr Berkshire’s documents, the cashier would take a card with three holes punched out in the shape of a triangle and place it in between the microfilm strip and a lightbulb. Each document on the strip would then be automatically pulled up to be aligned with the card, causing the light to shine through the holes on the card and then through the film itself. A photocell placed behind the film detected any light that came through and signalled this to the rest of the machine. For most of the documents, some light would get through as the <a id="page_156"></a>symbols on the film didn’t align with the holes on the card. But when the desired document appeared, the light shining through the card would be exactly blocked out by the pattern of black dots on the film. These mini eclipses meant no light would land on the photocell and this signalled to the rest of the machine, and to the cashier, that a match had been found. </p>
<p class="TXI">Goldberg’s approach required that the cashiers knew in advance exactly what symbol they were looking for and had a card to match it. Crude though it was, this style of template matching became the dominant approach for much of the history of artificial vision. When computers appeared on the scene, the form of the templates migrated from the physical to the digital. </p>
<p class="TXI">In a computer, images are represented as a grid of pixel values (see Figure 14). Each pixel value is a number indicating the intensity of the colour in the tiny square region of the picture it represents.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> In the digital world, a template is also just a grid of numbers, one that defines the desired pattern. So, the template for three dots in the shape of a triangle may be a grid of mostly zeros except for three precisely placed pixels with value one. The role of the light shining through the template card in Goldberg’s machine was replaced in the computer by a mathematical operation: multiplication. If each pixel value in the image is multiplied by the value at the same location in the template, the result can actually tell us if the image is a match.</p> <a id="page_157"></a>
<p class="image-fig" id="fig14.jpg">
<img alt="" src="Images/chapter-06-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 14</span>
</span></p>
<p class="TXI">Let’s say we are looking for a smiling face in a black and white image (where black pixels have a value of one and white have a value of zero). Given a template for the face, we can compare it with an image through multiplication. If the image is indeed of the face we are searching for, the values that comprise the template will be very similar to those in the image. Therefore, zeros in the template will be multiplied by zeros in the image and ones in the template will be multiplied by ones in the image. Adding up all the resulting values from this multiplication gives us the number of black pixels that are the same in the template and the image, which in the case of a match would be many. If the image we’re given is of a frowning face instead, some of the pixels around the mouth in the image won’t match the template. There, zeros in the template will be multiplied by ones in the image and vice versa. Because the product at those pixel locations will be zero, the sum across the whole image won’t be so high. In this way, a simple sum of products gives a measure of how much an image matches a template.</p><a id="page_158"></a>
<p class="TXI">This method found wide use in many different industries. Templates have been used to count crowd sizes by finding faces in a picture. Known geographical features have also been located in satellite images via templates. The number and model of cars that pass through an intersection can be tracked as well. With template matching, all we need to do is define what we want and multiplication will tell us if it’s a match. </p>
<p class="center">* * *</p>
<p class="TXT">Imagine a stadium – one just like where you’d watch a football game – but in this stadium, instead of screaming fans, the stands are full of screaming demons. And what they are screaming about isn’t players on the field, but rather an image. Specifically, each of these demons has its own preferred letter of the alphabet and when it sees something that looks like that letter on the field it shrieks. The louder the shriek the more the image on the field looks like the demon’s favourite letter. Up in the skybox is another demon. This one doesn’t look at the field or do any screaming itself, but merely observes all the other demons in the stadium. It finds the demon shrieking the loudest and determines that the image on the field must be that demon’s favourite letter.</p>
<p class="TXI">This is how Oliver Selfridge described the process of template matching at a 1958 conference. Selfridge was a mathematician, computer scientist and associate director of Lincoln Labs at MIT, a research centre focused on national security applications of technology. Selfridge didn’t publish many papers himself. He never finished his own PhD dissertation either (he did however end up <a id="page_159"></a>writing several children’s books; presumably these contained fewer demons). Despite this lack of academic output, his ideas infiltrated the research community nonetheless, largely due to the circles in which he moved. After earning a bachelor’s degree in mathematics from MIT at only 19 years old, Selfridge was advised in his PhD work by the notable mathematician Norbert Wiener and remained in contact with him. Selfridge also went on to supervise Marvin Minsky, the prominent AI researcher from <a href="chapter3.xhtml#chapter3">Chapter 3</a>. And as a graduate student, Selfridge was friends with Warren McCulloch and for a time lived with Walter Pitts (you’ll recall this pair of neuroscientists from <a href="chapter3.xhtml#chapter3">Chapter 3</a> as well). Selfridge benefited from letting his ideas marinate in this social stew of prominent scientists. </p>
<p class="TXI">To map Selfridge’s unique analogy to the concept of template matching, we just have to think of each demon as holding its own grid of numbers that represents the shape of its letter. They multiply their grid with the image and sum up those products (just as described above) and scream at a volume determined by that sum. Selfridge doesn’t give much of an indication as to why he chose to give such a demonic description of visual processing. His only reflection on it is to say: ‘We are not going to apologise for a frequent use of anthropomorphic or biomorphic terminology. They seem to be useful words to describe our notions.’<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p><a id="page_160"></a>
<p class="TXI">Most of the notions in Selfridge’s presentation were actually about how the template matching approach is flawed. The demons – each individually checking if their favourite letter was in sight – weren’t being very efficient. They each performed their own completely separate computations, but it didn’t have to be that way. Many of the shapes that a demon may look for in its search for its letter are also used by other demons. For example, both the ‘A’-preferring demon and the ‘H’-preferring demon would be on the lookout for a horizontal bar. So why not introduce a separate group of demons, ones whose templates and screams correspond to more basic features of the image such as horizontal bars, vertical lines, slanted lines, dots, etc. The letter demons would then just listen to those demons rather than look at the images themselves and decide how much to scream based on whether the basic shapes of their letter are being yelled about. </p>
<p class="TXI">From bottom to top Selfridge defined a new style of stadium that contained three types of demon: the ‘computational’ (those that look at the image and yell about basic shapes), ‘cognitive’ (those that listen to the computational demons and yell about letters) and ‘decision’ (the one that listens to the cognitive demons and decides which letter is present). The name Selfridge gave to the model as a whole – to this stack of shrieking demons – was Pandemonium.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup></p>
<p class="TXI">Nefarious nomenclature aside, Selfridge’s intuitions on visual processing proved insightful. While conceptually <a id="page_161"></a>simple, template matching is practically challenging. The number of templates needed grows with the number of objects you want to be able to spot. If each image needs to be compared to every filter, that’s a lot of calculations. Templates also need to more or less match the image exactly. But because of the myriad different patterns of light that the same object can produce on a retina or camera lens, it’s nearly impossible to know how every pixel in an image should look when a given object is present. This makes templates very hard to design for any but the simplest of patterns. </p>
<p class="TXI">These issues make template matching a challenge both for artificial visual systems and for the brain. The ideas on display in Pandemonium, however, represent a more distributed approach, as the features detected by the computational demons are shared across cognitive demons. The approach is also <span class="italic">hierarchical</span>. That is, Pandemonium breaks the problem of vision into two stages: first look for the simple things, then for the more complex. </p>
<p class="TXI">Together, these properties make the system more flexible overall. If Pandemonium was set up to recognise the letters of the first half of the alphabet, for example, it would be in a pretty good position to recognise the rest. This is because the low-level computational demons would already know what kinds of basic shapes letters are made of. The cognitive demon for a new letter would just need to figure out the right way to listen to the demons below it. In this way, the elementary features work like a vocabulary – or a set of building blocks – that can be combined and recombined to detect additional complex patterns. Without this hierarchical structure and sharing of low-level features, a basic <a id="page_162"></a>template-matching approach would need to produce a new template for each letter from scratch.</p>
<p class="TXI">The design of Pandemonium does pose some questions. For example, how does each computational demon know what basic shape to scream about? And how do the cognitive demons know to whom they should listen? Selfridge proposes that the system learn the answers to these questions through trial and error. If, for example, adjusting how the ‘A’-preferring demon listens to those below it makes it better at detecting ‘A’s, then keep those changes; otherwise don’t and try something new. Or, if adding a computational demon to scream about a new low-level pattern makes the whole system better at letter detection, then that new demon stays; otherwise it’s out. This is an arduous process of course and it’s not guaranteed to work, but when it does it has the desirable effect of creating a system that is customised – automatically – for the type of objects it needs to detect. The strokes that comprise the symbols of the Japanese alphabet, for example, differ from those in the English alphabet. A system that learns would discover the different basic patterns for each. No prior or special knowledge needed, just let the model take a stab at the task.</p>
<p class="TXI">Computer scientist Leonard Uhr was impressed enough with the ideas of Selfridge and colleagues that he wanted to share their work more broadly. In 1963, he wrote in the <span class="italic">Psychological Bulletin</span> to an audience of psychologists about the strides computer scientists were making on the vision front. In his article, ‘“Pattern recognition” computers as models for form perception’, he indicates that the models of the time were ‘actually already in the position of suggesting physiological and <a id="page_163"></a>psychological experiments’ and even warns that ‘it would be unfortunate if psychologists did not play any part in this theoretical development of their own science’. The article is concrete evidence of the intertwined relationship the two fields have always had. But such explicit public pleas for collaboration weren’t always needed. Sometimes personal relationships were enough.</p>
<p class="TXI">Jerome Lettvin was a neurologist and psychiatrist from Chicago, Illinois. He was also a friend of Selfridge, having shared a house with him and Pitts as a young man. A self-described ‘overweight slob’, Lettvin wanted to be a poet but appeased his mother’s wishes and became a doctor instead. The most rebellion he managed was to occasionally abandon his medical practice to engage in some scientific research.</p>
<p class="TXI">Inspired by the work of his friend and former cohabitant, in the late 1950s Lettvin set out to search for neurons that responded to low-level features – that is, to the types of things computational demons would scream about. The animal he chose to look at was the frog. Frogs use sight mostly to make quick reflexive responses to prey or to predators and as a result their visual system is relatively simple. </p>
<p class="TXI">Inside the retina, individual light-detecting photore­ceptors send their information to another population of cells called ganglion cells. Each photoreceptor connects to many ganglion cells and each ganglion cell gets inputs from many photoreceptors. But, crucially, all these inputs come from a certain limited region of space. This makes a ganglion cell responsive only to light that hits the retina in that specific location – and each cell has its own preferred location.</p> <a id="page_164"></a>
<p class="TXI">At this point in time, ganglion cells were assumed not to do much computation themselves. They were thought of mostly as a relay – just sending information about photoreceptor activity along to the brain like a mail carrier. Such a picture would fit within a template-matching view of visual processing. If the role of the brain was to compare visual information from the eye to a set of stored templates, it wouldn’t want that information distorted in any way by the ganglion cells. But if the ganglion cells were part of a hierarchy – where each level played a small role in the eventual detection of complex objects – they should be specialised to detect useful elementary visual patterns. Rather than relaying information verbatim, then, they should be actively processing and repackaging it.</p>
<p class="TXI">Lettvin found – by recording the activity of these ganglion cells and showing all kinds of moving objects and patterns to the frog – that the hierarchy hypothesis was true. In fact, in a 1959 paper ‘What the frog’s eye tells the frog’s brain’ he and his co-authors describe four different types of ganglion cells that each responded to a different simple pattern. Some responded to swift large movements, others to when light turned to dark and still others to curved objects that jittered about. These different categories of responses proved that the ganglion cells were specifically built to detect different elementary patterns. Not only did these findings align with Selfridge’s notions of low-level feature detectors, but they also supported the idea that these features are specific to the type of objects the system needs to detect. For example, the last class of cells responded best when a small dark object moved quickly in fits and starts around a fixed <a id="page_165"></a>background. After describing these in the paper, Lettvin remarked: ‘Could one better describe a system for detecting an accessible bug?’ </p>
<p class="TXI">Selfridge’s intuitions were proving to be correct. With Lettvin’s finding in frogs, the community started to conceive of the visual system more as a stack of screaming demons and less as a store of template cards.</p>
<p class="center">* * *</p>
<p class="TXT">Around the same time as Lettvin’s work, two doctors at the John Hopkins University School of Medicine in Baltimore were exploring vision in cats. A cat’s visual system is more like ours than a frog’s. It is tasked with challenging problems related to tracking prey and navigating the environment and is, as a result, more elaborate. The work of the cat visual system is thus stretched over many brain areas and the one that doctors David Hubel<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> and Torsten Wiesel focused on was the primary visual cortex. This region at the back of the brain represents one of the earlier stages of visual processing in mammals; it gets its input from another brain area – the thalamus – that gets input from the retina itself. </p>
<p class="TXI">Previous work had investigated how the neurons in the thalamus and retina of cats behave. These cells tend to respond best to simple dots: either a small area of light surrounded by dark or a small area of dark surrounded <a id="page_166"></a>by light. And, as in the frog, each neuron has a specific location the dot needs to be in for it to respond. </p>
<p class="TXI">Hubel and Wiesel had access to equipment for producing dots at different locations in order to explore such retinal responses. So, this is the equipment they used, even as they investigated brain areas well beyond the retina. The method for displaying dots included sliding a small glass or metal plate with different cut-out patterns over a screen in front of the eye. Hubel and Wiesel used this to show slide after slide of dots to their feline subject as they measured the activity of a neuron in its primary visual cortex. But the dots simply didn’t do it for this neuron – the cell wouldn’t make a peep in response to the slides. Then the experimenters noticed something strange: occasionally the neuron would respond – not to the slides – but to the changing of them. As one plate was slid out and another in, the shadow from the edge of the glass swept across the cat’s retina. This created a moving line that reliably excited the neuron in the primary visual cortex. One of the most iconic discoveries in neuroscience had just occurred, almost by accident. </p>
<p class="TXI">Decades later, reflecting on the serendipity of this discovery, Hubel remarked: ‘In a certain early phase of science a degree of sloppiness can be a huge advantage.’ But that phase quickly passed. By 1960 he and Wiesel had moved their operation to Boston, to help establish the department of neurobiology at Harvard University and embarked on years of careful investigation into the responses of neurons in the visual system. </p>
<p class="TXI">Expanding on their happy accident, Hubel and Wiesel dug deep into how this responsiveness to moving <a id="page_167"></a>lines worked. One of their first findings was that the neurons in the primary visual cortex each have a preferred <span class="italic">orientation</span> in addition to a preferred location. A neuron won’t respond to just any line that shows up in its favourite location. Horizontal-preferring neurons require a horizontal line, vertical-preferring neurons require vertical lines, 30-degree-slant-preferring neurons require 30-degree slanted lines, and so on and so on. To get a sense of what this means, you can hold a pen out horizontally in front of your face and move it up and down. You’ve just excited a group of neurons in your primary visual cortex. Tilt the pen another way and you’ll excite a different group (you’ve now got at-home, targeted brain stimulation for free!).</p>
<p class="TXI">With their realisation about orientation, Hubel and Wiesel had discovered the alphabet used by the cat brain to represent images. Flies have bug detectors and cats (and other mammals) have line detectors. However, they didn’t stop at just observing these responses, they went further to ask how the neurons in the primary visual cortex could come to have such responses. After all, the cells they get their inputs from – those in the thalamus – respond to dots, not lines. So where did the preference for lines come from? </p>
<p class="TXI">The solution was to assume that neurons in the cortex get a perfectly selected set of inputs from the thalamus. A line, of course, is nothing more than a set of appropriately arranged dots. Inputs to a neuron in the primary visual cortex therefore must come from a set of thalamus neurons wherein each one represents a dot in a row of dots. That way, the primary visual neuron would fire the most when a line was covering all those dots (see Figure 15). Just like <a id="page_168"></a>the cognitive demons listening for the shrieks of the demons that look for parts of their letter, neurons in the primary visual cortex listen for the activity of neurons in the thalamus that make up their preferred line. </p>
<p class="TXI">Hubel and Wiesel noticed another kind of neuron, too: ones that also had preferred orientations, but weren’t quite as strict about location. These neurons would respond if a line appeared anywhere in a region that was about four times larger than that of the other neurons they recorded. How could these neurons come to have this response? The answer, again, was to assume they got just the right inputs. In particular, a ‘complex’ neuron – as Hubel and Wiesel labelled these cells – just needed input from a group of regular (or ‘simple’) neurons. All these simple cells should have the same preferred orientations but slightly different preferred locations. That way, a complex cell would inherit the orientation preference of its inputs, but have a spatial preference that is larger than any single one of them. This spatial flexibility is important. If we want to know if the letter ‘A’ is in front of us, a little bit of jitter in the exact location of its lines shouldn’t really matter. Complex cells are built to discard jitter.</p>
<p class="image-fig" id="fig15.jpg">
<img alt="" src="Images/chapter-06-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 15</span>
</span></p>
<p class="TXI">The discovery of complex cells provided an additional piece of the puzzle as to how points of light become perception. In addition to the feature detection done by simple cells, pooling of inputs across space was added to the list of computations performed by the visual system. For all the work they did dissecting this system, Hubel and Wiesel were awarded the Nobel Prize in 1981. In his Nobel lecture, Hubel put their goals plainly: ‘Our idea originally was to emphasise the tendency toward <a id="page_169"></a>increased complexity as one moves centrally along the visual path, and the possibility of accounting for a cell’s behaviour in terms of its inputs.’<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> This approach, while simple, sufficed to capture many of the basic properties of the visual-processing pathway. </p>
<p class="center">* * *</p>
<p class="TXT">On the other side of the world – at Japan’s national public broadcasting organisation, NHK, located in Tokyo – Kunihiko Fukushima heard about the simple properties of the visual system. Fukushima was an engineer and part of the research arm of NHK. Because NHK was a broadcasting company (and was broadcasting visual and audio signals into the eyes and ears of humans) <a id="page_170"></a>it also had groups of neurophysiologists and psychologists on staff to study how sensory signals are received by the brain. These three groups – the psychologists, physiologists and engineers – would meet regularly to share the work of their respective fields. One day, a colleague of Fukushima’s decided to present the work of Hubel and Wiesel. </p>
<p class="TXI">When Fukushima saw this clear description of the roles of neurons in the visual system, he set out to implement the very same functions in a computer model. His model used images of simple white patterns on a black background as input. To approximate the work of the thalamus, a sheet of artificial neurons was created that responded to white dots in the image. This served as a way to get the image information into the network. From here the input to the simple cells needed to be calculated. </p>
<p class="TXI">To do so, Fukushima used the standard approach of making a grid of numbers that represent the to-be-detected pattern – which in the case of a simple cell is a line with a specific orientation. In engineering terms, this grid of numbers is known as a <span class="italic">filter</span>. To mimic the spatial preferences of simple cells, Fukushima applied this filter separately at each location in the image. Specifically, the activity of one simple cell was calculated as the sum of the thalamus activity at one location multiplied by the filter. Sliding the filter across the whole image created a set of simple cells all with the same preferred orientation but different preferred locations. This is a process known in mathematics as a <span class="italic">convolution</span>. </p>
<p class="TXI">By producing multiple filters – each representing a line with a different orientation – and convolving each with the image, Fukushima produced a full population <a id="page_171"></a>of simple cells each with its own preferred orientation and location, just like the brain. For the complex cells, he simply gave them strong inputs from a handful of simple cells that all represented the same orientation in nearby locations. That way they would be active if the orientation appeared in any of those locations. </p>
<p class="TXI">This first version of Fukushima’s model was pretty much a direct translation of the physiological findings of Hubel and Wiesel into mathematics and computer code – and, in a way, it worked. It could do some simple visual tasks like finding curved lines in a black and white image, but it was far from a complete visual system and Fukushima knew that. As he later recounted in an interview, after publishing this work in the late 1960s, Fukushima waited patiently to see what Hubel and Wiesel would discover next; he wanted to know what the later stages of visual processing did so he could add them to his model. </p>
<p class="TXI">But the famous pair of physiologists never provided that information. After their initial work cataloguing cell types, Hubel and Wiesel explored the responses of cells in other visual areas, but were never able to give as clean a description as they had for the primary visual cortex. They eventually moved on to studying how the visual system develops in young animals. </p>
<p class="TXI">Without the script provided by biology, Fukushima needed to improvise. The solution he devised was to take the structure he had – that of simple cells projecting to complex cells – and repeat it. Stacking more simple and complex cells on top of each over and over creates an extended hierarchy that visual information can be passed through. This means, specifically, a second round of ‘simple’ cells comes after the initial layer of complex <a id="page_172"></a>cells. This second layer of simple cells would be on the lookout not for simple features in the image, but rather for simple ‘features’ in the activity of the complex cells from which they get their input. They’d still use filters and convolutions, but just applied to the activity of the neurons below them. Then these simple cells would send inputs to their own complex cells that respond to the same features in a slightly larger region of space – and then the whole process starts again. </p>
<p class="TXI">Simple cells look for patterns; complex cells forgive a slight misplacement of those patterns. Simple, complex; simple, complex. Over and over. Repeating this riff leads to cells that are responsive to all kinds of patterns. For a second-layer simple cell to respond to the letter ‘L’, for example, it just needs to get input from a horizontal-preferring complex cell at one location and a vertical-preferring one at the location just above and to the left of it. A third-layer simple cell could then easily respond to a rectangle by getting input from two appropriately placed ‘L’-cells. Go further and further up the chain and cells start responding to larger and more complex patterns, including full shapes, objects and even scenes. </p>
<p class="TXI">The only problem with extending Hubel and Wiesel’s findings this way was that Fukushima didn’t actually know how the cells in the different layers should connect to each other. The filters – those grids of numbers that would determine how the simple cells at any given layer respond – had to be filled. But how? For this, Fukushima took a page out of Selfridge’s book of Pandemonium and turned to learning. </p>
<p class="TXI">Rather than using the kind of trial and error Selfridge proposed, Fukushima used a version of learning that <a id="page_173"></a>doesn’t require knowing the right answers. In this form of learning, the model is simply shown a series of images without being told what’s in them. The activity of all the artificial neurons is calculated in response to each image and the connections between neurons change depending on how active they are (this may remind you of the Hebbian style of learning discussed in <a href="chapter4.xhtml#chapter4">Chapter 4</a>). If a neuron was very active in response to a particular image, for example, the connections from its very active inputs would be strengthened. As a result, that neuron would respond strongly to that and similar images in the future. This makes neurons responsive to specific shapes and different neurons diverge to have different responses. The network is therefore able to pick out a diversity of patterns in the input images. </p>
<p class="TXI">In the end, Fukushima’s model contained three layers of simple and complex cells, and was trained using computer-generated images of the digits zero to four. He dubbed the network the ‘Neocognitron’ and published the results of it in the journal <span class="italic">Biological Cybernetics</span> in 1980. </p>
<p class="TXI">In their original papers, Hubel and Wiesel made a point of stressing that their classification system and nomenclature was not meant to be taken as gospel. The brain is complicated and dividing neurons into only two categories could in no way capture the full diversity of responses and functions. It was just for convenience and expediency of communication that they proceeded in such a way. Yet Fukushima found success in doing the exact thing Hubel and Wiesel warned against: he <span class="italic">did</span> collapse the brimming complexity of the brain’s visual system into two very simple computations. He did take these descriptions as true, or true enough, and even <a id="page_174"></a>stretched them beyond what they were meant to describe. </p>
<p class="TXI">This practice – of collapsing and then expanding, of shaking the leaves off a tree and using it to build a house – is what all theorists and engineers know to be necessary if progress is to be made. Fukushima wanted to build a functioning visual system in a computer. Hubel and Wiesel provided a description of the brain’s visual system to a first approximation. Sometimes the first approximation is enough. </p>
<p class="center">* * *</p>
<p class="TXT">In 1987, like in any other year, the people of Buffalo, New York, mailed countless bills, birthday cards and letters through their local post office. What the citizens of the town didn’t know, as they inked the 5-digit zip code of the recipient on to their envelope, was that this bit of their handwriting would be immortalised – digitised and stored on computers across the country for years to come. It would become part of a database for researchers trying to teach computers how to read human handwriting and, in turn, revolutionise artificial vision. </p>
<p class="TXI">Some of the researchers working on this project were at Bell Labs, a research company owned by the telecommunications company AT&amp;T, located in suburban New Jersey. Among the group of mostly physicists was a 28-year-old French computer scientist named Yann LeCun. LeCun had read about Fukushima and his Neocognitron, and he recognised how the simple repeating architecture of that model could solve many of the hard problems of vision.</p> <a id="page_175"></a>
<p class="TXI">LeCun also recognised, however, that the way the model learned its connections needed to change. In particular, he wanted to move back towards the approach of Selfridge and give the model access to images paired with the correct labels of which digit is in them. So, he tweaked some of the mathematical details of the model to make it amenable to a different kind of learning. In this type of learning, if the model misclassifies an image (for example, labels a two as a six), all the connections in the model – those grids of numbers that define what patterns are searched for – are updated in a way that makes them less likely to misclassify that image in the future. In this way, the model learns what patterns are important for identifying digits. This may sound familiar because what LeCun used was the backpropagation algorithm described in <a href="chapter3.xhtml#chapter3">Chapter 3</a>. Do this with enough images and the model as a whole becomes quite good at classifying images of handwritten digits, even ones it’s never seen before. </p>
<p class="TXI">LeCun and his fellow researchers unveiled the impressive results of their model, trained on the thousands of Buffalo digits, in 1989. The ‘convolutional neural network’ – the name given to this style of model – was born. </p>
<p class="TXI">Just like the template-matching approaches that came before it, convolutional neural networks found applications in the real world. In 1997, these networks formed a core part of a software system AT&amp;T developed to automate the processing of cheques at banks across America. By 2000, it was estimated that between 10 and 20 per cent of cheques in America were being processed by this software. In a charming example of science fulfilling its destiny, Goldberg’s dream of equipping <a id="page_176"></a>banks with synthetic visual systems came true some 70 years after the invention of his microfilm machine. </p>
<p class="TXI">The method for training convolutional neural networks is a data-hungry one and the model will only learn to be as good as what’s fed into it. Just as important as getting the right model, then, is getting the right data. That is why it was so crucial to collect real samples of real digits written by real people. The Bell Lab researchers could’ve done as Fukushima did and made computer-generated images of numbers. But those would hardly capture the diversity, the nuance or the sloppiness in how digits are written in the wild. The letters that passed through the Buffalo post office contained nearly 10,000 examples of true, human handwriting, giving the model what it needed to truly learn. Once computer scientists saw the importance of real data, they were spurred to collect even more. A dataset of six times as many digits – named MNIST – was collected shortly after the Buffalo set. Surprisingly, this dataset remains one of the most commonly used for quickly testing out new models and algorithms for artificial vision. The digits for MNIST were written by Maryland high school students and US census takers.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> And while the writers <span class="italic">were</span> told what their digits were being used for in this case, they almost certainly wouldn’t have expected their handwriting to still be used by computer scientists some 30 years later. </p>
<p class="TXI">Tests of convolutional neural networks didn’t stop at digits, but when making the jump to more involved images they hit a snag. In the early 2000s, networks much like LeCun’s were trained on another dataset of 60,000 images, <a id="page_177"></a>this time made of objects. The images were small and grainy – only 32x32 pixels – and could be of either airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships or trucks. While still a simple task for us, this marked a serious increase in difficulty for the networks. The full ambiguity inherent in resolving a three-dimensional world from two-dimensional input comes into play when real images of real objects are used. The same models that could learn to recognise digits struggled to make sense of these more realistic pictures. This brain-like approach to artificial vision was failing at the basic visual processing brains do every day. </p>
<p class="TXI">A tide turned, however, in 2012 when Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton from the University of Toronto used a convolutional neural network to win a major image-recognition contest known as the ImageNet Large Scale Visual Recognition Challenge. The contest consisted of labelling images – large (224x224 pixels), real-life pictures taken by people around the world and pulled from image-hosting websites like Flickr – as belonging to one of a thousand different possible object categories. On this very convincing test of visual skill, the convolutional neural network got 62 per cent correct, beating the second-place algorithm by 10 percentage points.</p>
<p class="TXI">How did the Toronto team do so well? Did they discover some new computation needed for vision? Did they find a magical technique to help the model learn its connections better? The truth, in this case, is actually much more banal. The difference between this convolutional neural network and the ones that came before it was mainly one of size. The Toronto team’s network had a total of over 650,000 artificial neurons in it – about 80 times the size of LeCun’s digit-recognising <a id="page_178"></a>one. This network was so large, in fact, that it required some clever engineering to fit the model in the memory of the computer chips that were used to run it. The model went big in another way, too. All those neurons meant a lot more data was needed to train the connections between them. The model learned from 1.2 million labelled images collected by computer science professor Fei-Fei Li as part of the ImageNet database. </p>
<p class="TXI">A watershed year came in 2012 for convolutional neural networks. While the Toronto team’s advances were technically just a quantitative leap – upping the number of neurons and images – the stunning performance enhancement made a qualitative difference in the field. After seeing what they were capable of, researchers flocked to study convolutional neural networks and to make them even better. This usually went in the same direction: making them bigger, but important tweaks to their structure and to how they learned were found as well. </p>
<p class="TXI">By 2015, a convolutional neural network reached the level of performance expected of a human in the image-classification competition (which isn’t actually 100 per cent; some of the images can be confusing). And convolutional neural networks now form the base of almost any image-processing software: facial recognition on social media, pedestrian detection in self-driving cars and even automatic diagnosis of diseases in X-ray images. In an amusing bending of science in on itself, convolutional neural networks have even been used <span class="italic">by neuroscientists</span> to help automatically detect where neurons are in pictures of brain tissue. Artificial neural networks are now looking at real ones. </p>
<p class="TXI">It would seem that engineers made a smart move turning to the brain for inspiration on how to build a <a id="page_179"></a>visual system. Fukushima’s attention to the functions of neurons – and his condensing of those functions into simple operations – has paid dividends. But when he was taking the first steps in the development of these models, the computing resources and the data to make them shine simply wasn’t available. Decades later, the next generation of engineers picked up the project and brought it across the finish line. As a result, current convolutional neural networks can finally do many of the tasks originally asked for by the MIT summer project in 1966. </p>
<p class="TXI">But just as Selfridge’s Pandemonium helped inspire visual neuroscientists, the relationship between convolutional neural networks and the brain does not go only one way. Neuroscientists have come to reap rewards from the effort computer scientists put into making models that can solve real visual problems. That’s because not only are these large, heavily trained convolutional neural networks good at spotting objects in images, they’re also good at predicting how the brain will respond to those same images. </p>
<p class="center">* * *</p>
<p class="TXT">Visual processing gets started in the primary visual cortex – where Hubel and Wiesel did their recordings – but many areas are involved after that. The primary visual cortex sends connections to (you guessed it) the secondary visual cortex. And after a few more relays, the information ends up in the temporal cortex, located just behind the temples. </p>
<p class="TXI">The temporal cortex has been associated with object recognition for a long time. As early as the 1930s, researchers noticed that damage to this brain area leads to some strange behaviour. Patients with temporal cortex damage are bad <a id="page_180"></a>at deciding what things are important to look at and so get easily distracted. They also don’t show normal emotional responses to images; they can see pictures most people would find terrifying and hardly blink. And when they want to explore objects, they may do so not by looking at them but by putting them in their mouths. </p>
<p class="TXI">Understanding of this brain area was refined through decades of careful observation of patients or animals with brain lesions and eventually by recording the activity of its neurons. This led to the conclusion that a subpart of the temporal cortex – the ‘inferior’ part at the bottom, also called ‘IT’ – is the main location for object understanding. People with damage to IT have mostly normal behaviour and vision, but with the more specific problem of being unable to appropriately name or recognise objects; they may, for example, fail to recognise the faces of friends or confuse the identity of objects that appear similar.</p>
<p class="TXI">Accordingly, the neurons in this area respond to objects. Some neurons have clear preferences; one may fire when a clock is present, another for a house, another for a banana, <span class="italic">etc.</span> But other cells are less scrutable. They may prefer portions of objects or respond similarly to two different objects that have some features in common. Some cells also care about the angle the object is seen from, perhaps firing most if the object is seen straight on, but others are more forgiving and respond to an object at almost any angle. Some care about the size and location of the object, others don’t. In total, IT is a grab-bag of neurons interested in objects. While they’re not always easy to interpret, such object-driven responses make IT seem like the top of the visual-processing hierarchy, the last stop on the visual system express.</p> <a id="page_181"></a>
<p class="TXI">Neuroscientists have tried for decades to understand exactly how IT comes to have these responses. Frequently they followed in the footsteps of Fukushima and built models with stacks of simple and complex cells, hoping that these computations would mimic those that lead to IT activity and make the activity perfectly predictable. This approach worked to an extent, but just like with the Neocognitron, the models were small and they learned their connections from a small set of small images. To make real progress, neuroscientists needed to scale up their models just the same way computer scientists did.</p>
<p class="TXI">In 2014, two separate groups of scientists – one led by Nikolaus Kriegeskorte</p>
<p class="TXI">at Cambridge University and one by James DiCarlo at MIT – did exactly that. They showed real and diverse images of objects to subjects (humans and monkeys) and recorded the activity of different areas of their visual systems as they viewed them. They also showed the same images to a large convolutional neural network trained to classify real images. What both groups found was that these computer models provided a great approximation to biological vision. Particularly, they showed that if you want to guess how a neuron in IT will respond to a specific image, the best bet – better than any previous method neuroscientists had tried – was to look at how the artificial neurons in the network responded to it. Specifically, the neurons in the last layer of the network best predicted the activity of IT neurons. What’s more, the neurons in the second-to-last layer best predicted the activity of neurons in V4 – the area that gives input to IT. The convolutional neural network, it seemed, was mimicking the brain’s visual hierarchy.</p> <a id="page_182"></a>
<p class="TXI">By showing such a striking alignment between the model and the brain, this research ushered in a revolution in the study of biological vision. It demonstrated that neuroscientists were broadly on the right track, a track that started with Lettvin and Hubel and Wiesel, but that they needed to be bigger and bolder. If they wanted a model that could explain how animals see objects, the model itself needed to be able to see objects. </p>
<p class="TXI">Going this way, though, symbolised an abandonment of principles that some theorists hold dear: a striving for elegance, simplicity and efficiency in models. There’s nothing elegant or efficient about 650,000 artificial neurons wired up in whatever way they found to work. Compared to some of the most beloved and beautiful equations in science, these networks are hulking, unsightly beasts. But, in the end, they work – and there is no guarantee that anything more elegant will. </p>
<p class="TXI">Selfridge’s work pushed biologists to see the visual system as a hierarchy and the experiments that resulted from this planted the seeds for the design of convolutional neural networks. These seeds were incubated in computer science and, in the end, the collaboration yielded fruit for both sides. In general, the desire for artificial systems that can do real visual tasks in the real world has pushed the study of biological vision in directions it may not have gone on its own. Engineers and computer scientists have always enjoyed having the brain’s visual system to look to – not only for inspiration, but for proof that this challenging problem is solvable. This mutual appreciation and influence makes the story of the study of vision a uniquely interwoven one.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-1" id="fn-1">1</a> ﻿That professor would be Marvin Minsky and the professor that wrote the memo was Seymour Papert, both key participants in ﻿﻿Chapter 3﻿﻿. Indeed, as you﻿’﻿ll see, there are many overlapping players and themes in the histories of artificial neural networks and artificial vision.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-2" id="fn-2">2</a> ﻿Primates are admittedly fairly unusual in this sense. Rodent brains, for example, lean more towards processing smell.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-3" id="fn-3">3</a> ﻿Actually, pixels in colour images are defined by three numbers corresponding to the intensities of the red, green and blue components. For simplicity, we﻿’﻿ll speak of pixels as being a single number, despite the fact that this is only true for grayscale images.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-4" id="fn-4">4</a> ﻿Though in response to a colleague﻿’﻿s remark on it, Selfridge commented: ﻿‘﻿All of us have sinned in Adam, we have eaten of the tree of the knowledge of good and evil, and the demonological allegory is a very old one, indeed.﻿’﻿﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-5" id="fn-5">5</a> ﻿From the Greek for ﻿‘﻿all the demons﻿’﻿, introduced in John Milton﻿’﻿s ﻿<span class="italic">Paradise Lost</span>﻿.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-6" id="fn-6">6</a> ﻿Hubel was actually quite interested in mathematics and physics, and was accepted into a PhD programme in physics at the same time he was accepted to medical school. Truly torn, he waited until the last possible day to make the choice.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-7" id="fn-7">7</a> ﻿Hubel and Wiesel did not, however, mention Lettvin or his pioneering work in the frog during this speech. This was an omission Selfridge referred to as ﻿‘﻿rotten manners, putting it very mildly﻿’﻿.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-8" id="fn-8">8</a> ﻿You can guess who had the neater handwriting.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<meta charset="utf-8"/>
<title>Contents</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="FMT" id="re_contents">Contents</p>
<p class="TOC-CH"><a href="chapter1.xhtml#chapter1" id="re_chapter1">Chapter 1:  Spherical Cows</a></p>
<p class="TOC-CH"><a href="chapter2.xhtml#chapter2" id="re_chapter2">Chapter 2:  How Neurons Get Their Spike</a></p>
<p class="TOC-CH"><a href="chapter3.xhtml#chapter3" id="re_chapter3">Chapter 3:  Learning to Compute</a></p>
<p class="TOC-CH"><a href="chapter4.xhtml#chapter4" id="re_chapter4">Chapter 4:  Making and Maintaining Memories</a></p>
<p class="TOC-CH"><a href="chapter5.xhtml#chapter5" id="re_chapter5">Chapter 5:  Excitation and Inhibition</a></p>
<p class="TOC-CH"><a href="chapter6.xhtml#chapter6" id="re_chapter6">Chapter 6:  Stages of Sight</a></p>
<p class="TOC-CH"><a href="chapter7.xhtml#chapter7" id="re_chapter7">Chapter 7:  Cracking the Neural Code</a></p>
<p class="TOC-CH"><a href="chapter8.xhtml#chapter8" id="re_chapter8">Chapter 8:  Movement in Low Dimensions</a></p>
<p class="TOC-CH"><a href="chapter9.xhtml#chapter9" id="re_chapter9">Chapter 9:  From Structure to Function</a></p>
<p class="TOC-CH"><a href="chapter10.xhtml#chapter10" id="re_chapter10">Chapter 10:  Making Rational Decisions</a></p>
<p class="TOC-CH"><a href="chapter11.xhtml#chapter11" id="re_chapter11">Chapter 11:  How Rewards Guide Actions</a></p>
<p class="TOC-CH"><a href="chapter12.xhtml#chapter12" id="re_chapter12">Chapter 12:  Grand Unified Theories of the Brain</a></p>
<p class="TOC-CH1"><a href="Mathematical.xhtml#Mathematical" id="re_Mathematical">Mathematical Appendix</a></p>
<p class="TOC-CH"><a href="ack.xhtml#ack" id="re_ack">Acknowledgements</a></p>
<p class="TOC-CH"><a href="bib.xhtml#bib" id="re_bib">Bibliography</a></p>
<p class="TOC-CH"><a href="index.xhtml#index" id="re_index">Index</a></p>
</body>
</html>