<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 7</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter7"><a href="contents.xhtml#re_chapter7">CHAPTER SEVEN</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter7">Cracking the Neural Code</a><a id="page_7"></a></p>
<p class="H1"><span class="bold"><span>Information theory and efficient coding</span></span></p>
<p class="EXTF">Whereas <span class="italic">the heart pumps blood and the lungs effect gas exchange, whereas the liver processes and stores chemicals and the kidney removes substances from the blood, the nervous system processes information.</span></p>
<p class="right">Summary of Neurosciences Research 
Program work session, 1968</p>
<p class="TXT-con">The goal of the 1968 Neurosciences Research <a id="page_183"></a>Program meeting was to discuss how individual and groups of neurons process information. The meeting’s summary, written by neuroscientists Theodore Bullock and Donald Perkel, does not push for any hard and fast conclusions. But it does lay out a wide world of possibilities for the representation, transformation, transmission and storage of information in the brain in a way that summarised the state of the field.</p>
<p class="TXI">As the quote from their summary implies, ascribing the role of information processing to the brain seems as natural as saying the heart pumps blood. Even before ‘information’ became a part of everyday vocabulary in the twentieth century, scientists still spoke implicitly of the information that nerves convey, often in the language of ‘messages’ and ‘signals’. An 1892 lecture to hospital employees, for example, explains that: ‘There are fibres <a id="page_184"></a>which convey messages from the various parts of the body to the brain’ and that some of these fibres ‘carry special kinds of messages as, for example, the nerves connected with the organs of special sense, which have been called the gateways of knowledge’. In the same vein, an 1870 publication describes the firing of motor neurons as ‘a message of the will to the muscle’ and even goes so far as to equate the nervous system with the dominant information-transmitting technology of the day: the telegraph. </p>
<p class="TXI">But the investigation into how the nervous system represents information only started in earnest about 40 years before Bullock and Perkel’s report, with the work of Edgar Adrian in the early twentieth century.</p>
<p class="TXI">Adrian was in many ways the image of a prim and proper man of science. By the time he was born in London in 1889, his family had been in England for more than 300 years – a lineage that included a sixteenth-century surgeon and several reverends and members of government. As a student, his brilliance was regularly acknowledged by his teachers. In addition to his focus on medicine during his university studies, he displayed skill in art, particularly painting and drawing. As a lecturer at Cambridge, he worked long hours in the lab and in the classroom. In his career as a physiologist, he was an undeniable success. At the age of 42 he won a Nobel Prize and in 1955 he was granted a title by Queen Elizabeth II, becoming Lord Adrian. </p>
<p class="TXI">But behind these formal awards and accolades was a restless and chaotic man. Adrian was a thrill-seeker who liked climbing mountains and driving fast cars. He was happy to experiment on himself, including keeping a <a id="page_185"></a>needle in his arm for two hours to try to measure muscle activity. He was known to play elaborate games of hide-and-seek with fellow students in the valleys of England’s Lake District. As a professor he was equally elusive. He avoided unscheduled meetings by hiding in his lab, forcing any enquiring students to try to catch him on his bike ride home. He was temperamental and when he needed to think he’d perch himself on a shelf in a dark cabinet. His lab mates and his family described his movements as rapid, jerky and almost constant. His mind could be equally darting. Over the course of his career he studied many different questions in many different animals: vision, pain, touch and muscle control in frogs, cats, monkeys and more. </p>
<p class="TXI">This inability to remain still, physically or mentally, may have been a key to his success. Through his varied studies on the activity of single nerves he was able to find certain general principles that would form the core of our understanding of the nervous system as a whole. In his 1928 book, <span class="italic">The Basis of Sensation</span>, Adrian explains his conclusions and the experiments that allowed him to reach them. The pages are peppered with talk of ‘signals’, ‘messages’ and even ‘information’, all mixed in with anatomical details of the nervous system and the technical challenges of capturing its activity. It was a mix of experimental advances and conceptual insights that would influence the field for decades to come.</p>
<p class="TXI">In <a href="chapter3.xhtml#chapter3">Chapter 3</a>, Adrian explains an experiment wherein he adds weight to a frog’s muscle to see how the ‘stretch’ receptors that track the muscle’s position would respond. Adrian recorded from the nerves that carry this signal from the receptors to the spinal cord. After applying <a id="page_186"></a>different weights, Adrian summarised his findings as follows: ‘The sensory message which travels to the central nervous system when a muscle is stretched … consists of a succession of impulses of the familiar type. The frequency with which the impulses recur depends on the strength of the stimulus, but the size of each impulse does not vary.’ This finding – that the size, shape or duration of an action potential emitted by these sensory neurons does not change, no matter how heavy or light the weight applied to the muscle is – Adrian referred to as the ‘all-or-nothing’ principle.</p>
<p class="TXI">Examples of the ‘all-or-nothing’ nature of neural impulses reappear throughout the book. In different species, for different nerves carrying different messages, the story is always the same. Action potentials don’t change based on the signal they’re conveying, but their frequency can. The spikes of a neuron are thus like an army of ants – each as identical as possible, their power coming mainly from their numbers. </p>
<p class="TXI">If the nature of an individual action potential is the same regardless of the strength or weakness of the sensory stimulus causing it, then one thing is certain: the size of the action potential does not carry information. With the contributions from Adrian, physiologists now felt comfortable embarking on a search for where exactly information was in nerves and how it got transmitted. </p>
<p class="TXI">There was only one problem: what <span class="italic">is</span> information? The blood that the heart pumps and the gases that lungs exchange are real, physical substances. They’re observable, tangible and measurable. For as commonly as we use it, ‘information’ is actually a rather vague and elusive concept. A precise definition of the word does not easily <a id="page_187"></a>come to mind for most people; it falls unfortunately into the ‘know it when you see it’ camp. Without a way to weigh information the way we can weigh fluids or gases, what hope could scientists have for a quantitative understanding of the brain’s central purpose?</p>
<p class="TXI">Between the time of Adrian’s book and Perkel and Bullock’s report, however, a quantitative definition of information had been found. It was born out of the scientific struggles of the Second World War and went on to transform the world in unexpected ways. Its application to the study of the brain was at times as rocky to execute as it was obvious to attempt. </p>
<p class="center">* * *</p>
<p class="TXT">Claude Shannon started at Bell Labs under a contract provided by the American military. It was 1941 and the National Defense Research Committee wanted scientists working on wartime technology. The seriousness of the work didn’t dampen Shannon’s naturally playful tendencies, though. He enjoyed juggling and while at Bell Labs was known to juggle around campus while riding a unicycle. </p>
<p class="TXI">Born in a small town in the American Midwest, Shannon grew up following his curiosity about all things science, mathematics and engineering anywhere it took him. As a child he played with radio parts and enjoyed number puzzles. As an adult he created a mathematical theory of juggling and a flame-powered Frisbee. He enjoyed chess and building machines that could play chess. A constant tinkerer, he made many gadgets, some more productive than others. On his desk at Bell Labs, <a id="page_188"></a>for example, he kept an ‘Ultimate Machine’: a box with a switch that, when flipped on, caused a mechanical hand to reach out and flip it back off.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> </p>
<p class="TXI">For his master’s degree, Shannon wrote a 72-page thesis entitled ‘A symbolic analysis of relay and switching circuits’ that would revolutionise electrical engineering. For his PhD, he turned his mathematical eye toward biology, working on ‘An Algebra for Theoretical Genetics’. But his topic at Bell Labs was cryptography. How to safely encode messages that would be transmitted through land, air and water was a natural topic of concern for the military. Bell Labs was a hub of cryptography research and even hosted the renowned code-cracker Alan Turing during Shannon’s time there. </p>
<p class="TXI">All this work on codes and messages got Shannon thinking broadly about the concept of communication. During the war, he proposed a method for understanding message-sending mathematically. Because of the necessary secrecy around cryptography research, however, his ideas were kept classified. In 1948, Shannon was finally able to publish the work and ‘A mathematical theory of communication’ became the founding document of a new field: information theory. </p>
<p class="TXI">Shannon’s paper describes a very generic communication system consisting of five simple parts. The first is an information <span class="italic">source</span>, which produces the message that will be sent. The next is the <span class="italic">transmitter</span>, <a id="page_189"></a>which is responsible for encoding the message into a form that can be sent across the third component, the <span class="italic">channel</span>. On the other end of the channel, a <span class="italic">receiver</span> decodes the information back into its original form and it is sent to its final <span class="italic">destination</span> (Figure 16). </p>
<p class="TXI">In this framework, the medium of the message is irrelevant. It could be songs over radio waves, words on a telegraph or images through the internet. As Shannon says, the components of his information-sending model are ‘suitably idealised from their physical counterparts’. This is possible because, in all of these cases, the fundamental problem of communication remains the same. It is the problem of ‘reproducing at one point either exactly or approximately a message selected at another point’.</p>
<p class="image-fig" id="fig16.jpg">
<img alt="" src="Images/chapter-07-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 16</span>
</span></p>
<p class="TXI">With this simple communication system in mind, Shannon aimed to formalise the study of information transmission. But, to mathematically approach the question of how information is communicated, he first had to define information mathematically. Building on previous work, Shannon discusses what desirable properties a measure of information should have. Some are practical: information shouldn’t be negative, for example, and its definition should be easy to work with mathematically. But the real constraint came from the need to capture an intuition about information: its reliance on a code.</p> <a id="page_190"></a>
<p class="TXI">Imagine a school where all students wear uniforms. Seeing a student show up in the same outfit every day provides very little information about their mood, their personality or the weather. On the other hand, in a school without uniforms, clothing choice has the ability to convey all this information and more. To someone wondering about the current temperature, for example, seeing a student in a sundress instead of a sweater can go a long way towards relieving that curiosity. In this way, clothing can be used as a code – it is a transmittable set of symbols that conveys meaning.</p>
<p class="TXI">The reason the uniformed students can’t carry this information is that a code requires options. There need to be multiple symbols in a code’s vocabulary (in this case, multiple outfits in a student’s wardrobe), each with their own meaning, for any of the symbols to have meaning. </p>
<p class="TXI">But it’s not just the number of symbols in a code that matters – it’s also how they’re used. Let’s say a student has two outfits: jeans and a T-shirt or a suit. If the student wears jeans and a T-shirt 99 per cent of the time, then there is not much information that can be gained from this wardrobe choice. You wouldn’t even need to see the student to be almost certain of what they’re wearing – it’s essentially a uniform. But the one day in a hundred where they show up in a suit tells you something important. It lets you know that day is somehow special. What this shows is that the rarer a symbol’s use, the more information it contains. Common symbols, on the other hand, can’t communicate much.</p>
<p class="TXI">Shannon wanted to capture this relationship between a symbol’s use and its information content. He therefore <a id="page_191"></a>defined a symbol’s information content in terms of the probability of it appearing. Specifically – to make the amount of information decrease as the probability of the symbol increases – he made a symbol’s information depend on the <span class="italic">inverse</span> of its probability. Because the inverse of a number is simply one divided by that number, a higher probability means a lower ‘inverse probability’. In this way, the more frequently the symbol is used, the lower its information will be. Finally, to meet his other mathematical constraints, he took the logarithm of this value. </p>
<p class="TXI">A logarithm, or ‘log’, is defined by its <span class="italic">base</span>. To take the base-10 log of a number, for example, you would ask: ‘To what power must I raise 10 in order to get this number?’ The base-10 log of 100 (written as log<sub>10</sub>100), is therefore 2, because 10 to the power of 2 (<span class="italic">i.e.</span>, 10x10) is 100. The base-10 log of 1,000 is thus 3. And the base-10 log of something in between 100 and 1,000 is in between 2 and 3. </p>
<p class="TXI">Shannon decided to use a base of <span class="italic">two</span> for his definition of information. To calculate the information in a symbol you must therefore ask: ‘To what power must I raise two in order to get the inverse of the symbol’s probability?’ Taking our student’s outfit of jeans and T-shirt as a symbol that appears with 0.99 probability, its information content is log<sub>2</sub>(1/0.99), which comes out to about 0.014. The suit that appears with only 0.01 probability, on the other hand, has an information content of log<sub>2</sub>(1/0.01) or roughly 6.64. Again, the lower the probability, the higher the information.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup></p><a id="page_192"></a>
<p class="TXI">But Shannon was interested in more than just the information in a single symbol – he wanted to study the information content of a code. A code is defined by its set of symbols and how frequently each is used. Shannon therefore defined the total information in a code as the sum of the information of all its symbols. Importantly, this sum is weighted – meaning the information from each symbol is multiplied by how frequently that symbol is used. </p>
<p class="TXI">Under this definition, the student’s clothing code would have a total information content of 0.99 x 0.014 (from the jeans and T-shirt) + 0.01 x 6.64 (from the suit) = 0.081. This can be thought of as the average amount of information we would receive each day by seeing the student’s outfit. If the student chose instead to wear their jeans 80 per cent of the time and their suit the other 20 per cent, their code would be different. And the average information content would be higher: 0.80 x log<sub>2</sub>(1/0.80) + 0.20 x log<sub>2</sub>(1/0.20) = 0.72. </p>
<p class="TXI">Shannon gave the average information rate of a code a name. He called it entropy. The official reason he gives for this is that his definition of information is related to the concept of entropy in physics, where it serves as a measure of disorder. On the other hand, Shannon was also known to claim – perhaps jokingly – that he was advised to call his new measure entropy because ‘no one understands entropy’ and therefore Shannon would likely always win arguments about his theory.</p>
<p class="TXI">Shannon’s entropy captures a fundamental trade-off inherent in maximising information. Rare things carry the most information, so you want as much of them as possible in your code. But the more you use a rare <a id="page_193"></a>symbol, the less rare it becomes. This fight fully defines the equation for entropy: decreasing the probability of the symbol makes the log of its inverse go up – a positive contribution to the information. But this number is then multiplied by that very same probability: this means that decreasing a symbol’s probability makes its contribution to information go down. To maximise entropy, then, we must make rare symbols as common as possible but no commoner. </p>
<p class="TXI">Shannon’s use of a base-two log makes the unit of information the <span class="italic">bit</span>. Bit is short for binary digit and, while Shannon’s paper sees the first known use of the word, he did not coin it (Shannon credits his Bell Labs colleague John Tukey with that honour).<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> The bit as a unit of information has a helpful and intuitive interpretation. Specifically, the average number of bits in a symbol is equal to the number of yes-or-no questions you need to ask in order to get that amount of information. </p>
<p class="TXI">For example, consider trying to find out the season in which someone was born. You may start by asking: ‘Is it a transitional season?’ If they say yes, you may then ask: ‘Is it spring?’ If they say yes to that, you have your answer; if they say no, you still have your answer: autumn. If they said no to the first question you’d follow the opposite route – asking if they were born in summer, <span class="italic">etc.</span> No matter the answer, it takes two yes-or-no questions to get it. Shannon’s entropy equation agrees. Assuming <a id="page_194"></a>people are equally likely to be born in each season, then each of these season ‘symbols’ will be used 25 per cent of the time. The information in each symbol is thus log<sub>2</sub>(1/0.25). This makes the average bits per symbol equal to two – the same as the number of questions. </p>
<p class="TXI">Part of designing a good communication system is designing a code that packs in a lot of information per symbol. To maximise the average information that a symbol in a code provides, we need to maximise the code’s entropy. But, as we saw, the definition of entropy has an inherent tension. To maximise it, rare symbols need to be the norm. What is the best way to satisfy this seemingly paradoxical demand? This tricky question turns out to have a simple answer. To maximise a code’s entropy, each of its symbols should be used the exact same amount. Have five symbols? Use each one-fifth of the time. A hundred symbols? Each should have a probability of 1/100th. Making each and every symbol equally likely balances the trade-off between rare and common communication. </p>
<p class="TXI">What’s more, the more symbols a code has the better. A code with two symbols, each used half the time, has an entropy of one bit per symbol (this makes sense according to our intuitive definition of a bit: thinking of one symbol as standing in for ‘yes’ and the other for ‘no’, each symbol answers one yes-or-no question). On the other hand, a code with 64 symbols – each used equally – has an entropy of six bits per symbol. </p>
<p class="TXI">As important as a good code is, encoding is only the start of a message’s journey. According to Shannon’s conception of communication, after information is encoded it still needs to be sent through a channel on <a id="page_195"></a>the way to its destination. Here is where the abstract aims of message-sending meet the physical limits of matter and materials.</p>
<p class="TXI">Consider the telegraph. A telegraph sends messages via short pulses of electric current passed over wires. The patterns of pulses – combinations of shorter ‘dots’ and longer ‘dashes’ – define the alphabet. In American Morse code, for example, a dot followed by a dash indicates the letter ‘A’, two dots and a dash means ‘U’. The physical constraints and imperfections of the wires carrying these messages, especially those sent long distances or under oceans, put a limit on the speed of information. Telegraph operators who typed too quickly were at risk of running their dots and dashes together, creating an unintelligible ‘hog Morse’ that would be worthless to the receiver of it. In practice, operators could safely send around 100 letters per minute on average.</p>
<p class="TXI">To create a practical measure of information rate, Shannon combined the inherent information rate of a code with the physical transmission rate of the channel it is sent over. For example, a code that provides five bits of information per symbol and is sent over a channel that can send 10 symbols per minute would have a total information rate of 50 bits per minute. The maximum rate at which information can be sent over a channel without errors is known as the channel’s capacity. </p>
<p class="TXI">Shannon’s publication enforced a clear structure on a notoriously nebulous concept. In this way, it set the stage for the increasing objectification of information in the decades to come. The immediate effects of Shannon’s work on information processing in the real world, however, were slight. It took more than two decades for <a id="page_196"></a>the technology that makes information transmission, storage and processing a constant part of everyday life to come about. And it took engineers time to figure out how to harness Shannon’s theory for practical effect in these devices. Information theory’s impact on biology, however, came much quicker.</p>
<p class="center">* * *</p>
<p class="TXT">The first application of information theory to biology was itself a product of war. Henry Quastler, an Austrian physician, was living and working in the US during the time of the Second World War. His response to the development of the atomic bomb was one of horror – and action. He left his private practice to start doing research on the medical and genetic effects of nuclear bombs. But he needed a way to quantify just how the information encoded in an organism was changed by exposure to radiation. ‘A godsend, these formulas, splendid! I can go on now,’ Quastler is said to have remarked upon learning about Shannon’s theory. He wrote a paper in 1949 – just a year after Shannon’s work was published – entitled ‘The information content and error rate of living things’. It set off the study of information in biology.</p>
<p class="TXI">Neuroscience was not slow to follow. In 1952, Warren McCulloch and physicist Donald MacKay published ‘The limiting information capacity of a neuronal link’. In this paper, they derive what they consider to be the most optimistic estimate of how much information a single neuron could carry. Based on the average time it takes to fire an action potential, the minimum time <a id="page_197"></a>needed in between firing and other physiological factors, MacKay and McCulloch calculated an upper bound of 2,900 bits per second. </p>
<p class="TXI">MacKay and McCulloch were quick to stress that this doesn’t mean neurons actually <span class="italic">are</span> conveying that much information, only that under the best possible circumstances they could. After their paper, many more publications followed, each aiming to work out the true coding capacity of the brain. So awash in attempts was the field that in 1967 neuroscientist Richard Stein wrote a paper both acknowledging the appeal of information theory for quantifying nervous transmission but also lamenting the ‘enormous discrepancies’ that have resulted from its application. Indeed, in the work that followed MacKay and McCulloch’s, estimates ranged from higher than their value – 4,000 bits per second per neuron – to significantly lower, as meagre as one-third of a bit per second. </p>
<p class="TXI">This diversity came, in part, from different beliefs about how the parts and patterns of nervous activity should be mapped on to the formal components of Shannon’s information theory. The biggest question centred on how to define a symbol. Which aspects of neural activity are actually carrying information and which are incidental? What, essentially, is the neural code? </p>
<p class="TXI">Adrian’s original finding – that it is not the height of the spike that matters – still held strong.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> But even under this constraint, options abounded. Starting from the basic <a id="page_198"></a>unit of an action potential, scientists were still able to devise many conceivable codes. MacKay and McCulloch began by thinking of the neural code as composed of only two symbols: spike or no spike. At each point in time a neuron would send one or the other symbol. But after calculating the information rate of such a code, MacKay and McCulloch realised they could do better. Thinking instead of the <span class="italic">time between spikes</span> as the code allowed a neuron to transmit much more information. In this coding scheme, if there were a 20-millisecond gap in between two spikes, this would symbolise something different from a 10-millisecond gap. It’s a scheme that creates many more possible symbols and it was with this style of coding that they made their estimate of 2,900 bits per second. </p>
<p class="TXI">Stein, in his attempt to clean up the cacophony of codes on offer at the time, focused on a third option for neural coding – the one that came from Adrian himself. Adrian, after establishing that action potentials don’t change as the stimulus does, claimed that: ‘In fact, the only way in which the message can be made to vary at all is by a variation in the total number of the impulses and in the frequency with which they recur.’ This style of coding – where it is the number of spikes produced in a certain amount of time that serves as the symbol – is known as frequency or rate-based coding. In his 1967 paper, Stein argues for the existence of a rate-based code and highlights its benefits, including higher error tolerance. </p>
<p class="TXI">But the debate over what the true neural code is did not end with Stein in 1967. Nor did it end with Bullock and Perkel’s meeting on information coding in the brain <a id="page_199"></a>a year later. In fact, in their report on that meeting, Bullock and Perkel include an Appendix that lays out dozens of possible neural codes and how they could be implemented. </p>
<p class="TXI">In truth, neuroscientists continue to spar and struggle over the neural code to this day. They host conferences centred on ‘Cracking the neural code’. They write papers with titles like ‘Seeking the neural code’, ‘Time for a new neural code?’ and even ‘Is there a neural code?’ They continue to find good evidence for Adrian’s original rate-based coding, but also some against it. Identifying the neural code can seem a more distant goal now than when MacKay and McCulloch wrote their first musings on it. </p>
<p class="TXI">In general, some evidence of rate-based coding can be found in most areas of the brain. The neurons that send information from the eye change their firing frequency based on the intensity of light. Neurons that encode smell fire in proportion to the concentration of their preferred odour. And, as Adrian showed, receptors in the muscle and those in the skin fire more with more pressure applied to them. But some of the strongest evidence for other coding schemes comes from sensory problems that require very specific solutions. </p>
<p class="TXI">When localising the source of a sound, for example, precise timing matters. Because of the distance between the two ears, sound coming from the left or right side will hit one ear just before it hits the other. This gap between the times of arrival at each ear – sometimes as brief as only a few millionths of a second – provides a clue for calculating where the sound came from. The medial superior olive (MSO), a tiny cluster of cells <a id="page_200"></a>located right in between the two ears, is responsible for performing this calculation.</p>
<p class="TXI">The neural circuit that can carry this out was posited by psychologist Lloyd Jeffress in 1948 and has been supported by many experiments since. Jeffress’ model starts with information coming from each ear in the form of a temporal code – that is, the exact timing of the spikes matters. In the MSO, cells that receive inputs from each ear compare the relative timing of these two inputs. For example, one cell may be set up to detect sounds that arrive at both ears simultaneously. To do so, the signals from each ear would need to take the exact same amount of time to reach this MSO cell. This cell then fires when it receives two inputs at the exact same time and this response indicates that the sound hit both ears at the same time (see Figure 17). </p>
<p class="TXI">The cell next to this one, however, receives slightly asymmetric inputs. That is, the nerve fibre from one ear needs to travel a <span class="italic">little</span> farther to reach this cell than the nerve from the other ear. Because of this, one of the temporal signals gets delayed. The extra length the signal travels determines just how much extra time it takes. Let’s say the signal from the left ear takes an extra 100 microseconds to reach this MSO cell. Then, the only way this cell will receive two inputs at once is if the sound hits the left ear 100 microseconds before it hits the right. Therefore, this cell’s response (which, like the other cell, only comes when it receives two inputs at once) would signal a 100-microsecond difference.</p>
<p class="TXI">Continuing this pattern, the next cell may respond to a 200-microsecond difference, the one after that 300 microseconds and so on. In total, the cells in the MSO form a map wherein those that signal short arrival time <a id="page_201"></a>differences fall at one end and those that signal long differences fall at the other. In this way, a temporal code has been transformed into a <span class="italic">spatial</span> code: the position of the active neuron in this map carries information about the source of the sound.</p>
<p class="image-fig" id="fig17.jpg">
<img alt="" src="Images/chapter-07-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 17</span>
</span></p>
<p class="TXI">For the question of why the neural code is such an enigma, the most likely answer – as with so many questions of the brain – is because it’s complicated. Some neurons, in some areas of the brain, under some circumstances, may be using a rate-based code. Other neurons, in other times and places, may be using a code based on the timing of spikes, or the time in between spikes, or some other code altogether. As a result, the thirst to crack <span class="italic">the</span> neural code will likely never be quenched. The brain, it seems, speaks in too many different languages. </p>
<p class="center">* * *</p>
<p class="TXT">Evolution did not furnish the nervous system with one single neural code, nor did it make it easy for scientists to <a id="page_202"></a>find the multitude of symbols it uses. But, according to British neuroscientist Horace Barlow, evolution did thankfully provide one strong guiding light for our understanding of the brain’s coding scheme. Barlow is known as one of the founders of the efficient coding hypothesis, the idea that – no matter what code the brain uses – it is always encoding information <span class="italic">efficiently</span>. </p>
<p class="TXI">Barlow was a trainee of Lord Adrian. He worked with him – when he could find him – as a student at Cambridge in 1947. Barlow had always had a keen interest in physics and mathematics but, to be practical, chose to study medicine.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> Yet throughout his studies he recognised how the influence from more quantitative subjects could drive questions in biology. It was a trait he considered in contrast to his mentor: ‘[Adrian] was not at all theoretically based; his attitude was that we had the means of recording from nerve fibres and we should just see what happens.’ </p>
<p class="TXI">Quickly taken in by Shannon’s equations when they came about, Barlow made several early contributions to the study of information in the brain. Rather than simply counting bits per second, however, Barlow’s use of information theory went deeper. The laws of information, in some respects, are as fundamental and constraining of biology as the laws of physics. From Barlow’s perspective, these equations could thus do more than merely <span class="italic">describe</span> the brain as it is, but rather <span class="italic">explain</span> how it came to be. So certain of its importance to neuroscience, Barlow compared trying to study the brain without focusing on <a id="page_203"></a>information processing to trying to understand a wing without knowing that birds fly. </p>
<p class="TXI">Barlow came to his efficient coding hypothesis by combining reflections on information theory with observations of biology. If the brain evolved within the constraints of information theory – and evolution tends to find pretty good solutions – then it makes sense to conclude that the brain is quite good at encoding information. ‘The safe course here is to assume that the nervous system is efficient,’ Barlow wrote in a 1961 paper. If this is true, any puzzle about why neurons are responding the way they are may be solved by assuming they are acting efficiently. </p>
<p class="TXI">But what does efficient information coding look like? For that, Barlow focused on the notion of redundancy. In Shannon’s framework, ‘redundancy’ refers to the size of the gap between the highest possible entropy a given set of symbols could have and the entropy they actually have. For example, if a code has two symbols and uses one of them 90 per cent of the time and the other only 10 per cent, its entropy is not as high as it could be. Sending the same symbol nine out of ten times is redundant. As we saw earlier, the code with the highest entropy would use each of those symbols 50 per cent of the time and would have a redundancy of zero. Barlow believed efficient brains reduce their redundancy as much as possible.</p>
<p class="TXI">The reason for this is that redundancy is a waste of resources. The English language, as it turns out, is incredibly redundant. A prime example of this is the letter ‘q’, which is almost always followed by ‘u’. The ‘u’ adds little if any information once we see the ‘q’ and is <a id="page_204"></a>therefore redundant. The redundancy of English means we could, in theory, be conveying the same amount of information with far fewer letters. In fact, in his original 1948 paper, Shannon estimated the redundancy of written English to be about 50 per cent. This is why, for example, ppl cn stll rd sntncs tht hv ll th vwls rmvd.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> </p>
<p class="TXI">In the nervous system, redundancy can come in the form of multiple neurons saying the same thing. Imagine one neuron represents the letter ‘q’ and another the letter ‘u’. The sight of ‘qu’ would thus make both these neurons fire. But if these two letters frequently appear together in the world, it would be more efficient of the brain to use just a single neuron to respond to them.</p>
<p class="TXI">Why should it matter if the brain is efficient with its encoding? One reason is energy costs. Every time a neuron fires a spike, the balance of charged particles inside and outside the cell gets thrown off. Restoring this balance takes energy: little pumps in the cell membrane have to chuck sodium ions out of the cell and pull potassium ones back in. Building up neuro­transmitters and expelling them from the cell with each spike also incurs a cost. In total, it’s estimated that up to three-fourths of the brain’s energy budget goes towards sending and receiving signals. And the brain – using 20 per cent of the body’s energy while accounting for only 2 per cent of its weight – is the most energetically expensive organ to run. With such a high energy bill, it <a id="page_205"></a>makes sense for the brain to be economical in how it uses its spikes. </p>
<p class="TXI">But to know how to send information efficiently, the brain needs to know what kind of information it normally needs to send. In particular, the brain needs to somehow determine when the information it is receiving from the world is redundant. Then it could simply not bother sending it on. This would keep the neural code efficient. Does the nervous system have the ability to track the statistics of the information it’s receiving and match its coding scheme to the world around it? One of Lord Adrian’s own findings – adaptation – suggests it does.</p>
<p class="TXI">In his experiments on muscle stretch receptors, Adrian noticed that ‘there is a gradual decline in the frequency of the discharge under a constant stimulus’. Specifically, while keeping the weight applied to the muscle constant, the firing rate of the nerve would decrease by about half over 10 seconds. Adrian called this phenomenon ‘adaptation’ and defined it as ‘a decline in excitability caused by the stimulus’. Noticing the effect in several of his experiments, he devoted a whole chapter to the topic in his 1928 book. </p>
<p class="TXI">Adaptation has since been found all over the nervous system. For example, the ‘waterfall effect’ is a visual illusion wherein the sight of movement in one direction then causes stationary objects to appear as though they are moving in the opposite direction. It’s so-named because it can happen after staring at the downward motion of a waterfall. The effect is believed to be the result of adaptation in the cells that represent the original motion direction: with these cells silenced by adaptation, <a id="page_206"></a>our perception is biased by the firing of cells that represent the opposite direction.</p>
<p class="TXI">In his 1972 paper, Barlow argues for adaptation as a means of increasing efficiency: ‘If sensory messages 
are to be given a prominence proportional to their informational value, mechanisms must exist for reducing the magnitude of representation of patterns which are constantly present, and this is presumably the underlying rationale for adaptive effects.’</p>
<p class="TXI">In other words – specifically, in the words of information theory – if the same symbol is being sent across the channel over and over, its presence no longer carries information. Therefore, it makes sense to stop sending it. And that is what neurons do: they stop sending spikes when they see the same stimulus over and over. </p>
<p class="TXI">Since the time that Barlow made the claim that cells should adapt their responses to the signals they’re receiving, techniques for tracking how neurons encode information have developed that allow for more direct and nuanced tests of this hypothesis. In 2001, for example, computational neuroscientist Adrienne Fairhall, along with colleagues at the NEC Research Institute in Princeton, New Jersey, investigated the adaptive abilities of visual neurons in flies. </p>
<p class="TXI">For their experiment, the researchers showed the flies a bar moving left and right on a screen. At first, the bar’s motion was erratic. At one moment it could be moving very quickly leftwards, and at the next it could go equally fast towards the right, or it could stay in that direction, or it could slow down entirely. In total, its range of possible speeds was large. After several seconds of such chaos, the bar then calmed down. Its movement became more constrained, never going too quickly in either direction. <a id="page_207"></a>Over the course of the experiment, the bar switched between such periods of erratic and calm movement several times. </p>
<p class="TXI">Looking at the activity of the neurons that respond to motion, the researchers found that the visual system rapidly adapts its code to the motion information it’s currently getting. Specifically, to be an efficient encoder, a neuron should always fire at its peak firing rate for the fastest motion it sees and at its lowest for the slowest.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> Thinking of different rates of firing as different symbols in the neural code, spreading the firing rates out this way ensures that all these symbols get used roughly equally. This maximises the entropy of the code. </p>
<p class="TXI">The problem is that the fastest motion during the period of calm is much slower than the fastest motion during the more erratic period. This means that the same exact speed needs to map to two different firing rates depending on the context it appears in. Strange as it is, this is just what Fairhall and colleagues saw. During the calm period, when the bar was moving at its fastest speed, the neuron fired at over 100 spikes per second. Yet when that same speed occurred during the erratic period, the neuron only fired about 60 times per second. To get the neuron back up to 100 spikes per second during the erratic period, the bar needed to move 10 times as fast. </p>
<p class="TXI">In addition, the researchers were able to quantify the amount of information carried by a spike before and after <a id="page_208"></a>the switch between these two types of motion. During the erratic period, the information rate was around 1.5 bits per spike. Immediately after the switch to calm movement, it dropped to just 0.8 bits per spike: the neuron, having not yet adapted to the new set of motion it was seeing, was an inefficient encoder. After just a fraction of a second of exposure to the calmer motion, however, the bits per spike went right back up to 1.5. The neuron needed just a small amount of time to monitor the range of speeds it was seeing and adapt its firing patterns accordingly. This experiment shows that, just as Barlow’s efficient coding theory suggests, adaptation ensures that all types of information are encoded efficiently. </p>
<p class="TXI">Neuroscientists also believe that the brain is built to produce efficient encodings on much longer timescales than the seconds to minutes of a sensory experience. Through both evolution and development, an organism has a chance to sample its environment and adapt its neural code to what is most important to it. By assuming that a given area of the brain is best-suited to represent relevant information as efficiently as possible, scientists are attempting to reverse engineer the evolutionary process.</p>
<p class="TXI">The 30,000 nerves that leave the human ear, for example, respond to different types of sounds. Some of the neurons prefer short blips of high-pitched noises, others prefer low-pitched noises. Some respond best when a soft sound gets louder, others when a loud sound gets softer and others still when a soft sound gets louder then softer again. Overall, each nerve fibre has a complex pattern of pitches and volumes that best drive its firing.</p>
<p class="TXI">Scientists know, for the most part, <span class="italic">how</span> the fibres end up with these responses. Tiny hairs connected to cells in <a id="page_209"></a>the inner ear move in response to sounds. Each cell responds to a different pitch based on where it is in a small, spiral-shaped membrane. The nerve fibres that leave the ear get inputs from these hairy cells. Each fibre combines pitches in its own way to make its unique, combined response profile. </p>
<p class="TXI">What is less clear, however, is <span class="italic">why</span> the fibres have these responses. That’s where ideas from information theory can help. </p>
<p class="TXI">If the brain is indeed reducing redundancy as Barlow suggests, then only a small number of neurons should be active at a time. Neuroscientists refer to this kind of activity as ‘sparse’.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> In 2002, computational neuroscientist Michael Lewicki asked whether the response properties of auditory nerves could be the result of the brain enforcing a sparse code – one specifically designed for the sounds an animal needs to process.</p>
<p class="TXI">To answer this, he first had to gather a collection of different natural sounds. One set of sounds came from a CD of vocalisations made by rainforest animals such as bats, manatees and marmosets; another was a compilation of ‘background’ noises like crunching leaves and snapping twigs; and the third was from a database of human voices reading English sentences. </p>
<p class="TXI">Lewicki then used an algorithm to decompose these complex sounds into a dictionary of short sound patterns. <a id="page_210"></a>The goal of the algorithm was to find the best decomposition – that is, one that can recreate each full, natural sound using as few of the short sound patterns as possible. In this way, the algorithm was seeking a sparse code. If the brain’s auditory system evolved to sparsely encode natural sounds, the sound patterns preferred by the auditory nerves should match those found by the algorithm. </p>
<p class="TXI">Lewicki found that creating a dictionary from just the animal noises alone produced sound patterns that didn’t match the biology. Specifically, the patterns the algorithm produced were too simple – representing just pure tones rather than the complex mix of pitches and volumes that human and animal auditory nerves tend to prefer. Applying the algorithm to a mix of animal noises and background sounds, however, did mimic the biology. This suggests that the coding scheme of the auditory system is indeed matched to these environmental sounds, allowing it to encode them efficiently. What’s more, Lewicki found that a dictionary made from human speech also reproduced the sound profiles preferred by biology. Lewicki took this as evidence for the theory that human speech evolved to make best use of the existing encoding scheme of the auditory system.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup></p>
<p class="center">* * *</p>
<p class="TXT">In 1959, Barlow presented his ideas about the brain’s information-processing properties to a group of sensory <a id="page_211"></a>researchers gathered at MIT. When the proceedings of this meeting were translated into Russian for a Soviet audience, Barlow’s contribution was conspicuously cut out. The Soviets, it turned out, had a problem with the use of information theory to understand the brain. Considered part of the ‘bourgeois pseudoscience’ of cybernetics, it ran counter to official Soviet philosophy by equating man with machine. Soviet leaders – and the sometimes scared scientists under their rule – openly critiqued this attitude as a foolish product of American capitalism. </p>
<p class="TXI">Though unique in its political motivations, the Soviets’ denunciation was far from the only critique of information theory in biology. In 1956, a short article entitled ‘The bandwagon’ cautioned against the over-excited application of information theory in fields such as psychology, linguistics, economics and biology. ‘Seldom do more than a few of nature’s secrets give way at one time. It will be all too easy for our somewhat artificial prosperity to collapse overnight when it is realised that the use of a few exciting words like <span class="italic">information</span>, <span class="italic">entropy</span>, <span class="italic">redundancy</span>, do not solve all our problems.’ The article was written by Shannon himself, just eight years after he unleashed information theory on to the world. </p>
<p class="TXI">Concerns about how apt the analogy is between Shannon’s framework and the brain have even come from the very scientists doing the analogising. In a 2000 article, Barlow warned that ‘the brain uses information in different ways from those common in communication engineering’. And Perkel and Bullock, in their original report, made a point of not committing themselves fully to Shannon’s definition of information but, rather, <a id="page_212"></a>treating the concept of ‘coding’ in the brain as a metaphor that may have varying degrees of usefulness.</p>
<p class="TXI">The caution is warranted. A particularly tricky part of Shannon’s system to map to the brain is the decoder. In a simple communication system, the receiver gets the encoded message through the channel and simply reverses the process of encoding in order to decode it. The recipient of a telegraph message, for example, would use the same reference table as the sender to know how to map dots and dashes back to letters. The system in the brain, however, is unlikely to be so symmetric. This is because the only ‘decoders’ in the brain are other neurons, and what they do with the signal they receive can be anyone’s guess. </p>
<p class="TXI">Take, for example, encoding in the retina. When a photon of light is detected, some of the cells in the retina (the ‘on’ cells) encode this through an increase in their firing rate while another set of cells (the ‘off’ cells) encodes it by decreasing their firing. If this joint up–down change in firing is the symbol the retina has designated to indicate the arrival of a photon, we may assume this is also the symbol that is ‘decoded’ by later brain areas. However, this does not seem to be the case.</p>
<p class="TXI">In 2019, a team of researchers from Finland genetically modified the cells in a mouse’s retina. Specifically, they made the ‘on’ cells less sensitive to photons. Now, when a photon hit, the ‘off’ cells would still decrease their firing, but the ‘on’ cells may or may not increase theirs. The question was: which set of cells would the brain listen to? The information about the photon would be there for the taking if the ‘off’ cells <a id="page_213"></a>were decoded. Yet the animals didn’t seem to use it. By assessing the animal’s ability to detect faint lights, it appeared that the brain was reading out the activity of the ‘on’ cells alone. If those cells didn’t signal that a photon was detected, the animal didn’t respond. The scientists took this to mean that the brain is not, at least in this case, decoding all of the encoded information. It ignores the signals the ‘off’ cells are sending. Therefore, the authors wrote, ‘at the sensitivity limit of vision, the decoding principles of the brain do not produce the optimal solution predicted by information theory’. Just because scientists can spot a signal in the spikes doesn’t mean it has meaning to the brain. </p>
<p class="TXI">There are many reasons this might be. One important one is that the brain is an information-<span class="italic">processing</span> machine. That is, it does not aim to merely reproduce messages sent along it, but rather to transform them into action for the animal. It is performing computations on information, not just communicating it. Expectations about how the brain works based solely on Shannon’s communication system therefore miss this crucial purpose. The finding that the brain is not optimally transmitting information does not necessarily indicate a flaw in its design. It was just designed for something else. </p>
<p class="TXI">Information theory, invented as a language for engineered communication systems, couldn’t be expected to translate perfectly to the nervous system. The brain is not a mere telephone line. Yet parts of the brain do engage in this more basic task of communication. Nerves do send signals. And they do so through some kind of code based on spike rates or spike times or spike <a id="page_214"></a>something. To glance at the brain from the vantage point of information theory, then, is a sensible endeavour – one that has yielded many insights and ideas. Stare too long, though, and the cracks in the analogy become visible. This is the reason for 
wariness. As a metaphor, the relationship between a communication system and the brain is thus most fruitful when not overextended.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-1" id="fn-1">1</a> ﻿Marvin Minsky, one of the authors of the ﻿<span class="italic">Perceptrons</span>﻿ book from ﻿﻿Chapter 3﻿﻿, was working under Shannon at the time and is credited with the design of the Ultimate Machine. Shannon reportedly convinced Bell Labs to produce several of them as gifts for AT&amp;T executives. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-2" id="fn-2">2</a> ﻿More on probability and its history in ﻿﻿Chapter 10﻿﻿.﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-3" id="fn-3">3</a> ﻿The base-two log wasn﻿’﻿t the only option for information. Prior to Shannon﻿’﻿s work, his colleague Ralph Hartley posited a definition of information using a base-10 log, which would﻿’﻿ve put information in terms of ﻿‘﻿decimal digits or ﻿‘﻿dits﻿’﻿ instead of bits. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-4" id="fn-4">4</a> ﻿Full disclosure: some modern neuroscientists are exploring the idea that action potentials actually ﻿<span class="italic">do</span>﻿ change in certain ways depending on what inputs the cell gets and that these changes ﻿<span class="italic">could</span>﻿ be part of the neural code. Science is never set in stone. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-5" id="fn-5">5</a> ﻿Barlow credits his mother Nora, granddaughter of Charles Darwin, with his interest in science. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-6" id="fn-6">6</a> ﻿In case you couldn﻿’﻿t: ﻿‘﻿people can still read sentences that have all the vowels removed﻿’﻿. Texting and tweeting are also great ways to see just how many letters can be removed from a word before causing problems. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-7" id="fn-7">7</a> ﻿Technically if the neuron has a preferred direction of motion ﻿–﻿ that is, it fires most strongly for, say, rightwards motion ﻿–﻿ it should fire at its peak rate for high speed in that direction and its lowest rate for high speed in the opposite direction. But the principle remains the same regardless. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-8" id="fn-8">8</a> ﻿Among neuroscientists, the ﻿‘﻿grandmother cell﻿’﻿ is considered the mascot of sparse coding. This fictional neuron is meant to be the one and only cell to fire when you see your grandmother (and fires in response to nothing else). Such an extreme example of efficient coding was devised by Jerome Lettvin (the frog guy from the last chapter) in order to vividly demonstrate the concept to his students. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-9" id="fn-9">9</a> ﻿If, while reading the last chapter, you wondered why it is that neurons in the visual system detect lines, information theory has an answer to that, too. In 1996, Bruno Olshausen and David Field applied a similar technique as Lewicki to show that lines are what you﻿’﻿d expect neurons to respond to if they are encoding images efficiently. ﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 3</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter3"><a href="contents.xhtml#re_chapter3">CHAPTER THREE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter3">Learning to Compute</a><a id="page_49"></a></p>
<p class="H1" id="b-9781472966445-ch222-sec3">
<span class="bold">
<span>McCulloch-Pitts, the Perceptron and 
artificial neural networks</span>
</span></p>
<p class="TXT">Cambridge University mathematician Bertrand Russell spent 10 years at the beginning of the twentieth century toiling towards a monumental goal: to identify the philosophical roots from which all of mathematics stems. Undertaken in collaboration with his former teacher Alfred Whitehead, this ambitious project produced a book, the <span class="italic">Principia Mathematica</span>, which was delivered to the publishers overdue and over budget. The authors themselves had to chip in towards the publishing costs just to get it done and they didn’t see any royalties for 40 years. </p>
<p class="TXI">But the financial hurdle was perhaps the smallest one to overcome in getting this opus finished. Russell had to fight against his own agitation with the scholarly material. According to his autobiography, he spent days staring at a blank sheet of paper and evenings contemplating jumping in front of a train. Work on the book also coincided with the dissolution of Russell’s marriage and a strain on his relationship with Whitehead – who, according to Russell, was fighting his own mental and marital battles at the time. The book was even physically demanding: Russell spent 12 hours a day at a desk writing out the intricate symbolism needed <a id="page_50"></a>to convey his complex mathematical ideas and when the time came to bring the manuscript to the publisher it was too large for him to carry. Despite it all, Russell and Whitehead eventually completed and published the text they hoped would tame the seemingly wild state of mathematics. </p>
<p class="TXI">The conceit of the <span class="italic">Principia</span> was that all of mathematics could be reduced to logic. In other words, Russell and Whitehead believed that a handful of basic statements, known as ‘expressions’, could be combined in just the right way to generate all the formalisms, claims and findings of mathematicians. These expressions didn’t stem from any observations of the real world. Rather, they were meant to be universal. For example, the expression: if X is true, then the statement ‘X is true or Y is true’ is also true. Such expressions are made up of propositions: fundamental units of logic that can be either true or false, written as letters like X or Y. These propositions are strung together with ‘Boolean’ operators<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> such as ‘and’, ‘or’ and ‘not’. </p>
<p class="TXI">In the first volume of the <span class="italic">Principia</span>, Russell and Whitehead provided fewer than two dozen of these abstract expressions. From these humble seeds, they built mathematics. They were even able to triumphantly conclude – after scores of symbol-filled pages – that 1+1=2. </p>
<p class="TXI">Russell and Whitehead’s demonstration that the full grandeur of mathematics could be captured with the simple <a id="page_51"></a>rules of logic<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> had immense philosophical implications as it provided proof of the power of logic. What’s more, it meant that a subsequent finding, made by a different pair of men some 30 years later, would have immense implications of its own. This finding said that neurons, simply via the nature of their anatomy and physiology, were implementing the rules of logic. It revolutionised the study of the brain and of intelligence itself. </p>
<p class="center">* * *</p>
<p class="TXT">When Detroit native Walter Pitts was just 12 years old, he was invited by Russell to join him as a graduate student at Cambridge University. The young boy had, the story goes, encountered a copy of the <span class="italic">Principia</span> after running into a library to avoid bullies. As he read, Pitts found what he believed to be errors in the work. So, he sent his notes on the subject off to Russell, who, presumably not knowing the boy’s age, then offered him the position. Pitts didn’t accept it. But a few years later, when Russell was visiting the University of Chicago, Pitts went to sit in on his lectures. Having fled an abusive family home to come to Chicago, Pitts decided not to return. He remained in the city, homeless. </p>
<p class="TXI">Luckily, the University of Chicago had another world-famous logician for Pitts to criticise – Rudolf Carnap. Again, Pitts wrote up notes – this time identifying issues in Carnap’s recent book <span class="italic">The Logical Syntax of Language</span> – and delivered them to Carnap’s office at the University of Chicago. Pitts didn’t stick around long enough to hear his <a id="page_52"></a>reaction, but Carnap, impressed, eventually chased down Pitts, whom he referred to as ‘that newsboy who understood logic’. On this occasion, the philosopher he critiqued actually did get Pitts to work with him. Though he never officially enrolled, Pitts functioned effectively as a graduate student for Carnap and fraternised with a group of scholars who were interested in the mathematics of biology.</p>
<p class="TXI">Warren McCulloch’s interest in philosophy took a more traditional form. Born in New Jersey, he studied the subject (along with psychology) at Yale and read many of the greats. He was most enamoured with Immanuel Kant and Gottfried Leibniz (whose ideas were very influential for Russell), and he read the <span class="italic">Principia</span> at the age of 25. But, despite the beard upon his long face, McCulloch was not a philosopher – he was a physiologist. He attended medical school in Manhattan and then went on to observe the panoply of ways in which the brain can break as a neurology intern at Bellevue Hospital and at the Rockland State Hospital psychiatric facility. In 1941, he joined the University of Illinois at Chicago as the director of the laboratory for basic research in the department of psychiatry. </p>
<p class="TXI">As with all great origin stories, there are conflicting accounts of how McCulloch and Pitts met. One claims that it happened when McCulloch spoke in front of a research group Pitts was a part of. Another story is that Carnap introduced them. Finally, a contemporary of the two men, Jerome Lettvin, claimed he introduced them and that all three bonded over a mutual love of Leibniz. In any case, by 1942, the 43-year-old McCulloch and his wife had taken the 18-year-old Pitts into their home, <a id="page_53"></a>and the two men were spending evenings drinking whisky and discussing logic. </p>
<p class="TXI">The wall between ‘mind’ and ‘body’ was strong among scientists in the early twentieth century. The mind was considered internal and intangible; the body, including the brain, was physical. Researchers on either side of this wall toiled diligently, but separately, at their own problems. Biologists, as we saw in the last chapter, were working hard to uncover the physical machinery of neurons: using pipettes and electrodes and chemicals to sort out what causes a spike and how. Psychiatrists, on the other hand, were attempting to uncover the machinery of the mind through lengthy sessions of Freudian psychoanalysis. Few on either side would attempt a glance over the wall at the other. They spoke separate languages and worked towards different goals. For most practitioners, the question of how neural building blocks could create the structure of the mind was not just unanswered, it was unasked. </p>
<p class="TXI">But McCulloch, from as early on as his time at medical school, had immersed himself in a crowd of scientists who did care about this question and allowed him the space to think about it. Eventually, through his physiological observations, he came up with a hunch. He saw in the emerging concepts of neuroscience a possible mapping to the notions of logic and computation he so adored in philosophy. To think of the brain as a computing device following the rules of logic – rather than just a bag of proteins and chemicals – would open the door to understanding thought in terms of neural activity.</p> <a id="page_54"></a>
<p class="TXI">Analytical skill, however, was not where McCulloch excelled. Some who knew him say he was too much of a romantic to be held down by such details. So, despite years of toying with these ideas in his mind and in conversation (even as a Bellevue intern he was accused of ‘trying to write an equation for the working of the brain’), McCulloch struggled with several technical issues of how to enact them. Pitts, however, was comparably unfazed by the analytical. As soon as he spoke with him about it, Pitts saw what approaches were needed to formally realise McCulloch’s intuitions. Not long after they met, one of the most influential papers on computation was written.</p>
<p class="TXI">‘A logical calculus of the ideas immanent in nervous activity’ was published in 1943. The paper is 17 pages long with many equations, only three references (one of which is to the <span class="italic">Principia</span>) and a single figure consisting of little neural circuits drawn by McCulloch’s daughter.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> </p>
<p class="TXI">The paper begins by reviewing the biology of neurons that was known at the time: neurons have cell bodies and axons; two neurons connect when the axon of the first meets the body of the second; through this connection one neuron provides input to the other; a certain amount of input is needed for a neuron to fire; a cell either fires a spike or it doesn’t – no half spikes or in-between spikes; and the input from some neurons – inhibitory neurons – has the power to prevent a cell from spiking.</p><a id="page_55"></a>
<p class="TXI">McCulloch and Pitts go on to explain how these biological details are congruent with Boolean logic. The core of their claim is that the activity state of each neuron – either firing or not – is like the truth value of a proposition – either true or false. In their own words, they ‘conceive of the response of any neuron as factually equivalent to a proposition which proposed its adequate stimulus’. </p>
<p class="TXI">By ‘its adequate stimulus’ they are referring to something about the world. Imagine a neuron in the visual cortex whose activity represents the statement ‘the current visual stimulus looks like a duck’. If that neuron is firing, that statement is true; if the neuron is not firing, it is false. Now imagine another neuron, in the auditory cortex, that represents the statement ‘the current auditory stimulus is quacking like a duck’. Again, if this neuron is firing, that statement is true, otherwise it is false. </p>
<p class="TXI">Now we can use the connections between neurons to enact Boolean operations. For example, by giving a third neuron inputs from both of these neurons, we could implement the rule ‘if it looks like a duck <span class="italic">and</span> it quacks like a duck, it’s a duck’. All we have to do is build the third neuron such that it will only fire if both of its input neurons are firing. That way, both ‘looks like a duck’ and ‘quacks like a duck’ have to be true in order for the conclusion represented by the third neuron (‘it’s a duck’) to be true. </p>
<p class="TXI">This describes the simple circuit needed to implement the Boolean operation ‘and’. McCulloch and Pitts in their paper show how to implement many others. To implement ‘or’ is very similar, however the strength of the connections from each neuron must be so strong <a id="page_56"></a>that one input alone is enough to make the output neuron fire. In this case, the ‘is a duck’ neuron would fire if the ‘looks like a duck’ neuron <span class="italic">or</span> the ‘quacks like a duck’ neuron (or both) were firing. The authors even show how to string together multiple Boolean operations. For example, to implement a statement like ‘X and not Y’, the neuron representing X connects to an output neuron with a strength enough to make it fire. But the neuron representing Y <span class="italic">inhibits</span> the output neuron, meaning it prevents it from firing. This way, the output neuron will only fire if the X-representing neuron <span class="italic">is</span> firing and the Y-representing neuron is <span class="italic">not</span> (see Figure 4). </p>
<p class="TXI">These circuits, which are meant to represent what networks of real neurons can do, became known as <span class="italic">artificial</span> neural networks. </p>
<p class="image-fig" id="fig4.jpg">
<img alt="" src="Images/chapter-01-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 4</span>
</span></p>
<p class="TXI">The ability to spot logic at play in the interactions of neurons came from McCulloch’s discerning eye. As a physiologist, he knew that neurons were more complex than his simple drawings and equations suggested. They had membranes, ion channels and forking paths of <a id="page_57"></a>dendrites. But the theory didn’t need their full complexity. So, like an impressionist painter using only the necessary strokes, he intentionally highlighted only the elements of neural activity required for the story he wanted to tell. In doing so, he demonstrated the artistry inherent to model-building; it is a subjective and creative process to decide which facts belong in the foreground. </p>
<p class="TXI">The radical story that McCulloch and Pitts told with their model – that neurons were performing a logical calculus – was the first attempt to use the principles of computation to turn the mind–body problem into a mind–body connection. Networks of neurons were now imbued with all the power of a formal logical system. Like a chain of falling dominoes, once certain truth values entered into a neural population (say, via sensory organs), a cascade of interactions could deduce the truth value of new and different statements. This meant a population of neurons could carry out endless computations: interpreting sensory inputs, developing conclusions, forming plans, reasoning through arguments, performing calculations and so on. </p>
<p class="TXI">With this step in their research, McCulloch and Pitts advanced the study of human thought and, at the same time, kicked it off its throne. The ‘mind’ lost its status as mysterious and ethereal once it was brought down to solid ground – that is, once its grand abilities were reduced to the firing of neurons. To adapt a quote from Lettvin, the brain could now be thought of as ‘a machine, meaty and miraculous, but still a machine’. More boldly still, McCulloch’s student Michael Arbib later remarked that the work ‘killed dualism’.</p><a id="page_58"></a>
<p class="TXI">Russell was known to lament that, despite the 20 years put into it and the impact it had on logicians and philosophers, the <span class="italic">Principia</span> had little effect on practising mathematicians. Its new take on the foundations of mathematics simply didn’t seem to mean much to those doing mathematics; it didn’t change their day-to-day work. The same could be said of McCulloch and Pitts’ discovery for neuroscientists of the time. Biologists, physiologists, anatomists – the scientists doing the labour of physically mining neurons for the details of their workings – didn’t take much from the theory. This was in part because it wasn’t obvious what experiments should follow from it. But it may also have stemmed from the very technical notation in the paper and its less-than-inviting writing style. In a review on nerve conduction written three years later, the author refers to the McCulloch-Pitts paper as ‘not for the lay reader’ and remarks that if this style of work is to be useful, it’s necessary for ‘physiologists to familiarise themselves with mathematical technology, or for mathematicians to elaborate at least their conclusions in a less formidable language’. The wall between mind and body may have come down, but the one between biologist and mathematician stood strong. </p>
<p class="TXI">There was a separate group of people – a group with the requisite technical know-how – who did take an interest in the logical calculus of neurons. In the post-war era, a series of meetings hosted by the philanthropic Macy Foundation brought together biologists and technologists, many of whom wished to use biological findings to build brain-like machines. McCulloch was an organiser of these meetings, and fellow attendees <a id="page_59"></a>included the ‘father of cybernetics’ Norbert Wiener and John von Neumann, the inventor of the modern computer architecture, who was directly inspired in its design by the McCulloch-Pitts neurons. As Lettvin described it 40 years later: ‘The whole field of neurology and neurobiology ignored the structure, the message and the form of McCulloch and Pitts’ theory. Instead, those who were inspired by it were those who were destined to become the aficionados of a new venture, now called Artificial Intelligence.’</p>
<p class="center">* * *</p>
<p class="EXTF">The <span class="italic">Navy last week demonstrated the embryo of an electronic computer named the Perceptron which, when completed in about a year, is expected to be the first non-living mechanism able to 
‘perceive, recognise, and identify its surroundings without human training or control.’ [...]</span> </p>
<p class="EXT-L">
<span class="italic">“‘Dr. Frank Rosenblatt, research psychologist at the Cornell Aeronautical Laboratory, Inc., Buffalo, NY, designer of the ­Perceptron, conducted the demonstration. The machine, he said, would be the first electronic device to think as the human brain. Like humans, Perceptron will make mistakes at first, ‘but it will grow wiser as it gains experience’, he said.</span></p>
<p class="TXT">This summary, from an article entitled ‘Electronic “brain” teaches itself’, appeared in the 13 July 1958 edition of the <span class="italic">New York Times</span>, opposite a letter to the editor about the ongoing debate on whether smoking causes cancer. Frank Rosenblatt, the 30-year-old architect of the project, was reaching beyond his training in experimental psychology to build a computer meant to rival the most advanced technology at the time.</p><a id="page_60"></a>
<p class="TXI">The computer in question was taller than the engineers who operated it and about twice as long. It was covered on either end in various control panels and readout mechanisms. Rosenblatt requested the services of three ‘professional people’ and an associated technical staff for 18 months to build it, and the estimated cost was $100,000 (around $870,000 today). The word ‘perceptron’, defined by Rosenblatt, is a generic term for a certain class of devices that can ‘recognise similarities or identities between patterns of optical, electrical or tonal information’. The Perceptron – the computer that was built in 1958 – was thus technically a subclass known as a ‘photoperceptron’ because it took as its input the output of a camera mounted on a tripod at one end of the machine. </p>
<p class="TXI">The Perceptron was, just like the models introduced in the McCulloch-Pitts paper, an artificial neural network. It was a simplified replica of what real neurons do and how they connect to each other. But rather than remaining a mathematical construct that exists only as the ink of equations on a page, the Perceptron was physically realised. The camera provided 400 inputs to this network in the form of a 20x20 grid of light sensors. Wires then randomly connected the output of these sensors to 1,000 ‘association units’ – small electrical circuits that summed up their inputs and switched to ‘on’ or ‘off’ as a result, just like a neuron. The output of these association units became the input to the ‘response units’, which themselves could be ‘on’ or ‘off’. The number of response units was equal to the number of mutually exclusive categories to which an image could belong. So, if the Navy wanted to use the Perceptron, say, to <a id="page_61"></a>determine if a jet was present in an image or not, there would be two response units: one for jet and one for no jet. At the end of the machine opposite the camera was a set of light bulbs that let the engineer know which of the response units was active – that is, which category the input belonged to. </p>
<p class="TXI">Implementing an artificial neural network this way was large and cumbersome, full of switches, plugboards and gas tubes. The same network made up of real neurons would be smaller than a grain of sea salt. But achieving this physical implementation was important. It meant that theories of how neurons compute could actually be tested in the real world on real data. Whereas the McCulloch-Pitts work was about proving a point in theory, the Perceptron put it into practice. </p>
<p class="TXI">Another important difference between the Perceptron and the McCulloch-Pitts network was that, as Rosenblatt told the <span class="italic">New York Times</span>, the Perceptron learns. In the McCulloch and Pitts paper, the authors make no reference to how the connectivity between the neurons comes to be. It is simply defined according to what logical function the network needs to carry out and it stays that way. For the Perceptron to learn, however, it must modify its connections.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> In fact, the Perceptron derives all its functionality from changing its connection strengths until they are just right. </p>
<p class="TXI">The type of learning the Perceptron engages in is known as ‘supervised’ learning. By providing pairs of inputs and outputs – say, a series of pictures and whether <a id="page_62"></a>they each contain a jet or not – the Perceptron learns to make this decision on its own. It does so by changing the strength of the connections – also known as the ‘weights’ – between the association units and the readouts. </p>
<p class="TXI">Specifically, when an image is provided to the network, it activates units first in the input layer, then in the association layer, and finally in the readout layer, indicating the network’s decision. If the network gets the classification wrong, the weights change according to these rules:</p>
<p class="NLF">1. If a readout unit is ‘off’ when it should be ‘on’, the connections from the ‘on’ association units to that readout unit are <span class="italic">strengthened</span>.</p>
<p class="NLL">2. If a readout unit is ‘on’ when it should be ‘off’, the connections from the ‘on’ association units to that readout unit are <span class="italic">weakened</span>.</p>
<p class="TXT">By following these rules, the network will start to correctly associate images with the category they belong to. If the network can learn to do this well, it will stop making errors and the weights will stop changing. </p>
<p class="TXI">This procedure for learning was, in many ways, the most remarkable part of the Perceptron. It was the conceptual key that could open all doors. Rather than needing to tell a computer exactly how to solve a problem, you need only show it some examples of that problem solved. This had the potential to revolutionise computing and Rosenblatt was not shy in saying so. He told the <span class="italic">New York Times</span> that Perceptrons would ‘be able to recognise people and call out their names’ and ‘to hear speech in one language and instantly translate it to <a id="page_63"></a>speech or writing in another language’. He also added that ‘it would be possible to build Perceptrons that could reproduce themselves on an assembly line and which would be “conscious” of their existence’. This was a bold statement, to say the least, and not everyone was happy with Rosenblatt’s public bravado. But the spirit of the claim – that a computer that could learn would expedite the solving of almost any problem – rang true.</p>
<p class="TXI">The power of learning, however, came with a price. Letting the system decide its own connectivity effectively divorced these connections from the concept of Boolean operators. The network <span class="italic">could</span> learn the connectivity that McCulloch and Pitts had identified as required for ‘and’, ‘or’, <span class="italic">etc.</span> But there was no requirement that it does, nor any need to understand the system in this light. Furthermore, while the association units in the Perceptron machine were designed to be only ‘on’ or ‘off’, the learning rule doesn’t actually require that they be this way. In fact, the activity level of these artificial neurons could be any positive number and the rule would still work.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> This makes the system more flexible, but without a binary ‘on’-‘off’ response it makes it harder to map the activity of these units to the binary truth values of propositions. Compared with the crisp and clear logic of the McCulloch-Pitts networks, the Perceptron was an uninterpretable mess. But it worked. Interpretability was sacrificed for ability.</p> <a id="page_64"></a>
<p class="TXI">The Perceptron machine and its associated learning procedure became a popular object of study in the burgeoning field of artificial intelligence. When it made the transition from a specific physical object (the Perceptron) to an abstract mathematical concept (the perceptron algorithm) the separate input and association layers were done away with. Instead, input units representing incoming data connected directly to the readout units and, through learning, these connections changed to make the network better at its task. How and what the perceptron in this simplified form could learn was studied from every angle. Researchers explored its workings mathematically using pen and paper, or physically by building their own perceptron machines, or – when digital computers finally became available – electronically by simulating it.</p>
<p class="TXI">The perceptron generated hope that humans could build machines that learn like we do; in this way it put the prospect of artificial intelligence within 
reach. Simultaneously, it provided a new way of understanding our own intelligence. It showed that artificial neural networks could compute without abiding by the strict rules of logic. If the perceptron could perceive without the use of propositions or operators, it follows that each neuron and connection in the brain needn’t have a clear role in terms of Boolean logic either. Instead, the brain could be working in a sloppier way, wherein, like the perceptron, the function of a network is distributed across its neurons and emerges out of the connections between them. This new approach to the study of the brain became known as ‘connectionism’.</p> <a id="page_65"></a>
<p class="TXI">The work of McCulloch and Pitts was an important stepping stone. As the first demonstration of how networks of neurons could think, it was responsible for getting neuroscience away from the shores of pure biology and into the sea of computation. This fact, rather than the veracity of its claims, is what earns it its place in history. The intellectual ancestor of McCulloch and Pitts’ work, the <span class="italic">Principia Mathematica</span>, could be said to have suffered a similar fate. In 1931, German mathematician Kurt Gödel published ‘On formally undecidable propositions of <span class="italic">Principia Mathematica</span> and related systems’. This paper took the <span class="italic">Principia Mathematica</span> as a starting point to show why its very goal – to explain all of mathematics from simple premises – was impossible to achieve. Russell and Whitehead had not, in fact, done what they believed they did.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> Gödel’s findings became known as the ‘incompleteness theorem’ and had a revolutionary effect on mathematics and philosophy. An effect that stemmed, in part, from Russell and Whitehead’s failed attempt.</p>
<p class="TXI">Russell and McCulloch were able to take the failings of their respective works in their stride. Pitts, on the other hand, was made of finer cloth. The realisation that the brain was not enacting the beautiful rules of logic tore him apart.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> This, along with pre-existing mental struggles and the end of a relationship with an important mentor, drove him to drink and experiment with other <a id="page_66"></a>drugs. He became erratic and delirious; he burned his work and withdrew from his friends. He died from the impacts of liver disease in 1969 – the same year McCulloch died. McCulloch was 70; Pitts was 46. </p>
<p class="TXI">* * *</p>
<p class="image-fig" id="fig5.jpg">
<img alt="" src="Images/chapter-01-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 5</span>
</span></p>
<p class="TXI">The cerebellum is a forest. Folded up neatly near where the spinal cord enters the skull, this bit of the brain is thick with different types of neurons, like different species of trees, all living in chaotic harmony (see Figure 5). The Purkinje cells are large, easily identified and heavily branched: from the body of these cells, dendrites stretch up and away, like a thousand alien hands raised in prayer. The granule cells are numerous and small – with cell bodies less than half the size of the Purkinje’s – but their reach is far. Their axons initially grow upwards, in parallel with the Purkinje cells’ dendrites. They then make a sharp right turn to run directly through the branches of the Purkinje cells, like power lines through treetops. This is where the granule cells make contact with the Purkinje cells: each Purkinje cell gets input from hundreds of <a id="page_67"></a>thousands of granule cells. Climbing fibres are axons that follow a longer path on their way to the Purkinje cells. These axons come from cells in a different brain region – the inferior olive – from which they navigate all the way to the bottom of the Purkinje cell bodies and creep up around them. Winding their way around the base of the Purkinje cell dendrites like ivy, the climbing fibres form connections. Unlike the granule cells, only a single climbing fibre targets each Purkinje cell. In the cerebellar landscape, Purkinje cells are thus central. They have scores of granule cells imposing on them from the top and a small yet precise set of climbing fibres closing in on them from the bottom. </p>
<p class="TXI">In its twisty, organic way, the circuitry of the cerebellum possesses an organisation and precision unbefitting of biology. It was in this biological wiring that James Albus, a PhD student in electrical engineering working at NASA, saw the principles of the perceptron at play. </p>
<p class="TXI">The cerebellum plays a crucial role in motor control; it helps with balance, coordination, reflexes and more. One of the most widely studied of its abilities is eye-blink conditioning. This is a trained reflex that can be found in everyday life. For example, if a determined parent or roommate tries to get you out of bed in the morning by pulling open the curtains, you’ll instinctively close your eyes in response to the sunlight. After a few days of this, simply the sound of the curtains being opened may be enough to make you blink in anticipation. </p>
<p class="TXI">In the lab, this process is studied in rabbits and the intruding sunlight is replaced by a small puff of air on the eye (just annoying enough to ensure they’ll want to avoid it). After several trials of playing a sound (such as a <a id="page_68"></a>short blip of a pure tone) and following it by this little air puff, the rabbit eventually learns to close its eyes immediately upon hearing the tone. Play the animal a new sound (for example, a clapping noise) that hasn’t been paired with air puffs and it won’t blink. This makes eye-blink conditioning a simple classification task: the rabbit has to decide if a sound it hears is indicative of an upcoming air puff (in which case the eyes should close) or if it is a neutral noise (in which case they can remain open). Disrupt the cerebellum and rabbits can’t learn this task. </p>
<p class="TXI">The Purkinje cells have the power to close the eyes. Specifically, a dip in their normally high firing rate will, via connections the Purkinje cells send out of this area, cause the eyes to shut. Based on this anatomy, Albus saw their place as the readout – that is, they indicate the outcome of the classification. </p>
<p class="TXI">The perceptron learns via supervision: it needs inputs and labels for those inputs to know when it has erred. Albus saw these two functions in the two different types of connections on to Purkinje cells. The granule cells pass along sensory signals; specifically, different granule cells fire depending on which sound is being played. The climbing fibres tell the cerebellum about the air puff; they fire when this annoyance is felt. Importantly, this means the climbing fibres signal an error. They indicate that the animal made a mistake in not closing its eyes when it should have. </p>
<p class="TXI">To prevent this error, the connections from the granule cells to the Purkinje cells need to change. In particular, Albus anticipated that any granule cells that were active before the climbing fibre was active (<span class="italic">i.e.</span>, before an error), <a id="page_69"></a>should weaken their connection to the Purkinje cell. That way, the next time those granule cells fire – <span class="italic">i.e.</span>, the next time the same sound is played – they <span class="italic">won’t</span> cause firing in the Purkinje cells. And that dip in Purkinje cell firing <span class="italic">will</span> cause the eyes to close. Through this changing of connection strengths, the animal learns from its past mistakes and avoids future air puffs to the eye.</p>
<p class="TXI">In this way, the Purkinje cell acts like a president advised by a cabinet of granule cell advisors. At first the Purkinje cell listens to all of them. But if it’s clear that some are providing bad advice – that is, their input is followed by negative news delivered by the climbing fibre – their influence over the Purkinje cell will fade. And the Purkinje cell will act better in the future. It is a process that directly mirrors the perceptron learning rule.</p>
<p class="TXI">When Albus proposed this mapping between the perceptron and the cerebellum in 1971,<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> his prediction about how the connections between granule cells and Purkinje cells should change was just that – a prediction. No one had directly observed this kind of learning in the cerebellum. But by the mid-1980s, evidence had piled up in Albus’ favour. It became clear that the strength of the connection between a granule cell and a Purkinje cell does decrease after an error. The particular molecular mechanisms of this process have even been revealed. We now know that granule cell inputs cause a receptor in <a id="page_70"></a>the membrane of the Purkinje cell to respond, effectively tagging which granule cell inputs were active at a given time. If a climbing fibre input comes later (during an air puff), it causes calcium to flood into the Purkinje cell. The presence of this calcium signals to all the tagged connections to decrease their strength. Patients with fragile X syndrome – a genetic disorder that leads to intellectual disabilities – appear to be missing a protein that regulates this connection from the granule cells on to the Purkinje cell. As a result, they have trouble learning tasks like eye-blink conditioning. </p>
<p class="TXI">The perceptron, with its explicit rules of how learning should proceed in a neural network, offered clear testable ideas for neuroscientists to hunt for – and find – in the brain. In doing so, it was able to connect science across scales. The smallest physical detail – calcium ions moving through the inside of a neuron, for example – inherits a much larger meaning in light of its role in computation. </p>
<p class="center">* * *</p>
<p class="TXT">The reign of the perceptron was cut short in 1969. And with a twist of Shakespearean irony, it was its namesake that killed it. </p>
<p class="TXI">
<span class="italic">Perceptrons</span> was written by Marvin Minsky and Seymour Papert, both mathematicians at the Massachusetts Institute of Technology. The book was subtitled <span class="italic">An Introduction to Computational Geometry</span> and had a simple abstract design on the cover. Minsky and Papert were drawn to write about the topic of perceptrons out of appreciation for Rosenblatt’s invention and a desire to explore it further. <a id="page_71"></a>In fact, Minsky and Papert met at a conference where they were presenting similar results from their explorations into how the perceptron learns. </p>
<p class="TXI">Papert was a native of South Africa with full cheeks, a healthy beard and not one, but <span class="italic">two</span>, PhDs in mathematics. He had a lifelong interest in education and how it could be transformed by computing. Minsky was less than a year older than Papert, with sharper features and large glasses. A New York native, he attended the Bronx High School of Science with Frank Rosenblatt; he was also mentored by McCulloch and Pitts. </p>
<p class="TXI">Minsky and Papert shared with McCulloch and Pitts the compulsive desire to formalise thinking. True advances in understanding computation, they believed, came from mathematical derivations. All the empirical success of the perceptron – whatever computing it was able to carry out or categories it was able to learn – meant next to nothing without a mathematical understanding of why and how it worked. </p>
<p class="TXI">At this time, the perceptron was attracting a lot of attention – and money – from the artificial intelligence community. But it wasn’t attracting the kind of mathematical scrutiny Minsky and Papert yearned for. The two were thus explicitly motivated to write their book by a desire to increase the rigour around the study of perceptrons – but also, as Papert later acknowledged, by some desire to decrease the reverence for them.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup></p>
<p class="TXI">The pages of <span class="italic">Perceptrons</span> are comprised mainly of proofs, theorems and derivations. Each contributes to a <a id="page_72"></a>story about the perceptron: defining what it is, what it can do and how it learns. Yet from the publication of these some 200 pages – a thorough exploration of the ins and outs of the perceptron’s workings – the message the community received was largely about its limitations. This is because Minsky and Papert had shown, conclusively, that certain simple computations were impossible for the perceptron to do. </p>
<p class="TXI">Consider a perceptron that has just two inputs, and each input can be ‘on’ or ‘off’. We want the perceptron to report if the two inputs are the same: to respond yes (<span class="italic">i.e.</span>, have its readout unit be on) if both inputs are on <span class="italic">or</span> if both inputs are off. But if one is on and the other is off, the readout unit should be off. Like sorting socks out of the laundry, the perceptron should only respond when it sees a matching pair.</p>
<p class="TXI">To make sure the readout doesn’t fire when only one input is on, the weights from each input need to be sufficiently low. They could, for example, each be half the amount needed to make the readout turn on. This way, when both are on, the readout <span class="italic">will</span> fire and it won’t fire when only one input is on. In this setup the readout is responding correctly for three of the four possible input conditions. But in the condition where both inputs are off, the readout will be off – an incorrect classification. </p>
<p class="TXI">As it turns out, no matter how much we fiddle with connection strengths, there is no way to satisfy all the needs of the classification at once. The perceptron simply cannot do it. And the problem with that is that no good model of the brain – or promising artificial intelligence – should fail at a task as simple as deciding if two things are the same or not.</p> <a id="page_73"></a>
<p class="TXI">Albus, whose paper on the cerebellum was published in 1971, knew of the limitations of the perceptron and knew that, despite these limitations, it was still powerful enough to be a model of the eye-blink conditioning task. But a model of the whole human brain, as Rosenblatt promised? Not possible.</p>
<p class="TXI">The portrait that Minsky and Papert painted forced researchers to see the perceptron’s powers clearly. Prior to the book, researchers were able to explore what the perceptron could do blindly, with the hope that the limits of its abilities were still far off, if they existed at all. Once the contours were put in stark relief, however, there was no denying that these boundaries existed, and that they existed much closer than expected. In truth, all this amounted to was an understanding of the perceptron – exactly what Minsky and Papert set out to do. But the end of ignorance around the perceptron meant the end of excitement around it as well. As Papert put it: ‘Being understood can be a fate as bad as death.’</p>
<p class="TXI">The period that followed the publication of <span class="italic">Perceptrons</span> is known as the ‘dark ages’ of connectionism. It was marked by significant decreases in funding to the research programmes that had grown out of Rosenblatt’s initial work. The neural network approach to building artificial intelligence was snuffed out. All the excessive promises, hopes and hype had to be retracted. Rosenblatt himself died tragically in a boating accident two years after the book was published and the field he helped build remained dormant for more than 10 years.</p>
<p class="TXI">But if the hype around the perceptron was excessive and ill informed, so too was the backlash against it. The limitations in Minsky and Papert’s book were true: the <a id="page_74"></a>perceptron in the form they were studying it was incapable of many things. But it didn’t need to keep that form. The same-or-not problem, for example, could be easily solved by adding an additional layer of neurons between the input and the readout. This layer could be composed of two neurons, one with weights that make it fire when both inputs are on and the other with weights that make it fire when both inputs are off. Now the readout neuron, which gets its input from these middle neurons, just needs to be active if one of the middle neurons is active. </p>
<p class="TXI">‘Multi-layer perceptrons’, as these new neural architectures were called, had the potential to bring connectionism back from the dead.<sup><a href="#fn-10" id="fnt-10">10</a>
</sup> But before a full resurrection was possible, one problem had to be solved: learning. The original perceptron algorithm provided the recipe for setting the connections between the input neurons and the readout neurons – that is, the learning rule was designed for a two-layered network. If the new breed of neural networks was going to have three, four, five or more layers, how should the connections between all those layers be set? (see Figure 6) Despite all the good features of the perceptron learning rule – its simplicity, the proof that it could work, the fact that it had been spotted in the wild of the cerebellum – it was unable to answer this question. Knowing that a multi-layer perceptron <span class="italic">could</span> solve more complex problems was not enough to deliver <a id="page_75"></a>on the grand promises of connectionism. What was needed was for it to <span class="italic">learn</span> to solve those problems.</p>
<p class="center">* * *</p>
<p class="TXT">The Easter Sunday of the connectionist revival story came in 1986. The paper ‘Learning representations by back-propagating errors’, written by two cognitive scientists from the University of California San Diego, David Rumelhart and Ronald Williams, and a computer scientist from Carnegie Mellon, Geoffrey Hinton, was published on 9 October in the journal <span class="italic">Nature</span>. It presented a solution to the exact problem the field had: how to train multi-layer artificial neural networks. The learning algorithm in the paper, called ‘backpropagation’, became widely used by the community at the time. And it remains to this day the dominant way in which artificial neural networks are trained to do interesting tasks.</p>
<p class="image-fig" id="fig6.jpg">
<img alt="" src="Images/chapter-01-image-03.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 6</span>
</span></p>
<p class="TXI">The original perceptron’s learning rule works because, with only two layers, it’s easy to see how to fix what’s gone wrong. If a readout neuron is off when it should be on, connections going from the input layer to that neuron <a id="page_76"></a>need to get stronger and vice versa. The relationship between these connections and the readout is thus clear. The backpropagation algorithm has a more difficult problem to solve. In a network with many layers between the input and readout, the relationships between all these connections and the readout aren’t as clear. Now instead of a president and his or her advisors, we have a president, their advisors, and the employees of those advisors. The amount of trust an advisor has in any given employee – <span class="italic">i.e.</span>, the strength of the connection from that employee to the advisor – will certainly have an impact, ultimately, on what the president does. But this impact is harder to directly see and harder to fix if the president feels something is going wrong. </p>
<p class="TXI">What was needed was an explicit way to calculate how any connection in the network would impact the readout layer. As it turns out, mathematics offers a neat way to do this. Consider an artificial neural network with three layers: input, middle and readout. How do the connections from the input to the middle layer impact the readout? We know the activity of the middle layer is a result of the activities of the input neurons and the weights of their connections to the middle layer. With this knowledge, writing an equation for how these weights affect the activity at the middle layer is straightforward. We also know that the readout neurons follow the same rule: their activity is determined by the activities of the middle neurons and the weights connecting the middle neurons to the readout. Therefore, an equation describing how these weights impact the readout is also easy to get. The only thing left to do is find a way to string these two equations together. That <a id="page_77"></a>way we’ll have an equation that tells us directly how the connections from the input to the middle layer impact the readout. </p>
<p class="TXI">When forming a train in the game of dominoes, the number on the end of one tile needs to match the number on the start of another in order for them to connect. The same is true for stitching together these equations. Here, the common term that connects the two equations is the activity of the middle layer: this activity both determines the activity of the readout and is determined by the input-to-middle connections. After joining these equations via the middle layer activity, the impact of the input-to-middle layer connections on the readout can be calculated directly. And this makes it easy to figure out how those connections should change when the readout is wrong. In calculus, this linking together of relationships is known as the ‘chain rule’ and it is the core of the backpropagation algorithm. </p>
<p class="TXI">The chain rule was discovered over 200 years ago by none other than the idol of McCulloch and Pitts, philosopher and polymath Gottfried Leibniz. Given how useful the rule is, its application to the training of multi-layer neural networks was no surprise. In fact, the backpropagation algorithm appears to have been invented at least three separate times before 1986. But the 1986 paper was part of a perfect storm that ensured its findings would spread far and wide. The first reason for this was the content of the paper itself. Not only did it show that neural networks could be trained this way, it also analysed the workings of networks trained on several cognitive tasks, such as understanding relations on a family tree. Another component of the success was the <a id="page_78"></a>increase in computational power that came in the 1980s; this was important for making the training of multi-layer neural networks practically possible for researchers. Finally, the same year the paper was published, one of its authors, Rumelhart, also published a book on connectionism that included the backpropagation algorithm. That book – written with a different Carnegie Mellon professor, James McClelland – went on to sell an estimated 40,000 copies by the mid-1990s. Its title, <span class="italic">Parallel Distributed Processing</span>, lent its name to the entire research agenda of building artificial neural networks in the late 1980s and early 1990s. </p>
<p class="TXI">For somewhat similar reasons, the story of artificial neural networks took an even more dramatic turn roughly a decade into the new millennium. The heaps of data accumulated in the internet age united with the computational power of the twenty-first century to supercharge progress in this field. Networks with more and more layers could suddenly be trained on more and more complex tasks. Such scaled-up models – referred to as ‘deep neural networks’ – are currently transforming artificial intelligence and neuroscience alike. </p>
<p class="TXI">The deep neural networks of today are based on the same basic understanding of neurons as those of McCulloch and Pitts. Beyond that base inspiration, though, they don’t aim to directly replicate the human brain. They aren’t trying to mimic its structure or anatomy, for example.<sup><a href="#fn-11" id="fnt-11">11</a>
</sup> But they do aim to mimic human behaviour and they’re getting quite good at it. When <a id="page_79"></a>Google’s popular language translation service started using a deep neural network approach in 2016, it reduced translation errors by 50 per cent. YouTube also uses deep neural networks to help its recommendation algorithm better understand what videos people want to see. And when Apple’s voice assistant Siri responds to a command, it is a deep neural network that is doing the listening and the speaking. </p>
<p class="TXI">In total, deep neural networks can now be trained to find objects in images, play games, understand preferences, translate between different languages, turn speech into written words and turn written words into speech. Not unlike the original Perceptron machine, the computers these networks run on fill up rooms. They’re located in server centres across the globe, where they hum away processing the world’s image, text and audio data. Rosenblatt may have been pleased to see that some of his grand promises to the <span class="italic">New York Times</span> were indeed fulfilled. They just required a scale nearly a thousand times what he had available at the time. </p>
<p class="TXI">The backpropagation algorithm was necessary to boost artificial neural networks to the point where they could reach near-human levels of performance on some tasks. As a learning rule for neural networks, it really works. Unfortunately, that doesn’t mean it works like the brain. While the perceptron learning rule was something that could be seen at play between real neurons, the backpropagation algorithm is not. It was designed as a mathematical tool to make artificial neural networks work, not a model of how the brain learns (and its inventors were very clear on that from the start). The reason for this is that real neurons can typically only <a id="page_80"></a>know about the activity of the neurons they’re connected to – not about the activity of the neurons those neurons connect to and so on and so on. For this reason, there is no obvious way for real neurons to implement the chain rule. They must be doing something different. </p>
<p class="TXI">For some researchers – particularly researchers in the field of artificial intelligence – the artificial nature of backpropagation is no problem. Their goal is to build computers that can think, by whatever means necessary. But for other scientists – neuroscientists in particular – finding the learning algorithm of the brain is paramount. We know the brain is good at getting better; we see it when we learn a musical instrument, how to drive or how to read a new language. The question is how.</p>
<p class="TXI">Because backpropagation is what we know works, some neuroscientists are starting there. They’re checking for signs that the brain is doing something <span class="italic">like</span> backpropagation, even if it can’t do it exactly. Inspiration comes from the success story of finding a perceptron at work in the cerebellum. There, clues were present in the anatomy; the different placement of the climbing fibres and granule cells pointed to a different role for each. Other brain areas display patterns of connectivity which may hint at how they are learning. For example, in the neocortex, some neurons have dendrites that stretch out way above them. Faraway regions of the brain send inputs to these dendrites. Do they carry with them information about how these neurons have impacted those that come after them in the brain’s neural network? Could this information be used to change the strength of the network’s connections? Both neuroscientists and artificial intelligence researchers hold out hope that the <a id="page_81"></a>brain’s version of backpropagation will be found and that, when it is, it can be copied to create algorithms that learn even better and faster than today’s artificial neural networks. </p>
<p class="TXI">In their hunt to understand how the mind learns from supervision, modern researchers are doing just what McCulloch did. They’re looking at the piles of facts we have about the biology of the brain and trying to see in it a computational structure. Today, they are guided in their search by the workings of artificial systems. Tomorrow, the findings from biology will again guide the building of artificial intelligence. This back-and-forth defines the symbiotic relationship between these two fields. Researchers looking to build artificial neural networks can take inspiration from the patterns found in biological ones, while neuroscientists can look to the study of artificial intelligence to identify the computational role of biological details. In this way, artificial neural networks keep the study of the mind and the brain connected.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-1" id="fn-1">1</a> ﻿Named after the English mathematician George Boole. While they used his ideas, Russell and Whitehead didn﻿’﻿t actually use the term ﻿‘﻿Boolean﻿’﻿, as it wasn﻿’﻿t coined until 1913.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-2" id="fn-2">2</a> ﻿At least that﻿’﻿s what it looked like at the time ﻿…﻿ More on this later.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-3" id="fn-3">3</a> ﻿The use of the word ﻿‘﻿circuit﻿’﻿ here differs from that in the last chapter. In addition to its meaning as an electrical circuit, neuroscientists also use the word to refer to a group of neurons connected in a specific way. ﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-4" id="fn-4">4</a> ﻿More on how learning ﻿–﻿ and memory ﻿–﻿ relies on a change in connections in the next chapter.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-5" id="fn-5">5</a> ﻿This can be thought of as representing the ﻿<span class="italic">rate</span>﻿ of spiking of a neuron, rather than if the neuron is emitting a spike or not. Using this type of artificial neuron only requires a small modification to the learning procedure.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-6" id="fn-6">6</a> ﻿The cracks in the ﻿<span class="italic">Principia</span>﻿﻿’﻿s foundation were noticeable even when it was published. Some of the ﻿‘﻿basic﻿’﻿ premises it had to assume were not really very basic and were hard to justify.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-7" id="fn-7">7</a> ﻿This realisation came even more directly from a study on the frog brain that Pitts was involved with. More on that study in ﻿﻿Chapter 6﻿﻿.﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-8" id="fn-8">8</a> ﻿The mapping is sometimes referred to as the ﻿‘﻿Marr-Albus-Ito﻿’﻿ theory of motor learning, named also after David Marr and Masao Ito, who both put forth similar models of how the cerebellum learns. ﻿</p>
<p class="FN1"><a href="chapter3.xhtml#fnt-9" id="fn-9">9</a> ﻿The particular words Papert used to describe his feelings about Perceptron-mania at the time were ﻿‘﻿hostility﻿’﻿ and ﻿‘﻿annoyance﻿’﻿.﻿</p>
<p class="FN2"><a href="chapter3.xhtml#fnt-10" id="fn-10">10</a> ﻿Technically they weren﻿’﻿t ﻿‘﻿new﻿’﻿. Minsky and Papert do reference multi-layer perceptrons in their book. However, they were dismissive about the potential powers of these devices and, unfortunately for science, did not encourage their further study. ﻿</p>
<p class="FN2"><a href="chapter3.xhtml#fnt-11" id="fn-11">11</a> ﻿With the exception of deep neural networks that are built to understand images, which we will hear all about in ﻿﻿Chapter 6﻿﻿.﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<title>Chapter 7</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter7"><a href="contents.xhtml#re_chapter7">CHAPTER SEVEN</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter7">Cracking the Neural Code</a><a id="page_7"></a></p>
<p class="H1"><span class="bold"><span>Information theory and efficient coding</span></span></p>
<p class="EXTF">Whereas <span class="italic">the heart pumps blood and the lungs effect gas exchange, whereas the liver processes and stores chemicals and the kidney removes substances from the blood, the nervous system processes information.</span></p>
<p class="right">Summary of Neurosciences Research 
Program work session, 1968</p>
<p class="TXT-con">The goal of the 1968 Neurosciences Research <a id="page_183"></a>Program meeting was to discuss how individual and groups of neurons process information. The meeting’s summary, written by neuroscientists Theodore Bullock and Donald Perkel, does not push for any hard and fast conclusions. But it does lay out a wide world of possibilities for the representation, transformation, transmission and storage of information in the brain in a way that summarised the state of the field.</p>
<p class="TXI">As the quote from their summary implies, ascribing the role of information processing to the brain seems as natural as saying the heart pumps blood. Even before ‘information’ became a part of everyday vocabulary in the twentieth century, scientists still spoke implicitly of the information that nerves convey, often in the language of ‘messages’ and ‘signals’. An 1892 lecture to hospital employees, for example, explains that: ‘There are fibres <a id="page_184"></a>which convey messages from the various parts of the body to the brain’ and that some of these fibres ‘carry special kinds of messages as, for example, the nerves connected with the organs of special sense, which have been called the gateways of knowledge’. In the same vein, an 1870 publication describes the firing of motor neurons as ‘a message of the will to the muscle’ and even goes so far as to equate the nervous system with the dominant information-transmitting technology of the day: the telegraph. </p>
<p class="TXI">But the investigation into how the nervous system represents information only started in earnest about 40 years before Bullock and Perkel’s report, with the work of Edgar Adrian in the early twentieth century.</p>
<p class="TXI">Adrian was in many ways the image of a prim and proper man of science. By the time he was born in London in 1889, his family had been in England for more than 300 years – a lineage that included a sixteenth-century surgeon and several reverends and members of government. As a student, his brilliance was regularly acknowledged by his teachers. In addition to his focus on medicine during his university studies, he displayed skill in art, particularly painting and drawing. As a lecturer at Cambridge, he worked long hours in the lab and in the classroom. In his career as a physiologist, he was an undeniable success. At the age of 42 he won a Nobel Prize and in 1955 he was granted a title by Queen Elizabeth II, becoming Lord Adrian. </p>
<p class="TXI">But behind these formal awards and accolades was a restless and chaotic man. Adrian was a thrill-seeker who liked climbing mountains and driving fast cars. He was happy to experiment on himself, including keeping a <a id="page_185"></a>needle in his arm for two hours to try to measure muscle activity. He was known to play elaborate games of hide-and-seek with fellow students in the valleys of England’s Lake District. As a professor he was equally elusive. He avoided unscheduled meetings by hiding in his lab, forcing any enquiring students to try to catch him on his bike ride home. He was temperamental and when he needed to think he’d perch himself on a shelf in a dark cabinet. His lab mates and his family described his movements as rapid, jerky and almost constant. His mind could be equally darting. Over the course of his career he studied many different questions in many different animals: vision, pain, touch and muscle control in frogs, cats, monkeys and more. </p>
<p class="TXI">This inability to remain still, physically or mentally, may have been a key to his success. Through his varied studies on the activity of single nerves he was able to find certain general principles that would form the core of our understanding of the nervous system as a whole. In his 1928 book, <span class="italic">The Basis of Sensation</span>, Adrian explains his conclusions and the experiments that allowed him to reach them. The pages are peppered with talk of ‘signals’, ‘messages’ and even ‘information’, all mixed in with anatomical details of the nervous system and the technical challenges of capturing its activity. It was a mix of experimental advances and conceptual insights that would influence the field for decades to come.</p>
<p class="TXI">In <a href="chapter3.xhtml#chapter3">Chapter 3</a>, Adrian explains an experiment wherein he adds weight to a frog’s muscle to see how the ‘stretch’ receptors that track the muscle’s position would respond. Adrian recorded from the nerves that carry this signal from the receptors to the spinal cord. After applying <a id="page_186"></a>different weights, Adrian summarised his findings as follows: ‘The sensory message which travels to the central nervous system when a muscle is stretched … consists of a succession of impulses of the familiar type. The frequency with which the impulses recur depends on the strength of the stimulus, but the size of each impulse does not vary.’ This finding – that the size, shape or duration of an action potential emitted by these sensory neurons does not change, no matter how heavy or light the weight applied to the muscle is – Adrian referred to as the ‘all-or-nothing’ principle.</p>
<p class="TXI">Examples of the ‘all-or-nothing’ nature of neural impulses reappear throughout the book. In different species, for different nerves carrying different messages, the story is always the same. Action potentials don’t change based on the signal they’re conveying, but their frequency can. The spikes of a neuron are thus like an army of ants – each as identical as possible, their power coming mainly from their numbers. </p>
<p class="TXI">If the nature of an individual action potential is the same regardless of the strength or weakness of the sensory stimulus causing it, then one thing is certain: the size of the action potential does not carry information. With the contributions from Adrian, physiologists now felt comfortable embarking on a search for where exactly information was in nerves and how it got transmitted. </p>
<p class="TXI">There was only one problem: what <span class="italic">is</span> information? The blood that the heart pumps and the gases that lungs exchange are real, physical substances. They’re observable, tangible and measurable. For as commonly as we use it, ‘information’ is actually a rather vague and elusive concept. A precise definition of the word does not easily <a id="page_187"></a>come to mind for most people; it falls unfortunately into the ‘know it when you see it’ camp. Without a way to weigh information the way we can weigh fluids or gases, what hope could scientists have for a quantitative understanding of the brain’s central purpose?</p>
<p class="TXI">Between the time of Adrian’s book and Perkel and Bullock’s report, however, a quantitative definition of information had been found. It was born out of the scientific struggles of the Second World War and went on to transform the world in unexpected ways. Its application to the study of the brain was at times as rocky to execute as it was obvious to attempt. </p>
<p class="center">* * *</p>
<p class="TXT">Claude Shannon started at Bell Labs under a contract provided by the American military. It was 1941 and the National Defense Research Committee wanted scientists working on wartime technology. The seriousness of the work didn’t dampen Shannon’s naturally playful tendencies, though. He enjoyed juggling and while at Bell Labs was known to juggle around campus while riding a unicycle. </p>
<p class="TXI">Born in a small town in the American Midwest, Shannon grew up following his curiosity about all things science, mathematics and engineering anywhere it took him. As a child he played with radio parts and enjoyed number puzzles. As an adult he created a mathematical theory of juggling and a flame-powered Frisbee. He enjoyed chess and building machines that could play chess. A constant tinkerer, he made many gadgets, some more productive than others. On his desk at Bell Labs, <a id="page_188"></a>for example, he kept an ‘Ultimate Machine’: a box with a switch that, when flipped on, caused a mechanical hand to reach out and flip it back off.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> </p>
<p class="TXI">For his master’s degree, Shannon wrote a 72-page thesis entitled ‘A symbolic analysis of relay and switching circuits’ that would revolutionise electrical engineering. For his PhD, he turned his mathematical eye toward biology, working on ‘An Algebra for Theoretical Genetics’. But his topic at Bell Labs was cryptography. How to safely encode messages that would be transmitted through land, air and water was a natural topic of concern for the military. Bell Labs was a hub of cryptography research and even hosted the renowned code-cracker Alan Turing during Shannon’s time there. </p>
<p class="TXI">All this work on codes and messages got Shannon thinking broadly about the concept of communication. During the war, he proposed a method for understanding message-sending mathematically. Because of the necessary secrecy around cryptography research, however, his ideas were kept classified. In 1948, Shannon was finally able to publish the work and ‘A mathematical theory of communication’ became the founding document of a new field: information theory. </p>
<p class="TXI">Shannon’s paper describes a very generic communication system consisting of five simple parts. The first is an information <span class="italic">source</span>, which produces the message that will be sent. The next is the <span class="italic">transmitter</span>, <a id="page_189"></a>which is responsible for encoding the message into a form that can be sent across the third component, the <span class="italic">channel</span>. On the other end of the channel, a <span class="italic">receiver</span> decodes the information back into its original form and it is sent to its final <span class="italic">destination</span> (Figure 16). </p>
<p class="TXI">In this framework, the medium of the message is irrelevant. It could be songs over radio waves, words on a telegraph or images through the internet. As Shannon says, the components of his information-sending model are ‘suitably idealised from their physical counterparts’. This is possible because, in all of these cases, the fundamental problem of communication remains the same. It is the problem of ‘reproducing at one point either exactly or approximately a message selected at another point’.</p>
<p class="image-fig" id="fig16.jpg">
<img alt="" src="Images/chapter-07-image-01.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 16</span>
</span></p>
<p class="TXI">With this simple communication system in mind, Shannon aimed to formalise the study of information transmission. But, to mathematically approach the question of how information is communicated, he first had to define information mathematically. Building on previous work, Shannon discusses what desirable properties a measure of information should have. Some are practical: information shouldn’t be negative, for example, and its definition should be easy to work with mathematically. But the real constraint came from the need to capture an intuition about information: its reliance on a code.</p> <a id="page_190"></a>
<p class="TXI">Imagine a school where all students wear uniforms. Seeing a student show up in the same outfit every day provides very little information about their mood, their personality or the weather. On the other hand, in a school without uniforms, clothing choice has the ability to convey all this information and more. To someone wondering about the current temperature, for example, seeing a student in a sundress instead of a sweater can go a long way towards relieving that curiosity. In this way, clothing can be used as a code – it is a transmittable set of symbols that conveys meaning.</p>
<p class="TXI">The reason the uniformed students can’t carry this information is that a code requires options. There need to be multiple symbols in a code’s vocabulary (in this case, multiple outfits in a student’s wardrobe), each with their own meaning, for any of the symbols to have meaning. </p>
<p class="TXI">But it’s not just the number of symbols in a code that matters – it’s also how they’re used. Let’s say a student has two outfits: jeans and a T-shirt or a suit. If the student wears jeans and a T-shirt 99 per cent of the time, then there is not much information that can be gained from this wardrobe choice. You wouldn’t even need to see the student to be almost certain of what they’re wearing – it’s essentially a uniform. But the one day in a hundred where they show up in a suit tells you something important. It lets you know that day is somehow special. What this shows is that the rarer a symbol’s use, the more information it contains. Common symbols, on the other hand, can’t communicate much.</p>
<p class="TXI">Shannon wanted to capture this relationship between a symbol’s use and its information content. He therefore <a id="page_191"></a>defined a symbol’s information content in terms of the probability of it appearing. Specifically – to make the amount of information decrease as the probability of the symbol increases – he made a symbol’s information depend on the <span class="italic">inverse</span> of its probability. Because the inverse of a number is simply one divided by that number, a higher probability means a lower ‘inverse probability’. In this way, the more frequently the symbol is used, the lower its information will be. Finally, to meet his other mathematical constraints, he took the logarithm of this value. </p>
<p class="TXI">A logarithm, or ‘log’, is defined by its <span class="italic">base</span>. To take the base-10 log of a number, for example, you would ask: ‘To what power must I raise 10 in order to get this number?’ The base-10 log of 100 (written as log<sub>10</sub>100), is therefore 2, because 10 to the power of 2 (<span class="italic">i.e.</span>, 10x10) is 100. The base-10 log of 1,000 is thus 3. And the base-10 log of something in between 100 and 1,000 is in between 2 and 3. </p>
<p class="TXI">Shannon decided to use a base of <span class="italic">two</span> for his definition of information. To calculate the information in a symbol you must therefore ask: ‘To what power must I raise two in order to get the inverse of the symbol’s probability?’ Taking our student’s outfit of jeans and T-shirt as a symbol that appears with 0.99 probability, its information content is log<sub>2</sub>(1/0.99), which comes out to about 0.014. The suit that appears with only 0.01 probability, on the other hand, has an information content of log<sub>2</sub>(1/0.01) or roughly 6.64. Again, the lower the probability, the higher the information.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup></p><a id="page_192"></a>
<p class="TXI">But Shannon was interested in more than just the information in a single symbol – he wanted to study the information content of a code. A code is defined by its set of symbols and how frequently each is used. Shannon therefore defined the total information in a code as the sum of the information of all its symbols. Importantly, this sum is weighted – meaning the information from each symbol is multiplied by how frequently that symbol is used. </p>
<p class="TXI">Under this definition, the student’s clothing code would have a total information content of 0.99 x 0.014 (from the jeans and T-shirt) + 0.01 x 6.64 (from the suit) = 0.081. This can be thought of as the average amount of information we would receive each day by seeing the student’s outfit. If the student chose instead to wear their jeans 80 per cent of the time and their suit the other 20 per cent, their code would be different. And the average information content would be higher: 0.80 x log<sub>2</sub>(1/0.80) + 0.20 x log<sub>2</sub>(1/0.20) = 0.72. </p>
<p class="TXI">Shannon gave the average information rate of a code a name. He called it entropy. The official reason he gives for this is that his definition of information is related to the concept of entropy in physics, where it serves as a measure of disorder. On the other hand, Shannon was also known to claim – perhaps jokingly – that he was advised to call his new measure entropy because ‘no one understands entropy’ and therefore Shannon would likely always win arguments about his theory.</p>
<p class="TXI">Shannon’s entropy captures a fundamental trade-off inherent in maximising information. Rare things carry the most information, so you want as much of them as possible in your code. But the more you use a rare <a id="page_193"></a>symbol, the less rare it becomes. This fight fully defines the equation for entropy: decreasing the probability of the symbol makes the log of its inverse go up – a positive contribution to the information. But this number is then multiplied by that very same probability: this means that decreasing a symbol’s probability makes its contribution to information go down. To maximise entropy, then, we must make rare symbols as common as possible but no commoner. </p>
<p class="TXI">Shannon’s use of a base-two log makes the unit of information the <span class="italic">bit</span>. Bit is short for binary digit and, while Shannon’s paper sees the first known use of the word, he did not coin it (Shannon credits his Bell Labs colleague John Tukey with that honour).<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> The bit as a unit of information has a helpful and intuitive interpretation. Specifically, the average number of bits in a symbol is equal to the number of yes-or-no questions you need to ask in order to get that amount of information. </p>
<p class="TXI">For example, consider trying to find out the season in which someone was born. You may start by asking: ‘Is it a transitional season?’ If they say yes, you may then ask: ‘Is it spring?’ If they say yes to that, you have your answer; if they say no, you still have your answer: autumn. If they said no to the first question you’d follow the opposite route – asking if they were born in summer, <span class="italic">etc.</span> No matter the answer, it takes two yes-or-no questions to get it. Shannon’s entropy equation agrees. Assuming <a id="page_194"></a>people are equally likely to be born in each season, then each of these season ‘symbols’ will be used 25 per cent of the time. The information in each symbol is thus log<sub>2</sub>(1/0.25). This makes the average bits per symbol equal to two – the same as the number of questions. </p>
<p class="TXI">Part of designing a good communication system is designing a code that packs in a lot of information per symbol. To maximise the average information that a symbol in a code provides, we need to maximise the code’s entropy. But, as we saw, the definition of entropy has an inherent tension. To maximise it, rare symbols need to be the norm. What is the best way to satisfy this seemingly paradoxical demand? This tricky question turns out to have a simple answer. To maximise a code’s entropy, each of its symbols should be used the exact same amount. Have five symbols? Use each one-fifth of the time. A hundred symbols? Each should have a probability of 1/100th. Making each and every symbol equally likely balances the trade-off between rare and common communication. </p>
<p class="TXI">What’s more, the more symbols a code has the better. A code with two symbols, each used half the time, has an entropy of one bit per symbol (this makes sense according to our intuitive definition of a bit: thinking of one symbol as standing in for ‘yes’ and the other for ‘no’, each symbol answers one yes-or-no question). On the other hand, a code with 64 symbols – each used equally – has an entropy of six bits per symbol. </p>
<p class="TXI">As important as a good code is, encoding is only the start of a message’s journey. According to Shannon’s conception of communication, after information is encoded it still needs to be sent through a channel on <a id="page_195"></a>the way to its destination. Here is where the abstract aims of message-sending meet the physical limits of matter and materials.</p>
<p class="TXI">Consider the telegraph. A telegraph sends messages via short pulses of electric current passed over wires. The patterns of pulses – combinations of shorter ‘dots’ and longer ‘dashes’ – define the alphabet. In American Morse code, for example, a dot followed by a dash indicates the letter ‘A’, two dots and a dash means ‘U’. The physical constraints and imperfections of the wires carrying these messages, especially those sent long distances or under oceans, put a limit on the speed of information. Telegraph operators who typed too quickly were at risk of running their dots and dashes together, creating an unintelligible ‘hog Morse’ that would be worthless to the receiver of it. In practice, operators could safely send around 100 letters per minute on average.</p>
<p class="TXI">To create a practical measure of information rate, Shannon combined the inherent information rate of a code with the physical transmission rate of the channel it is sent over. For example, a code that provides five bits of information per symbol and is sent over a channel that can send 10 symbols per minute would have a total information rate of 50 bits per minute. The maximum rate at which information can be sent over a channel without errors is known as the channel’s capacity. </p>
<p class="TXI">Shannon’s publication enforced a clear structure on a notoriously nebulous concept. In this way, it set the stage for the increasing objectification of information in the decades to come. The immediate effects of Shannon’s work on information processing in the real world, however, were slight. It took more than two decades for <a id="page_196"></a>the technology that makes information transmission, storage and processing a constant part of everyday life to come about. And it took engineers time to figure out how to harness Shannon’s theory for practical effect in these devices. Information theory’s impact on biology, however, came much quicker.</p>
<p class="center">* * *</p>
<p class="TXT">The first application of information theory to biology was itself a product of war. Henry Quastler, an Austrian physician, was living and working in the US during the time of the Second World War. His response to the development of the atomic bomb was one of horror – and action. He left his private practice to start doing research on the medical and genetic effects of nuclear bombs. But he needed a way to quantify just how the information encoded in an organism was changed by exposure to radiation. ‘A godsend, these formulas, splendid! I can go on now,’ Quastler is said to have remarked upon learning about Shannon’s theory. He wrote a paper in 1949 – just a year after Shannon’s work was published – entitled ‘The information content and error rate of living things’. It set off the study of information in biology.</p>
<p class="TXI">Neuroscience was not slow to follow. In 1952, Warren McCulloch and physicist Donald MacKay published ‘The limiting information capacity of a neuronal link’. In this paper, they derive what they consider to be the most optimistic estimate of how much information a single neuron could carry. Based on the average time it takes to fire an action potential, the minimum time <a id="page_197"></a>needed in between firing and other physiological factors, MacKay and McCulloch calculated an upper bound of 2,900 bits per second. </p>
<p class="TXI">MacKay and McCulloch were quick to stress that this doesn’t mean neurons actually <span class="italic">are</span> conveying that much information, only that under the best possible circumstances they could. After their paper, many more publications followed, each aiming to work out the true coding capacity of the brain. So awash in attempts was the field that in 1967 neuroscientist Richard Stein wrote a paper both acknowledging the appeal of information theory for quantifying nervous transmission but also lamenting the ‘enormous discrepancies’ that have resulted from its application. Indeed, in the work that followed MacKay and McCulloch’s, estimates ranged from higher than their value – 4,000 bits per second per neuron – to significantly lower, as meagre as one-third of a bit per second. </p>
<p class="TXI">This diversity came, in part, from different beliefs about how the parts and patterns of nervous activity should be mapped on to the formal components of Shannon’s information theory. The biggest question centred on how to define a symbol. Which aspects of neural activity are actually carrying information and which are incidental? What, essentially, is the neural code? </p>
<p class="TXI">Adrian’s original finding – that it is not the height of the spike that matters – still held strong.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> But even under this constraint, options abounded. Starting from the basic <a id="page_198"></a>unit of an action potential, scientists were still able to devise many conceivable codes. MacKay and McCulloch began by thinking of the neural code as composed of only two symbols: spike or no spike. At each point in time a neuron would send one or the other symbol. But after calculating the information rate of such a code, MacKay and McCulloch realised they could do better. Thinking instead of the <span class="italic">time between spikes</span> as the code allowed a neuron to transmit much more information. In this coding scheme, if there were a 20-millisecond gap in between two spikes, this would symbolise something different from a 10-millisecond gap. It’s a scheme that creates many more possible symbols and it was with this style of coding that they made their estimate of 2,900 bits per second. </p>
<p class="TXI">Stein, in his attempt to clean up the cacophony of codes on offer at the time, focused on a third option for neural coding – the one that came from Adrian himself. Adrian, after establishing that action potentials don’t change as the stimulus does, claimed that: ‘In fact, the only way in which the message can be made to vary at all is by a variation in the total number of the impulses and in the frequency with which they recur.’ This style of coding – where it is the number of spikes produced in a certain amount of time that serves as the symbol – is known as frequency or rate-based coding. In his 1967 paper, Stein argues for the existence of a rate-based code and highlights its benefits, including higher error tolerance. </p>
<p class="TXI">But the debate over what the true neural code is did not end with Stein in 1967. Nor did it end with Bullock and Perkel’s meeting on information coding in the brain <a id="page_199"></a>a year later. In fact, in their report on that meeting, Bullock and Perkel include an Appendix that lays out dozens of possible neural codes and how they could be implemented. </p>
<p class="TXI">In truth, neuroscientists continue to spar and struggle over the neural code to this day. They host conferences centred on ‘Cracking the neural code’. They write papers with titles like ‘Seeking the neural code’, ‘Time for a new neural code?’ and even ‘Is there a neural code?’ They continue to find good evidence for Adrian’s original rate-based coding, but also some against it. Identifying the neural code can seem a more distant goal now than when MacKay and McCulloch wrote their first musings on it. </p>
<p class="TXI">In general, some evidence of rate-based coding can be found in most areas of the brain. The neurons that send information from the eye change their firing frequency based on the intensity of light. Neurons that encode smell fire in proportion to the concentration of their preferred odour. And, as Adrian showed, receptors in the muscle and those in the skin fire more with more pressure applied to them. But some of the strongest evidence for other coding schemes comes from sensory problems that require very specific solutions. </p>
<p class="TXI">When localising the source of a sound, for example, precise timing matters. Because of the distance between the two ears, sound coming from the left or right side will hit one ear just before it hits the other. This gap between the times of arrival at each ear – sometimes as brief as only a few millionths of a second – provides a clue for calculating where the sound came from. The medial superior olive (MSO), a tiny cluster of cells <a id="page_200"></a>located right in between the two ears, is responsible for performing this calculation.</p>
<p class="TXI">The neural circuit that can carry this out was posited by psychologist Lloyd Jeffress in 1948 and has been supported by many experiments since. Jeffress’ model starts with information coming from each ear in the form of a temporal code – that is, the exact timing of the spikes matters. In the MSO, cells that receive inputs from each ear compare the relative timing of these two inputs. For example, one cell may be set up to detect sounds that arrive at both ears simultaneously. To do so, the signals from each ear would need to take the exact same amount of time to reach this MSO cell. This cell then fires when it receives two inputs at the exact same time and this response indicates that the sound hit both ears at the same time (see Figure 17). </p>
<p class="TXI">The cell next to this one, however, receives slightly asymmetric inputs. That is, the nerve fibre from one ear needs to travel a <span class="italic">little</span> farther to reach this cell than the nerve from the other ear. Because of this, one of the temporal signals gets delayed. The extra length the signal travels determines just how much extra time it takes. Let’s say the signal from the left ear takes an extra 100 microseconds to reach this MSO cell. Then, the only way this cell will receive two inputs at once is if the sound hits the left ear 100 microseconds before it hits the right. Therefore, this cell’s response (which, like the other cell, only comes when it receives two inputs at once) would signal a 100-microsecond difference.</p>
<p class="TXI">Continuing this pattern, the next cell may respond to a 200-microsecond difference, the one after that 300 microseconds and so on. In total, the cells in the MSO form a map wherein those that signal short arrival time <a id="page_201"></a>differences fall at one end and those that signal long differences fall at the other. In this way, a temporal code has been transformed into a <span class="italic">spatial</span> code: the position of the active neuron in this map carries information about the source of the sound.</p>
<p class="image-fig" id="fig17.jpg">
<img alt="" src="Images/chapter-07-image-02.jpg"/></p>
<p class="FC">
<span class="bold">
<span class="italic">Figure 17</span>
</span></p>
<p class="TXI">For the question of why the neural code is such an enigma, the most likely answer – as with so many questions of the brain – is because it’s complicated. Some neurons, in some areas of the brain, under some circumstances, may be using a rate-based code. Other neurons, in other times and places, may be using a code based on the timing of spikes, or the time in between spikes, or some other code altogether. As a result, the thirst to crack <span class="italic">the</span> neural code will likely never be quenched. The brain, it seems, speaks in too many different languages. </p>
<p class="center">* * *</p>
<p class="TXT">Evolution did not furnish the nervous system with one single neural code, nor did it make it easy for scientists to <a id="page_202"></a>find the multitude of symbols it uses. But, according to British neuroscientist Horace Barlow, evolution did thankfully provide one strong guiding light for our understanding of the brain’s coding scheme. Barlow is known as one of the founders of the efficient coding hypothesis, the idea that – no matter what code the brain uses – it is always encoding information <span class="italic">efficiently</span>. </p>
<p class="TXI">Barlow was a trainee of Lord Adrian. He worked with him – when he could find him – as a student at Cambridge in 1947. Barlow had always had a keen interest in physics and mathematics but, to be practical, chose to study medicine.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> Yet throughout his studies he recognised how the influence from more quantitative subjects could drive questions in biology. It was a trait he considered in contrast to his mentor: ‘[Adrian] was not at all theoretically based; his attitude was that we had the means of recording from nerve fibres and we should just see what happens.’ </p>
<p class="TXI">Quickly taken in by Shannon’s equations when they came about, Barlow made several early contributions to the study of information in the brain. Rather than simply counting bits per second, however, Barlow’s use of information theory went deeper. The laws of information, in some respects, are as fundamental and constraining of biology as the laws of physics. From Barlow’s perspective, these equations could thus do more than merely <span class="italic">describe</span> the brain as it is, but rather <span class="italic">explain</span> how it came to be. So certain of its importance to neuroscience, Barlow compared trying to study the brain without focusing on <a id="page_203"></a>information processing to trying to understand a wing without knowing that birds fly. </p>
<p class="TXI">Barlow came to his efficient coding hypothesis by combining reflections on information theory with observations of biology. If the brain evolved within the constraints of information theory – and evolution tends to find pretty good solutions – then it makes sense to conclude that the brain is quite good at encoding information. ‘The safe course here is to assume that the nervous system is efficient,’ Barlow wrote in a 1961 paper. If this is true, any puzzle about why neurons are responding the way they are may be solved by assuming they are acting efficiently. </p>
<p class="TXI">But what does efficient information coding look like? For that, Barlow focused on the notion of redundancy. In Shannon’s framework, ‘redundancy’ refers to the size of the gap between the highest possible entropy a given set of symbols could have and the entropy they actually have. For example, if a code has two symbols and uses one of them 90 per cent of the time and the other only 10 per cent, its entropy is not as high as it could be. Sending the same symbol nine out of ten times is redundant. As we saw earlier, the code with the highest entropy would use each of those symbols 50 per cent of the time and would have a redundancy of zero. Barlow believed efficient brains reduce their redundancy as much as possible.</p>
<p class="TXI">The reason for this is that redundancy is a waste of resources. The English language, as it turns out, is incredibly redundant. A prime example of this is the letter ‘q’, which is almost always followed by ‘u’. The ‘u’ adds little if any information once we see the ‘q’ and is <a id="page_204"></a>therefore redundant. The redundancy of English means we could, in theory, be conveying the same amount of information with far fewer letters. In fact, in his original 1948 paper, Shannon estimated the redundancy of written English to be about 50 per cent. This is why, for example, ppl cn stll rd sntncs tht hv ll th vwls rmvd.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> </p>
<p class="TXI">In the nervous system, redundancy can come in the form of multiple neurons saying the same thing. Imagine one neuron represents the letter ‘q’ and another the letter ‘u’. The sight of ‘qu’ would thus make both these neurons fire. But if these two letters frequently appear together in the world, it would be more efficient of the brain to use just a single neuron to respond to them.</p>
<p class="TXI">Why should it matter if the brain is efficient with its encoding? One reason is energy costs. Every time a neuron fires a spike, the balance of charged particles inside and outside the cell gets thrown off. Restoring this balance takes energy: little pumps in the cell membrane have to chuck sodium ions out of the cell and pull potassium ones back in. Building up neuro­transmitters and expelling them from the cell with each spike also incurs a cost. In total, it’s estimated that up to three-fourths of the brain’s energy budget goes towards sending and receiving signals. And the brain – using 20 per cent of the body’s energy while accounting for only 2 per cent of its weight – is the most energetically expensive organ to run. With such a high energy bill, it <a id="page_205"></a>makes sense for the brain to be economical in how it uses its spikes. </p>
<p class="TXI">But to know how to send information efficiently, the brain needs to know what kind of information it normally needs to send. In particular, the brain needs to somehow determine when the information it is receiving from the world is redundant. Then it could simply not bother sending it on. This would keep the neural code efficient. Does the nervous system have the ability to track the statistics of the information it’s receiving and match its coding scheme to the world around it? One of Lord Adrian’s own findings – adaptation – suggests it does.</p>
<p class="TXI">In his experiments on muscle stretch receptors, Adrian noticed that ‘there is a gradual decline in the frequency of the discharge under a constant stimulus’. Specifically, while keeping the weight applied to the muscle constant, the firing rate of the nerve would decrease by about half over 10 seconds. Adrian called this phenomenon ‘adaptation’ and defined it as ‘a decline in excitability caused by the stimulus’. Noticing the effect in several of his experiments, he devoted a whole chapter to the topic in his 1928 book. </p>
<p class="TXI">Adaptation has since been found all over the nervous system. For example, the ‘waterfall effect’ is a visual illusion wherein the sight of movement in one direction then causes stationary objects to appear as though they are moving in the opposite direction. It’s so-named because it can happen after staring at the downward motion of a waterfall. The effect is believed to be the result of adaptation in the cells that represent the original motion direction: with these cells silenced by adaptation, <a id="page_206"></a>our perception is biased by the firing of cells that represent the opposite direction.</p>
<p class="TXI">In his 1972 paper, Barlow argues for adaptation as a means of increasing efficiency: ‘If sensory messages 
are to be given a prominence proportional to their informational value, mechanisms must exist for reducing the magnitude of representation of patterns which are constantly present, and this is presumably the underlying rationale for adaptive effects.’</p>
<p class="TXI">In other words – specifically, in the words of information theory – if the same symbol is being sent across the channel over and over, its presence no longer carries information. Therefore, it makes sense to stop sending it. And that is what neurons do: they stop sending spikes when they see the same stimulus over and over. </p>
<p class="TXI">Since the time that Barlow made the claim that cells should adapt their responses to the signals they’re receiving, techniques for tracking how neurons encode information have developed that allow for more direct and nuanced tests of this hypothesis. In 2001, for example, computational neuroscientist Adrienne Fairhall, along with colleagues at the NEC Research Institute in Princeton, New Jersey, investigated the adaptive abilities of visual neurons in flies. </p>
<p class="TXI">For their experiment, the researchers showed the flies a bar moving left and right on a screen. At first, the bar’s motion was erratic. At one moment it could be moving very quickly leftwards, and at the next it could go equally fast towards the right, or it could stay in that direction, or it could slow down entirely. In total, its range of possible speeds was large. After several seconds of such chaos, the bar then calmed down. Its movement became more constrained, never going too quickly in either direction. <a id="page_207"></a>Over the course of the experiment, the bar switched between such periods of erratic and calm movement several times. </p>
<p class="TXI">Looking at the activity of the neurons that respond to motion, the researchers found that the visual system rapidly adapts its code to the motion information it’s currently getting. Specifically, to be an efficient encoder, a neuron should always fire at its peak firing rate for the fastest motion it sees and at its lowest for the slowest.<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> Thinking of different rates of firing as different symbols in the neural code, spreading the firing rates out this way ensures that all these symbols get used roughly equally. This maximises the entropy of the code. </p>
<p class="TXI">The problem is that the fastest motion during the period of calm is much slower than the fastest motion during the more erratic period. This means that the same exact speed needs to map to two different firing rates depending on the context it appears in. Strange as it is, this is just what Fairhall and colleagues saw. During the calm period, when the bar was moving at its fastest speed, the neuron fired at over 100 spikes per second. Yet when that same speed occurred during the erratic period, the neuron only fired about 60 times per second. To get the neuron back up to 100 spikes per second during the erratic period, the bar needed to move 10 times as fast. </p>
<p class="TXI">In addition, the researchers were able to quantify the amount of information carried by a spike before and after <a id="page_208"></a>the switch between these two types of motion. During the erratic period, the information rate was around 1.5 bits per spike. Immediately after the switch to calm movement, it dropped to just 0.8 bits per spike: the neuron, having not yet adapted to the new set of motion it was seeing, was an inefficient encoder. After just a fraction of a second of exposure to the calmer motion, however, the bits per spike went right back up to 1.5. The neuron needed just a small amount of time to monitor the range of speeds it was seeing and adapt its firing patterns accordingly. This experiment shows that, just as Barlow’s efficient coding theory suggests, adaptation ensures that all types of information are encoded efficiently. </p>
<p class="TXI">Neuroscientists also believe that the brain is built to produce efficient encodings on much longer timescales than the seconds to minutes of a sensory experience. Through both evolution and development, an organism has a chance to sample its environment and adapt its neural code to what is most important to it. By assuming that a given area of the brain is best-suited to represent relevant information as efficiently as possible, scientists are attempting to reverse engineer the evolutionary process.</p>
<p class="TXI">The 30,000 nerves that leave the human ear, for example, respond to different types of sounds. Some of the neurons prefer short blips of high-pitched noises, others prefer low-pitched noises. Some respond best when a soft sound gets louder, others when a loud sound gets softer and others still when a soft sound gets louder then softer again. Overall, each nerve fibre has a complex pattern of pitches and volumes that best drive its firing.</p>
<p class="TXI">Scientists know, for the most part, <span class="italic">how</span> the fibres end up with these responses. Tiny hairs connected to cells in <a id="page_209"></a>the inner ear move in response to sounds. Each cell responds to a different pitch based on where it is in a small, spiral-shaped membrane. The nerve fibres that leave the ear get inputs from these hairy cells. Each fibre combines pitches in its own way to make its unique, combined response profile. </p>
<p class="TXI">What is less clear, however, is <span class="italic">why</span> the fibres have these responses. That’s where ideas from information theory can help. </p>
<p class="TXI">If the brain is indeed reducing redundancy as Barlow suggests, then only a small number of neurons should be active at a time. Neuroscientists refer to this kind of activity as ‘sparse’.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> In 2002, computational neuroscientist Michael Lewicki asked whether the response properties of auditory nerves could be the result of the brain enforcing a sparse code – one specifically designed for the sounds an animal needs to process.</p>
<p class="TXI">To answer this, he first had to gather a collection of different natural sounds. One set of sounds came from a CD of vocalisations made by rainforest animals such as bats, manatees and marmosets; another was a compilation of ‘background’ noises like crunching leaves and snapping twigs; and the third was from a database of human voices reading English sentences. </p>
<p class="TXI">Lewicki then used an algorithm to decompose these complex sounds into a dictionary of short sound patterns. <a id="page_210"></a>The goal of the algorithm was to find the best decomposition – that is, one that can recreate each full, natural sound using as few of the short sound patterns as possible. In this way, the algorithm was seeking a sparse code. If the brain’s auditory system evolved to sparsely encode natural sounds, the sound patterns preferred by the auditory nerves should match those found by the algorithm. </p>
<p class="TXI">Lewicki found that creating a dictionary from just the animal noises alone produced sound patterns that didn’t match the biology. Specifically, the patterns the algorithm produced were too simple – representing just pure tones rather than the complex mix of pitches and volumes that human and animal auditory nerves tend to prefer. Applying the algorithm to a mix of animal noises and background sounds, however, did mimic the biology. This suggests that the coding scheme of the auditory system is indeed matched to these environmental sounds, allowing it to encode them efficiently. What’s more, Lewicki found that a dictionary made from human speech also reproduced the sound profiles preferred by biology. Lewicki took this as evidence for the theory that human speech evolved to make best use of the existing encoding scheme of the auditory system.<sup><a href="#fn-9" id="fnt-9">9</a>
</sup></p>
<p class="center">* * *</p>
<p class="TXT">In 1959, Barlow presented his ideas about the brain’s information-processing properties to a group of sensory <a id="page_211"></a>researchers gathered at MIT. When the proceedings of this meeting were translated into Russian for a Soviet audience, Barlow’s contribution was conspicuously cut out. The Soviets, it turned out, had a problem with the use of information theory to understand the brain. Considered part of the ‘bourgeois pseudoscience’ of cybernetics, it ran counter to official Soviet philosophy by equating man with machine. Soviet leaders – and the sometimes scared scientists under their rule – openly critiqued this attitude as a foolish product of American capitalism. </p>
<p class="TXI">Though unique in its political motivations, the Soviets’ denunciation was far from the only critique of information theory in biology. In 1956, a short article entitled ‘The bandwagon’ cautioned against the over-excited application of information theory in fields such as psychology, linguistics, economics and biology. ‘Seldom do more than a few of nature’s secrets give way at one time. It will be all too easy for our somewhat artificial prosperity to collapse overnight when it is realised that the use of a few exciting words like <span class="italic">information</span>, <span class="italic">entropy</span>, <span class="italic">redundancy</span>, do not solve all our problems.’ The article was written by Shannon himself, just eight years after he unleashed information theory on to the world. </p>
<p class="TXI">Concerns about how apt the analogy is between Shannon’s framework and the brain have even come from the very scientists doing the analogising. In a 2000 article, Barlow warned that ‘the brain uses information in different ways from those common in communication engineering’. And Perkel and Bullock, in their original report, made a point of not committing themselves fully to Shannon’s definition of information but, rather, <a id="page_212"></a>treating the concept of ‘coding’ in the brain as a metaphor that may have varying degrees of usefulness.</p>
<p class="TXI">The caution is warranted. A particularly tricky part of Shannon’s system to map to the brain is the decoder. In a simple communication system, the receiver gets the encoded message through the channel and simply reverses the process of encoding in order to decode it. The recipient of a telegraph message, for example, would use the same reference table as the sender to know how to map dots and dashes back to letters. The system in the brain, however, is unlikely to be so symmetric. This is because the only ‘decoders’ in the brain are other neurons, and what they do with the signal they receive can be anyone’s guess. </p>
<p class="TXI">Take, for example, encoding in the retina. When a photon of light is detected, some of the cells in the retina (the ‘on’ cells) encode this through an increase in their firing rate while another set of cells (the ‘off’ cells) encodes it by decreasing their firing. If this joint up–down change in firing is the symbol the retina has designated to indicate the arrival of a photon, we may assume this is also the symbol that is ‘decoded’ by later brain areas. However, this does not seem to be the case.</p>
<p class="TXI">In 2019, a team of researchers from Finland genetically modified the cells in a mouse’s retina. Specifically, they made the ‘on’ cells less sensitive to photons. Now, when a photon hit, the ‘off’ cells would still decrease their firing, but the ‘on’ cells may or may not increase theirs. The question was: which set of cells would the brain listen to? The information about the photon would be there for the taking if the ‘off’ cells <a id="page_213"></a>were decoded. Yet the animals didn’t seem to use it. By assessing the animal’s ability to detect faint lights, it appeared that the brain was reading out the activity of the ‘on’ cells alone. If those cells didn’t signal that a photon was detected, the animal didn’t respond. The scientists took this to mean that the brain is not, at least in this case, decoding all of the encoded information. It ignores the signals the ‘off’ cells are sending. Therefore, the authors wrote, ‘at the sensitivity limit of vision, the decoding principles of the brain do not produce the optimal solution predicted by information theory’. Just because scientists can spot a signal in the spikes doesn’t mean it has meaning to the brain. </p>
<p class="TXI">There are many reasons this might be. One important one is that the brain is an information-<span class="italic">processing</span> machine. That is, it does not aim to merely reproduce messages sent along it, but rather to transform them into action for the animal. It is performing computations on information, not just communicating it. Expectations about how the brain works based solely on Shannon’s communication system therefore miss this crucial purpose. The finding that the brain is not optimally transmitting information does not necessarily indicate a flaw in its design. It was just designed for something else. </p>
<p class="TXI">Information theory, invented as a language for engineered communication systems, couldn’t be expected to translate perfectly to the nervous system. The brain is not a mere telephone line. Yet parts of the brain do engage in this more basic task of communication. Nerves do send signals. And they do so through some kind of code based on spike rates or spike times or spike <a id="page_214"></a>something. To glance at the brain from the vantage point of information theory, then, is a sensible endeavour – one that has yielded many insights and ideas. Stare too long, though, and the cracks in the analogy become visible. This is the reason for 
wariness. As a metaphor, the relationship between a communication system and the brain is thus most fruitful when not overextended.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-1" id="fn-1">1</a> ﻿Marvin Minsky, one of the authors of the ﻿<span class="italic">Perceptrons</span>﻿ book from ﻿﻿Chapter 3﻿﻿, was working under Shannon at the time and is credited with the design of the Ultimate Machine. Shannon reportedly convinced Bell Labs to produce several of them as gifts for AT&amp;T executives. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-2" id="fn-2">2</a> ﻿More on probability and its history in ﻿﻿Chapter 10﻿﻿.﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-3" id="fn-3">3</a> ﻿The base-two log wasn﻿’﻿t the only option for information. Prior to Shannon﻿’﻿s work, his colleague Ralph Hartley posited a definition of information using a base-10 log, which would﻿’﻿ve put information in terms of ﻿‘﻿decimal digits or ﻿‘﻿dits﻿’﻿ instead of bits. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-4" id="fn-4">4</a> ﻿Full disclosure: some modern neuroscientists are exploring the idea that action potentials actually ﻿<span class="italic">do</span>﻿ change in certain ways depending on what inputs the cell gets and that these changes ﻿<span class="italic">could</span>﻿ be part of the neural code. Science is never set in stone. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-5" id="fn-5">5</a> ﻿Barlow credits his mother Nora, granddaughter of Charles Darwin, with his interest in science. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-6" id="fn-6">6</a> ﻿In case you couldn﻿’﻿t: ﻿‘﻿people can still read sentences that have all the vowels removed﻿’﻿. Texting and tweeting are also great ways to see just how many letters can be removed from a word before causing problems. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-7" id="fn-7">7</a> ﻿Technically if the neuron has a preferred direction of motion ﻿–﻿ that is, it fires most strongly for, say, rightwards motion ﻿–﻿ it should fire at its peak rate for high speed in that direction and its lowest rate for high speed in the opposite direction. But the principle remains the same regardless. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-8" id="fn-8">8</a> ﻿Among neuroscientists, the ﻿‘﻿grandmother cell﻿’﻿ is considered the mascot of sparse coding. This fictional neuron is meant to be the one and only cell to fire when you see your grandmother (and fires in response to nothing else). Such an extreme example of efficient coding was devised by Jerome Lettvin (the frog guy from the last chapter) in order to vividly demonstrate the concept to his students. ﻿</p>
<p class="FN1"><a href="chapter7.xhtml#fnt-9" id="fn-9">9</a> ﻿If, while reading the last chapter, you wondered why it is that neurons in the visual system detect lines, information theory has an answer to that, too. In 1996, Bruno Olshausen and David Field applied a similar technique as Lewicki to show that lines are what you﻿’﻿d expect neurons to respond to if they are encoding images efficiently. ﻿</p>
</body>
</html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML">
<head>
<meta charset="utf-8"/>
<title>Contents</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="FMT" id="re_contents">Contents</p>
<p class="TOC-CH"><a href="chapter1.xhtml#chapter1" id="re_chapter1">Chapter 1:  Spherical Cows</a></p>
<p class="TOC-CH"><a href="chapter2.xhtml#chapter2" id="re_chapter2">Chapter 2:  How Neurons Get Their Spike</a></p>
<p class="TOC-CH"><a href="chapter3.xhtml#chapter3" id="re_chapter3">Chapter 3:  Learning to Compute</a></p>
<p class="TOC-CH"><a href="chapter4.xhtml#chapter4" id="re_chapter4">Chapter 4:  Making and Maintaining Memories</a></p>
<p class="TOC-CH"><a href="chapter5.xhtml#chapter5" id="re_chapter5">Chapter 5:  Excitation and Inhibition</a></p>
<p class="TOC-CH"><a href="chapter6.xhtml#chapter6" id="re_chapter6">Chapter 6:  Stages of Sight</a></p>
<p class="TOC-CH"><a href="chapter7.xhtml#chapter7" id="re_chapter7">Chapter 7:  Cracking the Neural Code</a></p>
<p class="TOC-CH"><a href="chapter8.xhtml#chapter8" id="re_chapter8">Chapter 8:  Movement in Low Dimensions</a></p>
<p class="TOC-CH"><a href="chapter9.xhtml#chapter9" id="re_chapter9">Chapter 9:  From Structure to Function</a></p>
<p class="TOC-CH"><a href="chapter10.xhtml#chapter10" id="re_chapter10">Chapter 10:  Making Rational Decisions</a></p>
<p class="TOC-CH"><a href="chapter11.xhtml#chapter11" id="re_chapter11">Chapter 11:  How Rewards Guide Actions</a></p>
<p class="TOC-CH"><a href="chapter12.xhtml#chapter12" id="re_chapter12">Chapter 12:  Grand Unified Theories of the Brain</a></p>
<p class="TOC-CH1"><a href="Mathematical.xhtml#Mathematical" id="re_Mathematical">Mathematical Appendix</a></p>
<p class="TOC-CH"><a href="ack.xhtml#ack" id="re_ack">Acknowledgements</a></p>
<p class="TOC-CH"><a href="bib.xhtml#bib" id="re_bib">Bibliography</a></p>
<p class="TOC-CH"><a href="index.xhtml#index" id="re_index">Index</a></p>
</body>
</html>