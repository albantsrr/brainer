<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="en" xml:lang="en">
<head>
<title>Chapter 5</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter5"><a href="contents.xhtml#re_chapter5">CHAPTER FIVE</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter5">Excitation and Inhibition</a><a id="page_117"/></p>
<p class="H1" id="b-9781472966445-ch495-sec5">
<span class="bold">
<span>The balanced network and oscillations</span>
</span></p> 
<p class="TXT">Within nearly every neuron, a battle is raging. This fight – a struggle over the ultimate output of the neuron – pits the two fundamental forces of the brain against each other. It’s a battle of excitation versus inhibition. Excitatory inputs encourage a neuron to fire. Inhibitory inputs do the opposite: they push the neuron farther from its threshold for spiking. </p>
<p class="TXI">The balance between these two powers defines the brain’s activity. It determines which neurons fire and when. It shapes their rhythms – rhythms that are implicated in everything from attention to sleep to memory. Perhaps more surprisingly, the balance between excitation and inhibition can also explain a feature of the brain that has haunted scientists for decades: the notorious unreliability of neurons. </p>
<p class="TXI">Eavesdrop on a neuron that should be doing the same thing over and over – for example, one in the motor system that is producing the same movement repeatedly – and you’ll find its activity surprisingly irregular. Rather than repeating the same pattern of spikes verbatim each time, it will fire more on some attempts and less on others. </p>
<p class="TXI">Scientists learned about this peculiar habit of neurons early in the days of neural recordings. In 1932, physiologist Joseph Erlanger made an update to the equipment in his <a id="page_118"/>St Louis laboratory that let him record neural activity with 20 times the sensitivity previously available. He, along with his colleague E. A. Blair, was finally able to isolate individual neurons in the leg of a frog and record how they responded to precise pulses of electricity – 58 identical pulses per minute to be exact.</p>
<p class="TXI">To their surprise, Erlanger and Blair found that those identical pulses did not produce identical responses: a neuron may respond to one pulse of current, but not the next. There was still a relationship between the strength of the pulse and the response: when weak currents were used, for example, the neuron would respond, say, 10 per cent of the time, medium currents half the time and so on. But beyond these probabilities, how a neuron responded to any given pulse seemed a matter of pure chance. As the pair wrote in their 1933 paper in the <span class="italic">American Journal of Physiology</span>: ‘We were struck by the kaleidoscopic appearance of [responses] obtained from large nerves under absolutely constant conditions.’ </p>
<p class="TXI">This work was one of the first studies to systematically investigate the mysterious irregularity of the nervous system, but it would be far from the last. In 1964, for example, a pair of American scientists applied the same brushing motion to a monkey’s skin over and over. They reported that the activity of neurons responding to this motion appeared as ‘a series of irregularly recurring impulses, so that, in general, no ordered pattern can be detected by visual inspections’. </p>
<p class="TXI">In 1983, a group of researchers from Cambridge and New York noted that: ‘The variability of cortical neuron response[s] is known to be considerable.’ Their study of the visual system in cats and monkeys showed again that <a id="page_119"/>the neural response to repeats of the same image produced different results. The responses still had <span class="italic">some</span> relation to the stimulus – the cells still changed how much they fired <span class="italic">on average</span> to different images. But exactly which neuron would fire and when for any given instance seemed as unpredictable as next week’s weather. ‘Successive presentations of identical stimuli do not yield identical responses,’ the authors concluded. </p>
<p class="TXI">In 1998, two prominent neuroscientists even went so far as to liken the workings of the brain to the randomness of radioactive decay, writing that neurons have ‘more in common with the ticking of a Geiger counter than of a clock’. </p>
<p class="TXI">Decades of research and thousands of papers have resulted in a tidy message about just how messy the nervous system is. Signals coming into the brain appear to impinge on neurons already flickering on and off at their own whims. Inputs to these neurons can influence their activity, but not control it exactly – there will always be some element of surprise. This presumably useless chatter that distracts from the main message the neuron is trying to send is referred to by neuroscientists as ‘noise’. </p>
<p class="TXI">As Einstein famously said with regard to the new science of quantum mechanics: ‘God does not play dice.’ So why should the brain? Could there be any good reason for evolution to produce noisy neurons? Some philosophers have claimed that the noise in the brain could be a source of our free will – a way to overcome a view of the mind as subject to the same deterministic laws of any machine. Yet others disagree. As British philosopher Galen Strawson wrote: ‘It may be that some <a id="page_120"/>changes in the way one is are traceable … to the influence of indeterministic or random factors. But it is absurd to suppose that indeterministic or random factors, for which one is [by definition] in no way responsible, can in themselves contribute in any way to one’s being truly morally responsible for how one is.’ In other words, following decisions based on a coin flip isn’t exactly ‘free’ either. </p>
<p class="TXI">Other purposes for this unpredictability have been posited by scientists. Randomness, for example, can help to learn new things. If someone walks the same path to work every day, occasionally taking a random left turn could expose them to an unknown park, a new coffee shop or even a faster path. Neurons might benefit from a little exploration as well and noise lets them do that. </p>
<p class="TXI">In addition to the question of <span class="italic">why</span> neurons are noisy, the question of <span class="italic">how</span> they end up this way has preoccupied neuroscientists. Possible sources of noise exist outside the brain. Photoreceptors in the eye, for example, need to be hit by a certain number of photons before they respond. But even a constant source of light can’t guarantee that a constant stream of photons will reach the eye. In this way, the input to the nervous system itself could be unreliable. </p>
<p class="TXI">In addition, several elements of a neuron’s function depend on random processes. The electrical state of a neuron, for example, will change if the diffusion of ions in the fluid around it does. And neurons, like any other cells, are made up of molecular machines that don’t always function according to plan: necessary proteins might not be produced fast enough, moving parts may get stuck, <span class="italic">etc.</span> While these physical failures could <a id="page_121"/>contribute to the brain’s noisiness they don’t seem to fully account for it. In fact, when neurons are taken from the cortex and put into a Petri dish they behave remarkably more reliably: stimulating these neurons the same way twice will actually produce similar results. Therefore, basic lapses in cellular machinery – which can happen in the dish just as well as in the brain – seem insufficient to explain the noise that is normally observed. </p>
<p class="TXI">The books are therefore not balanced: the noise put in somehow doesn’t equal the noise produced. We could suspect this is just a curious error in accounting; perhaps there are a few extra unreliable cogs in the neural machinery, or inputs from the world are even less stable than we believe. Such mis-estimates could maybe make up the difference, if it weren’t for one small fact: the very nature of how neurons work makes them noise <span class="italic">reducers</span>. </p>
<p class="TXI">To understand this, imagine you and some friends are playing a game where the goal is simply to see how far you can collectively move a football down a long field before a timer runs out. None of you are very well practised and occasionally you make mistakes – one person misses a pass, another gets tired, someone else trips. You also occasionally exceed your own expectations by running extra fast or passing extra far. If the time allotted is small – say 30 seconds – such momentary lapses or advantages will have big effects on your distance. You could go 150m on one try and 20 the next. But if the time is large, say five minutes, these fluctuations in performance may simply balance each other out: a slow start could be made up for with an intense sprint at the end, or the gain from a long pass could be lost because of a fall. As a result, the longer the <a id="page_122"/>time, the more similar the distance will be on each try. In other words, the ‘noisiness’ of your athletic ability gets averaged out over time. </p>
<p class="TXI">Neurons find themselves in a similar situation. If a neuron gets enough input within a certain amount of time, it will fire a spike (see Figure 12). The input it gets is noisy because it comes from the firing of other neurons. So, the neuron may receive, say, five inputs at one moment, 13 the next and zero after that. Just like in the game example, if the neuron takes in this noisy input for a long time before deciding if it has enough to spike, the impact of the noise will be reduced. If it only uses a quick snapshot of its input, however, the noise will dominate.</p>
<p class="TXI">So how much time does a neuron combine its inputs over? About 20 milliseconds. That may not seem like long, but for a neuron it’s plenty. A spike only takes about 1 millisecond and a cell can be receiving many at a time from all of its different inputs. Therefore, neurons should be able to take the average over many snapshots of input before deciding to spike. </p>
<p class="TXI">Neuroscientists William Softky and Christof Koch used a simple mathematical model of a neuron – the ‘leaky integrate-and-fire’ model introduced in <a href="chapter2.xhtml#chapter2">Chapter 2</a> – to test just this. In their 1993 study, they simulated a neuron receiving inputs at irregular times. Yet the neuron itself – because it integrated these incoming spikes over time – still produced output spikes that were much more regular than the input it received. This means neurons do have the power to destroy noise – to take in noisy inputs while producing less noisy outputs.</p>
<p class="image-fig" id="fig12.jpg">
<img alt="" src="../images/fig12.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 12</span>
</span></p>
<p class="TXI">If neurons weren’t able to quench noise, then the unreliability of the brain wouldn’t be as much of a <a id="page_123"/>mystery. As mentioned before, we could assume that small amounts of randomness enter the brain – either from the outside world or from inside a cell – and spread through the system via the connections between neurons. If noisy inputs led to outputs that were just as, or possibly more, noisy, this would be a perfectly self-consistent story; noisy neurons would beget noisy neurons. But, according to Softky and Koch’s model, this is not what happens. When passed through a neuron, noise should get weaker. When passed over and over through a whole network of neurons, it should be expected to all but disappear. Yet everywhere neuroscientists look, there it is.</p>
<p class="TXI">Not only is the brain unpredictable, then, but it seems to be encouraging that unpredictability – going against the natural tendency of neurons to squash it. What’s keeping the randomness alive? Does the brain have a random-number generator? Some sort of hidden biological dice? Or, as scientists in the 1990s <a id="page_124"/>hypothesised, does all this disorder actually result from a more fundamental order, from the balance between excitation and inhibition?</p>
<p class="center">* * *</p>
<p class="TXT">It took Ernst Florey several trips to a Los Angeles horse butcher to uncover the source of inhibition in the brain.</p>
<p class="TXI">It was the mid-1950s when Florey, a German-born neurobiologist who emigrated to North America, was working on this question with his wife Elisabeth. At that time, the fact that neurons communicate by sending chemicals – called neurotransmitters – between each other was largely established. However, the only known neurotransmitters were excitatory – that is, they were chemicals that made a neuron <span class="italic">more</span> likely to fire. Yet since the mid-nineteenth century, it was known that some neurons can actually reduce the electrical activity of their targets. For example, the Weber brothers, Ernst and Eduard, showed in 1845 that electrically stimulating a nerve in the spinal cord could slow down the cells that controlled the beating of the heart, even bringing it to a standstill. This meant the chemical released by these neurons was inhibitory – it made cells less likely to fire. </p>
<p class="TXI">Florey needed specimens for his study on ‘factor I’, his name for the substance responsible for inhibition. So, he regularly rode his 1934 Chevrolet to a horse butcher and picked up some parts less favoured by their average customers: fresh brains and spinal cords. After extracting different substances from this nervous tissue, he checked what happened when each one was applied to living neurons taken from a crayfish. Eventually he identified <a id="page_125"/>some candidate chemicals that reliably quieted the crayfish neurons. This cross-species pairing was actually a bit of luck on Florey’s part. Neurotransmitters can’t always be assumed to function the same way across animals. But in this case, what was inhibitory for the horse was inhibitory for the crayfish. </p>
<p class="TXI">With the help of professional chemists, Florey then used tissue from yet another animal – 45kg (100lb) of cow brain to be precise – to purify ‘factor I’ down to its most basic chemical structure. In the end he was left with 18mg of gamma-aminobutyric acid. Gamma-aminobutyric acid (or GABA, as it is more commonly known) was the first identified inhibitory neurotransmitter. </p>
<p class="TXI">Whether a neurotransmitter is inhibitory or excitatory is really in the eye of the beholder – or more technically in the receptor of the target neuron. When a neurotransmitter is released from one neuron, the chemical travels the short distance across the synapse between that neuron and its target. It then attaches itself to receptors that line the membrane of the target neuron. These receptors are like little protein padlocks. They require the right key – that is, the correct neurotransmitter – to open. And once they open they’re pretty selective about who they let in. One kind of receptor that GABA attaches to, for example, only lets chloride ions into the cell. Chloride ions have a negative charge and letting more in makes it harder for the neuron to reach the electrical threshold it needs to fire. The receptors to which excitatory neurotransmitters attach let in positively charged ions, like sodium, which bring the neuron <span class="italic">closer</span> to threshold.</p> <a id="page_126"/>
<p class="TXI">Neurons tend to release the same neurotransmitter on to all of their targets, a principle known as Dale’s Law (named after Henry Hallett Dale who boldly guessed as much in 1934, a time at which only two neurotransmitters had even been identified). Neurons that release GABA are called ‘GABAergic’, although because GABA is the most prevalent inhibitory neurotransmitter in the adult mammalian brain, they’re frequently just called ‘inhibitory’. Excitatory transmitters are a bit more diverse, but the neurons that release them are still broadly classed as ‘excitatory’. Within an area of the cortex, excitatory and inhibitory neurons freely intermix, sending connections to, and receiving them from, each other.</p>
<p class="TXI">In 1991, after many of these facts of inhibition had been established, Florey wrote a retrospective on his role in the discovery of the first – and arguably most important – inhibitory neurotransmitter. He ended it with the sentence: ‘Whatever the brain does for the mind, we can be sure that GABA plays a major role in it.’ Likely unbeknownst to Florey, at the same time a theory that makes inhibition a key player in the production of the brain’s unpredictability was developing. </p>
<p class="center">* * *</p>
<p class="TXT">Returning to the analogy of the timed football game, imagine now that another team is added. Their goal is to fight against you, moving the ball to the opposite end of the field. When the clock stops, whoever is closer to their goal side wins. If the other team is also made up of your semi-athletic friends, then on average both teams <a id="page_127"/>would perform the same. The noisiness of your performance will still affect the outcome: your team may beat the other by a few metres on one try and get beaten by the same small amount on another. But on the whole, it will be a balanced and tame game. </p>
<p class="TXI">Now consider if the other team was made up of professional athletes – some of the strongest, fastest players around. In this case, you and your friends wouldn’t stand a chance; you’d be clobbered every time. This is why no one would bother watching a competition between career football players and the high school team, Tiger Woods vs your dad, or Godzilla against a literal moth. The outcome of all those matches is just too predictable to be interesting. In other words, unfair fights create consistency; fair fights are more fun. </p>
<p class="TXI">In the cortex, neurons get thousands of connections from both excitatory and inhibitory cells. Because of this, each individual force is strong and would consistently dominate if the other were any weaker. Without the presence of inhibition, for example, the hundreds of excitatory inputs bombarding a cell at any moment would make it fire almost constantly; on the other hand, inhibition alone would drive the cell down to a completely stagnant state. With huge power on each side, the true activity of the neuron is thus the result of a tug of war between giants. What’s happening in the neuron is indeed a balanced fight and it’s the kind you’d see at the Olympics rather than in a schoolyard.</p>
<p class="TXI">Tell this fact to a computer scientist and they may start to get worried. That’s because computer scientists know that taking the difference between too big, noisy numbers can cause big problems. In computers, numbers <a id="page_128"/>can only be represented with a certain level of precision. This means that some numbers need to be rounded off, introducing error – or noise – into the computation. For example, a computer with only three digits of precision may represent the number 18,231 as 1.82x10<sup>3</sup>; the leftover 31 is lost in the rounding error. When subtracting two roughly equal numbers, the effects of this rounding error can dominate the answer. For example, 18,231 minus 18,115 is 116, yet the computer would calculate this difference as 1.82x10<sup>3</sup> minus 1.81x10<sup>3</sup> which is only 10! That puts the computer off by 106. And the larger the numbers, the larger the error will be. For example, a computer with three-digit precision calculating 182,310 minus 181,150 would produce an answer that is 1,060 less than the true one. </p>
<p class="TXI">You reasonably wouldn’t feel comfortable if your bank or doctor’s office were doing their calculations this way. For this reason, programmers are taught to write their code in a way that avoids subtracting two very large numbers. Yet neurons are subtracting two large numbers – excitation minus inhibition – at every moment. Could such a ‘bug’ really be part of the brain’s operating system? </p>
<p class="TXI">Scientists had been toying with this idea for a while when, in 1994, Stanford neuroscientists Michael Shadlen and William Newsome decided to test it out. Similar to the work of Softky and Koch, Shadlen and Newsome built a mathematical model of a single neuron and fed it inputs. This time, however, the neuron got both noisy excitatory <span class="italic">and</span> noisy inhibitory inputs. When pitting these two forces against each other, sometimes excitation will win and sometimes inhibition will. Would the fight play out like a noisy calculation and create a neuron that <a id="page_129"/>fired erratically? Or would the neuron still be able to crush the noise in these inputs the way it had the excitatory inputs in Softky and Koch’s work? Shadlen and Newsome found that, indeed, given both these types of inputs – each coming in at the same, high rate – the neuron’s output was noisy. </p>
<p class="TXI">In a boxing match between amateurs, a momentary lapse in attention from one may let the other land a small hit. In a fight between pros, however, that same lapse could lead to a knockout. In general, the stronger that two competing powers are, the bigger the swings in the outcome of their competition. This is how the internal struggle between excitation and inhibition in a neuron can overpower its normal noise-crushing abilities. Because both sources are evenly matched, the neuron’s net input (that is the total excitation minus the total inhibition) is not very big on average. But because both sources are strong, the swings around that average are huge. At one moment the neuron can get pushed far above its threshold for firing and emit a spike. At the next it could be forced into silence by a wave of inhibition. These influences can make a neuron fire when it otherwise wouldn’t have or stay silent when it otherwise would have. In this way, a balance between excitation and inhibition creates havoc in a neuron and helps explain the variability of the brain.</p>
<p class="TXI">The simulation run by Shadlen and Newsome went a long way in helping to understand how neurons could remain noisy. But it didn’t quite go far enough. Real neurons get inputs from other real neurons. For the theory that noise results from a balance between excitation and inhibition to be correct, it thus has to <a id="page_130"/>work for a whole network of excitatory and inhibitory neurons. That means one in which each neuron’s input comes from the other neurons and its outputs go back to them too. Shadlen and Newsome’s simulation, however, was just of a single neuron, one that received inputs controlled by the model-makers. You can’t just look at the income and expenses of a single household and decide that the national economy is strong. Similarly, simulating a single neuron can’t guarantee that a network of neurons will work as needed. As we saw in the last chapter, in a system with a lot of moving parts, all of them need to move just right to get the desired outcome. </p>
<p class="TXI">To get a whole network to produce reliably noisy neurons requires coordination: each neuron needs to get excitatory and inhibitory input from its neighbours in roughly equal proportions. And the network needs to be <span class="italic">self-consistent</span> – that is, each neuron needs to produce the same amount of noise it receives, no more nor less. Could a network of interacting excitatory and inhibitory cells actually sustain the kind of noisy firing seen in the brain, or would the noise eventually peter out or explode?</p>
<p class="center">* * *</p>
<p class="TXT">When it comes to questions of self-consistency in networks, physicists know what to do. As we saw in the last chapter, physics is full of situations where self-consistency is important: gases made of large numbers of simple particles, for example, where each particle is influenced by all those around it and influences them all <a id="page_131"/>in return. So, techniques have been developed to make the maths of these interactions easier to work with.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">In the 1980s, Israeli physicist Haim Sompolinsky was using these techniques to understand the ways materials behave at different temperatures. But his interests eventually turned towards neurons. In 1996, Sompolinsky and fellow physicist-turned-neuroscientist Carl van Vreeswijk, applied the physicist’s approach to the question of balance in the brain. Mimicking the mathematics used to understand interacting particles, they wrote down some simple equations that represented a very large population of interacting excitatory and inhibitory cells. The population also received external inputs meant to represent connections coming from other brain areas.</p>
<p class="TXI">With their simple equations, it was possible for van Vreeswijk and Sompolinsky to mathematically define the kind of behaviour they wanted to see from the model. For example, the cells had to be able to keep themselves active, but not too active (they shouldn’t be firing non-stop, for example). In addition, they should respond to increases in external input by increasing their average firing rate. And, of course, the responses had to be noisy.</p>
<p class="TXI">Putting in these demands, van Vreeswijk and Sompolinsky then churned through the equations. They found that, to create a full network that will keep firing <a id="page_132"/>away irregularly at a reasonable rate, certain conditions had to be met. For example, the inhibitory cells need to have a stronger influence on the excitatory cells than the excitatory cells have on each other. Ensuring that the excitatory cells receive slightly more inhibition than excitation keeps the network activity in check. It’s also important that the connections between neurons be random and rare – each cell should only get inputs from, say, five or ten per cent of the other cells. This ensures no two neurons get locked into the same pattern of behaviour.</p>
<p class="TXI">None of the requirements van Vreeswijk and Sompolinsky found were unreasonable for a brain to meet. And when the pair ran a simulation of a network that met all of them, the necessary balance between excitation and inhibition emerged, and the simulated neurons looked as noisy as any real ones. Shadlen and Newsome’s intuition about how a single neuron can sustain noisy firing did in fact hold in a network of interacting neurons.</p>
<p class="TXI">Beyond just showing that balancing excitation and inhibition was possible in a network, van Vreeswijk and Sompolinsky also found a possible benefit of it: neurons in a tightly balanced network respond quickly to inputs. When a network is balanced, it’s like a driver with each foot pressed equally on the gas and the brake. This balance gets disrupted, however, if the amount of external input changes. Because the external inputs are excitatory – and they target the excitatory cells in the network more than the inhibitory ones – an increase in their firing is like a weight added to the gas pedal. The car then zooms off almost as quickly as the input came in. After this initial <span class="italic">whoosh</span> of a response, however, the <a id="page_133"/>network regains its balance. The explosion of excitation in the network causes the inhibitory neurons to fire more and – like adding an additional weight to the brake – the network resettles in a new equilibrium, ready to respond again. This ability to act so quickly in response to a changing input could help the brain accurately keep up with a changing world.</p>
<p class="TXI">Knowing that the maths works out is reassuring, but the real test of a theory comes from real neurons. Van Vreeswijk and Sompolinsky’s work makes plenty of predictions for neuroscientists to test, and that’s just what Michael Wehr and Anthony Zador at Cold Spring Harbor Laboratory did in 2003. The pair recorded from neurons in the auditory cortex of rats, which is responsible for processing sound, while different sounds were played to the animal. Normally when neuroscientists drop an electrode into the brain they’re trying to pick up on the output of neurons – that is, their spikes. But these researchers used a different technique to eavesdrop on the input a neuron was getting – specifically to see if the excitatory and inhibitory inputs balanced each other out.</p>
<p class="TXI">What they saw was that, right after the sound turned on, a strong surge of excitation came into the cell. It was followed almost immediately by an equal influx of inhibition – the brake that follows the gas. Therefore, increasing the input to this real network showed just the behaviour expected from the model. Even when using louder sounds that produced more excitation, the amount of inhibition that followed always matched it. Balance seemed to be emerging in the brain just as it did in the model.</p>
<p class="TXI">To explore another prediction of the model, scientists had to get a bit creative. Van Vreeswijk and Sompolinsky <a id="page_134"/>showed that, to make a well-balanced network, the strength of connections between neurons should depend on the total number of connections: with more connections, each connection can be weaker. Jérémie Barral and Alex Reyes from New York University wanted a way to change the number of connections in a network in order to test this hypothesis.</p>
<p class="TXI">Within a brain it’s hard to control just how neurons grow. So, in 2016, they decided to grow them in a Petri dish instead. It’s an experimental set-up that – in its simplicity, controllability and flexibility – is almost like a live version of a computer simulation. In order to control the number of connections, they simply put different numbers of neurons in the dish; dishes with more neurons packed in made more connections. They then monitored the activity of the neurons and checked their connection strengths. All the populations (which contained both excitatory and inhibitory cells) fired noisily, just as a balanced network should. But the connection strengths varied drastically. In a dish where each neuron only got about 50 connections, the connections were three times stronger than those with 500 connections. In fact, looking across all the populations, the average strength of a connection was roughly equal to one divided by the square root of the number of connections – exactly what van Vreeswijk and Sompolinsky’s theory predicted.</p>
<p class="TXI">As more and more evidence was sought, more was found for the belief that the brain was in a balanced state. But not all experiments went as the theory predicted; tight balance between excitation and inhibition wasn’t always seen. There is good reason to believe that certain <a id="page_135"/>brain areas engaged in certain tasks may be more likely to exhibit balanced behaviour. The auditory cortex, for example, needs to respond to quick changes in sound frequency to process incoming information. This makes the quick responsiveness of well-balanced neurons a good match. Other areas that don’t require such speed may find a different solution.</p>
<p class="TXI">The beauty of balance is that it takes a ubiquitous inhabitant of the brain – inhibition – and puts it to work solving an equally ubiquitous mystery – noise. And it does it all without any reliance on magic: that is, no hidden source of randomness. The noise comes even while neurons are responding just as they should.</p>
<p class="TXI">This counter-intuitive fact that good behaviour can produce bedlam is important. And it had been observed somewhere else before. Van Vreeswijk and Sompolinsky make reference to this history in the first word of the title of their paper: ‘<span class="italic">Chaos</span> in neuronal networks with balanced excitatory and inhibitory activity’.</p>
<p class="center">* * *</p>
<p class="TXT">Chaos didn’t exist in the 1930s: when neuroscientists were first realising how noisy neurons are, the mathematical theory to understand their behaviour hadn’t yet been discovered. When it was, it happened seemingly by chance. </p>
<p class="TXI">The Department of Meteorology at MIT was founded in 1941, just in time for Edward Lorenz. Lorenz, born in 1917 in a nice Connecticut neighbourhood to an engineer and a teacher, had an early interest in numbers, maps and the planets. He intended to continue on in <a id="page_136"/>mathematics after earning an undergraduate degree in it, but, as was the case for so many scientists of his time, the war intervened. In 1942 Lorenz was given the task of weather prediction for the US Army Air Corps. To learn how to do this, he took a crash course in meteorology at MIT. When he was done with the army he stayed with meteorology and remained at MIT: first as a PhD student, then a research scientist and finally a professor.</p>
<p class="TXI">If you’ve ever tried to plan a picnic, you know weather prediction is far from perfect. Academic meteorologists, who focus on the large-scale physics of the planet, hardly even consider day-to-day forecasting a goal. But Lorenz remained curious about it and about how a new technology – the computer – could help. </p>
<p class="TXI">The equations that describe the weather are many and complex. Churning through them by hand – to see how the weather right now will lead to the weather later – is an enormous, nearly impossible task (by the time you finish it the weather you’re predicting likely will have passed). But a computer could probably do it much faster. </p>
<p class="TXI">Starting in 1958, Lorenz put this to the test. He boiled the dynamics of weather down to 12 equations, picked some values to start them off with – say, westerly winds at 100km/hr – and let the mathematics run. He printed the output of the model on rolls of paper as it went along. It looked weather-like enough, with familiar ebbs and flows of currents and temperatures. One day he wanted to re-run a particular simulation to see how it would evolve over a longer period of time. Rather than starting it from the very beginning again, he figured he could start it part way along by <a id="page_137"/>putting in the values from the printout as the starting conditions. Impatience, sometimes, is the mother of discovery. </p>
<p class="TXI">The numbers that the computer printed out, however, weren’t the full thing. To fit more on the page, the printer cut the number of digits after the decimal point from six down to three. So, the numbers Lorenz put in for the second run of the simulation weren’t <span class="italic">exactly</span> where the model was before. But what could a few decimal points matter in a model of the whole world’s weather? Turns out quite a bit. After a few cranks of the mathematical machinery – about two months of weather changes in model time – this second run of the simulation was completely different from the first. What was hot was cold, what was fast was slow. What was supposed to be a replication turned into a revelation. </p>
<p class="TXI">Up until that point, scientists assumed that small changes only beget small changes. A little gust of wind at one point in time should have no power to move mountains later. Under that dogma, what Lorenz observed must’ve come from a mistake, a technical error made by the large, clunky computers of the day, perhaps. </p>
<p class="TXI">Lorenz was willing to see what was truly happening, however. As he wrote in 1991: ‘The scientist must always be on the lookout for other explanations than those that have been commonly disseminated.’ What Lorenz had observed was the true behaviour of the mathematics, as counter-intuitive as it seemed. In certain situations, small fluctuations <span class="italic">can</span> get amplified, making behaviour unpredictable. It’s not a mistake or an error; it’s just how complex systems work. Chaos – the name given to this <a id="page_138"/>phenomenon by mathematicians<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> – was real and it would do scientists well to try to understand it.</p>
<p class="TXI">Chaotic processes produce outputs that <span class="italic">look</span> random but in fact arise from perfect rule-following. The source of this deception is the upsetting truth that our ability to predict outcomes based on knowing the rules is far more limited than previously thought – especially if those rules are complex. In his book <span class="italic">Chaos: Making a New Science</span>, a sweeping history of how the field emerged, James Gleick wrote: ‘Traditionally, a dynamicist would believe that to write down a system’s equations is to understand the system … But because of the little bits of nonlinearity in these equations, a dynamicist would find himself helpless to answer the easiest practical questions about the future of the system.’ This imbues even the simplest systems of, say, interacting billiard balls or swinging pendulums with the potential to produce something surprising. He continued: ‘Those studying chaotic dynamics discovered that the disorderly behaviour of simple systems acted as a <span class="italic">creative</span> process. It generated complexity: richly organised patterns, sometimes stable and sometimes unstable, sometimes finite and sometimes infinite.’ </p>
<p class="TXI">Chaos was happening in the atmosphere – and if van Vreeswijk and Sompolinsky were right, it was happening in the brain, too. For this reason, explaining why the brain reacts to repeated inputs with a <a id="page_139"/>kaleidoscopic variety needn’t involve spotty cellular machinery. That’s not to say that there aren’t any sources of noise in the brain (such as unreliable ion channels or broken-down receptors), but just that an object as complex as the brain, with its interacting pools of excitation and inhibition, doesn’t require them to show rich and irregular responses. In fact, in their simulation of a network, all van Vreeswijk and Sompolinsky had to do was change the starting state of a single neuron – from firing to not, or vice versa – to create a completely different pattern of activity across the population.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> If a change so small can create such a disturbance, the brain’s ability to keep noise alive seems less mysterious.</p>
<p class="center">* * *</p>
<p class="TXT">In medical centres around the world, epilepsy patients spend several days – up to a week – stuck in small rooms. These ‘monitoring’ rooms are usually equipped with a TV – for the patients – and cameras that monitor patient movement – for the doctors. All day and night, the patients are connected to an electroencephalogram (EEG) machine that’s capturing their brain’s behaviour. They hope that the information gathered will help to treat their seizures.</p> <a id="page_140"/>
<p class="TXI">EEG electrodes, attached via stickers and tape to 
the scalp, monitor the electrical activity produced 
by the brain below. Each electrode provides one measurement – a complicated combination of the activity of many, many neurons at once. It’s a signal that varies over time like a seismograph. When patients are awake, the signal is a jagged and squiggly line: it moves slightly up and slightly down at random, but without any strong rhythm. When patients are asleep (particularly in deep dreamless sleep), the EEG makes waves: large movements upwards then downwards extending over a second or more. When the event of interest – a seizure – occurs, the movements are even starker. The signal traces out big, fast sweeps up and down, three to four times a second, like a kid scribbling frantically with a crayon. </p>
<p class="TXI">What are neurons doing to create these strong signals during a seizure? They’re working together. Like a well-trained military formation, they march in lockstep: firing in unison then remaining silent before firing again. The result is a repeated, synchronous burst of activity that drives the EEG signal up and down over and over again. In this way, a seizure is the opposite of randomness – it is perfect order and predictability. </p>
<p class="TXI">The same neurons that produce that seizure also produce the slow waves of sleep and the normal, noisy activity needed for everyday cognition. How can the same circuit exhibit these different behaviours? And how does it switch between them?</p>
<p class="TXI">In the late 1990s, French computational neuroscientist Nicolas Brunel set out to understand the different ways <a id="page_141"/>circuits can conduct themselves.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> Specifically, building off the work of van Vreeswijk and Sompolinsky, he wanted to investigate how models made of excitatory and inhibitory neurons behave. To do this, Brunel explored the <span class="italic">parameter space</span> of these models. </p>
<p class="TXI">Parameters are the knobs that can be turned on a model. They are values that define specific features, like the number of neurons in the network or how many inputs each gets. Like regular space, parameter space can be explored in many different directions, but here each direction corresponds to a different parameter. The two parameters Brunel chose to explore were, firstly, how much external input the network gets (<span class="italic">i.e.</span>, input from other brain areas) and, secondly, how strong the inhibitory connections in the network are compared with the excitatory ones. By changing each of these parameters a bit and churning through the equations, Brunel could check how the behaviour of the network depends on these values. </p>
<p class="TXI">Doing this for a bunch of different parameter values results in a map of the model’s behaviour. The latitude and longitude on this map (see Figure 13) correspond to the two parameters Brunel varied respectively. For the network at the middle of the map, the inhibition is exactly equal to the excitation and the input to the network is of medium strength. Moving to the left on the map, excitation becomes stronger than inhibition; <a id="page_142"/>move to the right and vice versa. Move upwards and the input to the network gets stronger, down and it’s weaker. Laid out this way, the network van Vreeswijk and Sompolinsky studied – with the inhibitory connections slightly stronger than excitatory ones – is just off to the right of the middle.</p>
<p class="image-fig" id="fig13.jpg">
<img alt="" src="../images/fig13.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 13</span>
</span></p>
<p class="TXI">Brunel surveyed this model landscape looking for any changes in the terrain: do certain sets of parameters make the network behave drastically differently? To find the first striking landmark you don’t have to travel far from van Vreeswijk and Sompolinsky’s original network. Crossing over from the region where inhibition is stronger into the one where excitation is, a sharp transition happens. In mathematics, these <a id="page_143"/>transitions are known as bifurcations. Like a steep cliff separating a grassy plain from a sea, bifurcations mark a quick change between two separate areas in parameter space. In Brunel’s map, the line where excitation and inhibition are equal separates the networks with irregular, noisy firing on the right from those with rigid, predictable firing on the left. Specifically, when inhibition gets too weak, the neurons in these networks stop their unique pitter-patter and start firing in unison. Their tight synchronous activity – with groups of neurons flicking on and off together – looks a lot like a seizure. </p>
<p class="TXI">Physiologists have known for centuries that certain substances act as convulsants – that is, they induce seizures. With the increased understanding of neurotransmitters that came in the mid-twentieth century, it became clear that many of these drugs interfered with inhibition. Bicuculline, for example, is found in plants across North America and stops GABA from attaching to its receptor. Thujone, present in low doses in absinthe, prevents GABA receptors from letting chloride ions in. Whatever the mechanism, in the end, these drugs are throwing the balance off in the brain, putting inhibitory influences at a disadvantage. Using his bird’s-eye view of the brain’s behaviour, Brunel could see how changing the brain’s parameters – through drugs or otherwise – moved it into different states. </p>
<p class="TXI">Travelling to the other end of Brunel’s map reveals yet another pattern of activity. In this realm, inhibition rules over excitation. If the external input remains at medium strength, the neurons remain noisy here. Move up or down, however, and two similar, but <a id="page_144"/>different, behaviours appear. With both high and low external input, the neurons show some cohesion. If you added up how many neurons were active at any given time, you’d see waves of activity: brief periods of more-than-average firing followed by less. But unlike the military precision of the seizure state, networks here are more like a percussion section made up of six-year-olds: there may be some organisation, but not everybody is playing together all the time. In fact, an individual neuron in these networks only participates in every third or fourth wave – and even then their timing isn’t always perfect. In this way, these states are both oscillating and noisy. </p>
<p class="TXI">The feature that differentiates behaviour in the top right corner of the map from that in the bottom right is the <span class="italic">frequency</span> of that oscillation. Drive the network with strong external inputs and its average activity will move up and down quickly – as fast as 180 times per second. The strong input drives the excitatory cells that drive the inhibitory cells to shut them down; then the inhibitory cells shut themselves down and the whole thing repeats. Reduce the network’s input and it oscillates more slowly, around 20 times a second. These slow oscillations occur because the external input to the network is so weak, and inhibition is so strong, that many neurons just don’t get enough input to fire. The ones that are firing, however, use their connections to slowly rev the network back up. If too many inhibitory cells get activated, though, the network becomes quiet again. </p>
<p class="TXI">Despite their superficial similarity to a seizure, these messy oscillations aren’t actually debilitating. In fact, scientists have observed oscillations in all different parts <a id="page_145"/>of the brain under all different conditions. Groups of neurons in the visual cortex, for example, can oscillate at a swift 60 times a second. The hippocampus (the memory-processing machine from last chapter) sometimes oscillates quickly and other times slowly. The olfactory bulb, where odours are processed, generates waves that range from once per second – aligning with inhalation – to a hundred times. Oscillations can be found everywhere if you care to look.</p>
<p class="TXI">Mathematicians are happy to see oscillations. This is because, to a mathematician, oscillations are approachable. Chaos and randomness are a challenge to capture with equations, but perfect periodicity is easy and it’s elegant. Over millennia, mathematicians have developed the equipment not just to describe oscillations, but to predict how they’ll interact and to spot them in signals that – to the untrained eye – may not look like oscillations at all. </p>
<p class="TXI">Nancy Kopell is a mathematician, or at least she used to be. Like her mother and sister before her, Kopell majored in mathematics as an undergraduate. She then got a PhD<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> in it from the University of California, Berkeley, in 1967, and became a professor of mathematics <a id="page_146"/>at Northeastern University in Boston. But after many years crossing back and forth between the border of mathematics and biology – taking problems from the latter to inspire ideas for the former – she started to feel more settled in the land of biology. As Kopell wrote in an autobiography: ‘My perspective started to shift, and I found myself at least as interested in the physiological phenomena as the mathematics problems that they generated. I didn’t stop thinking mathematically, but problems interested me less if I didn’t see the relevance to specific biological networks.’ Many of the biological networks that interested her were neural ones and throughout her career she’s studied all manner of oscillations in the brain.</p>
<p class="TXI">High-frequency oscillations are referred to by neuroscientists as ‘gamma’ waves. The reason for this is that Hans Berger, the inventor of the original EEG machine, called the big slow waves he could see by eye on his shoddy equipment ‘alpha’ waves and everything else ‘beta’; the scientists who came after him simply followed suit, giving new frequencies they found new Greek letters. Gamma waves, while fast, are usually small – or ‘low amplitude’ in technical terms. Their presence, detectable by a modern EEG or an electrode in the brain, is associated with an alert and attentive mind.</p>
<p class="TXI">In 2005, Kopell and colleagues came up with an explanation for how gamma oscillations could help the brain focus. Their theory stems from the idea that neurons representing the information you’re paying attention to should get a head start in the oscillation. Consider trying to listen to a phone call in the middle of a noisy room. Here, the signal you are paying attention <a id="page_147"/>to – the voice on the other end of the line – is competing against all the distracting sounds in the room. In Kopell’s model, the voice is represented by one group of excitatory cells and the background chatter by another. Both of these groups send connections to a common pool of inhibitory neurons and both get connections back from it in return. </p>
<p class="TXI">Importantly, the neurons representing the voice – because they are the object of attention – get a little more input than the ‘background’ neurons. This means they’ll fire first and more vigorously. If these ‘voice’ neurons fire in unison, they will – through their connections to the inhibitory cells – cause a big, sharp increase in inhibitory cell-firing. This wave of inhibition will then shut down the cells representing both the voice and the background noise. Because of this, the background neurons never get the chance to fire and therefore can’t interfere with the sound of the voice. It’s as though the voice neurons, by being the first to fire, are pushing themselves through a door and then slamming it shut on the background neurons. And as long as the voice neurons keep getting a little extra input, this process will repeat over and over – creating an oscillation. The background neurons will be forced to remain silent each time. This leaves just the clean sound of the voice in the phone as the only remaining signal. </p>
<p class="TXI">Beyond just this role in attention, neuroscientists have devised countless other ways in which oscillations could help the brain. These include uses in navigation, memory and movement. Oscillations are also supposed to make communication between brain areas better and help organise neurons into separately functioning <a id="page_148"/>groups. On top of this, theories abound for how oscillations go wrong in diseases like schizophrenia, bipolar disorder and autism.</p>
<p class="TXI">The omnipresence of oscillations may make it seem like their importance would go unquestioned, but this is far from the case. While several different roles for oscillations have been devised, many scientists remain sceptical. </p>
<p class="TXI">Part of the concern comes from the very first step: how oscillations are measured. Instead of recording from many neurons at once, many researchers interested in oscillations use an indirect measure that comes from the fluid that surrounds neurons. Specifically, when neurons are getting a lot of input, the composition of ions in this fluid changes and this can be used as a proxy for how active the population is. But the relationship between ion flows in this fluid and the real activity of neurons is complicated and not completely understood. This makes it hard to know if observed oscillations are actually happening. </p>
<p class="TXI">Scientists can also be swayed by the tools available to them. EEG has been around for a century and it makes the spotting of oscillations easy, even in human subjects; experiments can be performed in an afternoon on willing (usually undergraduate) research participants. As mentioned, a similar ease and familiarity applies to the mathematical tools of analysing oscillations. This may make researchers more likely to seek out these brainwaves, even in cases where they might not provide the best answers. To paraphrase the old adage, when a hammer is the easiest tool you have to use, you start looking for nails.</p> <a id="page_149"/>
<p class="TXI">Another issue is impact, especially when it comes to fast oscillations like gamma. If one brain state has stronger gamma waves than another, it means more neurons are firing as part of a wave in that state rather than sporadically on their own. But when these waves come so quickly, being part of one may only make a neuron fire a few milliseconds before or after it otherwise would have. Could that kind of temporal precision really matter? Or is all that matters the total number of spikes produced? Many elegant hypotheses about how oscillations can help haven’t been directly tested – and can be quite hard to test – so answers are unknown. </p>
<p class="TXI">As neuroscientist Chris Moore said in a 2019 <span class="italic">Science Daily</span> interview: ‘Gamma rhythms have been a huge topic of debate … Some greatly respected neuroscientists view gamma rhythms as the magic, unifying clock that aligns signals across brain areas. There are other equally respected neuroscientists that, colourfully, view gamma rhythms as the exhaust fumes of computation: They show up when the engine is running but they’re absolutely not important.’ </p>
<p class="TXI">Exhaust fumes may be produced when a car moves, but they’re not directly what’s making it go. Similarly, networks of neurons may produce oscillations when performing computations, but whether those oscillations are what’s doing the computing remains to be seen. </p>
<p class="TXI">As has been shown, the interaction between excitatory and inhibitory cells can create a zoo of different firing patterns. Putting these two forces in conflict has both benefits and risks. It grants the network the ability to respond with lightning speed and to generate the smooth <a id="page_150"/>rhythms needed for sleep. At the same time, it places the brain dangerously close to seizures and creates literal chaos. Making sense of such a multifaceted system can be a challenge. Luckily a diversity of mathematical methods – those developed for physics, meteorology and understanding oscillations – have helped tame the wild nature of neural firing.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-1" id="fn-1">1</a> ﻿Historically, this set of techniques went under the more obvious name of ﻿‘﻿self-consistent field theory﻿’﻿, but it﻿’﻿s now known as ﻿‘﻿mean-field theory﻿’﻿. The trick behind the mean-field approach is that you don﻿’﻿t need to provide an equation for each and every interacting particle in your system. Instead, you can study a ﻿‘﻿representative﻿’﻿ particle that receives its own output as input. That makes studying self-consistency a lot easier.﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-2" id="fn-2">2</a> ﻿In popular culture it﻿’﻿s better known as the ﻿‘﻿butterfly effect﻿’﻿, the idea that something as insignificant as a butterfly flapping its wings can change the whole course of history. ﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-3" id="fn-3">3</a> ﻿Again, the population will still produce the same amount of spikes ﻿<span class="italic">on average</span>﻿ in response to a given input. It﻿’﻿s just how those spikes are distributed across time and neurons that varies. If your neurons were truly following no rules for how they responded to inputs, you wouldn﻿’﻿t be able to be reading this right now. ﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-4" id="fn-4">4</a> ﻿Brunel, perhaps unsurprisingly at this point, started as a physicist. He learned about neuroscience during his PhD in the early 1990s, when a course exposed him to this new trend of applying the tools of physics to the brain.﻿</p>
<p class="FN1"><a href="chapter5.xhtml#fnt-5" id="fn-5">5</a> ﻿Kopell﻿’﻿s reasons for going to graduate school were somewhat unusual: ﻿‘﻿I had not entered college thinking about going to graduate school. But when my senior year arrived, I was not married and had nothing specific I wanted to do, so graduate school seemed like a good option.﻿’﻿ But the sexism she encountered there was perhaps more expected: ﻿‘﻿It was the unspoken but very widespread assumption that women in math were like dancing bears ﻿–﻿ perhaps they could do it, but not very well, and the attempt was an amusing spectacle.﻿’﻿﻿</p>
</body>
</html>