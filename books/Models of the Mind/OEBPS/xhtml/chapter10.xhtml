<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="en" xml:lang="en">
<head>
<title>Chapter 10</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter10"><a href="contents.xhtml#re_chapter10">CHAPTER TEN</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter10">Making Rational Decisions</a><a id="page_279"/></p>
<p class="H1" id="b-9781472966445-ch1077-sec11">
<span class="bold">
<span>Probability and Bayes’ rule</span>
</span></p>
<p class="TXT">When Hermann von Helmholtz was a small child in early nineteenth-century Prussia, he walked through his hometown of Potsdam with his mother. Passing a stand containing small dolls aligned in a row, he asked her to reach out and get one for him. His mother did not oblige, however, though not out of neglect or discipline. Rather, she couldn’t reach for the dolls because there were none. What the young Helmholtz was experiencing was an illusion; the ‘dolls’ he saw near him were actually <span class="italic">people</span> far away, at the top of the town’s church tower. ‘The circumstances were impressed on my memory,’ Helmholtz later wrote, ‘because it was by this mistake that I learned to understand the law of foreshortening in perspective.’</p>
<p class="TXI">Helmholtz went on to become a prominent physician, physiologist and physicist. One of his greatest contri­butions was the design of the ophthalmoscope, a tool to look inside the eye that is still used by doctors to this day. He also furthered understanding of colour vision with his work on the ‘trichromatic theory’ – the idea that three different cell types in the eye each respond to different wavelengths of light – through which he deduced that colour-blind patients must lack one of these cell types. Outside the eye, Helmholtz published a <a id="page_280"/>volume on acoustics, the experience of tones, how sound is conducted through the ear and the way it excites the nerves. By turning his characteristic thoughtfulness and precision to the study of the sense organs, Helmholtz repeatedly illuminated the physical mechanisms by which information from the world enters the mind. </p>
<p class="TXI">But the deeper question of how the mind uses that information always lingered with him. Inheriting a keen interest in philosophy from his father, Helmholtz’s worldview was impacted in several ways by the work of German philosopher Immanuel Kant. In Kant’s philosophy, the ‘Ding an sich’, or ‘thing-in-itself’, refers to the real objects out in the world – objects that can’t be experienced directly, but only through the impressions they make on our sense organs. But if two different conditions in the world – for example, a nearby doll or faraway person – can produce the same pattern of light hitting the eye, how does the mind decide which is the correct one to perceive? How, Helmholtz wanted to know, can perception form out of ambiguous or uncertain inputs? </p>
<p class="TXI">Ruminating on this question, Helmholtz concluded that a large amount of processing must go on between the point at which sensory information comes in and the moment it becomes a conscious experience. The result of this processing, he wrote, is ‘equivalent to a <span class="italic">conclusion</span>, to the extent that the observed action on our senses enables us to form an idea as to the possible cause of this action’. This idea became known as ‘unconscious inference’ because the objects in the world must be inferred by their effects on the sense organs. Taking further inspiration from Kant, Helmholtz proposed that this inference <a id="page_281"/>proceeded by interpreting the current sensory input in light of pre-existing knowledge about the world. In particular, just as his mistake with the dolls taught him about perspective, Helmholtz believed that experiences in the past can influence perceptions in the present.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> </p>
<p class="TXI">Despite being one of the most mathematically adept physiologists of all time, Helmholtz never defined unconscious inference mathematically. His ideas on the topic, while thorough, remained qualitative and mostly speculative. They were also rejected. Scientists at the time felt that the notion of ‘unconscious inference’ was a contradiction in terms. Inference, or decision-making, was a conscious process by default; it simply couldn’t be occurring beneath the surface. </p>
<p class="TXI">But Helmholtz would be vindicated, nearly 100 years after his death, by psychologists using mathematics originally developed more than 50 years before his birth. Unconscious inference – dressed in the equations of probability – would come to encapsulate the basic mechanisms of how humans perceive, decide and act. </p>
<p class="center">* * *</p>
<p class="TXT">It’s not uncommon for mathematical topics – even some of the most abstract – to have their origins in very practical professions. The tools of geometry arose from building and land surveying; ancient astronomers contributed to the concept of zero becoming commonplace; and the field of probability was born out of gambling.</p> <a id="page_282"/>
<p class="TXI">Girolamo Cardano was an Italian physician but, not unlike many educated men of the sixteenth century, he felt comfortable dabbling in a variety of subjects. According to his own count, he wrote well over a hundred books – most of them lost to time – with titles as far-ranging as <span class="italic">On the Seven Planets</span>, <span class="italic">On the Immortality of the Soul</span> and <span class="italic">On the Urine</span>. Regarding one of his books that would remain for posterity, Cardano wrote: ‘The book <span class="italic">On Games of Chance</span> I also wrote; why should not a man who is a gambler, a dicer, and at the same time an author, write a book on gaming?’ And a gambler Cardano was; the book reads more like a gaming manual borne from personal experience than a textbook. Yet it was, for its time, the most thorough treatment of the rules of probability available.</p>
<p class="TXI">Cardano focuses most of his mathematics on the casting of dice. He is quick to acknowledge that any of the six sides of a die is as likely as the others to show up, but that in practice they won’t always be found equally: ‘In six casts each point should turn up once; but since some will be repeated, it follows that others will not turn up.’ After working through examples of what to expect when rolling one, two or three dice, he concludes: ‘There is one general rule, namely, that we should consider the whole circuit, and the number of those casts which represents in how many ways the favourable result can occur, and compare that number to the remainder of the circuit.’ In other words, the probability that a certain result will happen can be calculated as the number of outcomes that lead to that result divided by the total number of possible outcomes.</p> <a id="page_283"/>
<p class="TXI">Take, for example, the rolling of two dice. If the rolling of one die has six possible outcomes, the rolling of two dice has 6 x 6 = 36 possible outcomes. If we say that our desired result is that, after rolling the two dice, their faces sum to three, then there are two possible outcomes that lead to that result: 1) the first die shows one and the second shows two; or 2) the first die shows two and the second shows one. The probability that we get our desired result is thus 2/36 or 1/18. </p>
<p class="TXI">According to Cardano: ‘Gambling is nothing but fraud and number and luck.’ So, in addition to discussing the numbers, he made sure to devote more than two chapters to the topic of fraud. Much of the focus was on how to notice a cheat: ‘The die may be dishonest either because it has been rounded off, or because it is too narrow 
(a fault which is plainly visible).’ The book also gave tips on how to handle a cheat when you spot one: ‘When you suspect fraud, play for small stakes, have spectators.’ Though it should be noted that Cardano’s autobiography offers a rather different take on how to react. Under the chapter entitled ‘Perils, Accidents and Persistent Treacheries’ he recalls a time when he noticed a man’s cards were marked and ‘I impetuously slashed his face with my poniard, though not deeply’. </p>
<p class="TXI">Importantly, Cardano made clear that most of his calculations of probabilities held only if the dice in use were honest, not if he were playing ‘in the house of a professional cheat’ (as he described the incident above). In that case, the probabilities would need to be ‘made so much the larger or smaller in the proportion to the departure from true equality’.</p><a id="page_284"/>
<p class="TXI">Accounting for different probabilities under different conditions – such as a cheating player – would later become known as <span class="italic">conditional probability</span>. Conditional probability can be thought of as a simple if-then statement. If you’re given the fact that X is true, then what’s the chance that Y is too? For example, <span class="italic">given</span> that the die is fair, the probability that a roll of it will produce a two is 1/6. Alternatively, the probability would be, say, 1/3 <span class="italic">given</span> that you’re playing with a cheat who has altered the die to prefer twos. The probability of an event thus depends on the circumstances it is <span class="italic">conditioned upon</span>. </p>
<p class="TXI">One topic that flummoxed mathematicians for centuries after Cardano was the question of <span class="italic">inverse probability</span>. Standard probability may be able to say how different dice create different chances, but the goal of inverse probability was to go the other way – to reverse the reasoning and find the cause behind the effects.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> For example, if Cardano didn’t know if he was in a game with a cheat or not, he could observe the rolls of the die to try to determine if it was biased. If one too many twos came up, he may have suspected that something was amiss (though hopefully he would have kept his poniard to himself).</p>
<p class="TXI">French mathematician Pierre-Simon Laplace worked on the issue of inverse probability intermittently over 40 years of his career. The culmination came in 1812 with the publication of <span class="italic">Théorie Analytique des Probabilités</span>. Here, <a id="page_285"/>Laplace demonstrates a simple rule that would come to be one of the most important and influential findings in mathematics. </p>
<p class="TXI">The rule says that if you want to know the probability that the die is weighted, you have to combine two different terms. The first is the probability that the rolls you’ve seen could come from a weighted die and the second is the probability of the die being weighted in the first place. More technically, this is usually stated as: the probability of your hypothesis (‘the die is weighted’) given your evidence (the rolls you’ve seen) is proportional to the probability of your evidence given your hypothesis (the odds you’d see those rolls if the die were weighted) times the probability of your hypothesis (how likely is the die to be weighted to begin with) (see Figure 22).</p>
<p class="TXI">Let’s say the die has come up ‘two’ three times in a row and you want to know if you’re being taken for a ride. With a fair die, the probability of that streak is 1/6 x 1/6 x 1/6 = 1/216. This would be called the probability of the evidence <span class="italic">given</span> the belief that the die is fair. On the other hand, the die may be weighted to roll a two, say, one-third of the time. The probability of the evidence <a id="page_286"/>
<span class="italic">given</span> the hypothesis the die is weighted this way would be 1/3 x 1/3 x 1/3 = 1/27. Comparing these numbers, it’s clear that three twos in a row is much more probable with a weighted die than with a fair one; it seems you may be playing with a cheat.</p>
<p class="TXI">But these numbers are insufficient. To draw a proper conclusion, the rule says we need to combine this with more information. Specifically, we need to multiply these numbers by the probability, in general, of the die being weighted or not. </p>
<p class="TXI">Let’s say in this case, your gambling partner is your closest friend of many years. You’d put the chance that they are using a weighted die at only 1 in 100. Multiplying the probability of getting three twos when using a weighted die times the low probability that the die is weighted, we get 1/27 x 1/100 = 1/2,700 or 0.00037. Doing this for the other hypothesis – that the die is unweighted – we get 1/216 x 99/100 = 0.0045. The second number being larger than the first, you’d be fair in concluding that your friend is not, in fact, a fraud. </p>
<p class="TXI">What this example shows is the power of the <span class="italic">prior</span>. The ‘prior’ is the name given to the probability of the hypothesis – in this case the probability your friend has altered the die. Running through the same equations, but assuming you are playing with a stranger who is as likely to cheat as not (that is, the probability of cheating is 0.5), the outcome is different: 0.019 versus 0.0023 in favour of a weighted die. In this way, a strong prior can be a deciding factor. </p>
<p class="TXI">The other term – the probability of the rolls given the hypothesis – is called the ‘likelihood’. It indicates how likely you’d be to see what you’ve seen if your hypothesis <a id="page_287"/>about the world were true. Its role in inverse probability reflects the fact that, to determine the cause of any effect, one must first know the likely effects of each cause.</p>
<p class="TXI">Both the likelihood and the prior on their own are incomplete. They represent different sources of knowledge: the evidence you have here and now versus an understanding accumulated over time. When they agree, the outcome is easy. Otherwise, they exert their influence in proportion to their certainty. In the absence of clear prior knowledge, the likelihood dominates the decision. When the pull from the prior is strong, it can make you hardly believe your own eyes. In the presence of a strong prior, extraordinary claims can only be believed with extraordinary evidence.</p>
<p class="TXI">‘When you hear hoof beats, think of horses, not zebras’ is a bit of advice frequently given to medical students. It’s meant to remind them that, of two diseases with similar symptoms, the more common one should be their first guess. It is also an excellent example of the rule of inverse probability in action. Whether you’re in the presence of a horse or of a zebra, you’ve got a similar chance of hearing hoof beats; in technical terms, the likelihoods in the two cases are the same. Given such ambiguous evidence, the decision falls into the hands of the prior and, in this case, prior knowledge says horses are more common and therefore the best guess.</p>
<p class="image-fig" id="fig22.jpg">
<img alt="" src="../images/fig22.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 22</span>
</span></p>
<p class="TXI">In the 200 years since the publication of his work, in papers, in textbooks and on classroom chalkboards, the equation for inverse probability that Laplace wrote down has been referred to as ‘Bayes’ rule’. Thomas Bayes was a Presbyterian minister in eighteenth-century England. Also an amateur mathematician, Bayes did <a id="page_288"/>work on the problem of inverse probability and he was able to solve a specific version of it. But all his thoughtfulness and calculations never quite got him to the form of Bayes’ rule we know today. What’s more, Bayes himself never published the work. An essay containing his thoughts on ‘a problem in the doctrine of chances’ was eventually sent to the Royal Society by a friend of his, another minister named Richard Price, in 1763, two years after Bayes’ death. Price put substantial work into turning Bayes’ notes into a proper essay; he wrote an introduction motivating the problem and added an extensive technical appendix (unfortunately all this effort did not prevent the essay from being referred to as ‘one of the more difficult works to read in the history of statistics’<sup><a href="#fn-3" id="fnt-3">3</a>
</sup>). Despite Laplace being alive at the time Bayes’ essay was published, he did not seem to be aware of it until after he had made substantial progress on his own.</p>
<p class="TXI">It could thus be said that the Reverend Bayes doesn’t quite deserve the empire that’s been posthumously gifted to him. It’s not clear he would’ve wanted it anyway. Bayes’ rule has not always fared well among scientists and philosophers. Much like Helmholtz’s work on unconscious inference, the equation has variably been underutilised and misunderstood. This was, initially, due to the difficulty of applying it. Laplace himself was able to use the rule on some problems of measurement in astronomy and also to <a id="page_289"/>support the long-running hypothesis that slightly more male than female babies are born on average. But, depending on the problem in question, applying Bayes’ rule could involve some complex calculus, making it an onerous approach before the modern computer was around to help. </p>
<p class="TXI">But the real struggle for Bayes’ rule came later – and ran deeper. While the validity of Laplace’s equation was unquestioned, how to <span class="italic">interpret</span> that equation occupied and divided statisticians for decades. According to philosopher of science Donald Gillies: ‘The dispute between the Bayesians and the anti-Bayesians has been one of the major intellectual controversies of the twentieth century.’ The biggest target in the crosshairs of the anti-Bayesians was the prior. Where – they wanted to know – does this information come from? In theory, it is world knowledge. In practice, it is someone’s knowledge. As the giant of twentieth-century statistics Ronald Fisher said, the assumptions that go into choosing the prior are ‘entirely arbitrary, nor has any method been put forward by which such assumptions can be made even with consistent uniqueness’. Without providing an unbiased, repeatable procedure for reaching a conclusion, Bayes’ rule was no rule at all. Because of this, the method was cast aside, branded – in a way that would be sure to scare off serious scientists – as ‘subjective’.</p>
<p class="TXI">Conceptual concerns have a habit of fading when exposed to the light of practical proof, however. And in the latter part of the twentieth century, Bayes’ rule was proving its worth. Actuaries, for example, came to realise that their rates were better calculated using the principles of inverse probability. In epidemiology, Bayes helped sort <a id="page_290"/>out the connection between smoking and lung cancer. And in the fight against the Nazis, code-breaker Alan Turing turned to Bayesian principles to uncover messages written with the ‘unbreakable’ Enigma machine. Bayes’ rule was emerging as a tamer of uncertainty anywhere that it reared its head. Practically speaking, the prior proved only a minor problem. It could be initialised with an educated guess and updated in light of new evidence (or, barring any knowledge at all, each hypothesis is simply given an equal chance). With its repeated success in spite of an active movement against it, Bayes’ rule certainly earns the title bestowed on it by Sharon McGrayne’s book, <span class="italic">The Theory That Would Not Die</span>.</p>
<p class="center">* * *</p>
<p class="TXT">When Bayes’ rule entered psychology it was not with a bang, but with a battering. No single publication carried it in. Rather, starting with the field of decision theory in the 1960s, multiple different lines of research employed it and explored it, until eventually the idea of the brain as Bayesian bloomed at the turn of the twenty-first century. </p>
<p class="TXI">Some of the early work on Bayesian principles in the brain came from an unlikely place: space. On its mission to get space travel off the ground, the National Aeronautics and Space Administration (NASA) knew that it would have to engineer more than just flight suits and jet engines. It also investigated the ‘human factors’ of flying – such as how pilots read flight equipment, sense their environment and interact with controls. Researching this problem in 1972, aeronautical engineer <a id="page_291"/>Renwick Curry wrote one of the earliest papers placing human perception in Bayesian terms. Specifically, he used Bayes’ rule to explain patterns in how humans perceive motion. Academic boundaries being what they are, however, few psychologists heard about it.</p>
<p class="TXI">Economics offered another way for Bayes to creep in. Ever eager to capture human behaviour in compact mathematical form, economists turned to Bayes’ rule as early as the 1980s. In ‘Are individuals Bayesian decision-makers?’, written by William Viscusi in 1985, workers were shown to either over-or underestimate the riskiness of specific jobs because they relied on their prior beliefs about how risky jobs are in general.</p>
<p class="TXI">Psychologists also spotted Bayes on the horizon via one of their former sources of inspiration. As we saw in <a href="chapter3.xhtml#chapter3">Chapter 3</a>, the study of the brain has been influenced by the field of formal logic. By the end of the twentieth century, in many ways, probability was the new logic – an improved way to assess how humans think. Instead of the harsh true-false dichotomy of Boolean logic, probability offers shades of grey. In this way, it aligns better with our own intuitions about our beliefs. As Laplace himself wrote: ‘Probability theory is nothing but common sense reduced to calculation.’ </p>
<p class="TXI">Of course, probability is a bit better than that because the rules are mathematically worked out to be the <span class="italic">best</span> form of common sense – and Bayes’ rule in particular is a prescription for how best to reason. </p>
<p class="TXI">It was on these grounds that John Anderson formally debuted a Bayesian approach to psychology, under a method he referred to as ‘rational analysis’. It was an idea that came to him in 1987 while he was in Australia, on <a id="page_292"/>sabbatical from his job as a professor of psychology and computer science at Carnegie Mellon University. Rational analysis, according to Anderson, stems from the belief that ‘there is a reason for the way the mind is’. Specifically, it posits that an understanding of how the mind works will best grow out of an understanding of where it came from. When it comes to Bayes, the reasoning starts with the fact that humans live in a messy, uncertain world. Yet – Anderson argues – humans have evolved within this world to behave as rationally as possible. Bayes’ rule is a description of how to reason rationally under conditions of uncertainty. Therefore, humans should be using Bayes’ rule. Put simply, if evolution has done its job, we should see Bayes’ rule in the brain.</p>
<p class="TXI">The details of just how the rule will be applied and to what problems depend on more specific features of the environment. As an example, Anderson offers a Bayesian theory of memory recall. It says that the probability of a particular memory being useful in a particular situation is found by combining: 1) how likely you’d be to find yourself in that situation if that memory were useful; with 2) a prior that assumes more recent memories are more likely to be useful. This choice of prior is meant to reflect the fact that humans come from a world where information has a shelf life; therefore, more recent memories are more likely to be of value. </p>
<p class="TXI">Importantly, under the rational analysis framework, ‘rational’ can be far from perfect. Memory, for example, certainly can fail us. But, according to this viewpoint, if we forget a fact from primary school 20 years on, we are not being irrational. Given the limited capacity of <a id="page_293"/>memory and the ever-changing world in which we live, it makes perfect sense to let old and little-used information go. In this way, the prior in a Bayesian model can be thought of as storing a shortcut. It’s an encoding of the basic stats of the world that can make decision-making faster, easier and – in most cases – more accurate. If, however, we find ourselves in a world that deviates from the one we’ve evolved and developed in, our priors can be misleading. ‘Think of horses’ is only good advice in a place with more horses than zebras.</p>
<p class="center">* * *</p>
<p class="TXT">In early 1993, a group of researchers met at the Chatham Bars Inn in Chatham, Massachusetts. The group included psychologists David Knill (a professor at the University of Pennsylvania who served as organiser) and Whitman Richards (a professor at MIT who was part of the first crop of PhD students in the Department of Psychology there in the 1960s). Also at the meeting were scientists trained in physiology and neuroscience like Heinrich Bülthoff, whose work was on the visual system of fruit flies; as well as engineers and mathematicians such as Alan Yuille, a student of Stephen Hawking.</p>
<p class="TXI">On the agenda for this eclectic bunch was a hunt for a new formal theory of perception – ideally one that could capture the complexities of the senses while also offering new, testable hypotheses. A complexity of particular concern was how the senses appear to be affected by more than just what meets the eye, ear or nose. That is, incoming sensory information combines with a rich set of background knowledge before <a id="page_294"/>perception is complete. According to Knill, no theory at the time was able to say precisely ‘how prior knowledge should be brought to bear upon the interpretation of sensory data’.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p>
<p class="TXI">The meeting birthed a book, published in 1996, the title of which reveals the solution the attendees settled on: <span class="italic">Perception as Bayesian Inference</span>. The seeds for this idea had, as we’ve seen, been scattered around for some time, growing in different ways in different fields. This was an opportunity to pull them together. The book presents a unified and clear approach to the Bayesian study of perception, focusing mainly on the sense of vision. Its success spawned countless research papers in the years that followed. If Anderson’s work on ‘rational analysis’ put Bayes on psychology’s map, this book gave it its own country.</p>
<p class="TXI">To understand the basics of Bayesian perception, consider an example. Light reflects off a flower and hits the eye. The wavelength of the light is around 670 nanometres (nm). It’s the task of the brain to figure out, given the wavelength it’s receiving, what the ‘thing-in-itself’ is, or what is really going on in the world. In Bayesian terms, this would be the probability of the hypothesis that a certain flower is present given that a wavelength of 670nm is hitting the eye. </p>
<p class="TXI">Bayes’ rule tells us what to do. First, we need to find out how likely we are to see that wavelength under different conditions. The likelihood of seeing 670nm light if the flower is blue and illuminated by white light <a id="page_295"/>is very low (blue light falls between 450 and 480nm). The likelihood of seeing 670nm light if the flower is red and illuminated by white light is quite high; 670nm is right in the middle of the red spectrum. However, the probability of seeing 670nm light if the flower is <span class="italic">white</span> and illuminated by <span class="italic">red</span> light is also quite high. Since both of these scenarios are just as likely to produce 670nm light, if we stop here, we may be quite unsure of which one is the better interpretation.</p>
<p class="TXI">But as good Bayesians, we remember the importance of the prior. The probability of a world illuminated by red light is, by most measures, quite small. White light, however, is a very common sight. The scenarios above that assume white light are thus much more probable. Multiplying the prior probability of different scenarios times the probability of seeing 670nm light in that scenario, we see that only one scores high on both of these measures. We therefore conclude that in front of us there is a red flower, illuminated by regular white light. </p>
<p class="TXI">Of course, ‘we’ don’t actually conclude that. This process, just as Helmholtz anticipated, happens unconsciously. The odds are weighed out of our sight and we know only the end result. It is, in this way, a never-ending procedure to produce perception – an underground production line in the mind. At each moment, probabilities are calculated and compared, each perception a bit of computation according to Bayes’ rule. </p>
<p class="TXI">With all the work that goes into perception, it’s no surprise that the brain can sometimes come up with odd results. In 2002, a team of researchers out of the US and Israel catalogued a series of common illusions that people fall prey to when trying to estimate the <a id="page_296"/>movement of an object. It included the fact that the shape of an object influences the direction we think it is moving in, that two items moving in different directions may appear as one and that dimmer objects appear to move more slowly. </p>
<p class="TXI">This may seem simply like a list of our failings, but the researchers found that all of these lapses could be explained by a simple Bayesian model. Particularly, these habits fall out of the calculation if we assume a specific prior: that motion is more likely to be slow than fast. Take, for example, the last illusion. When an object is hard to see, the evidence it provides about its movement is weak. In the absence of evidence, Bayes’ rule relies on the prior – and the prior says things move slowly. This bit of mathematics may explain why drivers have a tendency to speed up in the fog – with weak information about their own movement, they assume their speed is too slow. Importantly, the Bayesian approach recasts these tricks of the mind as traits of a rational calculation. It shows how some mistakes are actually reasonable guesses in an uncertain world. </p>
<p class="TXI">There is, however, another part to the process of perception. So far, we’ve simply assumed that the percept we experience should be the one with the highest probability. That’s a reasonable choice, but it is a choice nonetheless, and a different one could be made.</p>
<p class="image-fig" id="fig23.jpg">
<img alt="" src="../images/fig23.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 23</span>
</span></p>
<p class="TXI">Consider the Necker cube. This famous illusion (see Figure 23) admits more than one interpretation: the lower square could be seen as coming forwards, like a box faced slightly downwards, or it could be behind the plane of the page, suggesting a box tilted upwards. Both boxes are equally likely to produce this pattern of lines, so the <a id="page_297"/>decision about which is the true state would be strongly influenced by the prior. Let’s say, downwards-tilted boxes are a bit more likely in general. So, after applying Bayes’ rule, the probability of there being a downwards box when we see these lines is 0.51 and an upwards box 0.49. Taking the standard approach to mapping this to perception, we’d say the larger of the two probabilities wins – we see it as a downwards box, end of story. </p>
<p class="TXI">On the other hand, rather than choosing one interpretation and sticking to it, the brain could choose to alternate between the two. The box could appear downwards at one moment and upwards the next, switching back and forth repeatedly. In this case, the probabilities tell us not which interpretation to stick with, but rather <span class="italic">the amount of time</span> to spend in each. </p>
<p class="TXI">This switching is exactly what researchers at the University of Rochester (including David Knill) saw in an experiment in 2011. The experimenters overlaid two visual patterns such that it was unclear whether the first was on top of the second or vice versa – that is, the image could be interpreted two different ways. Asking people to indicate when their perception of the image <a id="page_298"/>switched from one view to the other, they could determine the amount of time spent seeing each. Assuming that either pattern was equally likely to be on top (that is, the prior probabilities are the same), Bayes’ rule says people should spend 50 per cent of their time seeing it each way. And this is exactly what they found. But to really test the predictive powers of Bayes’ rule, the scientists needed to move away from the 50-50 scenario. To do this, they manipulated the image to make one pattern appear slightly more on top than the other. This altered the likelihood – that is, the probability of seeing this image given that one or the other pattern truly was on top. The more they altered the image in this direction, the more time viewers spent seeing the preferred pattern on top – exactly in accordance with Bayes’ rule. </p>
<p class="TXI">As this study shows, a set of probabilities can be mapped to a perception in interesting ways – a mapping known by scientists as the decision function. Bayes’ rule itself doesn’t tell us what decision function to use; it only provides the probabilities. Perception could be collapsed to the interpretation with the highest probability, or it could not. Perception could be a sampling from different interpretations over time in accordance with their probabilities, or it could not. Overall, perception could be the result of any complex combination of probabilities. The output of Bayes’ rule therefore provides a rich representation of sensory information, one that the brain can use in any way that seems most reasonable. In this way, probabilities mean possibilities. </p>
<p class="TXI">Another benefit of thinking of the mind as dealing in probabilities is that it opens the door to quantifying a potentially elusive concept: confidence. Confidence is <a id="page_299"/>intuitively tied to evidence and certainty. When walking around in a dark room, where visual evidence is weak, we move slowly because we aren’t confident we won’t bump into a wall or table. In a brightly lit room, however, the strong influence of the clear visual evidence removes such doubts. The Bayesian confidence hypothesis formalises this intuition by saying that how confident a person is in their interpretation of the world is directly related to the probability of that interpretation given the evidence – that is, the output of Bayes’ rule. In the dark room where evidence is low, so is the probability of any given interpretation of the room and, therefore, so is confidence. </p>
<p class="TXI">Researchers from the UK tested just how well this Bayesian hypothesis matches the data in 2015. To do so, they asked people to look for a particular pattern in two different images that were quickly flashed one after the other. The subjects then reported which of the two images had the pattern and, importantly, how confident they were in their decision. The decisions and confidence of the humans were compared to the predictions of the Bayesian model and to predictions from two simpler mathematical models. Bayes’ rule provided the better match for the majority of the data, supporting the Bayesian confidence hypothesis. </p>
<p class="center">* * *</p>
<p class="TXT">‘In the laboratory, we like to simplify the enormous task of understanding how the brain works,’ said Dora Angelaki in an interview in 2014. ‘Traditionally, neuroscience has studied one sensory system at a time. But in the real world, this is not what happens.’</p> <a id="page_300"/>
<p class="TXI">Angelaki, originally from Crete, is a professor of Neuroscience at New York University. She credits her background in electrical engineering for her desire to seek out the underlying principles of how things work. As part of her research, she is correcting neuroscience’s bias towards simplicity by studying how the senses interact.</p>
<p class="TXI">The particular senses Angelaki seeks to combine are visual and vestibular. The vestibular system provides the little-known sixth sense of balance. Tucked deep in the ear, it is composed of a set of tiny tubes and stone-filled sacs. Through the sloshing of fluids in the tubes and the movement of the stones, the system offers – like the liquid in a level – a measurement of the head’s tilt and acceleration. This system works in tandem with the visual system to provide an overall sense of place, orientation and movement. When these two systems are out of whack, unpleasant sensations like motion sickness can occur. </p>
<p class="TXI">In her effort to understand the vestibular system, Angelaki has borrowed methods from an uncommon source: pilot training. Subjects in her experiments are strapped into a seat on a moving platform like the kind used in flight simulators. The platform can give them brief bursts of acceleration in different directions. At the same time a screen in front of them gives a visual sense of movement in the form of dots of light flowing past them – a visual not unlike that of jumping to lightspeed in <span class="italic">Star Wars</span>. While pilot training generally keeps the physical and visual motion aligned, Angelaki uses this set-up to see what the brain does when they disagree.</p>
<p class="TXI">Bayes’ rule offers a guess about that. Treating the visual and vestibular inputs as separate streams of evidence <a id="page_301"/>about the same external world, the mathematics of probability provides a simple means for how to combine them. Instead of a single likelihood term – like in the standard Bayes’ rule – the two likelihoods (one from each sense) are multiplied together. Let’s say your task is to determine if you’re moving to the left or to the right. To calculate the probability that you’re actually moving to the right – given some vestibular and visual inputs – you’d multiply the likelihood that you’d see that visual input if you were moving to the right <span class="italic">times</span> the likelihood that you’d receive that vestibular input if you were moving to the right. To complete the process, this value then goes on to be multiplied by the prior probability of rightwards movement. The same can be done for leftwards movement, and the two are compared.</p>
<p class="TXI">Just as a rumour transforms into a fact as you hear it from many different people, in Bayes’ rule getting the same information from multiple senses strengthens the belief in that information. When the moving platform and the display screen are both consistent with rightwards movement, both visual and vestibular likelihoods will be high and thus so will the outcome of their multiplication. This contributes to a confident conclusion of rightwards movement. If they’re put at odds – the platform moves right while the dots say left – then the vestibular likelihood would still say the probability of rightwards movement is high, but the visual one would say it’s low. Multiplying these leads to a middling result and only moderate confidence one way or the other. </p>
<p class="TXI">But, as with rumours, the reliability of the source matters. In her experiments, Angelaki can reduce the confidence subjects have in one or the other sensory <a id="page_302"/>inputs. To make the visual inputs less reliable, she simply makes them messier. That is, rather than all the dots moving together to give a strong sense of directed motion, some dots move randomly. The more dots that are random, the less reliable the visual information becomes. </p>
<p class="TXI">Looking at how that plays out with probabilities, we see that Bayes’ rule naturally titrates how much to rely on a source based on how reliable it is. If the dots were moving completely randomly, the visual input would provide no information about movement direction. In this case, the likelihood of the visual input given rightwards movement would be equal to the likelihood of it given leftwards movement. With equal likelihoods on both sides, the visual input wouldn’t weigh the decision either way. The tiebreaker would come from the vestibular inputs (and the prior). If, instead, 90 per cent of the dots moved randomly and 10 per cent indicated rightwards movement, then the likelihood of the visual input in support of rightwards movement would be slightly higher than that of leftwards. Now the visual input does weigh the decision – but only slightly. As the visual input becomes more reliable, its say in the decision grows. In this way, Bayes’ rule automatically puts more stock in a source in proportion to how certain it is.</p>
<p class="TXI">Investigating the conclusions people come to about their movement in these experiments, Angelaki and her lab have shown once again that, for the most part, humans behave as good Bayesians. When the visual evidence is weak, they depend more on their vestibular system. One caveat is that, while they use the visual information more as it becomes more reliable, they still don’t use it quite as much as Bayes’ rule would predict. That is, the vestibular input is consistently <a id="page_303"/>overemphasised – an effect found in monkeys as well. This could be a result of the fact the visual input is always a bit ambiguous: seeing dots moving past could be an effect of your own movement, or it could just be the dots moving. So, the vestibular input is, in general, a more trustworthy source and therefore worthy of more weight. </p>
<p class="center">* * *</p>
<p class="TXT">Once the Bayesian approach to perception was unleashed, it quickly spread to all corners of psychology. Like a Magic Eye illusion, staring at any data long enough can make the structure of Bayes’ rule pop out of it. As a result, priors and likelihoods abound in the study of the mind. </p>
<p class="TXI">As we’ve already seen, Bayes’ rule has been evoked to explain motion perception, the switching of ambiguous illusions like the Necker cube, confidence and the combination of vision and vestibular inputs. It’s also been adapted to account for how we can be tricked by ventriloquists, our sense of the passing of time and our ability to spot anomalies. It can even be stretched and expanded to cover such tasks as motor skill learning, language understanding and our ability to generalise. Such a unifying framework to describe so much of mental activity seems an unmitigated success. Indeed, according to philosopher of mind Michael Rescorla, the Bayesian approach is ‘our best current science of perception’. </p>
<p class="TXI">Yet, not all psychologists can be counted as devoted disciples of the Reverend Bayes. </p>
<p class="TXI">To some, a theory that explains everything is at risk of explaining nothing at all. The flip side of the flexibility <a id="page_304"/>of the Bayesian approach is that it can also be accused of having too many <span class="italic">free parameters</span>. The free parameters of a model are all its movable parts – all the choices that the researcher can make when using it. The same way that, if given enough strokes even the worst golfer could eventually get the ball in the cup, if given enough free parameters any model can fit any data. If a finding from a new experiment conflicts with an old one, for example, an over-parameterised model can easily twist its way around to encompass them both. If getting the model to fit the data is as easy as a bank making change, its success isn’t very interesting. A model that can say anything can never be wrong. As psychologists Jeffrey Bowers and Colin Davis wrote in their 2012 critique of the Bayesian approach: ‘This ability to describe the data accurately comes at the cost of falsifiability.’</p>
<p class="TXI">There are indeed many ways to squeeze parts of perception into a Bayesian package. Take, for example, the likelihood calculation. Computing a quantity like ‘the likelihood of seeing 670nm light given the presence of a red flower’ requires some knowledge and assumptions about how light reflects off different materials and how the eye absorbs it. Without a perfect understanding of the physical world, the model-maker must put in some of their own assumptions here. They could, therefore, wiggle these assumptions around a bit to match the data. Another source of choice is the decision function. As we saw earlier, the output of Bayes’ rule can be mapped to the perception and decision of an animal any number of ways. This option, too, has the power to make any action look Bayesian in theory. And then, of course, there are those pesky priors.</p><a id="page_305"/>
<p class="TXI">Just as they gave pause to statisticians in the twentieth century, priors have proven a challenge to psychologists of the twenty-first. If the assumption of a certain prior – for example, that motion is likely to be slow – helps explain a psychological phenomena, that can be taken as good evidence that the brain really uses that prior. But what if a different phenomena is best explained by a different prior – say, one that assumes motion is fast? Should it be assumed that the priors in our mind are constant across time and task? Or are they flexible and fluid? And how can we know?</p>
<p class="TXI">As a result of these concerns, some researchers have embarked on an exploration into the properties of priors. French cognitive scientist Pascal Mamassian has worked to investigate a particularly common one: the assumption that light comes from above. For more than two centuries, experiments and illusions have found that humans keep this implicit belief about the source of illumination in mind as they make sense of shadows in a scene. It’s a sensible guess, given the location of our dominant light source, the sun. More recently, experiments have revised this finding slightly and found that humans actually assume that light comes from above <span class="italic">and slightly to the left</span>. Mamassian conducted tests revealing this bias in the lab, but he also found a more creative way to interrogate it. Analysing 659 paintings from the Louvre museum in Paris, he found that in a full 84 per cent of portraits and 67 per cent of non-portrait paintings, the light source was indeed biased towards the left side. Artists may have come to prefer this setting exactly because it aligns with our intuitions, creating a more pleasant and interpretable painting.</p><a id="page_306"/>
<p class="TXI">Another open question about priors is their origin. Priors can serve as an efficient way to imprint facts about the world on our minds; but are these facts gifted to us from previous generations through our genes, or do we develop them in our own lifetimes? To test this, a study in 1970 raised chickens in an environment wherein all light came from below. If the prior assumption that light comes from overhead is learned in their lifetimes, these birds wouldn’t have it. How the animals interacted with visual stimuli, however, showed they did still assume light should be from above. This points in favour of an inherited prior. </p>
<p class="TXI">Humans, of course, aren’t chickens, and the development of our nervous system may allow for a bit more flexibility. Investigating the prior biases of children of various ages, psychologist James Stone found in 2010 that children as young as four did show a bias towards assuming overhead light, though it was weaker than that in adults. This bias grows steadily over the years to reach adult strength, suggesting that a partially innate prior may be fine-tuned by experience. Further in support of this flexibility, a team from the UK and Germany showed in 2004 that our grip on where light must be coming from can be loosened. Through training, participants were able to shift their prior beliefs about the source of the light by several degrees. </p>
<p class="TXI">Picking a particular prior and poking at it from different directions through a multitude of experiments helps verify it as a resilient and reliable effect. Each prior this is done for becomes less of a free parameter in the model and more fixed.</p> <a id="page_307"/>
<p class="TXI">Another question supporters of the Bayesian brain hypothesis need to address is ‘how?’</p>
<p class="TXI">While there are reasons to believe the brain <span class="italic">should</span> use Bayes’ rule, and there is evidence that it <span class="italic">does</span>, the question of how this plays out in neurons remains a lively area of research.</p>
<p class="TXI">When it comes to priors, scientists are looking for what cupboard in the brain stores these bits of background knowledge and how they get mixed into the neural decision-making process. One hypothesis is that it’s a simple numbers game. If a group of neurons are charged with representing something about the world – say, where in the environment a sound is coming from – each neuron may have its own preferred location. This means it responds the most when sound is coming from there. If the brain determines where the sound is by adding up the activity of all the neurons that prefer the same location, locations with more neurons will have an advantage. So, if the prior says that sound is more likely to come from central locations than the periphery, this can be implemented by simply increasing the number of neurons that prefer the centre. As it turns out, neuroscientists Brian Fischer and Jose Luis Peña found this exact scheme in the brains of owls in 2011. Identifying neural signatures of priors this way can give insights to where they come from and how they work.</p>
<p class="TXI">Theorists are building – and experimentalists are testing – many more hypotheses about how Bayes’ rule plays out in the brain. There are a multitude of ways that neurons can conspire to combine likelihoods and priors. <a id="page_308"/>These different hypotheses should not be considered in competition, nor should any single winner expect to be crowned at the end. Rather, while Bayes’ rule can be one-size-fits-all for capturing the outputs of perception, the physical underpinnings of this rule may come in many different shapes and styles.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-1" id="fn-1">1</a> ﻿In this way Helmholtz deviated from Kant, who believed that much of this world knowledge was innate rather than learned.﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-2" id="fn-2">2</a> ﻿Speaking about probability in terms of cause and effect was not uncommon at the time. But it is not, in general, a wise thing to do. The probability that you﻿’﻿ll be holding an umbrella given that the person next to you on the street is may be high, but the one is not the cause of the other. ﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-3" id="fn-3">3</a> ﻿The statistician and historian responsible for this assessment, Stephen Stigler, is also known for ﻿‘﻿Stigler﻿’﻿s law of eponymy﻿’﻿, which claims that a scientific law is never named after its true originator. The sociologist Robert Merton is believed to be the originator of this law. ﻿</p>
<p class="FN1"><a href="chapter10.xhtml#fnt-4" id="fn-4">4</a> ﻿Some of the existing theories of the time that failed to do this include the models of the visual system discussed in ﻿﻿Chapter 6﻿﻿.﻿</p>
</body>
</html>