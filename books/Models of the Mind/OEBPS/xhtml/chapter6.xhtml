<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="en" xml:lang="en">
<head>
<title>Chapter 6</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter6"><a href="contents.xhtml#re_chapter6">CHAPTER SIX</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter6">Stages of Sight</a><a id="page_151"/></p>
<p class="H1" id="b-9781472966445-ch612-sec6">
<span class="bold">
<span>The Neocognitron and convolutional 
neural networks</span>
</span></p>
<p class="EXTF">
<span class="italic">The summer vision project is an attempt to use our summer workers effectively in the construction of a significant part of a visual system. The particular task was chosen partly because it can be segmented into sub-problems which will allow individuals to work independently and yet participate in the construction of a system complex enough to be a real landmark in the development of ‘pattern recognition’.</span></p>
<p class="right">Vision Memo No. 100 from the 
Massachusetts Institute of Technology 
Artificial Intelligence Group, 1966</p>
<p class="TXT-con">The summer of 1966 was meant to be the summer that a group of MIT professors solved the problem of artificial vision. The ‘summer workers’ they planned to use so effectively for this project were a group of a dozen or so undergraduate students at the university. In their memo laying out the project’s plan, the professors provided several specific skills they wanted the computer system the students were developing to perform. It should be able to define different textures and lighting in an image, label parts as foreground and parts as background, and identify whatever objects <a id="page_152"/>were present. One professor<sup><a href="#fn-1" id="fnt-1">1</a>
</sup> purportedly described the aims more casually as ‘linking a camera to a computer and getting the computer to describe what it saw’. </p>
<p class="TXI">The goals of this project were not completed that summer. Nor the next. Nor many after that. Indeed, some of the core issues raised in the description of the summer project remain open problems to this day. The hubris on display in that memo is not surprising for its time. As discussed in <a href="chapter3.xhtml#chapter3">Chapter 3</a>, the 1960s saw an explosion in computing abilities and, in turn, naive hopes about automating even the most complex tasks. If computers could now do anything asked of them, it was just a matter of knowing what to ask for. With something as simple and immediate as vision, how hard could that be?</p>
<p class="TXI">The answer is very hard. The act of visual processing – of taking in light through our eyes and making sense of the external world that reflected it – is an immensely complex one. Common sayings like ‘right in front of your eyes’ or ‘in plain sight’, which are used to indicate the effortlessness of vision, are deceitful. They obscure the significant challenges even the most basic visual inputs pose for the brain. Any sense of ease we have regarding vision is an illusion, one that was hard won through millions of years of evolution. </p>
<p class="TXI">The problem of vision is specifically one of reverse engineering. In the back of the eye, in the retina, there is <a id="page_153"/>a wide flat sheet of cells called photoreceptors. These cells are sensitive to light. Each one indicates the presence or absence (and possibly wavelength) of the light hitting it at each moment by sending off a signal in the form of electrical activity. This two-dimensional flickering map of cellular activity is the only information from which the brain is allowed to reconstruct the three-dimensional world in front of it. </p>
<p class="TXI">Even something as simple as finding a chair in a room is a technically daunting endeavour. Chairs can be many different shapes and colours. They can also be nearby or far away, which makes their reflection on the retina larger or smaller. Is it bright in the room or dark? What direction is light coming from? Is the chair turned towards you or away? All of these factors impact the exact way in which photons of light hit the retina. But trillions of different patterns of light could end up meaning the same thing: a chair is there. The visual system somehow finds a way to solve this many-to-one mapping in less than a tenth of a second. </p>
<p class="TXI">At the time those MIT students were working to give the gift of sight to computers, physiologists were using their own tools to solve the mysteries of vision. This started with recording neural activity from the retina and moved on to neurons throughout the brain. With an estimated 30 per cent of the primate cortex playing some role in visual processing, this was no small undertaking.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> In the mid-twentieth century many of the scientists performing these experiments were based in <a id="page_154"/>the Boston area (many at MIT itself or just north of it, at Harvard) and they were quickly amassing a lot of data that they needed to somehow make sense of.</p>
<p class="TXI">Perhaps it was the physical proximity. Perhaps it was a tacit acknowledgement of the immense challenge they had each set for themselves. Perhaps in the early days the communities were just too small to keep to themselves. Whatever the reason, neuroscientists and computer scientists forged a long history of collaborating in their attempts to understand the fundamental questions of vision. The study of vision – of how patterns can be found in points of light – is full of direct influence from the biological to the artificial and vice versa. The harmony may not have been constant: when computer science embarked on methods that were useful but didn’t resemble the brain, the fields diverged. And when neuroscientists dig into the nitty-gritty detail of the cells, chemicals and proteins that carry out biological vision, computer scientists largely turn away. But the impacts of the mutual influence are still undeniable, and plainly visible in the most modern models and technologies. </p>
<p class="center">* * *</p>
<p class="TXT">The earliest efforts to automate vision came before modern computers. Though implemented in the form of mechanical gadgetry, some of the ideas that powered these machines prepared the field for the later emergence of computer vision. One such idea was <span class="italic">template matching</span>. </p>
<p class="TXI">In the 1920s, Emanuel Goldberg, a Russian chemist and engineer, set out to solve a problem banks and other offices had while searching their file systems for <a id="page_155"/>documents. At the time, documents were stored on microfilm – strips of 35mm film that contained tiny images of documents that could be projected to a larger screen for reading. The ordering of the documents on the film had little relation to their contents, so finding a desired document – such as a cancelled cheque from a particular bank customer – involved much unstructured searching. Goldberg turned to a crude form of ‘image processing’ to automate this process. </p>
<p class="TXI">Under Goldberg’s plan, cashiers entering a new cheque into the filing system would need to mark it with a special symbol that indicated its contents. For example, three black dots in a row meant the customer’s name started with ‘A’, three black dots in a triangle meant it started with ‘B’ and so on. Now, if a cashier wanted to find the last cheque submitted by a 
Mr Berkshire, for example, they just needed to find the cheques marked with a triangle. The triangle pattern was thus a template and the goal of Goldberg’s machine was to match it. </p>
<p class="TXI">Physically, these templates took the form of cards with holes punched in them. So, when looking for Mr Berkshire’s documents, the cashier would take a card with three holes punched out in the shape of a triangle and place it in between the microfilm strip and a lightbulb. Each document on the strip would then be automatically pulled up to be aligned with the card, causing the light to shine through the holes on the card and then through the film itself. A photocell placed behind the film detected any light that came through and signalled this to the rest of the machine. For most of the documents, some light would get through as the <a id="page_156"/>symbols on the film didn’t align with the holes on the card. But when the desired document appeared, the light shining through the card would be exactly blocked out by the pattern of black dots on the film. These mini eclipses meant no light would land on the photocell and this signalled to the rest of the machine, and to the cashier, that a match had been found. </p>
<p class="TXI">Goldberg’s approach required that the cashiers knew in advance exactly what symbol they were looking for and had a card to match it. Crude though it was, this style of template matching became the dominant approach for much of the history of artificial vision. When computers appeared on the scene, the form of the templates migrated from the physical to the digital. </p>
<p class="TXI">In a computer, images are represented as a grid of pixel values (see Figure 14). Each pixel value is a number indicating the intensity of the colour in the tiny square region of the picture it represents.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> In the digital world, a template is also just a grid of numbers, one that defines the desired pattern. So, the template for three dots in the shape of a triangle may be a grid of mostly zeros except for three precisely placed pixels with value one. The role of the light shining through the template card in Goldberg’s machine was replaced in the computer by a mathematical operation: multiplication. If each pixel value in the image is multiplied by the value at the same location in the template, the result can actually tell us if the image is a match.</p> <a id="page_157"/>
<p class="image-fig" id="fig14.jpg">
<img alt="" src="../images/fig14.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 14</span>
</span></p>
<p class="TXI">Let’s say we are looking for a smiling face in a black and white image (where black pixels have a value of one and white have a value of zero). Given a template for the face, we can compare it with an image through multiplication. If the image is indeed of the face we are searching for, the values that comprise the template will be very similar to those in the image. Therefore, zeros in the template will be multiplied by zeros in the image and ones in the template will be multiplied by ones in the image. Adding up all the resulting values from this multiplication gives us the number of black pixels that are the same in the template and the image, which in the case of a match would be many. If the image we’re given is of a frowning face instead, some of the pixels around the mouth in the image won’t match the template. There, zeros in the template will be multiplied by ones in the image and vice versa. Because the product at those pixel locations will be zero, the sum across the whole image won’t be so high. In this way, a simple sum of products gives a measure of how much an image matches a template.</p><a id="page_158"/>
<p class="TXI">This method found wide use in many different industries. Templates have been used to count crowd sizes by finding faces in a picture. Known geographical features have also been located in satellite images via templates. The number and model of cars that pass through an intersection can be tracked as well. With template matching, all we need to do is define what we want and multiplication will tell us if it’s a match. </p>
<p class="center">* * *</p>
<p class="TXT">Imagine a stadium – one just like where you’d watch a football game – but in this stadium, instead of screaming fans, the stands are full of screaming demons. And what they are screaming about isn’t players on the field, but rather an image. Specifically, each of these demons has its own preferred letter of the alphabet and when it sees something that looks like that letter on the field it shrieks. The louder the shriek the more the image on the field looks like the demon’s favourite letter. Up in the skybox is another demon. This one doesn’t look at the field or do any screaming itself, but merely observes all the other demons in the stadium. It finds the demon shrieking the loudest and determines that the image on the field must be that demon’s favourite letter.</p>
<p class="TXI">This is how Oliver Selfridge described the process of template matching at a 1958 conference. Selfridge was a mathematician, computer scientist and associate director of Lincoln Labs at MIT, a research centre focused on national security applications of technology. Selfridge didn’t publish many papers himself. He never finished his own PhD dissertation either (he did however end up <a id="page_159"/>writing several children’s books; presumably these contained fewer demons). Despite this lack of academic output, his ideas infiltrated the research community nonetheless, largely due to the circles in which he moved. After earning a bachelor’s degree in mathematics from MIT at only 19 years old, Selfridge was advised in his PhD work by the notable mathematician Norbert Wiener and remained in contact with him. Selfridge also went on to supervise Marvin Minsky, the prominent AI researcher from <a href="chapter3.xhtml#chapter3">Chapter 3</a>. And as a graduate student, Selfridge was friends with Warren McCulloch and for a time lived with Walter Pitts (you’ll recall this pair of neuroscientists from <a href="chapter3.xhtml#chapter3">Chapter 3</a> as well). Selfridge benefited from letting his ideas marinate in this social stew of prominent scientists. </p>
<p class="TXI">To map Selfridge’s unique analogy to the concept of template matching, we just have to think of each demon as holding its own grid of numbers that represents the shape of its letter. They multiply their grid with the image and sum up those products (just as described above) and scream at a volume determined by that sum. Selfridge doesn’t give much of an indication as to why he chose to give such a demonic description of visual processing. His only reflection on it is to say: ‘We are not going to apologise for a frequent use of anthropomorphic or biomorphic terminology. They seem to be useful words to describe our notions.’<sup><a href="#fn-4" id="fnt-4">4</a>
</sup></p><a id="page_160"/>
<p class="TXI">Most of the notions in Selfridge’s presentation were actually about how the template matching approach is flawed. The demons – each individually checking if their favourite letter was in sight – weren’t being very efficient. They each performed their own completely separate computations, but it didn’t have to be that way. Many of the shapes that a demon may look for in its search for its letter are also used by other demons. For example, both the ‘A’-preferring demon and the ‘H’-preferring demon would be on the lookout for a horizontal bar. So why not introduce a separate group of demons, ones whose templates and screams correspond to more basic features of the image such as horizontal bars, vertical lines, slanted lines, dots, etc. The letter demons would then just listen to those demons rather than look at the images themselves and decide how much to scream based on whether the basic shapes of their letter are being yelled about. </p>
<p class="TXI">From bottom to top Selfridge defined a new style of stadium that contained three types of demon: the ‘computational’ (those that look at the image and yell about basic shapes), ‘cognitive’ (those that listen to the computational demons and yell about letters) and ‘decision’ (the one that listens to the cognitive demons and decides which letter is present). The name Selfridge gave to the model as a whole – to this stack of shrieking demons – was Pandemonium.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup></p>
<p class="TXI">Nefarious nomenclature aside, Selfridge’s intuitions on visual processing proved insightful. While conceptually <a id="page_161"/>simple, template matching is practically challenging. The number of templates needed grows with the number of objects you want to be able to spot. If each image needs to be compared to every filter, that’s a lot of calculations. Templates also need to more or less match the image exactly. But because of the myriad different patterns of light that the same object can produce on a retina or camera lens, it’s nearly impossible to know how every pixel in an image should look when a given object is present. This makes templates very hard to design for any but the simplest of patterns. </p>
<p class="TXI">These issues make template matching a challenge both for artificial visual systems and for the brain. The ideas on display in Pandemonium, however, represent a more distributed approach, as the features detected by the computational demons are shared across cognitive demons. The approach is also <span class="italic">hierarchical</span>. That is, Pandemonium breaks the problem of vision into two stages: first look for the simple things, then for the more complex. </p>
<p class="TXI">Together, these properties make the system more flexible overall. If Pandemonium was set up to recognise the letters of the first half of the alphabet, for example, it would be in a pretty good position to recognise the rest. This is because the low-level computational demons would already know what kinds of basic shapes letters are made of. The cognitive demon for a new letter would just need to figure out the right way to listen to the demons below it. In this way, the elementary features work like a vocabulary – or a set of building blocks – that can be combined and recombined to detect additional complex patterns. Without this hierarchical structure and sharing of low-level features, a basic <a id="page_162"/>template-matching approach would need to produce a new template for each letter from scratch.</p>
<p class="TXI">The design of Pandemonium does pose some questions. For example, how does each computational demon know what basic shape to scream about? And how do the cognitive demons know to whom they should listen? Selfridge proposes that the system learn the answers to these questions through trial and error. If, for example, adjusting how the ‘A’-preferring demon listens to those below it makes it better at detecting ‘A’s, then keep those changes; otherwise don’t and try something new. Or, if adding a computational demon to scream about a new low-level pattern makes the whole system better at letter detection, then that new demon stays; otherwise it’s out. This is an arduous process of course and it’s not guaranteed to work, but when it does it has the desirable effect of creating a system that is customised – automatically – for the type of objects it needs to detect. The strokes that comprise the symbols of the Japanese alphabet, for example, differ from those in the English alphabet. A system that learns would discover the different basic patterns for each. No prior or special knowledge needed, just let the model take a stab at the task.</p>
<p class="TXI">Computer scientist Leonard Uhr was impressed enough with the ideas of Selfridge and colleagues that he wanted to share their work more broadly. In 1963, he wrote in the <span class="italic">Psychological Bulletin</span> to an audience of psychologists about the strides computer scientists were making on the vision front. In his article, ‘“Pattern recognition” computers as models for form perception’, he indicates that the models of the time were ‘actually already in the position of suggesting physiological and <a id="page_163"/>psychological experiments’ and even warns that ‘it would be unfortunate if psychologists did not play any part in this theoretical development of their own science’. The article is concrete evidence of the intertwined relationship the two fields have always had. But such explicit public pleas for collaboration weren’t always needed. Sometimes personal relationships were enough.</p>
<p class="TXI">Jerome Lettvin was a neurologist and psychiatrist from Chicago, Illinois. He was also a friend of Selfridge, having shared a house with him and Pitts as a young man. A self-described ‘overweight slob’, Lettvin wanted to be a poet but appeased his mother’s wishes and became a doctor instead. The most rebellion he managed was to occasionally abandon his medical practice to engage in some scientific research.</p>
<p class="TXI">Inspired by the work of his friend and former cohabitant, in the late 1950s Lettvin set out to search for neurons that responded to low-level features – that is, to the types of things computational demons would scream about. The animal he chose to look at was the frog. Frogs use sight mostly to make quick reflexive responses to prey or to predators and as a result their visual system is relatively simple. </p>
<p class="TXI">Inside the retina, individual light-detecting photore­ceptors send their information to another population of cells called ganglion cells. Each photoreceptor connects to many ganglion cells and each ganglion cell gets inputs from many photoreceptors. But, crucially, all these inputs come from a certain limited region of space. This makes a ganglion cell responsive only to light that hits the retina in that specific location – and each cell has its own preferred location.</p> <a id="page_164"/>
<p class="TXI">At this point in time, ganglion cells were assumed not to do much computation themselves. They were thought of mostly as a relay – just sending information about photoreceptor activity along to the brain like a mail carrier. Such a picture would fit within a template-matching view of visual processing. If the role of the brain was to compare visual information from the eye to a set of stored templates, it wouldn’t want that information distorted in any way by the ganglion cells. But if the ganglion cells were part of a hierarchy – where each level played a small role in the eventual detection of complex objects – they should be specialised to detect useful elementary visual patterns. Rather than relaying information verbatim, then, they should be actively processing and repackaging it.</p>
<p class="TXI">Lettvin found – by recording the activity of these ganglion cells and showing all kinds of moving objects and patterns to the frog – that the hierarchy hypothesis was true. In fact, in a 1959 paper ‘What the frog’s eye tells the frog’s brain’ he and his co-authors describe four different types of ganglion cells that each responded to a different simple pattern. Some responded to swift large movements, others to when light turned to dark and still others to curved objects that jittered about. These different categories of responses proved that the ganglion cells were specifically built to detect different elementary patterns. Not only did these findings align with Selfridge’s notions of low-level feature detectors, but they also supported the idea that these features are specific to the type of objects the system needs to detect. For example, the last class of cells responded best when a small dark object moved quickly in fits and starts around a fixed <a id="page_165"/>background. After describing these in the paper, Lettvin remarked: ‘Could one better describe a system for detecting an accessible bug?’ </p>
<p class="TXI">Selfridge’s intuitions were proving to be correct. With Lettvin’s finding in frogs, the community started to conceive of the visual system more as a stack of screaming demons and less as a store of template cards.</p>
<p class="center">* * *</p>
<p class="TXT">Around the same time as Lettvin’s work, two doctors at the John Hopkins University School of Medicine in Baltimore were exploring vision in cats. A cat’s visual system is more like ours than a frog’s. It is tasked with challenging problems related to tracking prey and navigating the environment and is, as a result, more elaborate. The work of the cat visual system is thus stretched over many brain areas and the one that doctors David Hubel<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> and Torsten Wiesel focused on was the primary visual cortex. This region at the back of the brain represents one of the earlier stages of visual processing in mammals; it gets its input from another brain area – the thalamus – that gets input from the retina itself. </p>
<p class="TXI">Previous work had investigated how the neurons in the thalamus and retina of cats behave. These cells tend to respond best to simple dots: either a small area of light surrounded by dark or a small area of dark surrounded <a id="page_166"/>by light. And, as in the frog, each neuron has a specific location the dot needs to be in for it to respond. </p>
<p class="TXI">Hubel and Wiesel had access to equipment for producing dots at different locations in order to explore such retinal responses. So, this is the equipment they used, even as they investigated brain areas well beyond the retina. The method for displaying dots included sliding a small glass or metal plate with different cut-out patterns over a screen in front of the eye. Hubel and Wiesel used this to show slide after slide of dots to their feline subject as they measured the activity of a neuron in its primary visual cortex. But the dots simply didn’t do it for this neuron – the cell wouldn’t make a peep in response to the slides. Then the experimenters noticed something strange: occasionally the neuron would respond – not to the slides – but to the changing of them. As one plate was slid out and another in, the shadow from the edge of the glass swept across the cat’s retina. This created a moving line that reliably excited the neuron in the primary visual cortex. One of the most iconic discoveries in neuroscience had just occurred, almost by accident. </p>
<p class="TXI">Decades later, reflecting on the serendipity of this discovery, Hubel remarked: ‘In a certain early phase of science a degree of sloppiness can be a huge advantage.’ But that phase quickly passed. By 1960 he and Wiesel had moved their operation to Boston, to help establish the department of neurobiology at Harvard University and embarked on years of careful investigation into the responses of neurons in the visual system. </p>
<p class="TXI">Expanding on their happy accident, Hubel and Wiesel dug deep into how this responsiveness to moving <a id="page_167"/>lines worked. One of their first findings was that the neurons in the primary visual cortex each have a preferred <span class="italic">orientation</span> in addition to a preferred location. A neuron won’t respond to just any line that shows up in its favourite location. Horizontal-preferring neurons require a horizontal line, vertical-preferring neurons require vertical lines, 30-degree-slant-preferring neurons require 30-degree slanted lines, and so on and so on. To get a sense of what this means, you can hold a pen out horizontally in front of your face and move it up and down. You’ve just excited a group of neurons in your primary visual cortex. Tilt the pen another way and you’ll excite a different group (you’ve now got at-home, targeted brain stimulation for free!).</p>
<p class="TXI">With their realisation about orientation, Hubel and Wiesel had discovered the alphabet used by the cat brain to represent images. Flies have bug detectors and cats (and other mammals) have line detectors. However, they didn’t stop at just observing these responses, they went further to ask how the neurons in the primary visual cortex could come to have such responses. After all, the cells they get their inputs from – those in the thalamus – respond to dots, not lines. So where did the preference for lines come from? </p>
<p class="TXI">The solution was to assume that neurons in the cortex get a perfectly selected set of inputs from the thalamus. A line, of course, is nothing more than a set of appropriately arranged dots. Inputs to a neuron in the primary visual cortex therefore must come from a set of thalamus neurons wherein each one represents a dot in a row of dots. That way, the primary visual neuron would fire the most when a line was covering all those dots (see Figure 15). Just like <a id="page_168"/>the cognitive demons listening for the shrieks of the demons that look for parts of their letter, neurons in the primary visual cortex listen for the activity of neurons in the thalamus that make up their preferred line. </p>
<p class="TXI">Hubel and Wiesel noticed another kind of neuron, too: ones that also had preferred orientations, but weren’t quite as strict about location. These neurons would respond if a line appeared anywhere in a region that was about four times larger than that of the other neurons they recorded. How could these neurons come to have this response? The answer, again, was to assume they got just the right inputs. In particular, a ‘complex’ neuron – as Hubel and Wiesel labelled these cells – just needed input from a group of regular (or ‘simple’) neurons. All these simple cells should have the same preferred orientations but slightly different preferred locations. That way, a complex cell would inherit the orientation preference of its inputs, but have a spatial preference that is larger than any single one of them. This spatial flexibility is important. If we want to know if the letter ‘A’ is in front of us, a little bit of jitter in the exact location of its lines shouldn’t really matter. Complex cells are built to discard jitter.</p>
<p class="image-fig" id="fig15.jpg">
<img alt="" src="../images/fig15.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 15</span>
</span></p>
<p class="TXI">The discovery of complex cells provided an additional piece of the puzzle as to how points of light become perception. In addition to the feature detection done by simple cells, pooling of inputs across space was added to the list of computations performed by the visual system. For all the work they did dissecting this system, Hubel and Wiesel were awarded the Nobel Prize in 1981. In his Nobel lecture, Hubel put their goals plainly: ‘Our idea originally was to emphasise the tendency toward <a id="page_169"/>increased complexity as one moves centrally along the visual path, and the possibility of accounting for a cell’s behaviour in terms of its inputs.’<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> This approach, while simple, sufficed to capture many of the basic properties of the visual-processing pathway. </p>
<p class="center">* * *</p>
<p class="TXT">On the other side of the world – at Japan’s national public broadcasting organisation, NHK, located in Tokyo – Kunihiko Fukushima heard about the simple properties of the visual system. Fukushima was an engineer and part of the research arm of NHK. Because NHK was a broadcasting company (and was broadcasting visual and audio signals into the eyes and ears of humans) <a id="page_170"/>it also had groups of neurophysiologists and psychologists on staff to study how sensory signals are received by the brain. These three groups – the psychologists, physiologists and engineers – would meet regularly to share the work of their respective fields. One day, a colleague of Fukushima’s decided to present the work of Hubel and Wiesel. </p>
<p class="TXI">When Fukushima saw this clear description of the roles of neurons in the visual system, he set out to implement the very same functions in a computer model. His model used images of simple white patterns on a black background as input. To approximate the work of the thalamus, a sheet of artificial neurons was created that responded to white dots in the image. This served as a way to get the image information into the network. From here the input to the simple cells needed to be calculated. </p>
<p class="TXI">To do so, Fukushima used the standard approach of making a grid of numbers that represent the to-be-detected pattern – which in the case of a simple cell is a line with a specific orientation. In engineering terms, this grid of numbers is known as a <span class="italic">filter</span>. To mimic the spatial preferences of simple cells, Fukushima applied this filter separately at each location in the image. Specifically, the activity of one simple cell was calculated as the sum of the thalamus activity at one location multiplied by the filter. Sliding the filter across the whole image created a set of simple cells all with the same preferred orientation but different preferred locations. This is a process known in mathematics as a <span class="italic">convolution</span>. </p>
<p class="TXI">By producing multiple filters – each representing a line with a different orientation – and convolving each with the image, Fukushima produced a full population <a id="page_171"/>of simple cells each with its own preferred orientation and location, just like the brain. For the complex cells, he simply gave them strong inputs from a handful of simple cells that all represented the same orientation in nearby locations. That way they would be active if the orientation appeared in any of those locations. </p>
<p class="TXI">This first version of Fukushima’s model was pretty much a direct translation of the physiological findings of Hubel and Wiesel into mathematics and computer code – and, in a way, it worked. It could do some simple visual tasks like finding curved lines in a black and white image, but it was far from a complete visual system and Fukushima knew that. As he later recounted in an interview, after publishing this work in the late 1960s, Fukushima waited patiently to see what Hubel and Wiesel would discover next; he wanted to know what the later stages of visual processing did so he could add them to his model. </p>
<p class="TXI">But the famous pair of physiologists never provided that information. After their initial work cataloguing cell types, Hubel and Wiesel explored the responses of cells in other visual areas, but were never able to give as clean a description as they had for the primary visual cortex. They eventually moved on to studying how the visual system develops in young animals. </p>
<p class="TXI">Without the script provided by biology, Fukushima needed to improvise. The solution he devised was to take the structure he had – that of simple cells projecting to complex cells – and repeat it. Stacking more simple and complex cells on top of each over and over creates an extended hierarchy that visual information can be passed through. This means, specifically, a second round of ‘simple’ cells comes after the initial layer of complex <a id="page_172"/>cells. This second layer of simple cells would be on the lookout not for simple features in the image, but rather for simple ‘features’ in the activity of the complex cells from which they get their input. They’d still use filters and convolutions, but just applied to the activity of the neurons below them. Then these simple cells would send inputs to their own complex cells that respond to the same features in a slightly larger region of space – and then the whole process starts again. </p>
<p class="TXI">Simple cells look for patterns; complex cells forgive a slight misplacement of those patterns. Simple, complex; simple, complex. Over and over. Repeating this riff leads to cells that are responsive to all kinds of patterns. For a second-layer simple cell to respond to the letter ‘L’, for example, it just needs to get input from a horizontal-preferring complex cell at one location and a vertical-preferring one at the location just above and to the left of it. A third-layer simple cell could then easily respond to a rectangle by getting input from two appropriately placed ‘L’-cells. Go further and further up the chain and cells start responding to larger and more complex patterns, including full shapes, objects and even scenes. </p>
<p class="TXI">The only problem with extending Hubel and Wiesel’s findings this way was that Fukushima didn’t actually know how the cells in the different layers should connect to each other. The filters – those grids of numbers that would determine how the simple cells at any given layer respond – had to be filled. But how? For this, Fukushima took a page out of Selfridge’s book of Pandemonium and turned to learning. </p>
<p class="TXI">Rather than using the kind of trial and error Selfridge proposed, Fukushima used a version of learning that <a id="page_173"/>doesn’t require knowing the right answers. In this form of learning, the model is simply shown a series of images without being told what’s in them. The activity of all the artificial neurons is calculated in response to each image and the connections between neurons change depending on how active they are (this may remind you of the Hebbian style of learning discussed in <a href="chapter4.xhtml#chapter4">Chapter 4</a>). If a neuron was very active in response to a particular image, for example, the connections from its very active inputs would be strengthened. As a result, that neuron would respond strongly to that and similar images in the future. This makes neurons responsive to specific shapes and different neurons diverge to have different responses. The network is therefore able to pick out a diversity of patterns in the input images. </p>
<p class="TXI">In the end, Fukushima’s model contained three layers of simple and complex cells, and was trained using computer-generated images of the digits zero to four. He dubbed the network the ‘Neocognitron’ and published the results of it in the journal <span class="italic">Biological Cybernetics</span> in 1980. </p>
<p class="TXI">In their original papers, Hubel and Wiesel made a point of stressing that their classification system and nomenclature was not meant to be taken as gospel. The brain is complicated and dividing neurons into only two categories could in no way capture the full diversity of responses and functions. It was just for convenience and expediency of communication that they proceeded in such a way. Yet Fukushima found success in doing the exact thing Hubel and Wiesel warned against: he <span class="italic">did</span> collapse the brimming complexity of the brain’s visual system into two very simple computations. He did take these descriptions as true, or true enough, and even <a id="page_174"/>stretched them beyond what they were meant to describe. </p>
<p class="TXI">This practice – of collapsing and then expanding, of shaking the leaves off a tree and using it to build a house – is what all theorists and engineers know to be necessary if progress is to be made. Fukushima wanted to build a functioning visual system in a computer. Hubel and Wiesel provided a description of the brain’s visual system to a first approximation. Sometimes the first approximation is enough. </p>
<p class="center">* * *</p>
<p class="TXT">In 1987, like in any other year, the people of Buffalo, New York, mailed countless bills, birthday cards and letters through their local post office. What the citizens of the town didn’t know, as they inked the 5-digit zip code of the recipient on to their envelope, was that this bit of their handwriting would be immortalised – digitised and stored on computers across the country for years to come. It would become part of a database for researchers trying to teach computers how to read human handwriting and, in turn, revolutionise artificial vision. </p>
<p class="TXI">Some of the researchers working on this project were at Bell Labs, a research company owned by the telecommunications company AT&amp;T, located in suburban New Jersey. Among the group of mostly physicists was a 28-year-old French computer scientist named Yann LeCun. LeCun had read about Fukushima and his Neocognitron, and he recognised how the simple repeating architecture of that model could solve many of the hard problems of vision.</p> <a id="page_175"/>
<p class="TXI">LeCun also recognised, however, that the way the model learned its connections needed to change. In particular, he wanted to move back towards the approach of Selfridge and give the model access to images paired with the correct labels of which digit is in them. So, he tweaked some of the mathematical details of the model to make it amenable to a different kind of learning. In this type of learning, if the model misclassifies an image (for example, labels a two as a six), all the connections in the model – those grids of numbers that define what patterns are searched for – are updated in a way that makes them less likely to misclassify that image in the future. In this way, the model learns what patterns are important for identifying digits. This may sound familiar because what LeCun used was the backpropagation algorithm described in <a href="chapter3.xhtml#chapter3">Chapter 3</a>. Do this with enough images and the model as a whole becomes quite good at classifying images of handwritten digits, even ones it’s never seen before. </p>
<p class="TXI">LeCun and his fellow researchers unveiled the impressive results of their model, trained on the thousands of Buffalo digits, in 1989. The ‘convolutional neural network’ – the name given to this style of model – was born. </p>
<p class="TXI">Just like the template-matching approaches that came before it, convolutional neural networks found applications in the real world. In 1997, these networks formed a core part of a software system AT&amp;T developed to automate the processing of cheques at banks across America. By 2000, it was estimated that between 10 and 20 per cent of cheques in America were being processed by this software. In a charming example of science fulfilling its destiny, Goldberg’s dream of equipping <a id="page_176"/>banks with synthetic visual systems came true some 70 years after the invention of his microfilm machine. </p>
<p class="TXI">The method for training convolutional neural networks is a data-hungry one and the model will only learn to be as good as what’s fed into it. Just as important as getting the right model, then, is getting the right data. That is why it was so crucial to collect real samples of real digits written by real people. The Bell Lab researchers could’ve done as Fukushima did and made computer-generated images of numbers. But those would hardly capture the diversity, the nuance or the sloppiness in how digits are written in the wild. The letters that passed through the Buffalo post office contained nearly 10,000 examples of true, human handwriting, giving the model what it needed to truly learn. Once computer scientists saw the importance of real data, they were spurred to collect even more. A dataset of six times as many digits – named MNIST – was collected shortly after the Buffalo set. Surprisingly, this dataset remains one of the most commonly used for quickly testing out new models and algorithms for artificial vision. The digits for MNIST were written by Maryland high school students and US census takers.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> And while the writers <span class="italic">were</span> told what their digits were being used for in this case, they almost certainly wouldn’t have expected their handwriting to still be used by computer scientists some 30 years later. </p>
<p class="TXI">Tests of convolutional neural networks didn’t stop at digits, but when making the jump to more involved images they hit a snag. In the early 2000s, networks much like LeCun’s were trained on another dataset of 60,000 images, <a id="page_177"/>this time made of objects. The images were small and grainy – only 32x32 pixels – and could be of either airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships or trucks. While still a simple task for us, this marked a serious increase in difficulty for the networks. The full ambiguity inherent in resolving a three-dimensional world from two-dimensional input comes into play when real images of real objects are used. The same models that could learn to recognise digits struggled to make sense of these more realistic pictures. This brain-like approach to artificial vision was failing at the basic visual processing brains do every day. </p>
<p class="TXI">A tide turned, however, in 2012 when Alex Krizhevsky, Ilya Sutskever and Geoffrey Hinton from the University of Toronto used a convolutional neural network to win a major image-recognition contest known as the ImageNet Large Scale Visual Recognition Challenge. The contest consisted of labelling images – large (224x224 pixels), real-life pictures taken by people around the world and pulled from image-hosting websites like Flickr – as belonging to one of a thousand different possible object categories. On this very convincing test of visual skill, the convolutional neural network got 62 per cent correct, beating the second-place algorithm by 10 percentage points.</p>
<p class="TXI">How did the Toronto team do so well? Did they discover some new computation needed for vision? Did they find a magical technique to help the model learn its connections better? The truth, in this case, is actually much more banal. The difference between this convolutional neural network and the ones that came before it was mainly one of size. The Toronto team’s network had a total of over 650,000 artificial neurons in it – about 80 times the size of LeCun’s digit-recognising <a id="page_178"/>one. This network was so large, in fact, that it required some clever engineering to fit the model in the memory of the computer chips that were used to run it. The model went big in another way, too. All those neurons meant a lot more data was needed to train the connections between them. The model learned from 1.2 million labelled images collected by computer science professor Fei-Fei Li as part of the ImageNet database. </p>
<p class="TXI">A watershed year came in 2012 for convolutional neural networks. While the Toronto team’s advances were technically just a quantitative leap – upping the number of neurons and images – the stunning performance enhancement made a qualitative difference in the field. After seeing what they were capable of, researchers flocked to study convolutional neural networks and to make them even better. This usually went in the same direction: making them bigger, but important tweaks to their structure and to how they learned were found as well. </p>
<p class="TXI">By 2015, a convolutional neural network reached the level of performance expected of a human in the image-classification competition (which isn’t actually 100 per cent; some of the images can be confusing). And convolutional neural networks now form the base of almost any image-processing software: facial recognition on social media, pedestrian detection in self-driving cars and even automatic diagnosis of diseases in X-ray images. In an amusing bending of science in on itself, convolutional neural networks have even been used <span class="italic">by neuroscientists</span> to help automatically detect where neurons are in pictures of brain tissue. Artificial neural networks are now looking at real ones. </p>
<p class="TXI">It would seem that engineers made a smart move turning to the brain for inspiration on how to build a <a id="page_179"/>visual system. Fukushima’s attention to the functions of neurons – and his condensing of those functions into simple operations – has paid dividends. But when he was taking the first steps in the development of these models, the computing resources and the data to make them shine simply wasn’t available. Decades later, the next generation of engineers picked up the project and brought it across the finish line. As a result, current convolutional neural networks can finally do many of the tasks originally asked for by the MIT summer project in 1966. </p>
<p class="TXI">But just as Selfridge’s Pandemonium helped inspire visual neuroscientists, the relationship between convolutional neural networks and the brain does not go only one way. Neuroscientists have come to reap rewards from the effort computer scientists put into making models that can solve real visual problems. That’s because not only are these large, heavily trained convolutional neural networks good at spotting objects in images, they’re also good at predicting how the brain will respond to those same images. </p>
<p class="center">* * *</p>
<p class="TXT">Visual processing gets started in the primary visual cortex – where Hubel and Wiesel did their recordings – but many areas are involved after that. The primary visual cortex sends connections to (you guessed it) the secondary visual cortex. And after a few more relays, the information ends up in the temporal cortex, located just behind the temples. </p>
<p class="TXI">The temporal cortex has been associated with object recognition for a long time. As early as the 1930s, researchers noticed that damage to this brain area leads to some strange behaviour. Patients with temporal cortex damage are bad <a id="page_180"/>at deciding what things are important to look at and so get easily distracted. They also don’t show normal emotional responses to images; they can see pictures most people would find terrifying and hardly blink. And when they want to explore objects, they may do so not by looking at them but by putting them in their mouths. </p>
<p class="TXI">Understanding of this brain area was refined through decades of careful observation of patients or animals with brain lesions and eventually by recording the activity of its neurons. This led to the conclusion that a subpart of the temporal cortex – the ‘inferior’ part at the bottom, also called ‘IT’ – is the main location for object understanding. People with damage to IT have mostly normal behaviour and vision, but with the more specific problem of being unable to appropriately name or recognise objects; they may, for example, fail to recognise the faces of friends or confuse the identity of objects that appear similar.</p>
<p class="TXI">Accordingly, the neurons in this area respond to objects. Some neurons have clear preferences; one may fire when a clock is present, another for a house, another for a banana, <span class="italic">etc.</span> But other cells are less scrutable. They may prefer portions of objects or respond similarly to two different objects that have some features in common. Some cells also care about the angle the object is seen from, perhaps firing most if the object is seen straight on, but others are more forgiving and respond to an object at almost any angle. Some care about the size and location of the object, others don’t. In total, IT is a grab-bag of neurons interested in objects. While they’re not always easy to interpret, such object-driven responses make IT seem like the top of the visual-processing hierarchy, the last stop on the visual system express.</p> <a id="page_181"/>
<p class="TXI">Neuroscientists have tried for decades to understand exactly how IT comes to have these responses. Frequently they followed in the footsteps of Fukushima and built models with stacks of simple and complex cells, hoping that these computations would mimic those that lead to IT activity and make the activity perfectly predictable. This approach worked to an extent, but just like with the Neocognitron, the models were small and they learned their connections from a small set of small images. To make real progress, neuroscientists needed to scale up their models just the same way computer scientists did.</p>
<p class="TXI">In 2014, two separate groups of scientists – one led by Nikolaus Kriegeskorte</p>
<p class="TXI">at Cambridge University and one by James DiCarlo at MIT – did exactly that. They showed real and diverse images of objects to subjects (humans and monkeys) and recorded the activity of different areas of their visual systems as they viewed them. They also showed the same images to a large convolutional neural network trained to classify real images. What both groups found was that these computer models provided a great approximation to biological vision. Particularly, they showed that if you want to guess how a neuron in IT will respond to a specific image, the best bet – better than any previous method neuroscientists had tried – was to look at how the artificial neurons in the network responded to it. Specifically, the neurons in the last layer of the network best predicted the activity of IT neurons. What’s more, the neurons in the second-to-last layer best predicted the activity of neurons in V4 – the area that gives input to IT. The convolutional neural network, it seemed, was mimicking the brain’s visual hierarchy.</p> <a id="page_182"/>
<p class="TXI">By showing such a striking alignment between the model and the brain, this research ushered in a revolution in the study of biological vision. It demonstrated that neuroscientists were broadly on the right track, a track that started with Lettvin and Hubel and Wiesel, but that they needed to be bigger and bolder. If they wanted a model that could explain how animals see objects, the model itself needed to be able to see objects. </p>
<p class="TXI">Going this way, though, symbolised an abandonment of principles that some theorists hold dear: a striving for elegance, simplicity and efficiency in models. There’s nothing elegant or efficient about 650,000 artificial neurons wired up in whatever way they found to work. Compared to some of the most beloved and beautiful equations in science, these networks are hulking, unsightly beasts. But, in the end, they work – and there is no guarantee that anything more elegant will. </p>
<p class="TXI">Selfridge’s work pushed biologists to see the visual system as a hierarchy and the experiments that resulted from this planted the seeds for the design of convolutional neural networks. These seeds were incubated in computer science and, in the end, the collaboration yielded fruit for both sides. In general, the desire for artificial systems that can do real visual tasks in the real world has pushed the study of biological vision in directions it may not have gone on its own. Engineers and computer scientists have always enjoyed having the brain’s visual system to look to – not only for inspiration, but for proof that this challenging problem is solvable. This mutual appreciation and influence makes the story of the study of vision a uniquely interwoven one.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-1" id="fn-1">1</a> ﻿That professor would be Marvin Minsky and the professor that wrote the memo was Seymour Papert, both key participants in ﻿﻿Chapter 3﻿﻿. Indeed, as you﻿’﻿ll see, there are many overlapping players and themes in the histories of artificial neural networks and artificial vision.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-2" id="fn-2">2</a> ﻿Primates are admittedly fairly unusual in this sense. Rodent brains, for example, lean more towards processing smell.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-3" id="fn-3">3</a> ﻿Actually, pixels in colour images are defined by three numbers corresponding to the intensities of the red, green and blue components. For simplicity, we﻿’﻿ll speak of pixels as being a single number, despite the fact that this is only true for grayscale images.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-4" id="fn-4">4</a> ﻿Though in response to a colleague﻿’﻿s remark on it, Selfridge commented: ﻿‘﻿All of us have sinned in Adam, we have eaten of the tree of the knowledge of good and evil, and the demonological allegory is a very old one, indeed.﻿’﻿﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-5" id="fn-5">5</a> ﻿From the Greek for ﻿‘﻿all the demons﻿’﻿, introduced in John Milton﻿’﻿s ﻿<span class="italic">Paradise Lost</span>﻿.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-6" id="fn-6">6</a> ﻿Hubel was actually quite interested in mathematics and physics, and was accepted into a PhD programme in physics at the same time he was accepted to medical school. Truly torn, he waited until the last possible day to make the choice.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-7" id="fn-7">7</a> ﻿Hubel and Wiesel did not, however, mention Lettvin or his pioneering work in the frog during this speech. This was an omission Selfridge referred to as ﻿‘﻿rotten manners, putting it very mildly﻿’﻿.﻿</p>
<p class="FN1"><a href="chapter6.xhtml#fnt-8" id="fn-8">8</a> ﻿You can guess who had the neater handwriting.﻿</p>
</body>
</html>