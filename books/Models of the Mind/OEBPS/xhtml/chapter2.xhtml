<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="en" xml:lang="en">
<head>
<title>Chapter 2</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter2"><a href="contents.xhtml#re_chapter2">CHAPTER TWO</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter2">How Neurons Get Their Spike</a><a id="page_17"/></p>
<p class="H1" id="b-9781472966445-ch99-sec2">
<span class="bold">
<span>Leaky integrate-and-fire and Hodgkin-Huxley neurons</span>
</span></p>
<p class="TXT">‘The laws of action of the nervous principle are totally different from those of electricity,’ concluded Johannes Müller more than 600 pages into his 1840 textbook <span class="italic">Handbuch der Physiologie des Menschen</span>. ‘To speak, therefore, of an electric current in the nerves, is to use quite as symbolical an expression as if we compared the action of the nervous principle with light or magnetism.’</p>
<p class="TXI">Müller’s book – a wide-ranging tour through the new and uncertain terrain of the field of physiology – was widely read. Its publication (especially its near-immediate translation into English under the title <span class="italic">Elements of Physiology</span>) cemented Müller’s reputation as a trusted teacher and scientist. </p>
<p class="TXI">Müller was a professor at Humboldt University of Berlin from 1833 until his death 25 years later. He had a broad interest in biology and strong intellectual views. He was a believer in vitalism, the idea that life relied on a <span class="italic">Lebenskraft</span>, or vital organising force, that went beyond mere chemical and physical interactions. This philosophy could be found streaking through his physiology. In his book, he claims not only that the activity of nerves is not electric in nature, but that it may ultimately be ‘imponderable’, the question of its essence ‘not capable of solution by physiological facts’.</p> <a id="page_18"/>
<p class="TXI">Müller, however, was wrong. Over the course of the following century, the spirit that animated the nerves would prove wholly reducible to the simple movement of charged particles. Electricity is indeed the ink in which the neural code is written. The nervous principle was perfectly ponderable after all.</p>
<p class="TXI">More than merely striking down Müller’s vitalism, the identification of this ‘bio-electricity’ in the nervous system provided an opportunity. By forging a path between the two rapidly developing studies of electricity and physiology, it allowed for the tools of the former to be applied to the problems of the latter. Specifically, equations – whittled down by countless experiments to capture the essential behaviours of wires, batteries and circuits – now offered a language in which to describe the nervous system. The two fields would come to share symbols, but their relationship was far more than the merely symbolic one Müller claimed. The proper study of the nervous system depended on collaboration with the study of electricity. The seeds of this collaboration, planted in the nineteenth century, would come to sprout in the twentieth and bloom in the twenty-first. </p>
<p class="center">* * *</p>
<p class="TXT">Walk into the home of an educated member of upper-class society in late eighteenth-century Europe and you may find, among shelves of other scientific tools and curiosities, a Leyden jar. Leyden jars, named after the Dutch town that was home to of one of their inventors, are glass jars like most others. However instead of storing jam or pickled vegetables, Leyden jars store charge. <a id="page_19"/>Developed in the mid-eighteenth century, these devices marked a turning point in the study of electricity. As a literal form of lightning in a bottle, they let scientists and non-scientists alike control and transmit electricity for the first time – sometimes doling out shocks large enough to cause nosebleeds or unconsciousness. </p>
<p class="TXI">While its power may be large, the Leyden jar’s design is simple (see Figure 1). The bottom portion of the inside of the jar is covered in a metal foil, as is the same region on the outside. This creates a sandwich of glass in between the two layers of metal. Through a chain or rod inserted at the top of the jar, the internal foil gets pumped full of charged particles. Particles of opposite charge are attracted to each other, so if the particles going into the jar are positively charged, for example, then negatively charged ones will start to accrue on the outside. The particles can never reach each other, however, because the glass of the jar keeps them apart. Like two neighbourhood dogs separated by a fence, they can only line up on either side of the glass, desperately wishing to be closer.</p><a id="page_20"/>
<p class="TXI">We would now call a device that stores charge like the Leyden jar a ‘capacitor’. The disparity in charge on either side of the glass creates a difference in potential energy known as voltage. Over time, as more and more charge is added to the jar, this voltage increases. If the glass barrier disappeared – or another path were provided for these particles to reach each other – that potential energy would turn into kinetic energy as the particles moved towards their counterparts. The higher the voltage was across the capacitor, the stronger this movement of charge – or current – would be. This is exactly how so many scientists and tinkerers ended up shocking themselves. By creating a link between the inside and outside of the jar with their hand, they opened a route for the flow of charged particles right through their body.</p> 
<p class="image-fig" id="fig1.jpg">
<img alt="" src="../images/fig1.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 1</span>
</span></p>
<p class="TXI">Luigi Galvani was an Italian scientist born in 1737. Strongly religious throughout his life, he considered joining the church before eventually studying medicine at the University of Bologna. There he was educated not just in techniques of surgery and anatomy, but also in the fashionable topic of electricity. The laboratory he kept in his home – where he worked closely with his wife Lucia, the daughter of one of his professors – contained devices for exploring both the biological and the electric: scalpels and microscopes, along with electrostatic machines and, of course, Leyden jars. For his medical experiments, Galvani – like students of biology for centuries before and after him – focused on frogs. The muscles in a frog’s legs can keep working after death, a desirable feature when trying to simultaneously understand the workings of an animal and dissect it.</p> <a id="page_21"/>
<p class="TXI">It was a result of his lab’s diversity – and potentially disorganisation – that landed Galvani in the pages of science textbooks. As the story goes, someone in the lab (possibly Lucia) touched a metal scalpel to the nerve of a dead frog’s leg at the exact moment that an errant spark from an electrical device caused the scalpel to carry charge. The leg muscles of the frog immediately contracted, an observation Galvani decided to enthusiastically pursue. In his 1791 book he describes many different preparations for his follow-up experiments on ‘animal electricity’, including comparing the efficacy of different types of metal in eliciting contractions and how he connected a wire to a frog’s nerve during a thunderstorm. He watched its legs contract with each lightning flash.</p>
<p class="TXI">There had always been some hints that life was making use of electricity. Ibn Rushd, a twelfth-century Muslim philosopher, anticipated several scientific findings when he noted that the ability of an electric fish to numb the fishermen in its waters may stem from the same force that pulls iron to a lodestone. And in the years before Galvani’s discovery, physicians were already exploring the application of electric currents to the body as a cure for everything from deafness to paralysis. But Galvani’s varied set of experiments took the study of bio-electricity beyond mere speculation and guesswork. He gathered the evidence to show that animal movement follows from the movement of electricity in the animal. He thus concluded that electricity was a force intrinsic to animals, a kind of fluid that flowed through their bodies as commonly as blood. </p>
<p class="TXI">In line with the spirit of amateur science at the time, upon hearing news of Galvani’s work many people set <a id="page_22"/>out to replicate it. Putting their personal Leyden jars in contact with any frog they could capture, curious laymen saw the same contractions and convulsions as Galvani did. So broad was the impact of Galvani’s work – and along with it the idea of electrical animation – it made its way into the mind of English writer Mary Shelley, forming part of the inspiration for her novel <span class="italic">Frankenstein</span>.</p>
<p class="TXI">A healthy dose of scientific scepticism, however, meant that not all of Galvani’s academic peers were so enthusiastically accepting of his claims. Alessandro Volta – an Italian physicist after whom ‘voltage’ was named – acknowledged that electricity could indeed cause muscle contractions in animals. But he denied that this means animals <span class="italic">normally</span> use electricity to move. Volta didn’t see in Galvani’s experiments any evidence that animals were producing their own electricity. In fact, he found that contact between two different metals could create many, nearly imperceptible, electric forces and therefore any test of animal electricity using metals in contact could be contaminated by externally generated electricity. As Volta wrote in an 1800 publication: ‘I found myself obliged to combat the pretended animal electricity of Galvani and to declare it an external electricity moved by the mutual contact of metals of different kinds’.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">Unfortunately for Galvani, Volta was a younger man, more willing to engage in public debate and on his way up in the field. He was a formidable scientific opponent. The power of Volta’s personality meant Galvani’s ideas, though correct in many ways, would be eclipsed for decades.</p> <a id="page_23"/>
<p class="TXI">Müller’s textbook came nearly 10 years after Volta’s death, but his objection to animal electricity followed similar lines. He simply didn’t believe electricity was the substance of nervous transmission and the weight of the evidence at the time couldn’t sway him. In addition to his vitalist tendencies, this stubbornness was perhaps due to Müller’s preference for observation over intervention. No matter how many examples of animals responding to externally applied electricity amassed over the years, it would never equal a direct observation of an animal generating its own electricity. ‘Observation is simple, indefatigable, industrious, upright, without any preconceived opinion,’ said Müller in his inaugural lecture at the University of Bonn. ‘Experiment is artificial, impatient, busy, digressive, passionate, unreliable.’ At the time, however, observation was impossible. No tool was powerful enough to pick up on the faint electrical signals carried by nerves in their natural state. </p>
<p class="TXI">That changed in 1847 when Emil du Bois-Reymond – one of Müller’s own students – fashioned a very sensitive galvanometer,<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> a device that measures current through its interaction with a magnetic field. His experiments were an attempt to replicate in nerves what Italian physicist Carlo Matteucci had recently observed in muscles. Using a galvanometer, Matteucci detected a small change in current coming from muscles after forcing them to contract. Searching for this signal in a nerve, however, demanded a stronger magnetic field to pick up the weaker current. In addition to designing proper insulation 
to prevent any interference from outside electricity, <a id="page_24"/>du Bois-Reymond had to coil more than a mile of wire by hand (producing more than eight times the coils of Matteucci) to get a magnetic field strong enough for his purposes. His handiwork paid off. With his galvanometer measuring its response, du Bois-Reymond stimulated 
a nerve in various ways – including electrically or 
with chemicals like strychnine – and monitored the galvanometer’s reading of how the nerve responded. Each time, he saw the galvanometer’s needle shoot up. Electricity had been spotted at work in the nervous system. </p>
<p class="TXI">Du Bois-Reymond was a showman as much as he was a scientist and he lamented the dry presentation styles of his fellow scientists. To spread the fruits of his labour, he built several public-ready demonstrations of bio-electricity, including a set-up where he could make a needle move by squeezing his arm in a jar of salt water. All this helped ensure that his findings would be noticed and that du Bois-Reymond would be fondly regarded by the minds of his time. As he said: ‘Popularisers of science persist in the public mind as memorial stones of human progress long after the waves of oblivion have surged over the originators of the soundest research.’ </p>
<p class="TXI">Luckily his research was sound as well. Particularly, the follow-up work du Bois-Reymond carried out with his student Julius Bernstein would seal the fate of the theory of nervous electricity. Du Bois-Reymond’s original experiment had succeeded in showing a signature of current change in an activated nerve. But Bernstein, through clever and careful experimental design, was able to both amplify the strength of the signal and record it at a finer timescale – creating the first true observation of the elusive nervous signal.</p><a id="page_25"/>
<p class="TXI">Bernstein’s experiments worked by first isolating a nerve and placing it on to his device. The nerve was then electrically stimulated at one end and Bernstein would look for the presence of any electrical activity some distance away. By recording with a precision of up to one-third of one-thousandth of a second, he saw how the nerve current changed characteristically over time after each stimulation. Depending on how far his recording site was from the stimulation site, there may be a brief pause as the electric event travelled down the nerve to reach the galvanometer. Once it got to where he was recording, however, he always saw the current rapidly decrease and then more slowly recover to its normal value.</p>
<p class="TXI">Bernstein’s result, published in the inaugural issue of the <span class="italic">European Journal of Physiology</span> in 1868, was the first known recording of what is now referred to as an ‘action potential’. An action potential is defined as a characteristic pattern of changes in the electrical properties of a cell. Neurons have action potentials. Certain other excitable cells, like those in the muscles or heart, do as well. </p>
<p class="TXI">This electrical disturbance travels across a cell’s membrane like a wave. In this way, action potentials help a cell carry a signal from one end of itself to the other. In the heart, for example, the ripple of an action potential helps coordinate a cell’s contraction. Action potentials are also a way for a cell to say something to other cells. In a neuron, when an action potential reaches the knobbly end of an outgrowth known as an axon, it pushes out neurotransmitters. These chemicals can reach other cells and trigger action potentials in them as well. In the case of the familiar frog nerve, the action potentials travelling down the leg lead to the <a id="page_26"/>release of neurotransmitters on to the leg muscle. Action potentials in the muscle then cause it to twitch. </p>
<p class="TXI">Bernstein’s work was the first word in a long story about the action potential. Now recognised as the core unit of communication in the nervous system, the action potential forms the basis of modern neuroscience. This quick blip of electrical activity connects the brain to the body, the body to the brain and links all the neurons of the brain in between. </p>
<p class="TXI">With his glimpsing of current changes coming from the nerve, du Bois-Reymond wrote: ‘If I do not greatly deceive myself, I have succeeded in realising [...] the hundred years’ dream of physicists and physiologists, to wit, the identity of the nervous principle with electricity.’ The nervous principle was indeed identified in the action potential. Yet du Bois-Reymond had committed himself to a ‘mathematico-physical method’ of explaining biology, and while he had established the physical, he had not quite solved the mathematical. With a growing sense among scientists that proper science involved quantification, the job of describing the physical properties of the nervous principle was far from done. Indeed, it would take roughly another hundred years to capture the essence of the nervous principle in equations. </p>
<p class="center">* * *</p>
<p class="TXT">In contrast to the experience of Johannes Müller, when Georg Ohm published a book of his scientific findings he lost his job. </p>
<p class="TXI">Ohm was born in 1789, the son of a locksmith. He studied for only a short time at the university of his <a id="page_27"/>hometown, Erlangen in Germany, and then spent years teaching mathematics and physics in various cities. Eventually, with an aim of becoming an academic, he started performing his own small experiments, particularly around the topic of electricity. For one test, he cut wires of various lengths out of different metals. He then applied voltage across the two ends of the wire and measured how much current flowed between them. Through this he was able to deduce a mathematical relationship between the length of a wire and its current: the longer the wire, the lower the current. </p>
<p class="TXI">By 1827, Ohm had collected this and other equations of electricity into his book, <span class="italic">The Galvanic Circuit Investigated Mathematically</span>. Quite contrary to its modern form, the study of electricity in the time of Ohm wasn’t a very mathematical discipline and Ohm’s peers didn’t like his attempts to make it one. One reviewer went so far as to say: ‘He who looks on the world with the eye of reverence must turn aside from this book as the result of an incurable delusion, whose sole effort is to detract from the dignity of nature.’ Having taken time away from his job to write the book in the hope it would land him a promotion, Ohm, with the book’s failure, ended up resigning instead. </p>
<p class="TXI">Ohm, however, was right. The key relationship he observed – that the current that runs through a wire is equal to the voltage across it divided by the wire’s resistance – is a cornerstone of modern electrical engineering taught to first-year physics students worldwide. This is now known as Ohm’s law and the standard unit of resistance is the ‘ohm’. Ohm wouldn’t know the full impact of his work in his lifetime, but he <a id="page_28"/>did eventually get some recognition. At the age of 63 he was finally appointed a professor of experimental physics at the University of Munich, two years before he died. </p>
<p class="TXI">Resistance, as the name suggests, is a measure of opposition. It’s a description of just how much a material impedes the course of current. Most materials have some amount of resistance, but, as Ohm noted, the physical properties of a material determine just how resistant it is. Longer wires have higher resistance; thicker ones have lower. Just as the narrowing of an hourglass slows the flow of sand, wires with higher resistance hinder the flow of charged particles.</p>
<p class="TXI">Louis Lapicque knew of Ohm’s law. Born in France in 1866, shortly after the first recording of an action potential, Lapicque completed his doctorate at the Paris Medical School. He wrote his thesis on liver function and iron metabolism. Though his studies were scientific, his interests ranged more broadly from history to politics to sailing; he sometimes even took his boat to conferences across the English Channel.</p>
<p class="TXI">It was around the start of the twentieth century that Lapique started studying the nerve impulse. It would be the beginning of a decades-long project with his student-turned-wife-and-colleague Marcelle de Heredia, which centred on the concept of time in nerves. One of their earliest questions was: how long does it take to activate a nerve? It was well established by then that applying voltage across a nerve<sup><a href="#fn-3" id="fnt-3">3</a>
</sup> caused a response – measured either as an action potential observed directly in the <a id="page_29"/>nerve or as a muscle twitch that resulted from it. It was also clear that the amount of voltage applied mattered: higher voltage and the nerve would respond quicker, lower and it would respond slower. But what was the exact <span class="italic">mathematical</span> relationship between the stimulation applied and the time it took to get a response? </p>
<p class="TXI">This may sound like something of a small research question, a curiosity not of much consequence, but it was Lapicque’s approach to it that mattered. Because a proper physiologist also needed to be an engineer – designing and building all manner of electrical devices for stimulating and recording from nerve fibres – Lapicque knew the rules of electricity. He knew about capacitors, resistance, voltage and Ohm’s law. And it was with this knowledge that he composed a mathematical concept of the nerve that would answer his question – and many more to come. </p>
<p class="TXI">Understanding of the membranes that enclose cells had grown in the decades before Lapicque’s work. It was becoming clear that these bundles of biological molecules worked a little bit like a brick wall: they didn’t let much through. Some of the particles they were capable of keeping apart included ions – atoms of different elements like chloride, sodium or potassium that carry a positive or negative charge. So, just as charged particles could build up on either side of the glass in a Leyden jar, so too could they accrue on the inside and outside of a cell. As Lapicque wrote in his 1907 paper: ‘These ideas lead, when treated in the simplest possible way, to already established equations for the polarisation of metal electrodes.’ </p>
<p class="TXI">He thus came to describe the nerve in terms of an ‘equivalent circuit’. (see Figure 2) That is, he assumed <a id="page_30"/>that different parts of the nerve acted like the different components of an electrical circuit. The first equivalence was made between the cell membrane and a capacitor, as the membrane could store charge in just the same way. But it was clear that these membranes weren’t acting as perfect capacitors; they couldn’t keep all of the charge apart. Instead, some amount of current seemed to flow between the inside and outside of the cell, allowing it to discharge slightly. A wire with some resistance could play this role. So Lapicque added a resistor to his circuit model of the nerve in parallel with the capacitor. This way, when current is injected into the circuit, some of that charge goes to the capacitor and some goes through the resistor. Trying to create a charge difference across the cell is therefore like pouring water into an imperfect bucket; much of it would stay in the bucket, but some would leak away.</p>
<p class="image-fig" id="fig2.jpg">
<img alt="" src="../images/fig2.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 2</span>
</span></p>
<p class="TXI">This analogy between a cell and a circuit made it possible for Lapicque to write down an equation. The equation described how the voltage across the cell <a id="page_31"/>membrane should change over time, based on how much voltage was being applied to it and for how long. With this formalisation in mind, he could calculate when the nerve would respond.</p>
<p class="TXI">For the data to test his equation on, Lapicque turned to the standard frog-leg experiment: he applied different amounts of voltage to the frog’s nerve and recorded the time it took to see a response. Lapicque assumed that when the frog nerve responded it was because the voltage across its membrane had reached a certain threshold. He therefore calculated how long his model would take to reach that threshold for each different voltage applied. Comparing the predictions from his model with the results of his experiments, Lapicque found a good match. He could predict just how long a certain voltage would need to be applied in order to make the nerve respond.</p>
<p class="TXI">Lapicque wasn’t the first to write down such an equation. A previous scientist, Georges Weiss, offered a guess as to how to describe this relationship between voltage and time. And it was a relatively good guess too; it deviated from Lapicque’s predictions only somewhat, for example, in the case of voltages applied for a long time. But just as the smallest clue at a crime scene can change the narrative of the whole event, this slight difference between the predictions of Lapicque’s equation and what came before actually signalled a deep difference in understanding.</p>
<p class="TXI">Unlike Lapicque’s, Weiss’ equation wasn’t inspired by the mechanics of a cell nor was it meant to be interpreted as an equivalent circuit. It was more a description of the data than a model of it. Whereas a descriptive equation is <a id="page_32"/>like a cartoon animation of an event – capturing its appearance but without any depth – a model is a re-enactment. A mathematical model of a nerve impulse thus needs to have the same moving parts as the nerve itself. Each variable should be mappable to a real physical entity and their interactions should mirror the real world as well. That is just what Lapicque’s equivalent circuit provided: an equation where the terms were interpretable.</p>
<p class="TXI">Others before Lapicque had seen the similarity between the electrical tools used to study the nerve and the nerve itself. Lapicque was building heavily on the work of Walther Nernst, who noticed that the membrane’s ability to separate ions could underlie the action potential. Another student of du Bois-Reymond, Ludimar Hermann, had spoken of the nerve in terms of capacitors and resistors. And even Galvani himself had a vision of a nerve that worked similarly to his Leyden jar. But with his explicit equivalent circuit and quantitative fit to data, Lapicque went a step further in making an argument for the nerve as a precise electrical device. As he wrote: ‘The physical interpretation that I reach today gives a precise meaning to several important previously known facts on excitability … It seems to me a reason to consider it a step in the direction of realism.’</p>
<p class="TXI">Due to their limited equipment, most neuroscientists of Lapicque’s time were recording from whole nerves. Nerves are bundles of many axons – the fibres through which individual neurons send their signals to other cells. Recording from many axons at once makes it easier to pick up the current changes they produce, but harder to see the detailed shape of those changes. Sticking an electrode into a single neuron, however, makes it possible <a id="page_33"/>to record the voltage across its membrane directly. Once the technology to observe individual neurons became available in the early twentieth century, the action potential came into much clearer view. </p>
<p class="TXI">One defining feature of the action potential noticed by English physiologist Edgar Adrian in the 1920s is the ‘all-or-nothing’ principle.<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> The all-or-nothing principle says that a neuron either emits an action potential or it doesn’t – nothing in between. In other words, any time a neuron gets enough input, the voltage across its membrane changes – and it changes in exactly the same way. So, just as a goal in hockey counts the same no matter how hard the puck is driven into the net, strongly stimulating a neuron doesn’t make its action potential any bigger or better. All stronger stimulation can do is make the neuron emit <span class="italic">more</span> of the exact same action potentials. In this way, the nervous system cares more about quantity than quality. </p>
<p class="TXI">The all-or-nothing nature of a neuron aligns with Lapicque’s intuition about a threshold. He knew that the voltage across the membrane needed to reach a certain value in order to see a response from the nerve. But once it got there, a response was a response. </p>
<p class="TXI">By the 1960s, the all-or-nothing principle was combined with Lapicque’s equation into a mathematical model known as a ‘leaky integrate-and-fire neuron’: ‘leaky’ because the presence of a resistor means some of the current leaks away; ‘integrate’ because the capacitor integrates the rest of it and stores it as charge; and ‘fire’ <a id="page_34"/>because when the voltage across the capacitor reaches the threshold the neuron ‘fires’, or emits an action potential. After each ‘firing’ the voltage is reset to its baseline, only to reach threshold again if more input is given to the neuron. </p>
<p class="TXI">While the model is simple, it can replicate features of how real neurons fire: for example, with strong and constant input the model neuron will fire action potentials repeatedly, with only a small delay between each one; if the input is low enough, however, it can remain on indefinitely without ever causing a single action potential. </p>
<p class="TXI">These model neurons can also be made to form connections – strung together such that the firing of one generates input to another. This provides modellers with a broader power: to replicate, explore and understand the behaviour of not just individual neurons but whole networks of them. </p>
<p class="TXI">Since their inception, such models have been used to understand countless aspects of the brain, including disease. Parkinson’s disease is a disorder that impacts the firing of neurons in the basal ganglia. Located deep in the brain, the basal ganglia are composed of a variety of regions with elaborate Latin names. When the input to one region – the striatum – is perturbed by Parkinson’s disease, it knocks the rest of the basal ganglia off balance. As a result of changes to the striatum, the subthalamic nucleus (another region of the basal ganglia) starts to fire more, which causes neurons in the globus pallidus external (yet another basal ganglia region) to fire. But those neurons send connections <span class="italic">back</span> to the subthalamic nucleus that <a id="page_35"/>
<span class="italic">prevent</span> those neurons from firing more – which also, in turn, shuts down the globus pallidus external itself. The result of this complicated web of connections is oscillations: neurons in this network fire more, then less, then more again. These rhythms appear to be related to the movement problems Parkinson’s patients have – tremors, slowed movements and rigidity. </p>
<p class="TXI">In 2011, researchers at the University of Freiburg built a computer model of these brain regions made up of 3,000 leaky integrate-and-fire model neurons. In the model, disturbing the cells that represented the striatum created the same problematic waves of activity seen in the subthalamic nuclei of Parkinson’s patients. With the model exhibiting signs of the disease, it could also be used to explore ways to treat it. For example, injecting pulses of input into the model’s subthalamic nucleus broke these waves down and restored normal activity. But the pulses had to be at just the right pace – too slow and the offending oscillations got worse, not better. Deep brain stimulation – a procedure where pulses of electrical activity are injected into the subthalamic nucleus of Parkinson’s patients – is known to help alleviate tremors. Doctors using this treatment also know that the rate of pulses has to be high, around 100 times per second. This model gives a hint as to why high rates of stimulation work better than lower ones. In this way, modelling the brain as a series of interconnected circuits illuminates how the application of electricity can fix its firing.</p>
<p class="TXI">Lapicque’s original interest was in the timing of neural firing. By piecing together the right components of an electrical circuit he captured the timing of action potentials correctly, but the creation of this circuit stand-in for a neuron did more than that. It formed <a id="page_36"/>a solid foundation on which to build towering networks of thousands of interconnecting cells. Now computers across the world churn through the equations of these faux neurons, simulating how real neurons integrate and fire in both health and disease. </p>
<p class="center">* * *</p>
<p class="TXT">In the summer of 1939, Alan Hodgkin set out on a small fishing boat off the southern coast of England. His goal was to catch some squid, but mostly what he got was seasick. </p>
<p class="TXI">At the time, Hodgkin, a research fellow at Cambridge University, had only just arrived at the Marine Biological Association in Plymouth ready to embark on a new project studying the electrical properties of the squid giant axon. In particular, he wished to know how the action potential got its characteristic up-down shape (frequently referred to as a ‘spike’<sup><a href="#fn-5" id="fnt-5">5</a>
</sup>). A few weeks later he was joined by an equally green student collaborator, Andrew Huxley. Luckily, the men eventually figured out when and where their subject matter could be found in the sea. </p>
<p class="TXI">Though Huxley was a student of Hodgkin’s, the men were only four years apart in age. Hodgkin looked the role of a proper English gentleman: long face, sharp eyes, his hair neatly parted and swept to the side. Huxley was a bit more boyish with round cheeks and heavy eyebrows. Both men had skill in biology and physics, though each came to this pairing from the opposite side. </p>
<p class="TXI">Hodgkin primarily studied biology, but in his last term he was encouraged by a zoology professor to learn <a id="page_37"/>as much mathematics and physics as he could. Hodgkin obliged, spending hours with textbooks on differential equations. Huxley was long interested in mechanics and engineering, but switched to a more biological track after a friend told him physiology classes would teach more lively and controversial topics. Huxley may have also been drawn to these subjects by the influence of his grandfather. Biologist Thomas Henry Huxley – known as ‘Darwin’s bulldog’ for his vociferous defence of Darwin’s theory of evolution – described physiology as ‘the mechanical engineering of living machines’. </p>
<p class="TXI">Lapicque’s model predicted when a cell would fire, but it still didn’t explain exactly what an action potential was. At the time of Hodgkin’s boat trip, the going theory of what happens when a neuron emits an action potential was still the one put forward by the original action potential observer himself, Julius Bernstein. It said that, at the time of this electrical event, the cell membrane temporarily breaks down. It therefore lets ions of all different kinds flow through, erasing the charge difference that is normally across it and creating the small current Bernstein saw with his galvanometer. </p>
<p class="TXI">But some of Hodgkin’s previous experiments on crabs told him this may not be quite right. He wanted to follow up on this work with the squid because the large size of the axon running along its mantle made precise measurements easier.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup> Sticking an electrode into this axon, Hodgkin and Huxley recorded the voltage changes <a id="page_38"/>that occurred during an action potential (See Figure 3). What they saw was a clear ‘overshoot’. That is, the voltage didn’t just go to zero, as would happen with a discharged capacitor, but rather it <span class="italic">reversed</span>. While a neuron normally has more positive charge on the outside of the cell than on the inside, during the peak of the action potential this pattern gets inverted and the inside becomes more positively charged than the outside. Simply letting more ions diffuse through the membrane wouldn’t lead to such a separation. Something more selective was at play.</p>
<p class="TXI">Only a short time after Hodgkin and Huxley made this discovery, their work was unfortunately interrupted. Hitler invaded Poland. The men needed to abandon the lab and join the war effort. Solving the mystery of the action potential would have to wait.</p>
<p class="image-fig" id="fig3.jpg">
<img alt="" src="../images/fig3.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 3</span>
</span></p>
<p class="TXI">When Hodgkin and Huxley returned to Plymouth eight years later the lab required a bit of reassembling: the building had been bombed in air raids and their equipment had been passed around to other scientists. But the men, each having picked up some extra quantitative skills as a result of their wartime assignments – Huxley performing data analysis for the <a id="page_39"/>Gunnery Division of the Royal Navy and Hodgkin developing radar systems for the Air Force – were eager to get back to work on the physical mechanisms of the nerve impulse.</p>
<p class="TXI">For many of the following years, Hodgkin and Huxley (helped by fellow physiologist Bernard Katz) played with ions. By removing a certain type of ion from the neuron’s environment, they could determine which parts of the action potential relied on which kinds of charged particles. A neuron kept in a bath with less sodium had less of an overshoot. Add extra potassium to the bath and the undershoot – an effect at the very end of the action potential when the inside of the cell becomes more negative than normal – disappeared. The pair also experimented with a technique that let them directly control the voltage across the cell membrane. Changing this balance of charge created large changes in the flow of ions into and out of the cell. Remove the difference in charge across the membrane and stores of sodium outside the cell suddenly swim inwards; keep the cell in this state a bit longer and potassium ions from inside the cell rush out. </p>
<p class="TXI">The result of all these manipulations was a model. Specifically, Hodgkin and Huxley condensed their hard-won knowledge of the nuances of neural membranes into the form of an equivalent circuit and with it a corresponding set of equations. This equivalent circuit was more complex than Lapicque’s, however. It had more moving parts as it aimed to explain not just <span class="italic">when</span> an action potential happens but the full shape of the event itself. The main difference, though, came down to resistance.</p> <a id="page_40"/>
<p class="TXI">In addition to the resistor Lapicque put in parallel with the membrane capacitor, Hodgkin and Huxley added two more – one specifically controlling the flow of sodium ions and the other controlling the flow of potassium ions. Such a separation of resistors assumed that different channels in the cell membrane were selectively allowing different ion types to pass. What’s more, the strength of these resistors – that is, the extent to which they block the flow of their respective ions – is not a fixed parameter in the model. Instead, they are dependent on the state of the voltage across the capacitor. The cell accomplishes this by opening or closing its ion channels as the voltage across its membrane changes. In this way, the membrane of the cell acts like the bouncer of a club: it assesses the population of particles on either side of itself and uses that to determine which ions get to enter and exit the cell. </p>
<p class="TXI">Having defined the equations of this circuit, Hodgkin and Huxley wanted to churn through the numbers to see if the voltage across the model’s capacitor really would mimic the characteristic whish and whoosh of an action potential. There was a problem, however. Cambridge was home to one of the earliest digital computers, a device that would’ve greatly sped up Hodgkin and Huxley’s calculations, but it was out of service. So, Huxley turned to a Brunsviga – a large, metal calculator powered by a hand-crank. As he sat for days putting in the value of the voltage at one point in time just to calculate what it would be at the next one-ten-thousandth of a second, Huxley actually found the work somewhat suspenseful. As he said in his Nobel lecture: ‘It was quite often exciting … Would the membrane <a id="page_41"/>potential get away into a spike, or die in a subthreshold oscillation? Very often my expectations turned out to be wrong, and an important lesson I learnt from these manual computations was the complete inadequacy of one’s intuition in trying to deal with a system of this degree of complexity.’ </p>
<p class="TXI">With the calculations complete, Hodgkin and Huxley had a set of artificial action potentials, the behaviour of which formed a near-perfect mirror image of a real neuron’s spike. </p>
<p class="TXI">When injected with current, the Hodgkin-Huxley model cell displays a complex dance of changing voltage and resistances. First, the input fights against the cell’s natural state: it adds some positive charge to the largely negative inside of the cell. If this initial disturbance in the membrane’s voltage is large enough – that is, if the threshold is met – sodium channels start opening and a glut of positively charged sodium ions flood into the cell. This creates a positive feedback loop: the influx of sodium ions pushes the inside of the cell more positive and the resulting change in voltage lowers the sodium resistance even more. Soon, the difference in charge across the membrane disappears. The inside of the cell is briefly as positive as the outside, and then more so – the ‘overshoot’. As this is happening potassium channels are opening, letting positively charged potassium ions fall out of the cell. The sodium and potassium channels work like saloon doors, one letting ions in and the other out, but now the potassium ions are moving quicker. The work of the potassium ions reverses the trend in voltage. As this exodus of potassium again makes the inside of the cell more negative, sodium channels close. The <a id="page_42"/>separation of charge across the membrane is being rebuilt. As the voltage nears its original value, positive charge continues to leak out of the still-open potassium channels – the ‘undershoot’. Eventually these too close, the voltage recovers and the cell has returned to normal, ready to fire again. The whole event takes less than one-half of one-hundredth of a second. </p>
<p class="TXI">According to Hodgkin, the pair built this mathematical model because ‘at first it might be thought that the response of a nerve to different electrical stimuli is too complicated and varied to be explained by these relatively simple conclusions’. But explain it they did. Like a juggler, the neuron combines simple parts in simple ways to create a splendidly intricate display. The Hodgkin-Huxley model makes clear that the action potential is a delicately controlled explosion occurring a billion times a second in your brain. </p>
<p class="TXI">The pair published their work – both experimental and computational – in a slew of <span class="italic">Journal of Physiology</span> papers in 1952. Eleven years later, they were awarded two-thirds of the Nobel Prize for ‘their discoveries concerning the ionic mechanisms involved in excitation and inhibition in the peripheral and central portions of the nerve cell membrane’. If doubts remained in the minds of any biologists about whether the nerve impulse was explainable in terms of ions and electricity, the work of Hodgkin and Huxley put those to rest. </p>
<p class="center">* * *</p>
<p class="TXT">‘The body <span class="italic">and dendrites</span> of a nerve cell are specialised for the reception and integration of information, <a id="page_43"/>which is conveyed as impulses that are fired from other nerve cells along their axons’ (emphasis added). With this modest sentence John Eccles, an Australian neurophysiologist and the third awardee alongside Hodgkin and Huxley, began his Nobel lecture. The lecture goes on to describe the intricacies of ion flows that occur when one cell sends information to another. </p>
<p class="TXI">What the lecture doesn’t discuss is dendrites. Dendrites are the wispy tendrils that grow out of a neuron’s cell body. These offshoots, like tree roots, branch and stretch and branch again to cover a wide area around the cell. A cell casts its dendritic net out among nearby cells to collect input from them. </p>
<p class="TXI">Eccles had a complex relationship with dendrites. The type of neuron he studied – found in the spinal cord of cats – had elaborate dendritic branches. They spanned roughly 20 times the size of the cell’s body in all directions. Yet Eccles didn’t believe this cellular root system was terribly relevant. He conceded that the parts of the dendrites nearest the cell body may have some use: axons from other neurons land on these regions and their inputs will get transported immediately into the cell body where they can contribute to causing an action potential. But, he claimed, those farther out were simply too distant to do much: their signal wouldn’t survive the journey to the cell body. Instead, he assumed the cell used these arms to absorb and expel charged particles in order to keep its overall chemical balance intact. In Eccles’ eyes, therefore, dendrites were – at most – a wick that carried a flame a short way to the cell body and, at least, a straw slurping up some ions. </p>
<p class="TXI">Eccles’ position on dendrites put him at odds with his student, Wilfrid Rall. Rall earned a degree in physics <a id="page_44"/>from Yale University in 1943 but, after his time working on the Manhattan Project, became interested in biology. He moved to New Zealand to work with Eccles on the effects of nerve stimulation in 1949. </p>
<p class="TXI">Given his background, Rall was quick to turn to mathematical analyses and simulations to understand a system as complex as a biological cell. And he was inspired and galvanised<sup><a href="#fn-7" id="fnt-7">7</a>
</sup> by the work of Hodgkin and Huxley, which he heard about when Hodgkin visited the University of Chicago where Rall was getting his master’s degree. With this mathematical model in mind, Rall suspected that dendrites were capable of more than Eccles gave them credit for. After his time in New Zealand, Rall devoted a good portion of his career to proving the power of dendrites – and, in turn, proving the power of mathematical models to anticipate discoveries in biology. </p>
<p class="TXI">Building on the analogy of the cell as an electrical circuit, Rall modelled the thin cords of dendrites as just what they looked like: cables. The ‘cable theory’ approach to dendrites treats each section of a dendrite as a very narrow wire, the width of which – just as Ohm discovered – determines its resistance. Stringing these sections together, Rall explored how electrical activity at the far end of a dendrite can make its way to the cell body or vice versa. </p>
<p class="TXI">Adding more parts to this mathematical model, however, meant more numbers to crunch. The National Institutes of Health (NIH) in Bethesda, Maryland, where Rall worked, didn’t have a digital computer suitable for some of his larger simulations. When Rall wanted to run <a id="page_45"/>the equations of a model with extensive dendrites, it was the job of Marjory Weiss, a programmer at the NIH, to drive a box of punch cards with the computer’s instructions to Washington DC to run them on a computer there. Rall couldn’t see the results of his model until she returned the next day. </p>
<p class="TXI">Through his elaborate mathematical explorations, Rall clearly showed – against the beliefs of Eccles – that a cell body with dendrites can have very different electrical properties than one without. A short description of Rall’s calculations, published in 1957, set off a years-long debate between the two men in the form of a volley of publications and presentations.<sup><a href="#fn-8" id="fnt-8">8</a>
</sup> Each pointed to experimental evidence and their own calculations to support their side. But slowly, over time, Eccles’ position shifted. By 1966, he had publicly accepted dendrites as a relevant cog in the neural machinery. Rall was right.</p>
<p class="TXI">Cable theory did more than just expose Eccles’ mistake. It also offered a way for Rall to explore in equations the many magical things that dendrites can do before the experimental techniques to do so were available. One important ability Rall identified was detecting order. Rall saw in his simulations that the order in which a dendrite gets input has important consequences for the cell’s response. If an input comes to a dendrite’s far end first, followed by more inputs closer and closer to the cell body, the cell may fire. If the <a id="page_46"/>pattern is reversed, however, it won’t. This is because inputs coming in far from the cell body take longer to reach it. So, starting the inputs at the far end means they all reach the cell body at the same time. This creates a big change in the membrane’s voltage and possibly a spike. Going the other way, however, inputs come in at different times; this creates only a middling disturbance in voltage. In a race where runners start at different times and locations, the only way to get them to cross the finish line together is to give the farther ones a head start. </p>
<p class="TXI">Rall made this prediction in 1964. In 2010, it was shown to be true in real neurons. To test Rall’s hypothesis, researchers at University College London took a sample of neurons from rat brains. Placing these neurons in a dish, they were able to carefully control the release of neurotransmitters on to specific portions of a dendrite – portions as little as five microns (or the width of a red blood cell) apart. When this input went from the end of a dendrite to its root, the cell spiked 80 per cent of the time. In the other direction, it responded only half as often.</p>
<p class="TXI">This work shows how even the smallest bit of biology has a purpose. That the sections of a dendrite can work like the keys of a piano – where the same notes can be played in different ways to different effect – gives neurons new tricks. Specifically, it imbues neurons with the ability to identify sequences. There are many occasions where inputs sweeping across the dendrite from one direction should be treated differently from inputs sweeping the other way. For example, neurons in the retina have this kind of ‘direction selectivity’. This lets them signal which way objects in the visual field are moving.</p><a id="page_47"/>
<p class="TXI">In many science classes, students are given small electrical circuit kits to play with. They can use wires of different resistances to connect up capacitors and batteries, maybe making a lightbulb light up or a fan spin. In much the same pick-and-play way, neuroscientists now build models of neurons. With the basic parts list of an electrical circuit, almost any observed property of a neuron’s activity can be mimicked. Rall helped add more parts to the kit.</p>
<p class="center">* * *</p>
<p class="TXT">If a standard neuron model is a small house built out of the bricks of electrical engineering, then the model constructed by the Blue Brain Project in 2015 is an entire metropolis. Eighty-two scientists across 12 institutions worked together as part of this unprecedented collaboration. Their goal was to replicate a portion of the rat brain about the size of a large grain of sand. They combed through previous studies and spent years performing their own experiments to collect every bit of data they could about the neurons in this region. They identified the ion channels they use, the length of their axons, the shapes of their dendrites, how closely they pack together and how frequently they connect. Through this, they identified 55 standard shapes that neurons can take, 11 different electrical response profiles they can have and a host of different ways they can interact.</p>
<p class="TXI">They used this data to build a simulation – a simulation that included more than 30,000 highly detailed model neurons forming 36 million connections. The full model required a specially built supercomputer to run through the billions of equations that defined it. Yet all of this <a id="page_48"/>complexity still stemmed from the same basic principles of Lapicque, Hodgkin, Huxley and Rall. A lead researcher on the project, Idan Segev, summarised the approach: ‘Use Hodgkin-Huxley in an extended way and build a simulation of the way these cells are active, to get the music – the electrical activity – of this network of neurons that should imitate the real biological network that you’re trying to understand.’ </p>
<p class="TXI">As the team showed in their publication documenting the work, the model was able to reproduce several features of the real biological network. The simulation showed similar sequences of firing patterns over time, a diversity of responses across cell types and oscillations. More than just replicating results of past experiments, this real-to-life model also makes it possible to explore new experiments quickly and easily. Recreating the biology in a computer makes virtual investigations of this brain region as simple as writing a few lines of code – an approach known as ‘in silico’ neuroscience.</p>
<p class="TXI">Running such simulations can only give good predictions if the model underlying them is a reasonable facsimile of biology. Thanks to Lapicque, we know that using the equations of an electrical circuit as a stand-in for a neuron is a solid foundation on which to build models of the brain. It was his analogy that set off the study of the nerve as an electrical device. And the extension of his analogy by countless other scientists – many trained in both physics and physiology – expanded its explanatory power even further. The nervous system – against Müller’s intuitions – is brought to life by the flow of electricity and the study of it has been undeniably animated by the study of electricity.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-1" id="fn-1">1</a> ﻿In the course of proving that contact between dissimilar metals generates electricity, Volta ended up inventing the battery. ﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-2" id="fn-2">2</a> ﻿Named, of course, after our man Galvani.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-3" id="fn-3">3</a> ﻿Applying voltage was an easier way to control the flow of charge than injecting current directly.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-4" id="fn-4">4</a> ﻿More on Adrian, and what his discovery meant about how neurons represent information, in ﻿﻿Chapter 7﻿﻿.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-5" id="fn-5">5</a> ﻿Spike, firing, activity, action potential ﻿–﻿ the sacred emission of a neuron goes by many names.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-6" id="fn-6">6</a> ﻿The ﻿‘﻿squid giant axon﻿’﻿ that Hodgkin and Huxley were studying is a particularly large (about the width of a marker tip) axon of a rather average-sized squid. It is not, as many students of neuroscience initially believe, the axon of a giant squid. ﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-7" id="fn-7">7</a> ﻿Also named after our man Galvani.﻿</p>
<p class="FN1"><a href="chapter2.xhtml#fnt-8" id="fn-8">8</a> ﻿According to Rall, Eccles even prevented his work from being published. With respect to a 1958 manuscript: ﻿‘﻿A negative referee persuaded the editors to reject this manuscript. The fact that this referee was Eccles, was clear from many marginal notes on the returned manuscript.﻿’﻿﻿</p>
</body>
</html>