<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:mml="http://www.w3.org/1998/Math/MathML" lang="en" xml:lang="en">
<head>
<title>Chapter 11</title>
<link href="../styles/stylesheet.css" rel="stylesheet" type="text/css"/>
<meta content="urn:uuid:e0000000-0000-0000-0000-000006607108" name="Adept.expected.resource"/>
</head>
<body>
<p class="CN" id="chapter11"><a href="contents.xhtml#re_chapter11">CHAPTER ELEVEN</a></p>
<p class="CT"><a href="contents.xhtml#re_chapter11">How Rewards Guide Actions</a><a id="page_309"/></p>
<p class="H1" id="b-9781472966445-ch1182-sec12">
<span class="bold">
<span>Temporal difference and reinforcement learning</span>
</span></p>
<p class="TXT">For much of his life as a scientist, Ivan Petrovich Pavlov had one passion: digestion. He started his academic work in 1870 with a thesis on pancreatic nerves. For 10 years as a professor of pharmacology in St Petersburg, he devised ways of measuring gastric juices in animals as they went about their life to show how secretions from different organs change in response to food or starvation. And by 1904, he was granted the Nobel prize ‘in recognition of his work on the physiology of digestion through which knowledge on vital aspects of the subject has been transformed and enlarged’. </p>
<p class="TXI">It is surprising, then, given all of his success in studying the gut, that Pavlov would go down in history as one of the most influential figures in psychology.</p>
<p class="TXI">Pavlov’s transition to studying the mind was, in a way, accidental. In an experiment designed to measure how dogs salivate in response to different foods, he noticed their mouths watering before the food even arrived – all it took was the sound of the footsteps of the assistant bringing in the bowls. This was not completely unusual. Much of Pavlov’s previous work looked at how the digestive system is influenced by the nervous system, but these were usually more obvious interactions such as the smell of food impacting stomach secretions – interactions <a id="page_310"/>that were plausibly thought to be innate to the animal. Drooling at the sound of footsteps isn’t a response hardwired into genes. It has to be learned. </p>
<p class="TXI">Pavlov was a strict and unforgiving scientist. When public shootings related to the Russian Revolution caused a colleague to be late to a meeting, Pavlov replied: ‘What difference does a revolution make when you have experiments to do in the laboratory?’ But this intensity could lend itself to meticulous work, and when he decided to follow up on these salivation observations he did so thoroughly and exhaustively. </p>
<p class="TXI">Pavlov would repeatedly present a dog with a neutral cue – like the ticking of a metronome or the sound of a buzzer (but not a bell, as is commonly thought; Pavlov relied only on stimuli that could be precisely controlled). He followed this neutral cue with food. After these pairings he would observe how much the dogs salivated in response to the cue alone. He wrote, in characteristic detail: ‘When the sounds from a beating metronome are allowed to fall upon the ear, a salivary secretion begins after nine seconds, and in the course of 45 seconds 11 drops have been secreted.’</p>
<p class="TXI">Varying the specifics of this procedure, Pavlov catalogued many features of the learning process. He asked questions like: ‘How many pairings of cue then food does it take to reliably learn?’ (around 20); ‘Does the timing between the cue and the food matter?’ (yes, the cue has to start before the food arrives but not too much before); ‘Does the cue need to be neutral?’ (no, the animals could learn to salivate in response to slightly negative cues, such as the application of a skin irritant); and many more.</p><a id="page_311"/>
<p class="TXI">This process – repeatedly pairing an upcoming reward with something usually unrelated to it until the two become linked – is known as classical or (unsurprisingly) ‘Pavlovian’ conditioning and it became a staple in early psychology research. Reviewers of Pavlov’s 1927 book outlining his methodology and results described his work as ‘of vital interest to all who study the mind and the brain’ and ‘remarkable both from the point of view of the exactness of his methods and the scientific insight shown in the sweeping character of his conclusions’. </p>
<p class="TXI">Pavlov’s work eventually fed into one of the biggest movements in twentieth-century science: behaviourism. According to behaviourism, psychology should not be defined as the study of the mind, but rather, as the study of behaviour. Behaviourists therefore prefer descriptions of observable external activity to any theorising about internal mental activity like thoughts, beliefs or emotions. To them, the behaviour of humans and animals can be understood as an elaborate set of reflexes – that is, mappings between inputs from the world to outputs produced by the animal. Conditioning experiments like Pavlov’s offered a clean way of quantifying these inputs and outputs, feeding into the behaviourism frenzy.</p>
<p class="TXI">After the publication of his book, therefore, many scientists were eager to replicate and build off Pavlov’s work. American psychologist B. F. Skinner, for example, learned about Pavlov through a book review written by famed sci-fi author H. G. Wells. Reading this article piqued Skinner’s interest in psychology and set him on the path to becoming a leading figure of the behaviourist <a id="page_312"/>movement, conducting countless precise examinations of behaviour in rats, pigeons and humans.<sup><a href="#fn-1" id="fnt-1">1</a>
</sup></p>
<p class="TXI">When any field of science amasses enough quantitative data, it eventually turns to mathematical modelling to make sense of it. Models find structure in piles of numbers; they can stitch together disparate findings and show how they arise from a unified process. In the decades after Pavlov, the amount of data being generated from behavioural experiments on learning made it ready for modelling. As William Estes, a prominent American psychologist working on the mathematics of learning, wrote in 1950, conditioning data ‘are sufficiently orderly and reproducible to support exact quantitative predictions of behaviour’. </p>
<p class="TXI">Another paper, published in 1951, agreed: ‘Among the branches of psychology, few are as rich as learning in quantity and variety of available data necessary for model building.’ This paper, ‘A mathematical model for simple learning’, was written by Robert Bush and Frederick Mosteller at the Laboratory of Social Relations at Harvard University. Bush was a physicist-turned-psychologist and Mosteller a statistician. Together, influenced by the work of Estes, they laid out a formula for learning associations between cues and rewards that would be the starting point for a series of increasingly elaborate models. Through the decades, the learning that these models capture became known as ‘reinforcement <a id="page_313"/>learning’. Reinforcement learning is an explanation for how complex behaviour arises when simple rewards and punishments are the only learning signals. It is, in many ways, the art of learning what to do without being told.</p>
<p class="center">* * *</p>
<p class="TXT">In their model, Bush and Mosteller focused on a specific measure of the learned association between the cue and reward: the probability of response. For Pavlov’s dogs, this is the probability of salivating in response to the buzzer. Bush and Mosteller used a simple equation to explain how that probability changes each time the reward is – or isn’t – given after the cue.</p>
<p class="TXI">Say you start with any random dog off the street (it is, in fact, rumoured that Pavlov got his subjects by stealing them off the streets). The probability that this dog will salivate at the sound of a buzzer starts at zero; it has no reason to suspect that the buzzer means food. Now you press the buzzer and then give the dog a piece of meat. According to the Bush-Mosteller model, after this encounter, the probability that the dog will salivate in response to the buzzer increases (see Figure 24). The exact amount that it increases depends on a parameter in the formula called the learning rate. Learning rates control the speed of the whole process. If the learning rate is very high, a single pairing could be enough to solidify the buzzer-food relationship in the dog’s mind. At more reasonable rates, however, the probability of salivating remains low after the first pairing – maybe it goes to 10 per cent – and raises each time the buzzer is followed by food.</p><a id="page_314"/>
<p class="TXI">Regardless of the value of the learning rate, however, the second time the buzzer is followed by food, the probability of salivating increases <span class="italic">less</span> than it did after the first time. So, if it went from 0 to 10 per cent after the first pairing, it would increase only another nine percentage points, to 19 per cent, after the second pairing. And only by about eight percentage points after the third. This reflects that, in the Bush-Mosteller model (and in the dogs), the change to the probability with each pairing depends on the value of the probability itself. In other words, learning depends on what is already learned. </p>
<p class="TXI">This is, from a certain angle, intuitive. Nothing new is learned from seeing the sun rise every day. To the extent that we believe something will happen, its actual happening has little effect on us. Anticipated rewards are no different. We don’t update our opinion of our boss, for example, if we get the same holiday bonus we’ve received for the past five years. And the dogs only update their response to the buzzer to the extent that the food that follows differs from what they expect. The power to change expectations comes only from violating them.</p>
<p class="image-fig" id="fig24.jpg">
<img alt="" src="../images/fig24.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 24</span>
</span></p><a id="page_315"/>
<p class="TXI">This violation can be for better or worse. To the dog, the first piece of post-‘buzz’ meat is a lovely surprise and one that has a big impact on its expectations. After repeated pairings, though, expectations shift and slobbering at the sound of the buzzer becomes second nature. At this point, the most impactful thing that could happen would be to hear the buzzer and <span class="italic">not</span> receive food. Such a deprivation would lead to a large decrease in the probability of future salivation – a decrease as large as the increase that occurred with the first pairing. This inverse side of reward-based learning, wherein the animal learns to <span class="italic">dis</span>associate a cue from a reward, is called extinction. With each presentation of the cue without the expected reward, the extinction process unbuilds the association, eventually extinguishing the learned response entirely. Bush and Mosteller made a point of showing that their model captures this process to a tee as well. </p>
<p class="TXI">At the same time that Bush and Mosteller were turning salivation information into equations, another man on the opposite side of the country was working to apply mathematics to some of the trickiest problems in business and industry. The deep and important connections between these works wouldn’t be realised for decades to come.</p>
<p class="center">* * *</p>
<p class="TXT">The RAND Corporation is an American think tank founded in 1948. A non-profit offshoot of the Douglas Aircraft Company, its central aim was to extend the collaboration between science and the military that blossomed out of necessity during the Second World War. The corporation’s name is appropriately generic (RAND literally stands for Research ANd Development) for the <a id="page_316"/>range of research projects it pursues. Over the years, RAND employees have made significant contributions to the fields of space exploration, economics, computing and even foreign relations.</p>
<p class="TXI">Richard Bellman worked at RAND as a research mathematician from 1952 to 1965. An admirer of the subject as early as his teenage years, Bellman’s path to becoming a mathematician was repeatedly interrupted by the Second World War. First, to lend support to the war effort, he left his postgraduate training at Johns Hopkins University to teach military electronics at the University of Wisconsin. He later moved to Princeton University, where he taught in the Army Specialized Training Program and worked on his own studies. He would eventually complete his PhD at Princeton, but not before he was drafted to work in Los Alamos as a theoretical physicist for the Manhattan Project. The intrusions didn’t seem to impact his career prospects much. He became a tenured professor at Stanford University just three years after the war at the age of only 28. </p>
<p class="TXI">Leaving the academic world for RAND at 32 was, in Bellman’s words, the difference between being ‘a traditional intellectual, or a modern intellectual using the results of my research for the problems of contemporary society’. At RAND his mathematical skill was applied to real-world problems. Problems like scheduling medical patients, organising production lines, devising long-term investment strategies or determining the purchasing plan for department stores. Bellman didn’t have to set foot in a hospital or on a factory floor to help with these issues, however. All of these problems – and a great many more – are huddled under one abstract mathematical umbrella. <a id="page_317"/>And, in the eyes of a mathematician, to be able to solve any of them is to solve them all.</p>
<p class="TXI">What these problems have in common is they are all ‘sequential decision processes’. In a sequential decision process, there is something to be maximised: patients seen, items produced, money made, orders shipped. And there are different steps that can be taken to do that. The goal is to determine which set of steps should be taken. How can the maximum be reached? What is the best way to climb the mountain?</p>
<p class="TXI">Without much previous work to draw on in this field, Bellman turned to a tried-and-true strategy in mathematics: he formalised an intuition.<sup><a href="#fn-2" id="fnt-2">2</a>
</sup> The mathematical conclusion this led him to is now known as the Bellman equation and the simple intuition it captures is that the best plan of action is the one in which all the steps are the best possible ones to take. Obvious though it may seem, when written in mathematics even banal statements can have power. </p>
<p class="TXI">To see how Bellman made use of this intuition, we have to understand how he framed the problem. Bellman first set out to define how good a plan was in terms of how much reward – be that money, widgets, shipments, <span class="italic">etc.</span> – it is likely to accrue. Let’s say you have a five-step plan. The total reward is the sum of the reward you get at each of those five steps. But after you’ve taken the first step, you now have a four-step plan. So, we could instead say that the total reward from the original five-step plan is the reward you get from taking the first step plus the <a id="page_318"/>total reward from the four-step plan. And the total reward from the four-step plan is just the reward from taking <span class="italic">its</span> first step plus the reward from the resulting three-step plan. And so on, and so on.</p>
<p class="TXI">By defining the reward of one plan in terms of the reward of another, Bellman made his definition <span class="italic">recursive</span>. A recursive process is one that contains itself. Consider, for example, alphabetisation. If you want to alphabetise a list of names, you’d start by sorting all the names according to their first letter. After that, you’d need to apply that <span class="italic">same</span> sorting process again on all the names starting with the same letter to sort them according to their second letter, and so on. This makes alphabetisation recursive. </p>
<p class="TXI">Recursion is a common trick in mathematics and computer science in part because recursive definitions are flexible; they can be made as long or as short as needed. The formula for calculating a plan’s total reward, for example, can just as easily be applied to a five-step plan as a 500-step one. Recursion is also a conceptually simple way to accomplish something potentially difficult. Like the turns of a spiral staircase, each step in a recursive definition is familiar but not identical, and we need only follow them down one by one to the end.</p>
<p class="TXI">Bellman’s framing contains two further insights that helped make his strategy effective for deploying on real-world problems. The first was to incorporate the very relatable fact that a reward you get immediately is worth more than a reward you get later. He did this by introducing a <span class="italic">discounting factor</span> into his recursive definition. So, whereas in the initial formula the reward <a id="page_319"/>from a five-step plan was equal to the reward from the first step plus the full reward from the four-step plan, an equation with discounting would say it is equal to the reward from the first step plus, maybe, 80 per cent of the reward from the four-step plan. Discounting is a way to weigh immediate gratification against delayed; it is 
‘a bird in the hand is worth two in the bush’ codified into mathematics. </p>
<p class="TXI">The second insight was more conceptual and more radical. It was a switch from focusing on rewards to focusing on <span class="italic">values</span>. </p>
<p class="TXI">To understand this switch, let’s consider the owner of a small business – a very small business. Angela is a busker on the New York City subway system. She knows she can play her electric violin for 20 minutes at certain subway stations before being chased away by the authorities, at which point she’s not allowed to return. Different stations, however, have different payouts. Tourist areas can be very lucrative whereas commuter stops for native New Yorkers yield far fewer donations. She’s leaving her house on Greenpoint Avenue in Brooklyn and wants to end up near a friend’s place in Bleecker Street. What path should she take to make the most money on the way to her destination? </p>
<p class="TXI">So far, we’ve noticed that, after starting from one position and taking a step in a plan, we find ourselves in circumstances broadly similar to how we began – except we are starting from a different position and have a different plan. In sequential decision-making, the different positions we can move through are called states and the steps in a plan are frequently referred to as actions. In Angela’s case, the states are the different <a id="page_320"/>subway stations she can be at. Each time Angela takes an action (for example, from station A to station B), she finds herself in a new state (station B) that both yields some reward (the amount of donations her playing gets) and provides her with a new set of possible actions (other stations to go to). In this way, states define what actions are available (you can’t go straight from Greenpoint Avenue to Times Square, for example) and actions determine what the next states are. </p>
<p class="TXI">This interplay – wherein the actions taken as part of a plan affect what actions will be available in the future – is part of what makes sequential decision processes so difficult. What Bellman did was to take this constellation of states, actions and rewards, and turn it on its head. Rather than talk about the reward expected from a series of actions, he focused on the value that any given state has.</p>
<p class="TXI">Value, as used colloquially, is a nebulous concept. It elicits ideas about money and worth, but also deeper notions of meaning and utility that can be hard to pin down. The Bellman equation, however, defines value precisely. Using the same recursive structure introduced earlier, Bellman defined the value of a state as the reward you get in that state plus the discounted value of the next state. You’ll notice, in this definition there is no explicit concept of a plan; value is defined by other values. </p>
<p class="TXI">Yet, this equation does rely on knowledge of the next state. Without a plan to say what action is taken, how do we know what the next state will be? This is where the original intuition – the idea that the best plan is made up of the best actions – comes into play. To calculate the value at the next state, you simply assume that the best <a id="page_321"/>possible action is taken. And the best possible action is the one that leads to the state with the highest value! When wrapped up in the language of value, the plan itself fades away. </p>
<p class="TXI">So how does this help Angela? Given a map of possible subway stations (see Figure 25) and the associated donations she expects to get from each, we can calculate a ‘value function’. A value function is simply the value associated with each state (in this case, each station). We can calculate this by starting at the end and working backwards. Once Angela reaches Bleecker Street, she will go straight to her friend’s house and not do any busking, so the reward she will get at her final destination is $0. Because there are no further states from this point, the value of Bleecker Street is also zero. Backing up from here, the values of Union Square and 34th Street can be calculated in terms of the reward expected there and the value of Bleecker Street. This process continues until the value for each station is calculated.</p>
<p class="image-fig" id="fig25.jpg">
<img alt="" src="../images/fig25.jpg"/></p>
<p class="FC"> 
<span class="bold">
<span class="italic">Figure 25</span>
</span></p>
<p class="TXI">With these values in hand, Angela can now plan her journey. Starting out from Greenpoint Avenue, she can take the train to either Court Square or Metropolitan Avenue. Which should she choose? Looking just at the possible rewards from each, Metropolitan Avenue seems the better choice, as it offers $10 versus Court Square’s $5. But looking at the value function, Court Square is the correct choice. This is because the value function cares about what states you can get yourself into in the future – and from Court Square Angela can go straight to the jackpot, Times Square. Angela could also go to Queen’s Plaza from Court Square, but that isn’t relevant here, because the value function assumes Angela is smart. It assumes that from Court Square she would go to <a id="page_322"/>Times Square because Times Square is the better choice. All in all, following the value function would take Angela through Court Square to Times Square then to 34th Street and finally on to her destination at Bleecker Street. In total, she will have earned $65 – the most that any path on this map could offer.</p>
<p class="TXI">Bellman’s move to focus on the value function was important because it corrected a flaw in the original framing of the problem. We started out by trying to calculate the total reward we could get from a given plan. But when solving a sequential decision process, we <a id="page_323"/>aren’t given a plan. In fact, a plan is exactly what we’re trying to find! Once we know the value function, though, the plan is simple: follow it. Like breadcrumbs left on a forest path, the value function tells you where to go. Anyone looking for the most reward needs only to greedily seek the next state with the highest value. All actions can be chosen based on this simple rule.</p>
<p class="TXI">Some interesting things happen as a result of the discounting that is part of the definition of value. For example, look at the options Angela has from Times Square. She can either go to 34th Street, get $20 and then end at Bleecker Street or she can go to 14th Street, get $8, then go to Union Square and get $12 and finally end at Bleecker Street. Both routes earn her $20 in total. But the value of 34th Street is 20 whereas the value of 14th Street is 17.6 (calculated as 8 + 0.8 x 12), indicating that 34th Street is the better option. This demonstrates how discounting future rewards can lead to plans with fewer steps; if there is only so much reward to get, it’s best to get it quicker rather than slower. Discounting also means that even big rewards will be ignored if they are too distant. If a train station in New Jersey could garner Angela $75, it may still not influence her choice as she leaves her house. The impact of a reward on a value function is like the ripple from a stone dropped in water. It’s felt strongest in the nearby states, but its power is diluted the farther away you go.<sup><a href="#fn-3" id="fnt-3">3</a>
</sup></p> <a id="page_324"/>
<p class="TXI">This technical definition of value – based on states and recursion and discounting factors – may seem a far cry from the word we use in everyday language. But those colloquial connotations are very much present in this equation. Why do we value money? Not because there is actually much pleasure to be had from the paper or the coin itself, but because of the future we can envision once we have that paper or coin. Money is worth only what it can get us later and what we can get later is baked into Bellman’s definition of value. </p>
<p class="TXI">Bellman’s work to frame sequential decision processes this way truly did allow him to become the ‘modern intellectual’ he aimed to be when moving to RAND. In the years after his first publications describing this solution, countless companies and government entities began to apply it out in the world. By the 1970s, Bellman’s ideas had been deployed on problems as diverse as sewer system design, airline scheduling and even the running of research departments at large companies such as Monsanto. The technique went under the name ‘dynamic programming’, a rather bland phrase Bellman actually coined with the aim of keeping some of the mathematics-phobic higher-ups in the military out of his hair. ‘The 1950s were not good years for mathematical research,’ Bellman wrote in his autobiography. ‘The RAND Corporation was employed by the Air Force, and the Air Force had [Charles] Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. … Thus, I thought dynamic programming was a good name. It was something not even a <a id="page_325"/>Congressman could object to. So I used it as an umbrella for my activities.’ </p>
<p class="TXI">In applying the method in each of these settings, engineers had to find a way to calculate the value function. In some cases, like the subway example given above, the landscape of the problem is simple enough that the calculation is straightforward. But simple problems are rarely realistic. The real world has a large number of potential states; these states can connect to each other in complex or even uncertain ways; and they can do so through many possible actions. Much effort was put into finding the value function in these trickier situations. Yet even with clever techniques, the application of dynamic programming usually pushed at the edge of computing power at the time. The calculation of the value function was always a bottleneck in the process. And without a way to find the value function, the full potential of Bellman’s contributions would remain unreached. </p>
<p class="center">* * *</p>
<p class="TXT">There is an irony to Pavlov’s legacy. Its immediate effect was to set off behaviourism, a movement with religious-like dedication to ignoring the mind and focusing only on directly measurable behaviour. Yet the lineage of mathematical models it spawned found success in the other direction, by going increasingly inside the mind; to capture reinforcement learning in equations required the use of terms representing hidden mental concepts.</p>
<p class="TXI">One of the well-known extensions to the Bush-Mosteller model came 20 years later, in 1972, and was developed by another duo, Yale psychologists Robert <a id="page_326"/>Rescorla and Allan Wagner. Rescorla and Wagner generalised the Bush-Mosteller model, making it applicable to a wider range of experimental settings and able to capture more findings. The first alteration they made was to the very measure the model was trying to explain. </p>
<p class="TXI">Bush and Mosteller’s ‘probability of response’ was too specific and too limited. Rescorla and Wagner instead aimed to capture a more abstract value which they referred to as ‘associative strength’. This strength of the association between a cue and a reward is something that would exist in the mind of the participant, making it not directly measurable, but different experiments could try to read it out in different ways. This could include measuring a probability of response, like the probability of salivating, but also other measures, such as the amount of salivation, or behaviours like barking or movement. In this way, Rescorla and Wagner subsumed the Bush-Mosteller model into a broader framework. </p>
<p class="TXI">The Rescorla-Wagner model also expanded to incorporate a known feature of conditioning experiments referred to as ‘blocking’. Blocking occurs when an initial cue is paired with a reward, and then a second cue is also given along with the first and both are paired with the reward. So, for example, after a dog had learned to associate the sound of the buzzer with food, the experimenter would then flash a light at the same time as the buzzer and then give the food. In Bush and Mosteller’s model, the cues were treated completely separately. So, if the light and buzzer were paired with food enough times, the dog should come to associate the light with food the same way it had learned the association with the buzzer. Then we would expect that presenting the light alone <a id="page_327"/>would cause the dog to salivate. This is not, in fact, what happens; the dogs don’t salivate in response to the light alone. The presence of the buzzer <span class="italic">blocks</span> the ability of the light to be associated with the food. </p>
<p class="TXI">This provides further proof that learning is driven by errors. In particular, errors about predicted reward. When the animal hears the buzzer, it knows food is coming. So, when the food arrives there is no error in its prediction of that reward. As we saw before, that means it doesn’t update its beliefs about the buzzer. But it also means it doesn’t update its beliefs about anything else either. Whether there was a light on at the same time as the buzzer or not is irrelevant. The light has no bearing on the predicted reward, the received reward or the difference between the two, which defines the prediction error – and without an error, everything is stuck as it is. Prediction error is the grease that oils the wheels of learning. </p>
<p class="TXI">Rescorla and Wagner thus made their update in associative strength between one cue and a reward dependent not just on that cue’s current associative strength, but on the sum of the associative strengths of all cues present. If one of these associative strengths is high (for example if the buzzer is present), then the presence of the reward wouldn’t change any of them (the association with the light is never learned). This summing across multiple cues is also something that would need to be done internal to the animal, further reflecting the rejection of behaviourism and a move to the mind. </p>
<p class="TXI">But the watershed moment in reinforcement learning came in the mid-1980s from the work of a ponytail-wielding Canadian computer scientist named Richard <a id="page_328"/>Sutton and his PhD adviser Andrew Barto. Sutton was educated in both psychology and computer science, and Barto spent a lot of his time reading the psychology literature. This proved a powerful combination as the work they did together pulled from and gave back to both fields. </p>
<p class="TXI">Sutton’s work removed the final tangible element of the model: the reward itself. Up until this point, the moment of learning centred around the time that 
a reward was given or denied. If you catch a whiff of the smoke from a blown-out candle and are then handed a piece of birthday cake, the association between the two strengthens. But a candle extinguished at the end of 
a religious ceremony likely doesn’t come with cake and so the association weakens. In either case, though, the cake itself is the important variable. Its presence or absence is key. Anything can serve as a cue, but the reward must be a primal one – food, water, sex. But once we come to associate smoke with birthday cake, we may notice some other regularities. For example, the smoke is usually preceded by singing and the singing may be preceded by people putting on silly hats. None of these things are rewards in and of themselves (especially the singing, at most parties), but they form a chain that attaches each in some degree to the primary reward. Knowing this information can be useful: if we want cake, being on the lookout for silly hats may help.</p>
<p class="TXI">Rescorla and Wagner had no way of allowing for this backing up of associations – no way, essentially, for a 
cue associated with a reward in one circumstance to 
play the role of a reward in another. But Sutton did. In 
Sutton’s algorithm – known as ‘temporal difference <a id="page_329"/>learning’ – beliefs are updated in response to any violation of expectations. Walking through your office hallway to your desk, for example, expectations about reward may be pretty low. But when you hear your co-workers in the conference room start the first verse of ‘Happy Birthday’, a violation has occurred. Beliefs must be updated; you are now in a state where reward is on the horizon. This is where temporal difference learning occurs. You may choose to enter the conference room, finish the song, smell the candles and eat the cake. In doing those actions, no further violations will have occurred – and therefore, no further learning. It is thus not the receipt of the reward itself that causes any changes. The only learning that happened was in that hallway, many steps away from the reward. </p>
<p class="TXI">What exactly is being learned here, though? What mental concept is it that got updated in the hallway? It’s not the association of a cue with a reward – not directly at least. Instead it’s more of a signal telling you the path to reward, should you follow the right steps in between. </p>
<p class="TXI">This may sound familiar because what temporal difference learning helps you learn is a value function. At each moment in time, according to this framing, we have expectations – essentially a sense of how far we are from reward – that define the value of the state we are in. As time passes or we take actions in the world we may find ourselves in new states, which have their own associated values. If we’ve correctly anticipated the value of those new states, then all’s well. But if the value of the current state is different from what we predicted it would be when we were in the state before, we’ve got an error. And errors induce learning. In particular, if the value of <a id="page_330"/>the current state is more or less than we expected it to be when we were in the previous state, we <span class="italic">change the value of the previous state</span>. That is, we take the surprise that occurred now and use it to change our belief about the past. That way, the next time we find ourselves in that previous state, we will better be able to predict the future. </p>
<p class="TXI">Consider driving to an amusement park. Here the value of your location is measured in terms of how far you are from this rewarding destination. As you leave your house, you expect to arrive in 40 minutes. You drive straight for five minutes and get on a highway. You now expect to arrive in 35 minutes. After 15 minutes on the highway, you take an exit. Your estimated time of arrival is now 20 minutes. But, when you get off that exit and turn on to a side street you hit a traffic jam. As you sit in your hardly moving car, you know you won’t be at the park for another 30 minutes. Your expected arrival time has now jumped up by 10 minutes – a significant error. </p>
<p class="TXI">What should be learned from this error? If you had an accurate view of the world, you would’ve anticipated 30 more minutes of driving at the moment you took the exit. So, temporal difference learning says you should <span class="italic">update the value of the state associated with that exit</span>. That is, you use the information received at one state (a traffic jam on the side street) to update your beliefs about the value of the state before (the exit). And this may mean that the next time you drive to this amusement park, you will avoid that exit and choose another instead. But it doesn’t take arriving at the amusement park 10 minutes late to learn from that mistake; the expectation of that happening at the sight of the traffic was enough.</p><a id="page_331"/>
<p class="TXI">What Sutton’s algorithm shows is that by mere exploration – simple trial and error – humans, animals or even an artificial intelligence can eventually learn the correct value function for the states they’re exploring. All it takes is updating expectations when expectations change – ‘learning a guess from a guess’ as Sutton describes it. </p>
<p class="TXI">As an extension of Bellman’s work on dynamic programming, temporal difference learning had the potential to solve real-world problems. Its simple moment-by-moment learning rule made it attractive from a computing perspective: it didn’t demand as much memory as programmes that needed to store the entire set of actions that preceded a reward before learning from it. It also worked. One of the biggest displays of its power was TD-Gammon, a computer program trained via temporal difference learning to play the board game backgammon. Board games are particularly useful tests of reinforcement learning because rewards frequently only come at the very end of a game, in the form of a win or loss. Using such a coarse and distant signal to guide strategy at the very first move is a challenge, but one that temporal difference learning could meet. Built in 1992 by Gerald Tesauro, a scientist at IBM, TD-Gammon played hundreds of thousands of games against itself, eventually reaching the level of an intermediate player without ever taking instructions from a human. Because it learned in isolation, it also developed strategies not tried by humans (who are generally influenced by each other’s gameplay to stick within a certain set of moves). In the end, TD-Gammon’s unusual moves actually influenced theory and understanding about the game of backgammon itself.</p><a id="page_332"/>
<p class="TXI">In 2013 another application of temporal difference learning made headlines, this time applied to video games. Scientists at the artificial intelligence research company DeepMind built a computer program that taught itself to play multiple games from the 1970s arcade system Atari. This artificial gamer got the full Atari experience. The only inputs to the algorithm were the pixels on the screen – it was given no special knowledge that some of those pixels may represent spaceships or ping-pong bats or submarines. The actions it was allowed to take included the standard buttons such as up, down, left, right, A, B; and the reward for the model came in terms of the score provided by the game it was playing. As this burdens the algorithm with a more challenging task than backgammon – which at least had the concepts of pieces and locations baked into the inputs to the model – the researchers combined temporal difference learning with deep neural networks (a method we encountered in <a href="chapter3.xhtml#chapter3">Chapter 3</a>).<sup><a href="#fn-4" id="fnt-4">4</a>
</sup> One version of this deep neural network had around 20,000 artificial neurons and, after weeks of learning, reached human-level performance on 29 out of 49 games tested. Because this Atari algorithm also learned asocially, it ended up with some interesting quirks, including discovering a clever trick for tunnelling through a wall in the brick-clearing game <span class="italic">Breakout</span>. </p>
<p class="TXI">While games are a flashy and fun way to demonstrate the power of this approach, its application didn’t stop there. After Google acquired DeepMind in 2014, it set reinforcement-learning algorithms to the task of <a id="page_333"/>minimising energy use in its massive data centres. The result was a 40 per cent decrease in energy used to cool the centres and likely hundreds of millions in savings over the years as a result. With a single-minded focus on achieving the goal at hand, reinforcement learning algorithms find creative and efficient solutions to hard problems. These alien minds can thus help devise plans humans would’ve never thought of. </p>
<p class="TXI">The paths of sequential decision-making and Pavlovian conditioning represent a victory of convergent scientific evolution. The trajectories of Bellman and Pavlov start with separate and substantial problems, each seething with their own demanding details. How should a hospital schedule its nurses and doctors to serve the most patients? What causes a dog to salivate when the sound of a buzzer hits its ears? These questions are seemingly worlds apart. But by peeling away the weight of the specifics – leaving only the bare bones of the problem to remain – their interlocking nature becomes clear. This is one of the roles of mathematics: to put questions disconnected in the physical world into the same conceptual space wherein their underlying similarities can shine through.</p>
<p class="TXI">The story of reinforcement learning is thus one of successful interdisciplinary interaction. It shows that psychology and engineering and computer science can work together to make progress on hard problems. It demonstrates how mathematics can be used to understand, and replicate, the ability of animals and humans to learn from their surroundings. The story would be a remarkable one as it is, if it ended there. But it doesn’t end there.</p>
<p class="center">* * *</p><a id="page_334"/>
<p class="TXT-con">Octopamine is a molecule found in the nervous systems of many insects, molluscs and worms. It is so named because of its initial discovery in the salivary glands of the octopus in 1948. In the brain of a bee, octopamine is released upon run-ins with nectar. In the early 1990s, Terry Sejnowski, a professor at the Salk Institute in San Diego, California, and two of his lab members, Read Montague and Peter Dayan, were thinking about octopamine. In particular, they built a model – a computer simulation of bee behaviour – that was centred on the neuron in the bee brain that releases octopamine. A bee’s choices about what flowers to land on or avoid, they proposed, could be explained by a Rescorla-Wagner model of learning and a neural circuit including the octopamine neuron could be the hardware that implements it. But as they worked out this octopamine puzzle, the team heard of another study, conducted some 6,000 miles away by a German professor named Wolfram Schultz, on octopamine’s chemical cousin dopamine.</p>
<p class="TXI">You may be familiar with dopamine. It has a bit of a reputation in popular culture. Countless news articles refer to it as something like ‘our brain’s pleasure and reward-related chemical’ or talk about how everyday activities like eating a cupcake cause ‘a surge of the reward chemical dopamine to hit the decision-making area of the brain’. It’s branded as the pleasure molecule and it’s not uncommon for products to be pedalled under its powerful name. Pop stars have named albums and songs after it. ‘Dopamine diets’ claim (without evidence) to provide foods that boost dopamine while keeping you slim. And the tech start-up Dopamine Labs promised to increase user engagement in phone apps by <a id="page_335"/>doling out squirts of the neurotransmitter. This poor celebrity chemical has also been badly maligned – referred to as the source of all addictions and maladaptive behaviours. Online communities like The Dopamine Project have cropped up aiming to provide ‘better living through dopamine awareness’. And some Silicon Valley dwellers have even attempted ‘dopamine fasts’ as a respite from constant over-stimulation.</p>
<p class="TXI">While it is true that a release of dopamine can accompany rewards, that is far from the whole story. What Schultz’s study in particular showed was a case in which the neurons that dole out dopamine were <span class="italic">silent</span> when a reward was given.</p>
<p class="TXI">Specifically, Schultz trained monkeys to reach their arm out in front of them in order to receive some juice.<sup><a href="#fn-5" id="fnt-5">5</a>
</sup> During this training process, he recorded the activity of a population of dopamine-releasing neurons tucked into the underside of the brain. Schultz observed that at the end of training – when the animals knew they would get some juice by making a reach – these neurons showed no response at all to the juice reward being delivered. </p>
<p class="TXI">When Schultz first published these results he didn’t have a clear explanation for why the dopamine neurons were behaving this way, but the members of the Sejnowski lab did. And they reached out to Schultz to embark on a collaboration that would test the hypothesis that dopamine neurons encode the prediction errors necessary for temporal difference learning. It would be <a id="page_336"/>the start of what Sejnowski referred to as ‘one of the most exciting scientific periods of my life’. </p>
<p class="TXI">Dayan and Montague worked to reanalyse Schultz’s data through the lens of learning algorithms. They focused on the simplest of Schultz’s experiments, which consisted of a light at the desired reach location turning on and, if the animal reached to it, a drop of juice was delivered half a second later. What they wanted to know was how the response of the dopamine neurons changed as the animal came to learn this association. But they were also interested in a particular circumstance after learning: what happens when the juice doesn’t follow the light. If the animals learned the light–juice association, they would know to expect it and if the juice didn’t show up that would be a significant prediction error. Did the dopamine neurons reflect that? </p>
<p class="TXI">The neurons that release dopamine tend to fire around five spikes per second when nothing much is going on. At the start of the learning process, right after the animal got what seemed like a surprise shot of juice after making an arm movement, that rate jumped up briefly to about 20 spikes per second. The light that came before the movement, however, elicited nothing. But after enough pairings, once the animal came to understand how the light and the reach and the juice were all related, that pattern shifted. The dopamine neurons stopped responding to the juice. This is a change perfectly in line with the notion that they signal prediction error, because once the animal can correctly predict the juice there is no more error. And they started responding to the light. Why? Because the light had become associated with the reward but – crucially – they had no idea when it would <a id="page_337"/>come on. When it did arrive it was an error. Specifically, it’s an error in the predicted value of the state of the animal. Sitting in the experimental chair going about its life, the monkey expects the next moment to be more or less similar to the current one. When the light turns on, that expectation is violated. Like hearing the first few bars of ‘Happy Birthday’ in your office hallway, it’s a pleasant surprise, but a surprise nonetheless.</p>
<p class="TXI">The final analysis – done while sporadically omitting the juice after the reach – was to see how <span class="italic">unpleasant</span> surprises were encoded. If dopamine is encoding errors, it should indicate when things are worse than expected as well. And with the juice absent, the neurons did just that. They had a dip in their firing right at the time the juice would’ve been delivered. Specifically, the neurons would go from five to 20 spikes per second in response to the light; then as the animal reached out its arm they’d return back to five. But, about half a second after the reach, when it was clear there was no juice coming, they’d shut off completely. An expectation had been violated and the dopamine neurons were letting it be known.</p>
<p class="TXI">This study showed that the firing of dopamine neurons can signal the errors – both positive and negative – about predicted values that are needed for learning. It was thus an important point in shifting the understanding of dopamine from a pleasure molecule to a pedagogical one.</p>
<p class="TXI">If the point of encoding the error is to learn from it, though, where does that learning happen? It turns out that’s a bit hard to pin down because these dopamine-releasing neurons release dopamine in many corners of the brain; their projections burrow through the brain like pipework, touching regions near and far. Still, <a id="page_338"/>a location that seems particularly important is the striatum. The striatum is a group of neurons that serves as the primary input to a collection of brain areas involved in guiding movement and actions. Neurons in the striatum contribute to the production of behaviour by associating sensory inputs with actions or actions with other actions.</p>
<p class="TXI">As we saw in <a href="chapter4.xhtml#chapter4">Chapter 4</a>, Hebbian learning is an easy way for associations between ideas to become encoded in the connections between neurons. Under Hebbian rules, if one neuron regularly fires before another, the weight of the connection from the first to the second is strengthened. In reinforcement learning, however, we need more than just to know that two events happened close in time. We need to know how those events relate to reward. Specifically, we only want to update the strength between a cue and an action (for example, seeing a light and reaching for it) if that pairing turns out to be associated with reward. </p>
<p class="TXI">So, the neurons in the striatum don’t follow basic Hebbian learning. Instead they follow a modified form wherein the firing of one neuron before another only strengthens their connection if it happens <span class="italic">in the presence of dopamine</span>. Dopamine – which encodes the error signal needed for updating values – is thus also required for the physical changes needed for updating that occur at the synapse. In this way, dopamine truly does act as a lubricant for learning. </p>
<p class="TXI">Having the language of temporal difference learning in which to talk about the functioning of the brain has altered the conversation on clinical topics such as addiction. One theory, put forth in 2004 by neuroscientist <a id="page_339"/>David Redish, tries to explain the addictive properties of drugs like amphetamine and cocaine in terms of the effects they have on dopamine release. It posits that these drugs cause a release of dopamine that is independent of the true prediction error. Specifically, by overdriving the dopamine neurons, these drugs send the false signal to the rest of the brain that the drug experience is always better than expected. This errant error signal still drives learning, pushing the estimated value of states associated with drug use higher and higher. Deforming the value function in this way is guaranteed to have detrimental effects on behaviour like the ones seen in addiction.<sup><a href="#fn-6" id="fnt-6">6</a>
</sup></p>
<p class="center">* * *</p>
<p class="TXT">David Marr was a British neuroscientist with a background in mathematics. His book, <span class="italic">Vision</span>: ﻿A <span class="italic">Computational Investigation into the Human Representation and Processing of Visual Information</span> was published in 1982, two years after his death. In the first chapter, he lays out the components needed for a successful analysis of a neural system. According to Marr, to understand any bit of the brain we should be able to explain it on each of three levels: computational, algorithmic and implementational. The computational level asks what is the overall purpose of this system, that is, what is it trying to do? The algorithmic level asks how, <span class="italic">i.e.</span>, through what steps, does it achieve this <a id="page_340"/>goal. And finally, the implementational level asks specifically what bits of the system – what neurons, neurotransmitters, <span class="italic">etc.</span> – carry out these steps.</p>
<p class="TXI">An explanation encompassing all of Marr’s levels is an aspiration towards which many neuroscientists strive. The systems that carry out reinforcement learning are a rare case where they can come within striking distance of this high bar. At the computational level reinforcement learning has a simple answer: maximise reward. This is what Bellman recognised as the goal of sequential decision processes and what following the value function should get you. But how do we learn the value function? That’s where temporal difference learning comes in. The work of Bush, Mosteller, Resorla, Wagner and Sutton all turned stacks of data from conditioning experiments into strings of symbols that could describe the algorithm needed to do the learning part of reinforcement learning. On the implementation level, dopamine neurons take on the task of calculating prediction error and the signals they send to other brain areas control the associations learned there. In this way, a satisfying understanding of a fundamental ability – to learn from rewards – was achieved by tunnelling towards the topic from many different angles.</p>
<p class="H1">Notes</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-1" id="fn-1">1</a> ﻿The kind of conditioning Skinner is most associated with is known as ﻿‘﻿operant conditioning﻿’﻿, which involves performing an action before getting a reward. The line between operant and Pavlovian conditioning is sometimes sharp, sometimes blurred and information in this chapter will at times relate to both.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-2" id="fn-2">2</a> ﻿Interestingly, Bellman was aware of Bush and Mosteller﻿’﻿s publications, but his work on these problems was developed independently of that.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-3" id="fn-3">3</a> ﻿Because it controls the balance between caring about now versus the future, the strength of discounting can have sizable impacts on value and therefore on which actions are chosen. Scientists have posited that disorders such as addiction or ADHD can be understood in terms of inappropriate reward discounting. More on addiction later.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-4" id="fn-4">4</a> ﻿Specifically, they used a deep convolutional neural network which, as we saw in ﻿﻿Chapter 6﻿﻿, is used to model the visual system. ﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-5" id="fn-5">5</a> ﻿This is actually an example of the ﻿‘﻿operant﻿’﻿ form of conditioning mentioned earlier, because the animals need to make a reach to receive their reward.﻿</p>
<p class="FN1"><a href="chapter11.xhtml#fnt-6" id="fn-6">6</a> ﻿This theory can explain several aspects of addiction, but one of its big predictions failed. If these drugs lead to non-stop prediction error, then the blocking phenomena described previously shouldn﻿’﻿t be seen when drugs are used as reward. An experiment in rats indicated, however, that blocking still does occur.﻿</p>
</body>
</html>