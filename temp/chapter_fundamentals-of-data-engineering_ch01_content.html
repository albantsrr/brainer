<h2>Objectifs d'apprentissage</h2>

<p>À la fin de ce chapitre, vous serez capable de :</p>
<ul>
  <li><strong>Comprendre</strong> ce qu'est le data engineering et son rôle dans l'écosystème des données</li>
  <li><strong>Identifier</strong> les étapes du cycle de vie de l'ingénierie des données (génération, stockage, ingestion, transformation, diffusion)</li>
  <li><strong>Analyser</strong> l'évolution historique du data engineering depuis les data warehouses jusqu'aux architectures modernes</li>
  <li><strong>Différencier</strong> le data engineering du data science et comprendre leur collaboration</li>
  <li><strong>Reconnaître</strong> les compétences essentielles d'un data engineer et les profils professionnels</li>
</ul>

<h2>Pourquoi c'est important</h2>

<p>Le data engineering est devenu l'une des disciplines les plus recherchées dans l'industrie technologique. Sans infrastructure de données robuste, le data science et l'analyse ne peuvent pas exister en production. Les data engineers construisent les fondations qui permettent aux organisations de transformer leurs données brutes en insights actionnables.</p>

<p>Comprendre le data engineering est essentiel même si vous n'êtes pas data engineer : les data scientists doivent collaborer efficacement avec les data engineers, les product managers doivent comprendre les contraintes techniques, et les architectes doivent concevoir des systèmes alignés avec le cycle de vie des données.</p>

<p>Ce chapitre vous donnera une vision claire du métier, de son évolution, et des compétences requises pour réussir dans ce domaine en pleine expansion.</p>

<h2>Qu'est-ce que le data engineering ?</h2>

<h3>Définition et périmètre</h3>

<p>Le <strong>data engineering</strong> est le développement, l'implémentation et la maintenance de systèmes et processus qui transforment des données brutes en informations de haute qualité et cohérentes, prêtes à être utilisées pour l'analyse et le machine learning.</p>

<p>Malgré la popularité croissante du data engineering, une grande confusion persiste sur sa définition exacte. Une recherche Google en 2022 pour "what is data engineering?" retourne plus de 91 000 résultats différents ! Cette diversité de définitions reflète la nature évolutive et multifacette de la discipline.</p>

<h3>Le cycle de vie de l'ingénierie des données</h3>

<p>Plutôt que de se focaliser sur les technologies spécifiques, le data engineering moderne s'organise autour d'un concept central : le <strong>cycle de vie de l'ingénierie des données</strong>. Ce cycle comprend cinq étapes principales :</p>

<pre><code class="language-mermaid">
graph LR
    A[Génération] --> B[Stockage]
    B --> C[Ingestion]
    C --> D[Transformation]
    D --> E[Diffusion]
    E -.-> A

    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style C fill:#e8f5e9
    style D fill:#fff3e0
    style E fill:#fce4ec
</code></pre>

<ol>
  <li><strong>Génération</strong> : création des données par les systèmes sources (applications, IoT, APIs, etc.)</li>
  <li><strong>Stockage</strong> : conservation des données dans des systèmes persistants (bases de données, data lakes, etc.)</li>
  <li><strong>Ingestion</strong> : collecte et transfert des données depuis les sources vers les systèmes de stockage</li>
  <li><strong>Transformation</strong> : nettoyage, enrichissement, agrégation et structuration des données</li>
  <li><strong>Diffusion</strong> : mise à disposition des données pour l'analyse, le ML, les dashboards, etc.</li>
</ol>

<h3>Les courants sous-jacents (undercurrents)</h3>

<p>En plus des cinq étapes du cycle de vie, le data engineering repose sur des <strong>courants sous-jacents</strong> qui traversent toutes les étapes :</p>

<ul>
  <li><strong>Sécurité</strong> : protection des données sensibles, chiffrement, contrôle d'accès</li>
  <li><strong>Gestion des données</strong> : gouvernance, qualité, catalogage, métadonnées</li>
  <li><strong>DataOps</strong> : automatisation, monitoring, CI/CD pour les pipelines de données</li>
  <li><strong>Architecture des données</strong> : conception de systèmes scalables et maintenables</li>
  <li><strong>Orchestration</strong> : coordination et planification des workflows de données</li>
  <li><strong>Software engineering</strong> : bonnes pratiques de développement logiciel appliquées aux données</li>
</ul>

<pre><code class="language-mermaid">
graph TD
    Lifecycle[Cycle de Vie des Données]

    Lifecycle --> Stages[Étapes Principales]
    Lifecycle --> Undercurrents[Courants Sous-jacents]

    Stages --> Gen[Génération]
    Stages --> Store[Stockage]
    Stages --> Ingest[Ingestion]
    Stages --> Transform[Transformation]
    Stages --> Serve[Diffusion]

    Undercurrents --> Security[Sécurité]
    Undercurrents --> DataMgmt[Gestion des données]
    Undercurrents --> DataOps[DataOps]
    Undercurrents --> Arch[Architecture]
    Undercurrents --> Orch[Orchestration]
    Undercurrents --> SWE[Software Engineering]

    style Lifecycle fill:#e1f5ff
    style Stages fill:#fff4e1
    style Undercurrents fill:#f3e5f5
</code></pre>

<h3>Exemple pratique : pipeline de données e-commerce</h3>

<p>Prenons l'exemple d'une entreprise e-commerce qui veut analyser le comportement de ses clients :</p>

<pre><code class="language-mermaid">
sequenceDiagram
    participant App as Application Web
    participant Events as Event Stream
    participant Lake as Data Lake
    participant DW as Data Warehouse
    participant BI as Dashboard BI

    App->>Events: Génération (clicks, achats)
    Events->>Lake: Ingestion (temps réel)
    Lake->>Lake: Stockage (S3, format Parquet)
    Lake->>DW: Transformation (ETL nightly)
    DW->>BI: Diffusion (SQL queries)
    BI-->>App: Insights → personnalisation
</code></pre>

<p>Dans cet exemple :</p>
<ul>
  <li><strong>Génération</strong> : l'application web génère des événements (clics, ajouts au panier, achats)</li>
  <li><strong>Ingestion</strong> : les événements sont capturés en temps réel via Kafka</li>
  <li><strong>Stockage</strong> : les données brutes sont stockées dans un data lake (S3) au format Parquet</li>
  <li><strong>Transformation</strong> : un job ETL nocturne nettoie, agrège et structure les données</li>
  <li><strong>Diffusion</strong> : les données transformées alimentent des dashboards BI et des modèles ML</li>
</ul>

<h3>Erreurs fréquentes</h3>

<ul>
  <li><strong>Erreur :</strong> Se focaliser uniquement sur les technologies sans comprendre le cycle de vie → <strong>Solution :</strong> Penser d'abord aux besoins métier et au flux de données, ensuite choisir les technologies appropriées</li>
  <li><strong>Erreur :</strong> Négliger les courants sous-jacents (sécurité, qualité, DataOps) → <strong>Solution :</strong> Intégrer ces aspects dès la conception, pas après coup</li>
</ul>

<h2>Évolution du data engineering</h2>

<h3>Les débuts : 1980-2000, des data warehouses au web</h3>

<p>Les racines du data engineering remontent aux <strong>data warehouses</strong> des années 1970-1980. Bill Inmon a officiellement inventé le terme "data warehouse" en 1990, après le développement des bases de données relationnelles et du SQL par IBM, puis popularisé par Oracle.</p>

<p>Les techniques de modélisation de Ralph Kimball et Bill Inmon, créées dans les années 1990, sont toujours largement utilisées aujourd'hui. Cette époque a vu l'apparition des premières bases de données massivement parallèles (MPP) capables de traiter des volumes de données sans précédent.</p>

<p>Les rôles précurseurs des data engineers modernes incluaient :</p>
<ul>
  <li><strong>BI Engineer</strong> : création de rapports et dashboards</li>
  <li><strong>ETL Developer</strong> : développement de pipelines de données</li>
  <li><strong>Data Warehouse Engineer</strong> : gestion de l'infrastructure du data warehouse</li>
</ul>

<h3>Début des années 2000 : naissance du data engineering contemporain</h3>

<p>Au début des années 2000, les survivants de la bulle dot-com (Google, Yahoo, Amazon) ont rapidement dépassé les limites des systèmes monolithiques traditionnels. Ils avaient besoin de solutions <strong>scalables</strong>, <strong>disponibles</strong>, <strong>fiables</strong> et <strong>économiques</strong>.</p>

<p><strong>Le "big bang" du big data :</strong></p>

<pre><code class="language-mermaid">
timeline
    title Innovations Fondatrices du Big Data
    2003 : Google File System : Stockage distribué à grande échelle
    2004 : MapReduce (Google) : Paradigme de traitement parallèle
    2006 : Apache Hadoop : Open source par Yahoo : Démocratisation du big data
    2006 : AWS S3 et EC2 : Cloud computing public : Ressources à la demande
</code></pre>

<p>Ces publications de Google ont constitué le "big bang" des technologies de données modernes. L'article sur MapReduce a introduit un nouveau paradigme de traitement de données ultra-scalable qui a inspiré tout un écosystème.</p>

<h3>Mécanisme interne : pourquoi MapReduce a révolutionné le traitement</h3>

<p>MapReduce a résolu un problème fondamental : comment traiter des pétaoctets de données en utilisant des milliers de serveurs bon marché, tout en gérant automatiquement les pannes ?</p>

<p>Le principe :</p>
<ol>
  <li><strong>Map</strong> : diviser le travail en tâches parallèles indépendantes</li>
  <li><strong>Shuffle</strong> : regrouper les résultats intermédiaires par clé</li>
  <li><strong>Reduce</strong> : agréger les résultats pour produire la sortie finale</li>
</ol>

<p>Cette approche permet de traiter des datasets massifs en parallèle sur des clusters de milliers de machines, avec une tolérance aux pannes intégrée.</p>

<h3>Années 2000-2010 : l'ère du big data engineering</h3>

<p>Apache Hadoop, open-sourcé en 2006, a démocratisé le big data. Pour la première fois, n'importe quelle entreprise avait accès aux mêmes outils que les géants de la tech. Un écosystème riche s'est développé : Hive, Pig, HBase, Cassandra, Spark, Storm, etc.</p>

<p>Le <strong>big data engineer</strong> devait maîtriser :</p>
<ul>
  <li>Le développement logiciel (Java, Scala, Python)</li>
  <li>L'administration de clusters massifs (HDFS, YARN)</li>
  <li>Les frameworks distribués (MapReduce, Spark)</li>
</ul>

<blockquote>
<p>Dan Ariely a résumé l'engouement pour le big data : "Le big data, c'est comme le sexe chez les adolescents : tout le monde en parle, personne ne sait vraiment comment faire, tout le monde pense que les autres le font, donc tout le monde prétend le faire."</p>
</blockquote>

<p>Le problème ? La complexité. Gérer un cluster Hadoop nécessitait des équipes entières de big data engineers coûtant des millions de dollars par an, souvent pour un retour sur investissement limité.</p>

<pre><code class="language-mermaid">
graph TD
    A[Popularité du Big Data] -->|Peak| B[2014-2016]
    B --> C[Complexité élevée]
    C --> D[Coûts de maintenance]
    D --> E[Simplification nécessaire]
    E --> F[Cloud managé]
    F --> G[Modern Data Stack]

    style B fill:#ffcccc
    style G fill:#ccffcc
</code></pre>

<h3>Années 2020 : ingénierie du cycle de vie des données</h3>

<p>Aujourd'hui, le rôle de data engineer évolue vers celui de <strong>data lifecycle engineer</strong>. Les frameworks monolithiques (Hadoop, Spark, Informatica) laissent place à des outils décentralisés, modulaires, managés et hautement abstraits.</p>

<p>Le <strong>Modern Data Stack</strong> représente cette évolution :</p>

<pre><code class="language-mermaid">
graph LR
    Sources[Sources de Données] --> Fivetran[Ingestion: Fivetran/Airbyte]
    Fivetran --> Snowflake[Stockage: Snowflake/BigQuery]
    Snowflake --> DBT[Transformation: dbt]
    DBT --> Looker[Visualisation: Looker/Tableau]
    DBT --> ML[ML: Databricks/SageMaker]

    style Fivetran fill:#e3f2fd
    style Snowflake fill:#f3e5f5
    style DBT fill:#e8f5e9
    style Looker fill:#fff3e0
</code></pre>

<p>Les data engineers modernes passent moins de temps à maintenir l'infrastructure et plus de temps sur :</p>
<ul>
  <li>La sécurité et la conformité (GDPR, CCPA)</li>
  <li>La gestion et la gouvernance des données</li>
  <li>Le DataOps et l'automatisation</li>
  <li>L'architecture et l'orchestration</li>
  <li>La qualité et la découvrabilité des données</li>
</ul>

<h2>Data engineering et data science</h2>

<h3>Deux disciplines complémentaires</h3>

<p>Le data engineering et le data science sont souvent confondus, mais ce sont des disciplines distinctes avec des objectifs différents :</p>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Data Engineering</th>
      <th>Data Science</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Objectif principal</strong></td>
      <td>Construire et maintenir l'infrastructure de données</td>
      <td>Extraire des insights et créer des modèles prédictifs</td>
    </tr>
    <tr>
      <td><strong>Focus</strong></td>
      <td>Pipeline, scalabilité, fiabilité</td>
      <td>Analyse, modélisation, expérimentation</td>
    </tr>
    <tr>
      <td><strong>Compétences clés</strong></td>
      <td>Software engineering, systèmes distribués, bases de données</td>
      <td>Statistiques, machine learning, domaine métier</td>
    </tr>
    <tr>
      <td><strong>Langages</strong></td>
      <td>Python, Java, Scala, SQL</td>
      <td>Python, R, SQL</td>
    </tr>
    <tr>
      <td><strong>Livrable</strong></td>
      <td>Données fiables et accessibles</td>
      <td>Modèles, insights, recommandations</td>
    </tr>
  </tbody>
</table>

<h3>Hiérarchie des besoins en data science</h3>

<p>Le data engineering est la fondation du data science. Sans infrastructure de données robuste, impossible de faire du machine learning en production.</p>

<pre><code class="language-mermaid">
graph TD
    ML[Machine Learning / AI]
    Analytics[Analytics / BI]
    Aggregate[Agrégation / Labels]
    Explore[Exploration / Transformation]
    Move[Déplacement / Stockage]
    Collect[Collecte]

    ML --> Analytics
    Analytics --> Aggregate
    Aggregate --> Explore
    Explore --> Move
    Move --> Collect

    DE1[Data Engineering]
    DE2[Data Engineering]
    DE3[Data Engineering]

    DE1 -.-> Collect
    DE2 -.-> Move
    DE3 -.-> Explore

    style ML fill:#4caf50
    style Analytics fill:#8bc34a
    style Aggregate fill:#cddc39
    style Explore fill:#ffeb3b
    style Move fill:#ffc107
    style Collect fill:#ff9800
    style DE1 fill:#e3f2fd
    style DE2 fill:#e3f2fd
    style DE3 fill:#e3f2fd
</code></pre>

<p>Cette hiérarchie, inspirée de la pyramide des besoins de Maslow appliquée aux données, montre que :</p>
<ul>
  <li>Les couches basses (collecte, stockage, transformation) sont le domaine du data engineering</li>
  <li>Les couches hautes (analytics, ML) sont le domaine du data science</li>
  <li>Sans fondations solides, les étages supérieurs s'effondrent</li>
</ul>

<h3>Collaboration efficace</h3>

<p>La collaboration entre data engineers et data scientists est essentielle :</p>

<ul>
  <li><strong>Data engineers</strong> fournissent des données propres, fiables, et documentées</li>
  <li><strong>Data scientists</strong> remontent les besoins en nouvelles données et features</li>
  <li><strong>Ensemble</strong>, ils déploient des modèles ML en production avec monitoring et re-training automatique</li>
</ul>

<h3>Exemple pratique : déploiement d'un modèle ML</h3>

<pre><code class="language-mermaid">
sequenceDiagram
    participant DS as Data Scientist
    participant DE as Data Engineer
    participant Infra as Infrastructure
    participant Users as Utilisateurs

    DS->>DE: Besoin de features X, Y, Z
    DE->>Infra: Création pipeline de features
    Infra-->>DS: Features disponibles
    DS->>DS: Entraînement du modèle
    DS->>DE: Modèle prêt pour production
    DE->>Infra: Déploiement + monitoring
    Infra->>Users: Prédictions en temps réel
    Infra-->>DE: Alertes si drift détecté
    DE->>DS: Notification pour re-training
</code></pre>

<h2>Compétences et profils du data engineer</h2>

<h3>Compétences techniques essentielles</h3>

<p>Un data engineer moderne doit maîtriser un large éventail de compétences techniques :</p>

<ul>
  <li><strong>Langages de programmation</strong> : Python (incontournable), SQL (fondamental), Java/Scala (pour Spark), bash/shell scripting</li>
  <li><strong>Bases de données</strong> : SQL (PostgreSQL, MySQL), NoSQL (MongoDB, Cassandra), NewSQL (Spanner, CockroachDB)</li>
  <li><strong>Systèmes de stockage</strong> : Object storage (S3, GCS), Data lakes, Data warehouses (Snowflake, BigQuery, Redshift)</li>
  <li><strong>Frameworks de traitement</strong> : Spark, Flink, Beam pour le batch et streaming</li>
  <li><strong>Orchestration</strong> : Airflow, Prefect, Dagster pour gérer les workflows</li>
  <li><strong>Cloud platforms</strong> : AWS, GCP, ou Azure avec leurs services managés</li>
  <li><strong>Infrastructure as Code</strong> : Terraform, CloudFormation pour provisionner l'infrastructure</li>
  <li><strong>CI/CD</strong> : GitHub Actions, GitLab CI, Jenkins pour automatiser les déploiements</li>
</ul>

<h3>Compétences non-techniques (soft skills)</h3>

<p>Les compétences humaines sont tout aussi importantes :</p>

<ul>
  <li><strong>Communication</strong> : traduire les besoins métier en solutions techniques, expliquer les contraintes aux stakeholders</li>
  <li><strong>Collaboration</strong> : travailler avec data scientists, analysts, product managers, software engineers</li>
  <li><strong>Pragmatisme</strong> : choisir la solution "assez bonne" plutôt que la perfection technique inutile</li>
  <li><strong>Apprentissage continu</strong> : le paysage technologique évolue constamment</li>
</ul>

<h3>Les trois profils de data engineer</h3>

<p>Il existe trois grands profils de data engineers, selon leur focus principal :</p>

<pre><code class="language-mermaid">
graph TD
    DE[Data Engineer]

    DE --> T1[Type 1: SQL-focused]
    DE --> T2[Type 2: Big Data-focused]
    DE --> T3[Type 3: Full-stack]

    T1 --> T1a[Bases relationnelles]
    T1 --> T1b[ETL/ELT avec SQL]
    T1 --> T1c[Data warehouses]

    T2 --> T2a[Hadoop ecosystem]
    T2 --> T2b[Spark, Flink]
    T2 --> T2c[Systèmes distribués]

    T3 --> T3a[Pipeline E2E]
    T3 --> T3b[ML en production]
    T3 --> T3c[Software eng + Data]

    style T1 fill:#e3f2fd
    style T2 fill:#f3e5f5
    style T3 fill:#e8f5e9
</code></pre>

<p><strong>Type 1 : SQL-focused</strong></p>
<ul>
  <li>Focus sur les bases de données relationnelles et SQL</li>
  <li>ETL/ELT avec des outils comme dbt, Informatica, Talend</li>
  <li>Parfait pour les use cases analytics et BI traditionnels</li>
</ul>

<p><strong>Type 2 : Big Data-focused</strong></p>
<ul>
  <li>Expertise en systèmes distribués (Hadoop, Spark, Kafka)</li>
  <li>Traitement de volumes massifs (pétaoctets)</li>
  <li>Langages : Java, Scala, Python pour les frameworks big data</li>
</ul>

<p><strong>Type 3 : Full-stack Data Engineer</strong></p>
<ul>
  <li>Combine software engineering et data engineering</li>
  <li>Construit des pipelines end-to-end</li>
  <li>Déploie des modèles ML en production</li>
  <li>Le profil le plus recherché dans les startups et scale-ups</li>
</ul>

<h3>Erreurs fréquentes</h3>

<ul>
  <li><strong>Erreur :</strong> Vouloir maîtriser toutes les technologies → <strong>Solution :</strong> Se spécialiser dans un domaine (SQL/Big Data/Full-stack) puis élargir progressivement</li>
  <li><strong>Erreur :</strong> Négliger les soft skills et la communication → <strong>Solution :</strong> Le data engineering est autant un problème humain que technique</li>
  <li><strong>Erreur :</strong> Choisir la technologie avant de comprendre le problème → <strong>Solution :</strong> Toujours partir des besoins métier</li>
</ul>

<h2>Synthèse</h2>

<p>Ce chapitre a posé les fondations du data engineering moderne :</p>

<ul>
  <li><strong>Définition</strong> : Le data engineering est le développement et la maintenance de systèmes qui transforment des données brutes en informations exploitables</li>
  <li><strong>Cycle de vie</strong> : Génération → Stockage → Ingestion → Transformation → Diffusion, avec des courants sous-jacents (sécurité, DataOps, architecture, etc.)</li>
  <li><strong>Évolution</strong> : Des data warehouses des années 1980 au big data des années 2010, jusqu'au Modern Data Stack actuel avec abstraction et simplicité</li>
  <li><strong>Relation avec le data science</strong> : Le data engineering construit les fondations, le data science extrait les insights</li>
  <li><strong>Compétences</strong> : Maîtrise technique (SQL, Python, cloud, orchestration) + soft skills (communication, collaboration, pragmatisme)</li>
</ul>

<p><strong>À retenir :</strong> Le data engineering est passé d'un focus sur les technologies (Hadoop, Spark) à un focus sur le cycle de vie des données et la création de valeur métier. Les data engineers modernes sont des architectes de systèmes de données qui équilibrent performance, fiabilité, scalabilité et maintenabilité.</p>

<p><strong>Prochaines étapes :</strong> Le chapitre 2 approfondit le cycle de vie de l'ingénierie des données et ses courants sous-jacents. Vous découvrirez comment concevoir des architectures de données robustes et évolutives.</p>

<pre><code class="language-mermaid">
graph TD
    Root[Data Engineering]

    Root --> Definition[Définition]
    Root --> Lifecycle[Cycle de Vie]
    Root --> Evolution[Évolution]
    Root --> Skills[Compétences]

    Definition --> D1[Systèmes de données]
    Definition --> D2[Qualité et fiabilité]
    Definition --> D3[Support analytics/ML]

    Lifecycle --> L1[5 étapes]
    Lifecycle --> L2[Courants sous-jacents]

    Evolution --> E1[1980-2000: Data Warehouse]
    Evolution --> E2[2000-2010: Big Data]
    Evolution --> E3[2020s: Modern Stack]

    Skills --> S1[Techniques]
    Skills --> S2[Soft skills]
    Skills --> S3[3 profils]

    style Root fill:#e1f5ff
    style Definition fill:#fff4e1
    style Lifecycle fill:#f3e5f5
    style Evolution fill:#e8f5e9
    style Skills fill:#fff3e0
</code></pre>