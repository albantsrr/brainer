<h2>Objectifs d'apprentissage</h2>

<p>À la fin de ce chapitre, vous serez capable de :</p>
<ul>
  <li><strong>Comprendre</strong> ce qu'est l'ingénierie des données et son rôle dans l'écosystème data moderne</li>
  <li><strong>Identifier</strong> les compétences et activités clés d'un data engineer</li>
  <li><strong>Analyser</strong> les trois niveaux de maturité data d'une organisation</li>
  <li><strong>Distinguer</strong> les data engineers de type A (abstraction) et de type B (build) selon leurs spécialisations</li>
  <li><strong>Reconnaître</strong> les différents acteurs avec lesquels un data engineer collabore au quotidien</li>
</ul>

<h2>Pourquoi c'est important</h2>

<p>L'ingénierie des données est devenue l'une des disciplines les plus stratégiques dans le domaine de la tech. Sans infrastructure data robuste, impossible de faire de la data science, du machine learning ou de l'analytique en production. Les data engineers construisent les fondations qui permettent aux organisations de transformer leurs données brutes en avantage compétitif.</p>

<p>Comprendre ce qu'est l'ingénierie des données et son écosystème est essentiel, que vous soyez développeur, data scientist, chef de projet ou entrepreneur. Cette connaissance vous permettra de mieux collaborer avec les équipes data, d'évaluer la maturité data de votre organisation, et d'identifier les compétences nécessaires pour construire des systèmes data efficaces.</p>

<p>Ignorer l'importance de l'ingénierie des données conduit à des projets de data science qui échouent en production, des analyses basées sur des données de mauvaise qualité, et des coûts d'infrastructure exponentiels. Ce chapitre vous donne les clés pour éviter ces pièges.</p>

<h2>Qu'est-ce que l'ingénierie des données ?</h2>

<h3>Concept fondamental</h3>

<p>L'<strong>ingénierie des données</strong> (data engineering) est le développement, l'implémentation et la maintenance de systèmes et processus qui ingèrent des données brutes et produisent des informations de haute qualité et cohérentes pour des usages en aval comme l'analyse et le machine learning.</p>

<p>L'ingénierie des données se situe à l'intersection de plusieurs disciplines :</p>
<ul>
  <li>Sécurité des données</li>
  <li>Gestion des données (data management)</li>
  <li>DataOps (opérations data)</li>
  <li>Architecture des données</li>
  <li>Orchestration</li>
  <li>Génie logiciel</li>
</ul>

<p>Un <strong>data engineer</strong> gère le cycle de vie complet des données, depuis l'extraction des systèmes sources jusqu'à la mise à disposition pour les utilisateurs finaux.</p>

<h3>Mécanisme interne : le cycle de vie des données</h3>

<p>Le cycle de vie de l'ingénierie des données se compose de plusieurs étapes interconnectées :</p>

<pre><code class="language-mermaid">
graph LR
    A[Génération] -->|Sources| B[Stockage]
    B --> C[Ingestion]
    C --> D[Transformation]
    D --> E[Servir les données]
    E --> F[Analyses & ML]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#e8f5e9
    style D fill:#f3e5f5
    style E fill:#fce4ec
    style F fill:#e0f2f1
</code></pre>

<p>Les <strong>undercurrents</strong> (courants sous-jacents) traversent toutes ces étapes :</p>
<ul>
  <li><strong>Sécurité</strong> : protection des données sensibles, contrôle d'accès</li>
  <li><strong>Data management</strong> : gouvernance, qualité, métadonnées</li>
  <li><strong>DataOps</strong> : automatisation, monitoring, gestion des incidents</li>
  <li><strong>Architecture</strong> : design des systèmes, scalabilité, résilience</li>
  <li><strong>Orchestration</strong> : coordination des pipelines, gestion des dépendances</li>
  <li><strong>Software engineering</strong> : versioning, tests, CI/CD</li>
</ul>

<h3>Exemple pratique</h3>

<p>Prenons l'exemple d'une plateforme e-commerce qui souhaite analyser le comportement de ses utilisateurs :</p>

<ol>
  <li><strong>Génération</strong> : Les utilisateurs génèrent des événements (clics, achats, recherches) capturés par l'application web</li>
  <li><strong>Stockage</strong> : Les événements sont stockés dans une base transactionnelle (PostgreSQL) et un data lake (S3)</li>
  <li><strong>Ingestion</strong> : Un pipeline batch (Airflow) ou streaming (Kafka) extrait ces données toutes les heures</li>
  <li><strong>Transformation</strong> : Les données brutes sont nettoyées, enrichies et agrégées (DBT, Spark)</li>
  <li><strong>Servir</strong> : Les données transformées alimentent un data warehouse (Snowflake, BigQuery) accessible via SQL</li>
  <li><strong>Utilisation</strong> : Les data analysts créent des dashboards (Tableau) et les data scientists entraînent des modèles de recommandation</li>
</ol>

<p>Le data engineer est responsable de la conception, du déploiement et de la maintenance de toute cette infrastructure.</p>

<h3>Évolution du métier</h3>

<p>L'ingénierie des données a émergé dans les années 2010 avec la montée du Big Data et de la data science. Historiquement, on distingue deux profils :</p>

<blockquote>
  <p><strong>Type A (Abstraction)</strong> : Focalisé sur SQL, bases relationnelles, ETL, data warehousing. Gère principalement des outils managés et des abstractions de haut niveau.</p>
</blockquote>

<blockquote>
  <p><strong>Type B (Build)</strong> : Focalisé sur les technologies Big Data (Hadoop, Spark, Kafka), systèmes distribués, traitement à grande échelle. Construit des solutions custom et gère l'infrastructure bas niveau.</p>
</blockquote>

<p>Aujourd'hui, la distinction s'estompe : les outils modernes (Snowflake, Databricks, Fivetran) abstraient la complexité technique, permettant aux data engineers de se concentrer sur la valeur métier plutôt que sur l'infrastructure.</p>

<h2>Compétences et activités d'un data engineer</h2>

<h3>Concept fondamental</h3>

<p>Un data engineer moderne doit maîtriser un ensemble de compétences techniques et organisationnelles couvrant les "undercurrents" de l'ingénierie des données. Contrairement au passé où il fallait être expert en quelques technologies monolithiques (Hadoop, Teradata), les data engineers actuels doivent naviguer dans un écosystème d'outils spécialisés et en constante évolution.</p>

<h3>Compétences techniques essentielles</h3>

<ul>
  <li><strong>SQL et modélisation de données</strong> : Maîtrise approfondie de SQL, design de schémas, normalisation/dénormalisation</li>
  <li><strong>Langages de programmation</strong> : Python pour scripting et orchestration, parfois Scala/Java pour le Big Data</li>
  <li><strong>Systèmes distribués</strong> : Compréhension des architectures distribuées, partitioning, réplication</li>
  <li><strong>Systèmes de stockage</strong> : Bases relationnelles (PostgreSQL), NoSQL (MongoDB, Cassandra), data warehouses (Snowflake, BigQuery), data lakes (S3, ADLS)</li>
  <li><strong>Traitement de données</strong> : Batch (Spark, DBT) et streaming (Kafka, Flink)</li>
  <li><strong>Orchestration</strong> : Airflow, Dagster, Prefect pour gérer les workflows</li>
  <li><strong>Cloud computing</strong> : AWS, Azure ou GCP selon l'organisation</li>
  <li><strong>DataOps</strong> : CI/CD pour data pipelines, monitoring, observabilité</li>
</ul>

<h3>Exemple pratique : journée type d'un data engineer</h3>

<p>Une journée type peut inclure :</p>

<ol>
  <li><strong>Morning standup</strong> : Point avec l'équipe data sur les incidents et priorités</li>
  <li><strong>Debugging</strong> : Investigation d'un pipeline qui a échoué cette nuit (timeout API, changement de schéma source)</li>
  <li><strong>Code review</strong> : Revue d'une PR proposant un nouveau pipeline d'ingestion depuis Salesforce</li>
  <li><strong>Architecture meeting</strong> : Discussion sur la migration du data warehouse vers Snowflake</li>
  <li><strong>Development</strong> : Implémentation d'une transformation DBT pour agréger les métriques produit</li>
  <li><strong>Documentation</strong> : Mise à jour du data catalog avec les métadonnées des nouvelles tables</li>
  <li><strong>Collaboration</strong> : Support à un data analyst bloqué sur une requête SQL complexe</li>
</ol>

<p>Le quotidien oscille entre développement (40%), debugging/maintenance (30%), collaboration (20%), et veille technologique (10%).</p>

<h3>Erreurs fréquentes</h3>

<ul>
  <li><strong>Erreur</strong> : Se concentrer uniquement sur la technique en négligeant la communication avec les stakeholders → <strong>Solution</strong> : Passer 20% de son temps à comprendre les besoins métier et à former les utilisateurs</li>
  <li><strong>Erreur</strong> : Construire des solutions over-engineered pour des problèmes simples → <strong>Solution</strong> : Privilégier les outils managés et les solutions standard avant de construire du custom</li>
  <li><strong>Erreur</strong> : Ignorer la qualité et la documentation des données → <strong>Solution</strong> : Mettre en place du data quality testing (Great Expectations) et maintenir un data catalog à jour</li>
</ul>

<h2>Maturité data des organisations</h2>

<h3>Concept fondamental</h3>

<p>La <strong>maturité data</strong> d'une organisation n'est pas corrélée à son âge ou son chiffre d'affaires, mais à sa capacité à exploiter les données comme avantage compétitif. Une startup peut être plus mature qu'une entreprise centenaire si elle a construit une culture et une infrastructure data efficaces.</p>

<p>Nous distinguons trois niveaux de maturité data qui impactent directement le rôle et les priorités d'un data engineer.</p>

<h3>Les trois niveaux de maturité</h3>

<pre><code class="language-mermaid">
graph TD
    A[Stage 1: Starting with Data] -->|Croissance| B[Stage 2: Scaling with Data]
    B -->|Maturation| C[Stage 3: Leading with Data]

    A1[Équipe : 1-5 personnes] --> A
    A2[Objectif : Get traction] --> A
    A3[Data engineer = Généraliste] --> A

    B1[Équipe : 10-50 personnes] --> B
    B2[Objectif : Scale infrastructure] --> B
    B3[Rôles spécialisés apparaissent] --> B

    C1[Équipe : 50+ personnes] --> C
    C2[Objectif : Innovation & automation] --> C
    C3[Data as competitive advantage] --> C

    style A fill:#ffcdd2
    style B fill:#fff9c4
    style C fill:#c8e6c9
</code></pre>

<h3>Stage 1 : Starting with Data</h3>

<p><strong>Caractéristiques :</strong></p>
<ul>
  <li>Équipe data très petite (1-5 personnes)</li>
  <li>Objectifs flous ou non définis</li>
  <li>Architecture et infrastructure en phase de planning</li>
  <li>Adoption et utilisation faibles</li>
</ul>

<p><strong>Rôle du data engineer :</strong></p>
<ul>
  <li>Généraliste qui porte plusieurs casquettes (data scientist, software engineer)</li>
  <li><strong>Priorité #1</strong> : Aller vite et apporter de la valeur rapidement</li>
  <li>Construire une base solide avant de faire du ML (erreur fréquente : vouloir faire du ML prématurément)</li>
  <li>Identifier les quick wins pour démontrer la valeur des données</li>
</ul>

<p><strong>Technologies typiques :</strong> PostgreSQL + Metabase + Python scripts schedulés avec cron. L'objectif est la simplicité et la rapidité de mise en place.</p>

<h3>Stage 2 : Scaling with Data</h3>

<p><strong>Caractéristiques :</strong></p>
<ul>
  <li>L'organisation a adopté les données et les utilise activement</li>
  <li>Les besoins data dépassent les capacités de l'infrastructure existante</li>
  <li>Multiplication des cas d'usage et des utilisateurs</li>
  <li>Émergence de problèmes de scalabilité et de qualité</li>
</ul>

<p><strong>Rôle du data engineer :</strong></p>
<ul>
  <li>Établir des <strong>pratiques formelles</strong> de data engineering</li>
  <li>Créer des systèmes <strong>scalables</strong> et <strong>robustes</strong></li>
  <li>Migrer vers des <strong>architectures off-the-shelf</strong> éprouvées</li>
  <li>Former l'organisation à consommer et exploiter les données</li>
  <li>Passer du mode "firefighting" au mode "prévention"</li>
</ul>

<p><strong>Technologies typiques :</strong> Data warehouse cloud (Snowflake, BigQuery), orchestration (Airflow), DBT pour transformations, outils de BI modernes (Looker, Tableau).</p>

<h3>Stage 3 : Leading with Data</h3>

<p><strong>Caractéristiques :</strong></p>
<ul>
  <li>Les données sont au cœur de la stratégie d'entreprise</li>
  <li>Infrastructure data mature et automatisée</li>
  <li>Culture data généralisée dans toute l'organisation</li>
</ul>

<p><strong>Rôle du data engineer :</strong></p>
<ul>
  <li>Créer des <strong>systèmes d'automatisation</strong> et de <strong>self-service</strong></li>
  <li>Se concentrer sur les <strong>aspects stratégiques</strong> (innovation, optimisation)</li>
  <li>Construire une <strong>data platform</strong> où les équipes peuvent travailler en autonomie</li>
  <li>Implémenter des pratiques avancées : data mesh, data contracts, observabilité</li>
  <li><strong>Focus</strong> : Réduire les coûts, augmenter l'efficacité, innover</li>
</ul>

<p><strong>Technologies typiques :</strong> Data lakehouse (Databricks), data catalog (Atlan, DataHub), data quality automation (Great Expectations, Monte Carlo), feature stores pour ML (Feast).</p>

<h3>Exemple pratique</h3>

<p>Comparaison d'une même demande métier ("Je veux un dashboard des ventes") selon le niveau de maturité :</p>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Approche</th>
      <th>Délai</th>
      <th>Qualité</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Stage 1</strong></td>
      <td>Requête SQL manuelle + Google Sheets partagé</td>
      <td>2 jours</td>
      <td>Données potentiellement obsolètes, processus manuel</td>
    </tr>
    <tr>
      <td><strong>Stage 2</strong></td>
      <td>Pipeline Airflow + DBT + dashboard Metabase, refresh quotidien</td>
      <td>1 semaine</td>
      <td>Données fiables, automatisé, documenté</td>
    </tr>
    <tr>
      <td><strong>Stage 3</strong></td>
      <td>Utilisateur crée lui-même le dashboard via data catalog + semantic layer</td>
      <td>1 heure</td>
      <td>Self-service, données certifiées, refresh temps réel</td>
    </tr>
  </tbody>
</table>

<h3>Erreurs fréquentes</h3>

<ul>
  <li><strong>Erreur</strong> : Adopter des technologies de Stage 3 alors qu'on est en Stage 1 → <strong>Solution</strong> : Choisir des outils adaptés à sa maturité actuelle</li>
  <li><strong>Erreur</strong> : Se focaliser uniquement sur la tech et ignorer la culture data → <strong>Solution</strong> : Investir dans la formation et l'évangélisation</li>
  <li><strong>Erreur</strong> : Ne pas anticiper la transition vers le stage suivant → <strong>Solution</strong> : Planifier l'évolution de l'architecture dès le début</li>
</ul>

<h2>Collaboration et écosystème</h2>

<h3>Concept fondamental</h3>

<p>Un data engineer ne travaille jamais seul. Il collabore avec de nombreux acteurs techniques et non-techniques, chacun ayant des besoins et des contraintes spécifiques. Comprendre cet écosystème est essentiel pour être efficace.</p>

<h3>Les acteurs clés et leurs interactions</h3>

<pre><code class="language-mermaid">
graph TD
    DE[Data Engineer]

    DE <-->|Besoins analytiques| DA[Data Analyst]
    DE <-->|Modèles ML| DS[Data Scientist]
    DE <-->|Features & déploiement| MLE[ML Engineer]
    DE <-->|APIs & infra| SWE[Software Engineer]
    DE <-->|Architecture & standards| ARCH[Data Architect]
    DE <-->|Priorités & roadmap| PM[Product Manager]
    DE <-->|Besoins métier| BIZ[Business Stakeholders]
    DE <-->|Infra & sécurité| OPS[DevOps/IT]

    style DE fill:#4fc3f7,stroke:#01579b,stroke-width:3px
    style DA fill:#fff9c4
    style DS fill:#c5e1a5
    style MLE fill:#a5d6a7
    style SWE fill:#ce93d8
    style ARCH fill:#90caf9
    style PM fill:#ffab91
    style BIZ fill:#ef9a9a
    style OPS fill:#bcaaa4
</code></pre>

<h3>Profils et interactions détaillés</h3>

<p><strong>Data Analysts</strong></p>
<ul>
  <li><strong>Rôle</strong> : Explorent et analysent les données pour générer des insights métier</li>
  <li><strong>Interaction</strong> : Demandent l'accès à de nouvelles sources de données, remontent des problèmes de qualité, ont besoin de dashboards et de datasets propres</li>
  <li><strong>Attentes envers le data engineer</strong> : Données fiables, bien documentées, accessibles via SQL</li>
</ul>

<p><strong>Data Scientists</strong></p>
<ul>
  <li><strong>Rôle</strong> : Construisent des modèles prédictifs et des algorithmes de ML</li>
  <li><strong>Interaction</strong> : Ont besoin de datasets historiques pour l'entraînement, de features engineering, d'aide pour mettre les modèles en production</li>
  <li><strong>Attentes envers le data engineer</strong> : Pipelines de features reproductibles, infrastructure pour servir les modèles</li>
</ul>

<p><strong>ML Engineers</strong></p>
<ul>
  <li><strong>Rôle</strong> : Industrialisent les modèles ML (déploiement, monitoring, retraining)</li>
  <li><strong>Interaction</strong> : Collaboration étroite sur les pipelines de features, le serving des modèles, l'observabilité</li>
  <li><strong>Overlap</strong> : Dans certaines organisations, data engineers et ML engineers fusionnent</li>
</ul>

<p><strong>Software Engineers</strong></p>
<ul>
  <li><strong>Rôle</strong> : Développent les applications qui génèrent ou consomment les données</li>
  <li><strong>Interaction</strong> : Coordination sur les schémas d'APIs, les événements, l'intégration de données dans les applications</li>
  <li><strong>Attentes envers le data engineer</strong> : APIs de données performantes, documentation claire des datasets disponibles</li>
</ul>

<p><strong>Product Managers</strong></p>
<ul>
  <li><strong>Rôle</strong> : Définissent la stratégie produit et les priorités</li>
  <li><strong>Interaction</strong> : Demandent des métriques produit, priorisent les projets data</li>
  <li><strong>Attentes envers le data engineer</strong> : Estimation réaliste des délais, traduction des besoins techniques en valeur métier</li>
</ul>

<p><strong>Business Stakeholders</strong></p>
<ul>
  <li><strong>Rôle</strong> : Executives, équipes métier qui utilisent les données pour prendre des décisions</li>
  <li><strong>Interaction</strong> : Demandent des rapports, des analyses ad-hoc, challengent la qualité des données</li>
  <li><strong>Attentes envers le data engineer</strong> : Disponibilité des données, réactivité sur les demandes critiques</li>
</ul>

<h3>Exemple pratique : projet de dashboard exécutif</h3>

<p>Un CEO demande un dashboard temps réel des KPIs business. Voici les interactions du data engineer :</p>

<ol>
  <li><strong>Avec le Product Manager</strong> : Clarifier les métriques exactes attendues et la fréquence de refresh acceptable</li>
  <li><strong>Avec les Software Engineers</strong> : Vérifier que les événements nécessaires sont bien loggés par les applications</li>
  <li><strong>Avec le Data Architect</strong> : Valider que l'architecture proposée (streaming Kafka → OLAP database) est alignée avec la stratégie long terme</li>
  <li><strong>Avec le DevOps</strong> : Provisionner l'infrastructure nécessaire (cluster Kafka, base OLAP)</li>
  <li><strong>Avec le Data Analyst</strong> : Comprendre les calculs métier complexes (ex: churn, LTV)</li>
  <li><strong>Avec le CEO (stakeholder)</strong> : Présenter une démo du dashboard, itérer sur les visualisations</li>
</ol>

<p>Ce projet nécessite de jongler entre des compétences techniques (architecture, code) et des compétences relationnelles (communication, pédagogie, gestion des attentes).</p>

<h3>Erreurs fréquentes</h3>

<ul>
  <li><strong>Erreur</strong> : Travailler en silo sans consulter les stakeholders → <strong>Solution</strong> : Organiser des points réguliers avec les utilisateurs des données</li>
  <li><strong>Erreur</strong> : Sur-promettre des délais irréalistes pour plaire au management → <strong>Solution</strong> : Être transparent sur la complexité et négocier des livrables par phases</li>
  <li><strong>Erreur</strong> : Utiliser un jargon technique avec des non-tech → <strong>Solution</strong> : Adapter son langage à l'audience, utiliser des analogies métier</li>
</ul>

<h2>Synthèse</h2>

<p>Ce chapitre a posé les fondations de l'ingénierie des données moderne :</p>

<ul>
  <li><strong>Définition</strong> : L'ingénierie des données est le développement et la maintenance de systèmes qui transforment des données brutes en informations de qualité pour l'analyse et le ML</li>
  <li><strong>Cycle de vie</strong> : Génération → Stockage → Ingestion → Transformation → Servir, avec 6 undercurrents (sécurité, data management, DataOps, architecture, orchestration, software engineering)</li>
  <li><strong>Compétences</strong> : Mélange de SQL, programmation (Python), systèmes distribués, cloud, et soft skills (communication, pédagogie)</li>
  <li><strong>Maturité data</strong> : Trois stages qui dictent les priorités du data engineer — Stage 1 (get traction), Stage 2 (scale), Stage 3 (lead & automate)</li>
  <li><strong>Collaboration</strong> : Interactions constantes avec data analysts, data scientists, software engineers, product managers et stakeholders business</li>
  <li><strong>Évolution du métier</strong> : Passage des technologies monolithiques (Hadoop) aux outils managés modernes (Snowflake, DBT), permettant de se concentrer sur la valeur métier</li>
</ul>

<pre><code class="language-mermaid">
mindmap
  root((Data Engineering))
    Définition
      Systèmes & Processus
      Données brutes → Infos qualité
      Intersection de 6 disciplines
    Cycle de Vie
      Génération
      Stockage
      Ingestion
      Transformation
      Servir
    Compétences
      SQL & Modélisation
      Python & Code
      Systèmes Distribués
      Cloud & DataOps
    Maturité
      Stage 1 Starting
      Stage 2 Scaling
      Stage 3 Leading
    Collaboration
      Data Analysts
      Data Scientists
      Software Engineers
      Stakeholders
</code></pre>

<p><strong>À retenir :</strong> L'ingénierie des données n'est pas qu'une affaire de technologie. C'est avant tout une question de comprendre les besoins métier, de choisir les bons outils pour le bon niveau de maturité, et de collaborer efficacement avec un écosystème varié. Un bon data engineer est autant un technicien qu'un communicant et un problem solver.</p>

<p><strong>Prochaines étapes :</strong> Le chapitre 2 explore le cycle de vie complet de l'ingénierie des données en détail, avec un focus sur les undercurrents (sécurité, data management, DataOps, etc.) qui traversent chaque étape. Vous apprendrez comment ces principes s'appliquent concrètement à la construction de systèmes data robustes et scalables.</p>
